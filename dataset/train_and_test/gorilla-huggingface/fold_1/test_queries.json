[
    {
        "id": "gorilla_huggingface_query_176",
        "query": "The customer is writing a book about the solar system and just needs the answer to a question: How long does it take for Mars to orbit the sun?",
        "instruction": "Given a `question answering` task, retrieve tools that utilize natural language processing techniques to extract answers from a provided context, specifically targeting factual information about the solar system.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='philschmid/distilbert-onnx')",
                    "api_arguments": {
                        "model": "philschmid/distilbert-onnx"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "onnx"
                    ],
                    "example_code": {
                        "Compute": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})"
                    },
                    "performance": {
                        "dataset": "squad",
                        "accuracy": "F1 score: 87.1"
                    },
                    "description": "This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.",
                    "id": "huggingface_api_475",
                    "name": "philschmid/distilbert-onnx"
                },
                "id": "gorilla_huggingface_tool_471",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 1: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 2: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 4: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 5: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 6: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 8: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.",
            "Tool 9: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 10: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 11: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 12: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 13: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.",
            "Tool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 15: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.",
            "Tool 16: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 17: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 18: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 19: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code."
        ],
        "tool_view": {
            "description": "A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "parameters": {},
            "response": "The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \n\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \n\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.",
            "examples": [
                {
                    "user_intent": "I have an image of a street scene and I want to know how many cars are visible in it.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/street_scene.jpg",
                            "question": "How many cars are there in this image?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this photo of a kitchen and tell me what color the refrigerator is.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/kitchen.jpg",
                            "question": "What color is the refrigerator?"
                        }
                    }
                },
                {
                    "user_intent": "I want to analyze a picture of a park and find out if there are any people sitting on benches.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/park.jpg",
                            "question": "Are there people sitting on benches?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 1: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 2: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 4: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 5: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 6: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 8: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.\nTool 9: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 10: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 11: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 12: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 13: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.\nTool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 15: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.\nTool 16: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 17: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 18: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 19: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\", \"parameters\": {}, \"response\": \"The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \\n\\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \\n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \\n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \\n\\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.\", \"examples\": [{\"user_intent\": \"I have an image of a street scene and I want to know how many cars are visible in it.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/street_scene.jpg\", \"question\": \"How many cars are there in this image?\"}}}, {\"user_intent\": \"Look at this photo of a kitchen and tell me what color the refrigerator is.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/kitchen.jpg\", \"question\": \"What color is the refrigerator?\"}}}, {\"user_intent\": \"I want to analyze a picture of a park and find out if there are any people sitting on benches.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/park.jpg\", \"question\": \"Are there people sitting on benches?\"}}}]}\n\n# User Query\nThe customer is writing a book about the solar system and just needs the answer to a question: How long does it take for Mars to orbit the sun?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Find the orbital period of Mars around the Sun.",
                    "tool_description": "Retrieve a factual answer to a specific astronomy-related question using a language model capable of answering general knowledge questions.",
                    "expected_response": "A concise textual answer stating the orbital period of Mars in Earth days or years."
                }
            ],
            "extracted_arguments": {
                "celestial_body": "The name of the planet or celestial object whose orbital period is being queried.",
                "question_topic": "The specific property or fact being asked about, e.g., orbital period around the Sun."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_14",
        "query": "Our client is a legal firm that needs assistance in extracting specific information from a large number of legal documents. Automate the process of answering questions related to these documents.",
        "instruction": "Given a `document question answering` task, retrieve tools that automate the extraction of information from legal documents by leveraging models capable of processing textual and visual inputs to answer queries accurately based on document content.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')",
                    "api_arguments": "question, image",
                    "python_environment_requirements": "transformers, torch, datasets, tokenizers",
                    "example_code": "",
                    "performance": {
                        "dataset": "None",
                        "accuracy": {
                            "Loss": 4.3167
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.",
                    "id": "huggingface_api_136",
                    "name": "layoutlmv2-base-uncased_finetuned_docvqa"
                },
                "id": "gorilla_huggingface_tool_129",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 1: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 2: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 3: A fine-tuned version of Microsoft’s LayoutLMv3-large model designed for multimodal document question answering, capable of extracting and classifying information from documents with high accuracy, achieving over 98% accuracy on the SROIE dataset.",
            "Tool 4: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 5: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 6: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 7: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 8: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 9: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 10: This tool provides a fine-tuned model for document question answering, built on the LayoutLMv2 architecture. It allows for extracting answers from documents based on textual queries, utilizing a pretrained version of the model specifically fine-tuned for information extraction tasks.",
            "Tool 11: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 12: The `microsoft/trocr-large-handwritten` tool is a Transformer-based model for optical character recognition (OCR) that converts handwritten images into text. It uses an image Transformer encoder to process visual input and a text Transformer decoder to generate corresponding text, making it suitable for digitizing handwritten documents or extracting text from images.",
            "Tool 13: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 14: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 15: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 17: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.",
            "Tool 18: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 19: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis."
        ],
        "tool_view": {
            "description": "Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "parameters": {
                "tokenizer": {
                    "type": "object",
                    "description": "A tokenizer object used to preprocess input text for the RoBERTa-based Question Answering model. This tokenizer converts text into token IDs that the model can understand.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('Rakib/roberta-base-on-cuad')"
                }
            },
            "response": "The tool is designed for **Question Answering** using a model fine-tuned specifically on the CUAD dataset, which focuses on legal documents. The model is based on the **RoBERTa** architecture, and it can extract answers from legal contracts and other legal documents.\n\n### Expected Response Structure and Meaning:\n\n1. **Predicted Answer**: \n   - The primary output will contain the **answer** to the question posed. This is typically a text string that represents the model's best guess for the correct response based on the input question and context (i.e., the legal document).\n   \n2. **Start Position**:\n   - This component indicates the **starting position** in the context (the legal document) where the answer begins. It is typically represented as an index within the text.\n\n3. **End Position**:\n   - The **end position** represents the index of where the answer finishes in the context text.\n\n4. **Confidence Score** (Optional but common in NLP models):\n   - The model may also include a **confidence score** or probability indicating how certain it is about the predicted answer. This score would typically range from 0 to 1, with higher values indicating greater confidence in the prediction.\n\n### General Behavior:\n- Given a **question** and a **context** (i.e., a segment of a legal document), the tool's output will typically consist of the most likely answer found within that context, along with the positions where the answer starts and ends in the provided context. If the model cannot find a clear answer, it might either output an empty string or indicate an uncertain result (possibly with a low confidence score).",
            "examples": [
                {
                    "user_intent": "I have a legal contract and want to know the termination clause details.",
                    "tool_call": {
                        "name": "Rakib/roberta-base-on-cuad",
                        "arguments": {
                            "context": "This Agreement may be terminated by either party with thirty (30) days written notice...",
                            "question": "What are the termination terms in this contract?"
                        }
                    }
                },
                {
                    "user_intent": "Extract the liability limitations from this service agreement.",
                    "tool_call": {
                        "name": "Rakib/roberta-base-on-cuad",
                        "arguments": {
                            "context": "The liability of the Company shall be limited to direct damages not exceeding $10,000...",
                            "question": "What limitations of liability does the agreement specify?"
                        }
                    }
                },
                {
                    "user_intent": "Check the governing law stated in this contract document.",
                    "tool_call": {
                        "name": "Rakib/roberta-base-on-cuad",
                        "arguments": {
                            "context": "This Agreement shall be governed by and construed in accordance with the laws of the State of New York...",
                            "question": "Which law governs this contract?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 1: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 2: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 3: A fine-tuned version of Microsoft’s LayoutLMv3-large model designed for multimodal document question answering, capable of extracting and classifying information from documents with high accuracy, achieving over 98% accuracy on the SROIE dataset.\nTool 4: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 5: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 6: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 7: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 8: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 9: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 10: This tool provides a fine-tuned model for document question answering, built on the LayoutLMv2 architecture. It allows for extracting answers from documents based on textual queries, utilizing a pretrained version of the model specifically fine-tuned for information extraction tasks.\nTool 11: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 12: The `microsoft/trocr-large-handwritten` tool is a Transformer-based model for optical character recognition (OCR) that converts handwritten images into text. It uses an image Transformer encoder to process visual input and a text Transformer decoder to generate corresponding text, making it suitable for digitizing handwritten documents or extracting text from images.\nTool 13: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 14: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 15: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 17: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.\nTool 18: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 19: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\", \"parameters\": {\"tokenizer\": {\"type\": \"object\", \"description\": \"A tokenizer object used to preprocess input text for the RoBERTa-based Question Answering model. This tokenizer converts text into token IDs that the model can understand.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('Rakib/roberta-base-on-cuad')\"}}, \"response\": \"The tool is designed for **Question Answering** using a model fine-tuned specifically on the CUAD dataset, which focuses on legal documents. The model is based on the **RoBERTa** architecture, and it can extract answers from legal contracts and other legal documents.\\n\\n### Expected Response Structure and Meaning:\\n\\n1. **Predicted Answer**: \\n   - The primary output will contain the **answer** to the question posed. This is typically a text string that represents the model's best guess for the correct response based on the input question and context (i.e., the legal document).\\n   \\n2. **Start Position**:\\n   - This component indicates the **starting position** in the context (the legal document) where the answer begins. It is typically represented as an index within the text.\\n\\n3. **End Position**:\\n   - The **end position** represents the index of where the answer finishes in the context text.\\n\\n4. **Confidence Score** (Optional but common in NLP models):\\n   - The model may also include a **confidence score** or probability indicating how certain it is about the predicted answer. This score would typically range from 0 to 1, with higher values indicating greater confidence in the prediction.\\n\\n### General Behavior:\\n- Given a **question** and a **context** (i.e., a segment of a legal document), the tool's output will typically consist of the most likely answer found within that context, along with the positions where the answer starts and ends in the provided context. If the model cannot find a clear answer, it might either output an empty string or indicate an uncertain result (possibly with a low confidence score).\", \"examples\": [{\"user_intent\": \"I have a legal contract and want to know the termination clause details.\", \"tool_call\": {\"name\": \"Rakib/roberta-base-on-cuad\", \"arguments\": {\"context\": \"This Agreement may be terminated by either party with thirty (30) days written notice...\", \"question\": \"What are the termination terms in this contract?\"}}}, {\"user_intent\": \"Extract the liability limitations from this service agreement.\", \"tool_call\": {\"name\": \"Rakib/roberta-base-on-cuad\", \"arguments\": {\"context\": \"The liability of the Company shall be limited to direct damages not exceeding $10,000...\", \"question\": \"What limitations of liability does the agreement specify?\"}}}, {\"user_intent\": \"Check the governing law stated in this contract document.\", \"tool_call\": {\"name\": \"Rakib/roberta-base-on-cuad\", \"arguments\": {\"context\": \"This Agreement shall be governed by and construed in accordance with the laws of the State of New York...\", \"question\": \"Which law governs this contract?\"}}}]}\n\n# User Query\nOur client is a legal firm that needs assistance in extracting specific information from a large number of legal documents. Automate the process of answering questions related to these documents.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Automatically extract answers to specific questions from multiple legal documents.",
                    "tool_description": "Use a RoBERTa-based question answering model fine-tuned on the CUAD dataset to process legal documents and retrieve answers to user-specified questions.",
                    "expected_response": "For each question and document pair, return the predicted answer text, the start and end positions within the document, and optionally a confidence score indicating the model's certainty."
                }
            ],
            "extracted_arguments": {
                "documents": "A collection of legal document texts to be processed by the model.",
                "questions": "A list of questions or queries for which answers are required from the legal documents.",
                "tokenizer": "A tokenizer object to preprocess input text for the RoBERTa-based QA model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_119",
        "query": "I need to estimate CO2 emissions from vehicles based on their characteristics, such as engine size, transmission type, and miles traveled.",
        "instruction": "Given a `CO2 emissions estimation` task, retrieve tools that process vehicle characteristics to estimate emissions by leveraging models capable of analyzing features like engine size, transmission type, and miles traveled to provide accurate predictions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Tabular Classification",
                    "api_call": "AutoModel.from_pretrained('datadmg/autotrain-test-news-44534112235')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "datadmg/autotrain-data-test-news",
                        "accuracy": 0.333
                    },
                    "description": "This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality.",
                    "id": "huggingface_api_857",
                    "name": "datadmg/autotrain-test-news-44534112235"
                },
                "id": "gorilla_huggingface_tool_850",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 1: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 2: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 3: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 4: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 5: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.",
            "Tool 6: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 7: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.",
            "Tool 8: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 9: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "Tool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 11: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 12: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 13: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 14: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 15: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 16: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 17: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.",
            "Tool 18: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset."
        ],
        "tool_view": {
            "description": "A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "parameters": {},
            "response": "The tool produces a **tabular classification prediction** output based on input data. Specifically, its response structure would typically consist of the following components:  \n\n1. **Predictions**:  \n   - A list, array, or Series containing the predicted class labels for each row in the input dataset.  \n   - Each element corresponds to a single input record, representing the model’s estimated carbon emissions category.  \n   - The predicted labels are likely categorical (multi-class), reflecting the classes used during model training.\n\n2. **Optional Additional Outputs (not explicitly defined but plausible)**:  \n   - **Probabilities or confidence scores**: Some classification models can provide a probability distribution over all possible classes for each input record.  \n   - **Metadata about input-output mapping**: For example, row identifiers or indexes corresponding to the input data.\n\nThe core expectation is that after passing a preprocessed tabular dataset to the model, the tool returns a structured set of predictions indicating the predicted CO2 emission class for each input row. No additional formatting, aggregation, or postprocessing is explicitly defined in the documentation.",
            "examples": [
                {
                    "user_intent": "I have a CSV file with vehicle specs and I want to predict their CO2 emissions using the trained model.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "vehicle_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "Predict carbon emissions for a dataset of industrial machines stored in 'machines.csv'.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "machines.csv"
                        }
                    }
                },
                {
                    "user_intent": "I want to estimate CO2 emissions for my fleet of delivery trucks using the provided model.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "delivery_trucks.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 1: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 2: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 3: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 4: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 5: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\nTool 6: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 7: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.\nTool 8: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 9: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\nTool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 11: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 12: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 13: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 14: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 15: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 16: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 17: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.\nTool 18: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\", \"parameters\": {}, \"response\": \"The tool produces a **tabular classification prediction** output based on input data. Specifically, its response structure would typically consist of the following components:  \\n\\n1. **Predictions**:  \\n   - A list, array, or Series containing the predicted class labels for each row in the input dataset.  \\n   - Each element corresponds to a single input record, representing the model\\u2019s estimated carbon emissions category.  \\n   - The predicted labels are likely categorical (multi-class), reflecting the classes used during model training.\\n\\n2. **Optional Additional Outputs (not explicitly defined but plausible)**:  \\n   - **Probabilities or confidence scores**: Some classification models can provide a probability distribution over all possible classes for each input record.  \\n   - **Metadata about input-output mapping**: For example, row identifiers or indexes corresponding to the input data.\\n\\nThe core expectation is that after passing a preprocessed tabular dataset to the model, the tool returns a structured set of predictions indicating the predicted CO2 emission class for each input row. No additional formatting, aggregation, or postprocessing is explicitly defined in the documentation.\", \"examples\": [{\"user_intent\": \"I have a CSV file with vehicle specs and I want to predict their CO2 emissions using the trained model.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"vehicle_data.csv\"}}}, {\"user_intent\": \"Predict carbon emissions for a dataset of industrial machines stored in 'machines.csv'.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"machines.csv\"}}}, {\"user_intent\": \"I want to estimate CO2 emissions for my fleet of delivery trucks using the provided model.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"delivery_trucks.csv\"}}}]}\n\n# User Query\nI need to estimate CO2 emissions from vehicles based on their characteristics, such as engine size, transmission type, and miles traveled.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Estimate CO2 emissions for vehicles based on their characteristics like engine size, transmission type, and miles traveled.",
                    "tool_description": "Use a pre-trained multi-class classification model to predict CO2 emissions from tabular vehicle data containing features such as engine size, transmission type, and miles traveled.",
                    "expected_response": "A set of predicted CO2 emission classes for each vehicle record, possibly including confidence scores for each prediction."
                }
            ],
            "extracted_arguments": {
                "input_data": "A tabular dataset containing vehicle characteristics including engine size, transmission type, and miles traveled as input features."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_361",
        "query": "Can you help me find the best model to fill in the gap in my legal document? I want a smaller model with higher efficiency but maintains a high level of accuracy.",
        "instruction": "Given a `gap-filling in legal documents` task, retrieve tools that specialize in Natural Language Processing, particularly fill-mask functionalities, to process legal text inputs and offer smaller, efficient models that maintain high accuracy while filling in missing information.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "nlpaueb/legal-bert-small-uncased"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModel"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-small-uncased')\nmodel = AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')",
                    "performance": {
                        "dataset": "Legal Corpora",
                        "accuracy": "Comparable to larger models"
                    },
                    "description": "LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.",
                    "id": "huggingface_api_685",
                    "name": "nlpaueb/legal-bert-small-uncased"
                },
                "id": "gorilla_huggingface_tool_680",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 1: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 2: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 3: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 4: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 5: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 6: A Hugging Face Transformers model designed for zero-shot text classification, allowing users to assign labels to text without prior task-specific training. It is a distilled version of BART-large-MNLI optimized for efficiency while maintaining high accuracy on natural language inference tasks.",
            "Tool 7: A fine-tuned version of Microsoft’s LayoutLMv3-large model designed for multimodal document question answering, capable of extracting and classifying information from documents with high accuracy, achieving over 98% accuracy on the SROIE dataset.",
            "Tool 8: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 9: A YOLOv8-based model designed for extracting tables from document images, capable of detecting both bordered and borderless tables. Trained on a dedicated table-extraction dataset, it provides high accuracy in identifying table structures and can be used to automate table detection and data extraction from images.",
            "Tool 10: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.",
            "Tool 11: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.",
            "Tool 12: This tool provides a pretrained Hugging Face Transformers model for token-level entity extraction, enabling identification of named entities in text. It uses the DistilBERT architecture, was trained with AutoTrain, and achieves high accuracy on relevant datasets, making it suitable for natural language processing tasks that require precise entity recognition.",
            "Tool 13: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.",
            "Tool 14: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.",
            "Tool 15: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 16: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.",
            "Tool 17: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.",
            "Tool 18: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 19: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text."
        ],
        "tool_view": {
            "description": "LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pre-trained model to load. This specifies which version of the LEGAL-BERT model will be used for fill-mask tasks.",
                    "required": true,
                    "example": "nlpaueb/legal-bert-small-uncased"
                }
            },
            "response": "The tool performs a **fill-mask task** using a lightweight BERT model trained specifically on legal text. Its response is not explicitly defined in the JSON, but based on standard fill-mask functionality in Transformers, a typical response would have the following structure:\n\n- **Predictions List**: A list or array of candidate tokens or words that could replace the masked token in the input text.\n  - Each entry represents a single candidate replacement.\n  \n- **Each Prediction Entry** typically contains:\n  1. **Token**: The actual token or word that the model predicts as a possible replacement for the mask.\n  2. **Token ID**: The numerical identifier corresponding to the token in the model’s vocabulary.\n  3. **Score / Probability**: A numerical value representing the model’s confidence or likelihood that this token correctly fills the masked position. Higher values indicate higher confidence.\n  4. **Sequence**: (Optional, depending on the implementation) The full input sentence with the mask replaced by the predicted token, showing the context with the prediction applied.\n\n- **Overall Semantics**: The response ranks the candidate tokens by likelihood, allowing users to see the most probable words the model would use to complete the masked position. This output can be used for tasks like legal text completion, suggestion, or analysis in the legal domain.\n\nIn essence, the response provides **a ranked set of possible words to fill the masked token, along with their confidence scores and optional reconstructed sequences**.",
            "examples": [
                {
                    "user_intent": "I want to predict the missing legal term in this contract sentence: 'The tenant shall pay the ___ on time.'",
                    "tool_call": {
                        "name": "nlpaueb/legal-bert-small-uncased",
                        "arguments": {
                            "pretrained_model_name_or_path": "nlpaueb/legal-bert-small-uncased",
                            "input_text": "The tenant shall pay the [MASK] on time."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank in this legal statement: 'The court hereby ___ the motion for summary judgment.'",
                    "tool_call": {
                        "name": "nlpaueb/legal-bert-small-uncased",
                        "arguments": {
                            "pretrained_model_name_or_path": "nlpaueb/legal-bert-small-uncased",
                            "input_text": "The court hereby [MASK] the motion for summary judgment."
                        }
                    }
                },
                {
                    "user_intent": "Complete the missing word in this clause from a lease agreement: 'The landlord is responsible for maintaining the ___ of the property.'",
                    "tool_call": {
                        "name": "nlpaueb/legal-bert-small-uncased",
                        "arguments": {
                            "pretrained_model_name_or_path": "nlpaueb/legal-bert-small-uncased",
                            "input_text": "The landlord is responsible for maintaining the [MASK] of the property."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 1: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 2: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 3: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 4: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 5: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 6: A Hugging Face Transformers model designed for zero-shot text classification, allowing users to assign labels to text without prior task-specific training. It is a distilled version of BART-large-MNLI optimized for efficiency while maintaining high accuracy on natural language inference tasks.\nTool 7: A fine-tuned version of Microsoft’s LayoutLMv3-large model designed for multimodal document question answering, capable of extracting and classifying information from documents with high accuracy, achieving over 98% accuracy on the SROIE dataset.\nTool 8: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 9: A YOLOv8-based model designed for extracting tables from document images, capable of detecting both bordered and borderless tables. Trained on a dedicated table-extraction dataset, it provides high accuracy in identifying table structures and can be used to automate table detection and data extraction from images.\nTool 10: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.\nTool 11: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.\nTool 12: This tool provides a pretrained Hugging Face Transformers model for token-level entity extraction, enabling identification of named entities in text. It uses the DistilBERT architecture, was trained with AutoTrain, and achieves high accuracy on relevant datasets, making it suitable for natural language processing tasks that require precise entity recognition.\nTool 13: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.\nTool 14: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.\nTool 15: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 16: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.\nTool 17: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\nTool 18: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 19: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pre-trained model to load. This specifies which version of the LEGAL-BERT model will be used for fill-mask tasks.\", \"required\": true, \"example\": \"nlpaueb/legal-bert-small-uncased\"}}, \"response\": \"The tool performs a **fill-mask task** using a lightweight BERT model trained specifically on legal text. Its response is not explicitly defined in the JSON, but based on standard fill-mask functionality in Transformers, a typical response would have the following structure:\\n\\n- **Predictions List**: A list or array of candidate tokens or words that could replace the masked token in the input text.\\n  - Each entry represents a single candidate replacement.\\n  \\n- **Each Prediction Entry** typically contains:\\n  1. **Token**: The actual token or word that the model predicts as a possible replacement for the mask.\\n  2. **Token ID**: The numerical identifier corresponding to the token in the model\\u2019s vocabulary.\\n  3. **Score / Probability**: A numerical value representing the model\\u2019s confidence or likelihood that this token correctly fills the masked position. Higher values indicate higher confidence.\\n  4. **Sequence**: (Optional, depending on the implementation) The full input sentence with the mask replaced by the predicted token, showing the context with the prediction applied.\\n\\n- **Overall Semantics**: The response ranks the candidate tokens by likelihood, allowing users to see the most probable words the model would use to complete the masked position. This output can be used for tasks like legal text completion, suggestion, or analysis in the legal domain.\\n\\nIn essence, the response provides **a ranked set of possible words to fill the masked token, along with their confidence scores and optional reconstructed sequences**.\", \"examples\": [{\"user_intent\": \"I want to predict the missing legal term in this contract sentence: 'The tenant shall pay the ___ on time.'\", \"tool_call\": {\"name\": \"nlpaueb/legal-bert-small-uncased\", \"arguments\": {\"pretrained_model_name_or_path\": \"nlpaueb/legal-bert-small-uncased\", \"input_text\": \"The tenant shall pay the [MASK] on time.\"}}}, {\"user_intent\": \"Fill in the blank in this legal statement: 'The court hereby ___ the motion for summary judgment.'\", \"tool_call\": {\"name\": \"nlpaueb/legal-bert-small-uncased\", \"arguments\": {\"pretrained_model_name_or_path\": \"nlpaueb/legal-bert-small-uncased\", \"input_text\": \"The court hereby [MASK] the motion for summary judgment.\"}}}, {\"user_intent\": \"Complete the missing word in this clause from a lease agreement: 'The landlord is responsible for maintaining the ___ of the property.'\", \"tool_call\": {\"name\": \"nlpaueb/legal-bert-small-uncased\", \"arguments\": {\"pretrained_model_name_or_path\": \"nlpaueb/legal-bert-small-uncased\", \"input_text\": \"The landlord is responsible for maintaining the [MASK] of the property.\"}}}]}\n\n# User Query\nCan you help me find the best model to fill in the gap in my legal document? I want a smaller model with higher efficiency but maintains a high level of accuracy.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to fill in missing words in a legal document using a lightweight, efficient model.",
                    "tool_description": "Perform a fill-mask task on legal text using a smaller, faster BERT-based model pre-trained on legal data, providing high-accuracy predictions for missing tokens.",
                    "expected_response": "A ranked list of candidate tokens to fill the masked positions in the legal text, including their confidence scores and optional reconstructed sequences."
                }
            ],
            "extracted_arguments": {
                "pretrained_model_name_or_path": "The identifier or path for the small, efficient LEGAL-BERT model to use for filling masked tokens in legal text.",
                "input_text": "The legal document sentence containing a masked token where the missing word should be predicted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_211",
        "query": "I am designing a quiz app that caters to blind users. The app should interpret the images and answer questions based on the image contents.",
        "instruction": "Given a `visual question answering` task, retrieve tools that can interpret and analyze images to answer questions based on their content, ensuring accessibility for users with visual impairments.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A VisualBERT model for Visual Question Answering.",
                    "id": "huggingface_api_484",
                    "name": "uclanlp/visualbert-vqa"
                },
                "id": "gorilla_huggingface_tool_480",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 1: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 2: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 3: A Latent Diffusion Model that generates and modifies images based on text prompts, allowing users to inpaint or alter specific regions of an image using a mask. It leverages a pretrained text encoder to interpret prompts and produce high-quality, customizable visual content.",
            "Tool 4: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 5: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 6: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 7: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 8: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 9: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 10: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 11: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 12: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.",
            "Tool 13: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 14: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.",
            "Tool 15: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 16: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 17: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 19: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data."
        ],
        "tool_view": {
            "description": "A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "parameters": {},
            "response": "The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \n\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \n\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.",
            "examples": [
                {
                    "user_intent": "I have an image of a street scene and I want to know how many cars are visible in it.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/street_scene.jpg",
                            "question": "How many cars are there in this image?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this photo of a kitchen and tell me what color the refrigerator is.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/kitchen.jpg",
                            "question": "What color is the refrigerator?"
                        }
                    }
                },
                {
                    "user_intent": "I want to analyze a picture of a park and find out if there are any people sitting on benches.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/park.jpg",
                            "question": "Are there people sitting on benches?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 1: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 2: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 3: A Latent Diffusion Model that generates and modifies images based on text prompts, allowing users to inpaint or alter specific regions of an image using a mask. It leverages a pretrained text encoder to interpret prompts and produce high-quality, customizable visual content.\nTool 4: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 5: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 6: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 7: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 8: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 9: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 10: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 11: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 12: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.\nTool 13: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 14: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.\nTool 15: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 16: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 17: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 19: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\", \"parameters\": {}, \"response\": \"The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \\n\\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \\n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \\n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \\n\\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.\", \"examples\": [{\"user_intent\": \"I have an image of a street scene and I want to know how many cars are visible in it.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/street_scene.jpg\", \"question\": \"How many cars are there in this image?\"}}}, {\"user_intent\": \"Look at this photo of a kitchen and tell me what color the refrigerator is.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/kitchen.jpg\", \"question\": \"What color is the refrigerator?\"}}}, {\"user_intent\": \"I want to analyze a picture of a park and find out if there are any people sitting on benches.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/park.jpg\", \"question\": \"Are there people sitting on benches?\"}}}]}\n\n# User Query\nI am designing a quiz app that caters to blind users. The app should interpret the images and answer questions based on the image contents.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Interpret images and answer questions about their content for blind users.",
                    "tool_description": "Use a Visual Question Answering (VQA) model to analyze an image along with a natural language question and generate a textual answer describing the image content.",
                    "expected_response": "A textual answer describing the relevant details of the image in response to the input question, potentially including confidence scores or metadata."
                }
            ],
            "extracted_arguments": {
                "image_input": "The image file or visual content to be interpreted by the model.",
                "question_text": "The natural language question about the content of the image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_86",
        "query": "I need a system that extracts all the well-known named entities such as person names, locations, and organizations from news articles.",
        "instruction": "Given a `named entity extraction` task, retrieve tools capable of processing text inputs to reliably identify and categorize well-known entities such as person names, locations, and organizations from the provided content, ensuring accurate and contextually appropriate entity recognition.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load('flair/ner-english')\n\n# make example sentence\nsentence = Sentence('George Washington went to Washington')\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)",
                    "performance": {
                        "dataset": "conll2003",
                        "accuracy": "93.06"
                    },
                    "description": "This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_426",
                    "name": "flair/ner-english"
                },
                "id": "gorilla_huggingface_tool_422",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 1: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 2: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 3: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 4: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 5: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 6: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 7: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 8: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 9: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.",
            "Tool 10: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 11: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 12: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 13: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 14: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 15: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 16: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 17: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 18: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.",
            "Tool 19: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content."
        ],
        "tool_view": {
            "description": "A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "parameters": {},
            "response": "The tool returns a list of named entities extracted from the input text. Each item in the list represents a single entity and typically includes the following components:\n\n1. **`entity` / `label`**: A string indicating the type of the recognized entity, such as a person, location, organization, or other predefined category used by the model.\n\n2. **`score`**: A numeric confidence score representing the model’s certainty that the entity has been correctly identified and classified. Higher scores indicate higher confidence.\n\n3. **`start`**: An integer specifying the index of the first character of the entity in the original input text.\n\n4. **`end`**: An integer specifying the index of the character immediately after the last character of the entity in the original input text.\n\n5. **`word` / `text`**: The substring from the input text that corresponds to the recognized entity.\n\nThe response is structured as an array of such entity objects, allowing downstream processing to identify, highlight, or categorize entities within the input text.",
            "examples": [
                {
                    "user_intent": "Extract all the names of people and locations from this sentence: 'Alice moved to Paris last year.'",
                    "tool_call": {
                        "name": "dslim/bert-base-NER-uncased",
                        "arguments": {
                            "text": "Alice moved to Paris last year."
                        }
                    }
                },
                {
                    "user_intent": "Identify organizations and locations mentioned in the news headline: 'Google announced a new office in Berlin.'",
                    "tool_call": {
                        "name": "dslim/bert-base-NER-uncased",
                        "arguments": {
                            "text": "Google announced a new office in Berlin."
                        }
                    }
                },
                {
                    "user_intent": "Find all named entities in a paragraph describing recent events in New York involving John and Microsoft.",
                    "tool_call": {
                        "name": "dslim/bert-base-NER-uncased",
                        "arguments": {
                            "text": "John attended a conference hosted by Microsoft in New York to discuss AI developments."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 1: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 2: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 3: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 4: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 5: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 6: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 7: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 8: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 9: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.\nTool 10: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 11: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 12: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 13: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 14: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 15: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 16: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 17: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 18: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.\nTool 19: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\", \"parameters\": {}, \"response\": \"The tool returns a list of named entities extracted from the input text. Each item in the list represents a single entity and typically includes the following components:\\n\\n1. **`entity` / `label`**: A string indicating the type of the recognized entity, such as a person, location, organization, or other predefined category used by the model.\\n\\n2. **`score`**: A numeric confidence score representing the model\\u2019s certainty that the entity has been correctly identified and classified. Higher scores indicate higher confidence.\\n\\n3. **`start`**: An integer specifying the index of the first character of the entity in the original input text.\\n\\n4. **`end`**: An integer specifying the index of the character immediately after the last character of the entity in the original input text.\\n\\n5. **`word` / `text`**: The substring from the input text that corresponds to the recognized entity.\\n\\nThe response is structured as an array of such entity objects, allowing downstream processing to identify, highlight, or categorize entities within the input text.\", \"examples\": [{\"user_intent\": \"Extract all the names of people and locations from this sentence: 'Alice moved to Paris last year.'\", \"tool_call\": {\"name\": \"dslim/bert-base-NER-uncased\", \"arguments\": {\"text\": \"Alice moved to Paris last year.\"}}}, {\"user_intent\": \"Identify organizations and locations mentioned in the news headline: 'Google announced a new office in Berlin.'\", \"tool_call\": {\"name\": \"dslim/bert-base-NER-uncased\", \"arguments\": {\"text\": \"Google announced a new office in Berlin.\"}}}, {\"user_intent\": \"Find all named entities in a paragraph describing recent events in New York involving John and Microsoft.\", \"tool_call\": {\"name\": \"dslim/bert-base-NER-uncased\", \"arguments\": {\"text\": \"John attended a conference hosted by Microsoft in New York to discuss AI developments.\"}}}]}\n\n# User Query\nI need a system that extracts all the well-known named entities such as person names, locations, and organizations from news articles.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract named entities such as person names, locations, and organizations from a news article.",
                    "tool_description": "Use a Named Entity Recognition (NER) model to identify and extract well-known entities like person names, locations, and organizations from the given text.",
                    "expected_response": "A list of named entities extracted from the text, including their type (e.g., person, location, organization), score, and position within the text."
                }
            ],
            "extracted_arguments": {
                "text": "The input news article or text from which named entities need to be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_116",
        "query": "We are developing a medical records analysis software that automatically recognizes biomedical entities from physicians' case reports.",
        "instruction": "Given a `biomedical entity recognition` task, retrieve tools that perform Named Entity Recognition specifically for biomedical texts by processing the case reports to automatically identify and classify biomedical entities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)",
                        "tokenizer": "AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification"
                    },
                    "example_code": "pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)",
                    "performance": {
                        "dataset": "Maccrobat",
                        "accuracy": "Not provided"
                    },
                    "description": "An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.",
                    "id": "huggingface_api_404",
                    "name": "d4data/biomedical-ner-all"
                },
                "id": "gorilla_huggingface_tool_400",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 1: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 2: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 3: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 4: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 5: A Hugging Face Transformers image classification model fine-tuned for detecting diabetic retinopathy from medical images, enabling automated analysis of retinal scans with an accuracy of 77.44%.",
            "Tool 6: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 7: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 8: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 9: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 10: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.",
            "Tool 11: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 12: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 13: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 14: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 15: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 16: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 17: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 18: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.",
            "Tool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently."
        ],
        "tool_view": {
            "description": "A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pretrained token classification model to load for named entity recognition, specified by its model identifier.",
                    "required": true,
                    "example": "d4data/biomedical-ner-all"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.",
                    "required": true,
                    "example": "d4data/biomedical-ner-all"
                },
                "aggregation_strategy": {
                    "type": "string",
                    "description": "The strategy to aggregate token-level predictions into entity-level results. Common strategies include 'simple', 'first', 'average', etc.",
                    "required": true,
                    "example": "simple"
                }
            },
            "response": "The tool produces a **Named Entity Recognition (NER) response** for a given input text. The response is typically structured as a sequence of entities detected in the text, with each entity containing several key pieces of information:\n\n1. **Entity text or span** – The exact substring from the input text that has been recognized as an entity.  \n2. **Entity type or label** – The category or class of the entity, corresponding to one of the 107 biomedical entity types the model was trained to recognize. Examples include medical conditions, medications, procedures, or anatomical terms.  \n3. **Start and end positions** – Integer indices indicating the position of the entity text within the input string. The `start` index marks the first character of the entity, and the `end` index marks the position immediately after the last character of the entity.  \n4. **Score or confidence** – A numeric value representing the model’s confidence in the predicted entity label. This is usually a probability between 0 and 1.  \n\nBecause the tool is used with an **aggregation strategy of \"simple\"**, overlapping or subword tokens may be merged so that each entity in the response corresponds to a coherent, human-readable span rather than fragmented subword tokens.\n\nOverall, the response is a list of such entity dictionaries, providing a structured identification of biomedical entities present in the input text.",
            "examples": [
                {
                    "user_intent": "I want to extract all biomedical entities like diseases, medications, and procedures from a patient's clinical note.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "The patient was prescribed 5 mg of amlodipine daily and showed improvement in hypertension symptoms within 2 weeks."
                        }
                    }
                },
                {
                    "user_intent": "Analyze a set of clinical trial abstracts to automatically identify genes, proteins, and other biomedical entities.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "BRCA1 mutations are linked to a higher risk of breast cancer, and patients often undergo genetic counseling before starting therapy."
                        }
                    }
                },
                {
                    "user_intent": "Detect and highlight any medical procedures and conditions mentioned in a cardiology case report.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "After undergoing catheter ablation, the patient reported no recurrence of palpitations at the 6-month follow-up."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 1: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 2: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 3: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 4: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 5: A Hugging Face Transformers image classification model fine-tuned for detecting diabetic retinopathy from medical images, enabling automated analysis of retinal scans with an accuracy of 77.44%.\nTool 6: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 7: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 8: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 9: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 10: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.\nTool 11: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 12: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 13: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 14: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 15: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 16: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 17: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 18: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.\nTool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pretrained token classification model to load for named entity recognition, specified by its model identifier.\", \"required\": true, \"example\": \"d4data/biomedical-ner-all\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.\", \"required\": true, \"example\": \"d4data/biomedical-ner-all\"}, \"aggregation_strategy\": {\"type\": \"string\", \"description\": \"The strategy to aggregate token-level predictions into entity-level results. Common strategies include 'simple', 'first', 'average', etc.\", \"required\": true, \"example\": \"simple\"}}, \"response\": \"The tool produces a **Named Entity Recognition (NER) response** for a given input text. The response is typically structured as a sequence of entities detected in the text, with each entity containing several key pieces of information:\\n\\n1. **Entity text or span** \\u2013 The exact substring from the input text that has been recognized as an entity.  \\n2. **Entity type or label** \\u2013 The category or class of the entity, corresponding to one of the 107 biomedical entity types the model was trained to recognize. Examples include medical conditions, medications, procedures, or anatomical terms.  \\n3. **Start and end positions** \\u2013 Integer indices indicating the position of the entity text within the input string. The `start` index marks the first character of the entity, and the `end` index marks the position immediately after the last character of the entity.  \\n4. **Score or confidence** \\u2013 A numeric value representing the model\\u2019s confidence in the predicted entity label. This is usually a probability between 0 and 1.  \\n\\nBecause the tool is used with an **aggregation strategy of \\\"simple\\\"**, overlapping or subword tokens may be merged so that each entity in the response corresponds to a coherent, human-readable span rather than fragmented subword tokens.\\n\\nOverall, the response is a list of such entity dictionaries, providing a structured identification of biomedical entities present in the input text.\", \"examples\": [{\"user_intent\": \"I want to extract all biomedical entities like diseases, medications, and procedures from a patient's clinical note.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"The patient was prescribed 5 mg of amlodipine daily and showed improvement in hypertension symptoms within 2 weeks.\"}}}, {\"user_intent\": \"Analyze a set of clinical trial abstracts to automatically identify genes, proteins, and other biomedical entities.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"BRCA1 mutations are linked to a higher risk of breast cancer, and patients often undergo genetic counseling before starting therapy.\"}}}, {\"user_intent\": \"Detect and highlight any medical procedures and conditions mentioned in a cardiology case report.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"After undergoing catheter ablation, the patient reported no recurrence of palpitations at the 6-month follow-up.\"}}}]}\n\n# User Query\nWe are developing a medical records analysis software that automatically recognizes biomedical entities from physicians' case reports.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to automatically extract biomedical entities such as diseases, medications, procedures, and anatomical terms from physicians' case reports.",
                    "tool_description": "Use a Named Entity Recognition (NER) model trained on biomedical text to identify and classify entities within unstructured medical case reports, producing structured outputs for downstream analysis.",
                    "expected_response": "A list of recognized biomedical entities, each with its text span, entity type, start and end positions in the text, and confidence score."
                }
            ],
            "extracted_arguments": {
                "model": "The pretrained token classification model identifier used for biomedical NER.",
                "tokenizer": "The pretrained tokenizer corresponding to the NER model, converting text into model-readable tokens.",
                "aggregation_strategy": "The method to combine token-level predictions into coherent entity-level results."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_464",
        "query": "The marketing team wants a tool to quickly classify new advertisement videos.",
        "instruction": "Given a `video classification` task, retrieve tools that utilize video inputs to categorize and classify content by employing advanced computer vision models to understand and label advertisement videos.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')",
                    "api_arguments": [
                        "images"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, TimesformerForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(8, 3, 224, 224))\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\ninputs = processor(images=video, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-600",
                        "accuracy": null
                    },
                    "description": "TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_318",
                    "name": "facebook/timesformer-base-finetuned-k600"
                },
                "id": "gorilla_huggingface_tool_314",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 1: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 2: A multilingual zero-shot text classification tool that uses the XLM-RoBERTa large model fine-tuned on natural language inference data across 15 languages. It allows users to classify text into custom categories without prior task-specific training by providing candidate labels and an optional hypothesis template, making it useful for quickly categorizing text in multiple languages.",
            "Tool 3: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 4: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 5: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 7: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.",
            "Tool 8: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.",
            "Tool 9: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 10: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 11: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 12: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 13: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 14: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.",
            "Tool 15: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 16: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 17: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 18: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 19: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch."
        ],
        "tool_view": {
            "description": "VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "parameters": {
                "video": {
                    "type": "array",
                    "description": "A sequence of video frames to be processed by the model. Each frame should be a 3-channel image array, typically in the shape [num_frames, height, width, channels].",
                    "required": true,
                    "example": "list(np.random.randn(16, 3, 224, 224))"
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors to return after processing the video. Common options include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool returns an object containing the outputs of the VideoMAE model for a given input video. The response typically includes the following components:\n\n1. **loss** (optional):  \n   - A scalar value representing the reconstruction loss calculated over the masked patches when the model is used in pre-training mode.  \n   - Present if the input includes a mask for masked autoencoding (e.g., `bool_masked_pos`).\n\n2. **logits** or **predictions** (optional, depending on configuration):  \n   - A tensor containing the predicted values for each patch or the entire video sequence.  \n   - These can be used for downstream classification or reconstruction tasks.\n\n3. **hidden_states** (optional):  \n   - A sequence of tensors representing the intermediate representations of the video at each layer of the Transformer encoder.  \n   - Can be used for feature extraction or further processing.\n\n4. **reconstructed_video** (optional):  \n   - A tensor containing the reconstructed pixel values for masked patches if the model is being used for masked autoencoding.\n\n5. **attention_maps** (optional):  \n   - Tensors representing attention weights from the Transformer layers, useful for interpretability.\n\nIn principle, the response is a structured object where each field provides either a training signal (like `loss`), the model’s output predictions (`logits` or `reconstructed_video`), or internal representations (`hidden_states`, `attention_maps`). The presence of each field depends on the inputs provided and whether the model is used for pre-training, feature extraction, or downstream classification.",
            "examples": [
                {
                    "user_intent": "I want to use the VideoMAE model for video classification on a set of 16-frame videos.",
                    "tool_call": {
                        "name": "videomae-base-short-ssv2",
                        "arguments": {
                            "video": "list(np.random.randn(16, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Can you show me how to use the VideoMAE model to process a batch of random video data?",
                    "tool_call": {
                        "name": "videomae-base-short-ssv2",
                        "arguments": {
                            "video": "list(np.random.randn(16, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I need to test the VideoMAE model with my video data in PyTorch. How do I integrate it into my workflow?",
                    "tool_call": {
                        "name": "videomae-base-short-ssv2",
                        "arguments": {
                            "video": "list(np.random.randn(16, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 1: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 2: A multilingual zero-shot text classification tool that uses the XLM-RoBERTa large model fine-tuned on natural language inference data across 15 languages. It allows users to classify text into custom categories without prior task-specific training by providing candidate labels and an optional hypothesis template, making it useful for quickly categorizing text in multiple languages.\nTool 3: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 4: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 5: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 7: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.\nTool 8: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\nTool 9: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 10: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 11: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 12: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 13: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 14: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.\nTool 15: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 16: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 17: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 18: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 19: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\", \"parameters\": {\"video\": {\"type\": \"array\", \"description\": \"A sequence of video frames to be processed by the model. Each frame should be a 3-channel image array, typically in the shape [num_frames, height, width, channels].\", \"required\": true, \"example\": \"list(np.random.randn(16, 3, 224, 224))\"}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors to return after processing the video. Common options include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool returns an object containing the outputs of the VideoMAE model for a given input video. The response typically includes the following components:\\n\\n1. **loss** (optional):  \\n   - A scalar value representing the reconstruction loss calculated over the masked patches when the model is used in pre-training mode.  \\n   - Present if the input includes a mask for masked autoencoding (e.g., `bool_masked_pos`).\\n\\n2. **logits** or **predictions** (optional, depending on configuration):  \\n   - A tensor containing the predicted values for each patch or the entire video sequence.  \\n   - These can be used for downstream classification or reconstruction tasks.\\n\\n3. **hidden_states** (optional):  \\n   - A sequence of tensors representing the intermediate representations of the video at each layer of the Transformer encoder.  \\n   - Can be used for feature extraction or further processing.\\n\\n4. **reconstructed_video** (optional):  \\n   - A tensor containing the reconstructed pixel values for masked patches if the model is being used for masked autoencoding.\\n\\n5. **attention_maps** (optional):  \\n   - Tensors representing attention weights from the Transformer layers, useful for interpretability.\\n\\nIn principle, the response is a structured object where each field provides either a training signal (like `loss`), the model\\u2019s output predictions (`logits` or `reconstructed_video`), or internal representations (`hidden_states`, `attention_maps`). The presence of each field depends on the inputs provided and whether the model is used for pre-training, feature extraction, or downstream classification.\", \"examples\": [{\"user_intent\": \"I want to use the VideoMAE model for video classification on a set of 16-frame videos.\", \"tool_call\": {\"name\": \"videomae-base-short-ssv2\", \"arguments\": {\"video\": \"list(np.random.randn(16, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Can you show me how to use the VideoMAE model to process a batch of random video data?\", \"tool_call\": {\"name\": \"videomae-base-short-ssv2\", \"arguments\": {\"video\": \"list(np.random.randn(16, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I need to test the VideoMAE model with my video data in PyTorch. How do I integrate it into my workflow?\", \"tool_call\": {\"name\": \"videomae-base-short-ssv2\", \"arguments\": {\"video\": \"list(np.random.randn(16, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nThe marketing team wants a tool to quickly classify new advertisement videos.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify advertisement videos to identify their content or category.",
                    "tool_description": "Use a pre-trained video classification model to process advertisement videos and generate predicted category labels.",
                    "expected_response": "A structured object containing predicted class labels for each input video, optionally including logits, hidden states, or attention maps depending on the model configuration."
                }
            ],
            "extracted_arguments": {
                "video": "A sequence of video frames representing the advertisement, formatted as a 3-channel array suitable for input to a video classification model.",
                "return_tensors": "The format of the output tensors, such as PyTorch ('pt') or TensorFlow ('tf') tensors."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_410",
        "query": "Our company's goal is to predict carbon emissions based on the given features of the compound.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools that can utilize tabular regression models to process compound features and predict carbon emissions accurately, emphasizing the model's ability to handle structured data inputs and deliver quantified emission outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Hugging Face",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "kochetkovIT/autotrain-data-ironhack",
                        "accuracy": {
                            "Loss": 2.603,
                            "R2": 0.013,
                            "MSE": 6.776,
                            "MAE": 1.666,
                            "RMSLE": 0.502
                        }
                    },
                    "description": "A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.",
                    "id": "huggingface_api_870",
                    "name": "kochetkovIT/autotrain-ironhack-49741119788"
                },
                "id": "gorilla_huggingface_tool_863",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 1: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 2: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 3: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 4: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 5: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 6: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 7: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 8: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 9: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 10: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 11: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 12: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 13: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 14: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 15: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 16: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.",
            "Tool 17: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 18: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 19: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries."
        ],
        "tool_view": {
            "description": "A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "parameters": {
                "data": {
                    "type": "DataFrame",
                    "description": "A tabular dataset containing the input features for which carbon emissions predictions are to be made. Columns should match the features expected by the model.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_1': [1.2, 3.4], 'feat_2': [5.6, 7.8]})"
                }
            },
            "response": "The tool produces a numeric prediction output corresponding to carbon emissions for the input tabular data. The response structure can be described as follows:  \n\n- **Type:** Array or list of numeric values.  \n- **Length:** Equal to the number of rows in the input dataset; each entry corresponds to a single data instance.  \n- **Elements:** Each element represents the predicted carbon emission value for the respective input row.  \n- **Data Semantics:**  \n  - Each numeric value reflects the model’s regression output, estimating carbon emissions based on the input features.  \n  - Values are continuous and can be used for further analysis, evaluation, or aggregation.  \n\nThe tool does not return additional metadata by default (such as confidence intervals or feature importances) in the standard prediction call; it strictly outputs the predicted numeric values corresponding to the input dataset.",
            "examples": [
                {
                    "user_intent": "Predict the carbon emissions for my factory dataset using the trained model.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "factory_emissions_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV of vehicle specifications and want to estimate their carbon footprint.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "vehicle_specs.csv"
                        }
                    }
                },
                {
                    "user_intent": "Calculate predicted carbon emissions for a new dataset of industrial plants.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "industrial_plants_data.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 1: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 2: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 3: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 4: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 5: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 6: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 7: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 8: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 9: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 10: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 11: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 12: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 13: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 14: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 15: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 16: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\nTool 17: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 18: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 19: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\", \"parameters\": {\"data\": {\"type\": \"DataFrame\", \"description\": \"A tabular dataset containing the input features for which carbon emissions predictions are to be made. Columns should match the features expected by the model.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_1': [1.2, 3.4], 'feat_2': [5.6, 7.8]})\"}}, \"response\": \"The tool produces a numeric prediction output corresponding to carbon emissions for the input tabular data. The response structure can be described as follows:  \\n\\n- **Type:** Array or list of numeric values.  \\n- **Length:** Equal to the number of rows in the input dataset; each entry corresponds to a single data instance.  \\n- **Elements:** Each element represents the predicted carbon emission value for the respective input row.  \\n- **Data Semantics:**  \\n  - Each numeric value reflects the model\\u2019s regression output, estimating carbon emissions based on the input features.  \\n  - Values are continuous and can be used for further analysis, evaluation, or aggregation.  \\n\\nThe tool does not return additional metadata by default (such as confidence intervals or feature importances) in the standard prediction call; it strictly outputs the predicted numeric values corresponding to the input dataset.\", \"examples\": [{\"user_intent\": \"Predict the carbon emissions for my factory dataset using the trained model.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"factory_emissions_data.csv\"}}}, {\"user_intent\": \"I have a CSV of vehicle specifications and want to estimate their carbon footprint.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"vehicle_specs.csv\"}}}, {\"user_intent\": \"Calculate predicted carbon emissions for a new dataset of industrial plants.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"industrial_plants_data.csv\"}}}]}\n\n# User Query\nOur company's goal is to predict carbon emissions based on the given features of the compound.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict carbon emissions based on the features of a compound.",
                    "tool_description": "Predict carbon emissions using a tabular regression model based on the provided compound features.",
                    "expected_response": "A list of predicted carbon emission values corresponding to each compound's feature data."
                }
            ],
            "extracted_arguments": {
                "data": "A tabular dataset containing the input features for compounds, where each row corresponds to a single compound and columns match the expected features for the model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_416",
        "query": "Detect if there are any harmful messages in a chat room.",
        "instruction": "Given a `text analysis` task, retrieve tools that utilize natural language processing to detect and classify potentially harmful or toxic messages within text data by processing the content and returning relevant classifications or insights.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline(model='martin-ha/toxic-comment-model')",
                    "api_arguments": {
                        "model_path": "martin-ha/toxic-comment-model"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\nmodel_path = martin-ha/toxic-comment-model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))",
                    "performance": {
                        "dataset": "held-out test set",
                        "accuracy": 0.94,
                        "f1-score": 0.59
                    },
                    "description": "This model is a fine-tuned version of the DistilBERT model to classify toxic comments.",
                    "id": "huggingface_api_392",
                    "name": "martin-ha/toxic-comment-model"
                },
                "id": "gorilla_huggingface_tool_388",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 1: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 2: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 3: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 4: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 5: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 6: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "Tool 7: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 8: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 9: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 10: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 11: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 12: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.",
            "Tool 13: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 14: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 15: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 16: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 17: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 18: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 19: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions."
        ],
        "tool_view": {
            "description": "A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "parameters": {
                "model_path": {
                    "type": "string",
                    "description": "The identifier or path of the pre-trained model to load for toxic comment classification, typically from Hugging Face model hub.",
                    "required": true,
                    "example": "martin-ha/toxic-comment-model"
                }
            },
            "response": "The tool returns a structured classification of input text with respect to toxicity. The typical response is a list of objects, each representing a predicted label along with an associated confidence score. The structure and meaning of each component are as follows:\n\n- **List of classification results**: The response is an array where each element corresponds to one possible label predicted by the model for the given input text. The array may contain one or multiple elements depending on whether the pipeline is configured to return the top label only or multiple possible labels.\n\n- **`label`**: A string indicating the category assigned to the input text. In this context, the labels represent toxicity classes, such as \"toxic,\" \"severe_toxic,\" \"obscene,\" or other similar categories depending on the model’s fine-tuning.\n\n- **`score`**: A numeric value (typically a float between 0 and 1) representing the model’s confidence in the prediction of that label. Higher values indicate greater confidence that the input text belongs to the corresponding category.\n\nOverall, the response communicates which toxicity categories the model associates with the input text and how confident it is in each classification. The information is suitable for downstream tasks such as filtering, moderation, or further text analysis.",
            "examples": [
                {
                    "user_intent": "Check if this comment is toxic: 'You're so stupid, I can't believe you wrote that.'",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "You're so stupid, I can't believe you wrote that."
                        }
                    }
                },
                {
                    "user_intent": "Analyze this customer review and see if it contains any toxic language: 'I hate this product, it's the worst thing I've ever bought!'",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "I hate this product, it's the worst thing I've ever bought!"
                        }
                    }
                },
                {
                    "user_intent": "I want to scan multiple forum posts for toxicity and flag any harmful comments.",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "I can't stand anyone who disagrees with me. They're all idiots."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 1: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 2: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 3: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 4: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 5: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 6: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\nTool 7: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 8: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 9: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 10: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 11: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 12: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.\nTool 13: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 14: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 15: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 16: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 17: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 18: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 19: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\", \"parameters\": {\"model_path\": {\"type\": \"string\", \"description\": \"The identifier or path of the pre-trained model to load for toxic comment classification, typically from Hugging Face model hub.\", \"required\": true, \"example\": \"martin-ha/toxic-comment-model\"}}, \"response\": \"The tool returns a structured classification of input text with respect to toxicity. The typical response is a list of objects, each representing a predicted label along with an associated confidence score. The structure and meaning of each component are as follows:\\n\\n- **List of classification results**: The response is an array where each element corresponds to one possible label predicted by the model for the given input text. The array may contain one or multiple elements depending on whether the pipeline is configured to return the top label only or multiple possible labels.\\n\\n- **`label`**: A string indicating the category assigned to the input text. In this context, the labels represent toxicity classes, such as \\\"toxic,\\\" \\\"severe_toxic,\\\" \\\"obscene,\\\" or other similar categories depending on the model\\u2019s fine-tuning.\\n\\n- **`score`**: A numeric value (typically a float between 0 and 1) representing the model\\u2019s confidence in the prediction of that label. Higher values indicate greater confidence that the input text belongs to the corresponding category.\\n\\nOverall, the response communicates which toxicity categories the model associates with the input text and how confident it is in each classification. The information is suitable for downstream tasks such as filtering, moderation, or further text analysis.\", \"examples\": [{\"user_intent\": \"Check if this comment is toxic: 'You're so stupid, I can't believe you wrote that.'\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"You're so stupid, I can't believe you wrote that.\"}}}, {\"user_intent\": \"Analyze this customer review and see if it contains any toxic language: 'I hate this product, it's the worst thing I've ever bought!'\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"I hate this product, it's the worst thing I've ever bought!\"}}}, {\"user_intent\": \"I want to scan multiple forum posts for toxicity and flag any harmful comments.\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"I can't stand anyone who disagrees with me. They're all idiots.\"}}}]}\n\n# User Query\nDetect if there are any harmful messages in a chat room.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Check if messages in a chat room are toxic or harmful.",
                    "tool_description": "Use a fine-tuned DistilBERT model to classify chat messages for toxicity, detecting offensive or harmful content in natural language inputs.",
                    "expected_response": "A list of classification results for each message, including labels indicating toxicity categories and confidence scores for each prediction."
                }
            ],
            "extracted_arguments": {
                "text": "The chat messages or conversation content to be analyzed for harmful or toxic language."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_114",
        "query": "A chat service needs a way to compare and cluster similar sentences from users in different languages. Find a suitable feature extraction method to achieve this.",
        "instruction": "Given a `multilingual sentence clustering` task, retrieve tools that perform feature extraction to compare and cluster similar sentences by processing multilingual text inputs and generating language-agnostic embeddings.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('rasa/LaBSE')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.",
                    "id": "huggingface_api_21",
                    "name": "rasa/LaBSE"
                },
                "id": "gorilla_huggingface_tool_21",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 2: This tool uses the \"sentence-transformers/distilbert-base-nli-mean-tokens\" model to convert sentences and paragraphs into 768-dimensional dense vector representations, enabling tasks such as clustering and semantic search. It leverages the Hugging Face Transformers framework for feature extraction and requires the `sentence-transformers` library.",
            "Tool 3: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 4: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 5: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 6: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 7: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.",
            "Tool 8: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 9: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 10: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 11: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 12: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 13: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "Tool 14: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 15: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 16: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 17: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 18: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 19: mT5 is a multilingual text-to-text model based on the T5 architecture, pre-trained on a large Common Crawl dataset covering 101 languages. It uses a unified text-to-text framework to perform a wide range of multilingual NLP tasks, such as translation, summarization, and text generation, enabling consistent input-to-output processing across different languages."
        ],
        "tool_view": {
            "description": "A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pretrained question answering model to load from the Transformers library.",
                    "required": true,
                    "example": "deepset/xlm-roberta-large-squad2"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or identifier of the tokenizer corresponding to the pretrained model, used to preprocess text inputs for question answering.",
                    "required": true,
                    "example": "deepset/xlm-roberta-large-squad2"
                }
            },
            "response": "The tool provides a model for extractive question answering based on the XLM-RoBERTa architecture. The response from this tool is likely to consist of the following components:\n\n1. **Answer Text**: The tool returns a text response that provides the most relevant answer to the question based on the provided context. This answer is extracted from the context paragraph using the model's understanding.\n\n2. **Start Position**: The tool may return the start position (as an index) of the answer within the context text. This represents where the answer begins in the context, useful for locating the specific segment.\n\n3. **End Position**: The tool may also return the end position (as an index) of the answer within the context text, indicating where the answer ends.\n\n4. **Confidence Scores** (optional): The tool might include confidence scores (e.g., a probability or a score indicating the model's certainty about the extracted answer). These scores provide insight into how confident the model is in its response.\n\nIn summary, the tool's response contains the most relevant text answer to the question, possibly along with its positions in the context and confidence metrics. These components help identify both the extracted information and the level of certainty regarding that information.",
            "examples": [
                {
                    "user_intent": "I want to find the answer to a question from a paragraph about AI advancements.",
                    "tool_call": {
                        "name": "deepset/xlm-roberta-large-squad2",
                        "arguments": {
                            "model_name": "deepset/xlm-roberta-large-squad2",
                            "tokenizer": "deepset/xlm-roberta-large-squad2",
                            "question": "What are the main benefits of AI in healthcare?",
                            "context": "AI is increasingly used in healthcare to improve diagnostics, personalize treatment plans, and optimize hospital workflows, leading to better patient outcomes and reduced costs."
                        }
                    }
                },
                {
                    "user_intent": "Answer a question in German using a multilingual model.",
                    "tool_call": {
                        "name": "deepset/xlm-roberta-large-squad2",
                        "arguments": {
                            "model_name": "deepset/xlm-roberta-large-squad2",
                            "tokenizer": "deepset/xlm-roberta-large-squad2",
                            "question": "Welche Vorteile bietet erneuerbare Energie?",
                            "context": "Erneuerbare Energien wie Wind- und Solarenergie helfen, den CO2-Ausstoß zu reduzieren und die Abhängigkeit von fossilen Brennstoffen zu verringern."
                        }
                    }
                },
                {
                    "user_intent": "Check if the model can handle tricky questions that might not have an answer in the text.",
                    "tool_call": {
                        "name": "deepset/xlm-roberta-large-squad2",
                        "arguments": {
                            "model_name": "deepset/xlm-roberta-large-squad2",
                            "tokenizer": "deepset/xlm-roberta-large-squad2",
                            "question": "What caused the extinction of dinosaurs in the 1800s?",
                            "context": "Dinosaurs became extinct around 66 million years ago, likely due to an asteroid impact and volcanic activity."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 2: This tool uses the \"sentence-transformers/distilbert-base-nli-mean-tokens\" model to convert sentences and paragraphs into 768-dimensional dense vector representations, enabling tasks such as clustering and semantic search. It leverages the Hugging Face Transformers framework for feature extraction and requires the `sentence-transformers` library.\nTool 3: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 4: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 5: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 6: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 7: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.\nTool 8: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 9: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 10: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 11: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 12: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 13: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\nTool 14: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 15: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 16: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 17: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 18: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 19: mT5 is a multilingual text-to-text model based on the T5 architecture, pre-trained on a large Common Crawl dataset covering 101 languages. It uses a unified text-to-text framework to perform a wide range of multilingual NLP tasks, such as translation, summarization, and text generation, enabling consistent input-to-output processing across different languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pretrained question answering model to load from the Transformers library.\", \"required\": true, \"example\": \"deepset/xlm-roberta-large-squad2\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or identifier of the tokenizer corresponding to the pretrained model, used to preprocess text inputs for question answering.\", \"required\": true, \"example\": \"deepset/xlm-roberta-large-squad2\"}}, \"response\": \"The tool provides a model for extractive question answering based on the XLM-RoBERTa architecture. The response from this tool is likely to consist of the following components:\\n\\n1. **Answer Text**: The tool returns a text response that provides the most relevant answer to the question based on the provided context. This answer is extracted from the context paragraph using the model's understanding.\\n\\n2. **Start Position**: The tool may return the start position (as an index) of the answer within the context text. This represents where the answer begins in the context, useful for locating the specific segment.\\n\\n3. **End Position**: The tool may also return the end position (as an index) of the answer within the context text, indicating where the answer ends.\\n\\n4. **Confidence Scores** (optional): The tool might include confidence scores (e.g., a probability or a score indicating the model's certainty about the extracted answer). These scores provide insight into how confident the model is in its response.\\n\\nIn summary, the tool's response contains the most relevant text answer to the question, possibly along with its positions in the context and confidence metrics. These components help identify both the extracted information and the level of certainty regarding that information.\", \"examples\": [{\"user_intent\": \"I want to find the answer to a question from a paragraph about AI advancements.\", \"tool_call\": {\"name\": \"deepset/xlm-roberta-large-squad2\", \"arguments\": {\"model_name\": \"deepset/xlm-roberta-large-squad2\", \"tokenizer\": \"deepset/xlm-roberta-large-squad2\", \"question\": \"What are the main benefits of AI in healthcare?\", \"context\": \"AI is increasingly used in healthcare to improve diagnostics, personalize treatment plans, and optimize hospital workflows, leading to better patient outcomes and reduced costs.\"}}}, {\"user_intent\": \"Answer a question in German using a multilingual model.\", \"tool_call\": {\"name\": \"deepset/xlm-roberta-large-squad2\", \"arguments\": {\"model_name\": \"deepset/xlm-roberta-large-squad2\", \"tokenizer\": \"deepset/xlm-roberta-large-squad2\", \"question\": \"Welche Vorteile bietet erneuerbare Energie?\", \"context\": \"Erneuerbare Energien wie Wind- und Solarenergie helfen, den CO2-Aussto\\u00df zu reduzieren und die Abh\\u00e4ngigkeit von fossilen Brennstoffen zu verringern.\"}}}, {\"user_intent\": \"Check if the model can handle tricky questions that might not have an answer in the text.\", \"tool_call\": {\"name\": \"deepset/xlm-roberta-large-squad2\", \"arguments\": {\"model_name\": \"deepset/xlm-roberta-large-squad2\", \"tokenizer\": \"deepset/xlm-roberta-large-squad2\", \"question\": \"What caused the extinction of dinosaurs in the 1800s?\", \"context\": \"Dinosaurs became extinct around 66 million years ago, likely due to an asteroid impact and volcanic activity.\"}}}]}\n\n# User Query\nA chat service needs a way to compare and cluster similar sentences from users in different languages. Find a suitable feature extraction method to achieve this.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to encode sentences from users in multiple languages into semantic vectors for comparison and clustering.",
                    "tool_description": "Use a multilingual sentence embedding model to convert text inputs into dense vector representations that capture semantic meaning across different languages.",
                    "expected_response": "A set of fixed-length vectors representing the semantic content of each input sentence, suitable for similarity computation and clustering."
                },
                {
                    "user_intent": "Cluster or group similar sentences based on their semantic vectors.",
                    "tool_description": "Apply a clustering or similarity analysis method on the sentence embeddings to identify groups of semantically similar sentences across languages.",
                    "expected_response": "A mapping of sentences to clusters or similarity scores between sentence pairs."
                }
            ],
            "extracted_arguments": {
                "input_sentences": "A list of sentences provided by users in one or more languages, represented as text strings.",
                "embedding_model": "The pretrained multilingual model used to convert sentences into dense semantic vectors.",
                "clustering_method": "The algorithm or method applied to group similar vectors together, such as k-means or hierarchical clustering."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_315",
        "query": "Write a welcome email to a new employee joining the company.",
        "instruction": "Given a `text generation` task, retrieve tools that facilitate natural language processing to create personalized and professional email content by processing textual inputs and generating coherent outputs suitable for welcoming new employees.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "nlp('Once upon a time...')",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random mt5 model for text generation",
                    "id": "huggingface_api_14",
                    "name": "lewtun/tiny-random-mt5"
                },
                "id": "gorilla_huggingface_tool_14",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 1: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 2: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 4: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 5: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 6: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 7: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 8: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.",
            "Tool 9: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 10: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.",
            "Tool 11: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 12: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.",
            "Tool 13: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 14: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 15: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.",
            "Tool 16: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 17: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 18: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 19: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts."
        ],
        "tool_view": {
            "description": "A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "parameters": {
                "inputs": {
                    "type": "string",
                    "description": "The text input that will be processed by the model to perform entity extraction. This is typically the raw text containing entities (like company names) that the model will extract.",
                    "required": true,
                    "example": "I love AutoTrain"
                }
            },
            "response": "The tool produces a **token classification response** typical of Hugging Face Transformers models for entity extraction. The response structure can be described as follows:\n\n1. **Token-level outputs**:  \n   The model returns a list of predictions corresponding to each token in the input text. Each element in this list represents a single token and contains information about the model’s classification of that token.\n\n2. **Typical fields per token**:  \n   - **`word` or `token`**: The actual text of the token from the input sentence.  \n   - **`entity` or `label`**: The predicted entity label assigned to this token (e.g., company name, organization, or non-entity). Labels usually follow the IOB or BIO tagging scheme.  \n   - **`score` or `confidence`**: A probability or confidence score indicating how confident the model is in its predicted label.  \n   - **`start` and `end`** (optional depending on implementation): Character offsets in the original input string that indicate the span of text covered by the token.\n\n3. **Aggregated entities (optional)**:  \n   Some implementations may post-process token-level outputs to group consecutive tokens with the same entity label into a single entity mention, returning:  \n   - **`entity_group` or `entity`**: The combined label for the entity span.  \n   - **`text`**: The reconstructed text of the entity from consecutive tokens.  \n   - **`start` and `end`**: Character offsets for the span of the entity in the original text.  \n   - **`score`**: Confidence score for the grouped entity, often computed as an aggregate of token-level scores.\n\n4. **Overall semantics**:  \n   The response allows a user to identify **which parts of the input text correspond to company names**, along with model confidence and positional information, enabling downstream tasks such as extraction, highlighting, or database population.  \n\nIn summary, the response is a **structured token-level prediction** that may optionally be aggregated into entity spans, providing the predicted label, confidence, and positional information for each recognized entity in the text.",
            "examples": [
                {
                    "user_intent": "Extract the company names mentioned in the sentence 'I recently started working at OpenAI and Microsoft.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "I recently started working at OpenAI and Microsoft."
                        }
                    }
                },
                {
                    "user_intent": "Identify entities representing companies in the text 'Tesla, SpaceX, and Neuralink are all led by Elon Musk.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "Tesla, SpaceX, and Neuralink are all led by Elon Musk."
                        }
                    }
                },
                {
                    "user_intent": "Find all company names in the paragraph: 'Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 1: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 2: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 4: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 5: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 6: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 7: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 8: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.\nTool 9: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 10: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.\nTool 11: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 12: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.\nTool 13: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 14: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 15: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.\nTool 16: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 17: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 18: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 19: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\", \"parameters\": {\"inputs\": {\"type\": \"string\", \"description\": \"The text input that will be processed by the model to perform entity extraction. This is typically the raw text containing entities (like company names) that the model will extract.\", \"required\": true, \"example\": \"I love AutoTrain\"}}, \"response\": \"The tool produces a **token classification response** typical of Hugging Face Transformers models for entity extraction. The response structure can be described as follows:\\n\\n1. **Token-level outputs**:  \\n   The model returns a list of predictions corresponding to each token in the input text. Each element in this list represents a single token and contains information about the model\\u2019s classification of that token.\\n\\n2. **Typical fields per token**:  \\n   - **`word` or `token`**: The actual text of the token from the input sentence.  \\n   - **`entity` or `label`**: The predicted entity label assigned to this token (e.g., company name, organization, or non-entity). Labels usually follow the IOB or BIO tagging scheme.  \\n   - **`score` or `confidence`**: A probability or confidence score indicating how confident the model is in its predicted label.  \\n   - **`start` and `end`** (optional depending on implementation): Character offsets in the original input string that indicate the span of text covered by the token.\\n\\n3. **Aggregated entities (optional)**:  \\n   Some implementations may post-process token-level outputs to group consecutive tokens with the same entity label into a single entity mention, returning:  \\n   - **`entity_group` or `entity`**: The combined label for the entity span.  \\n   - **`text`**: The reconstructed text of the entity from consecutive tokens.  \\n   - **`start` and `end`**: Character offsets for the span of the entity in the original text.  \\n   - **`score`**: Confidence score for the grouped entity, often computed as an aggregate of token-level scores.\\n\\n4. **Overall semantics**:  \\n   The response allows a user to identify **which parts of the input text correspond to company names**, along with model confidence and positional information, enabling downstream tasks such as extraction, highlighting, or database population.  \\n\\nIn summary, the response is a **structured token-level prediction** that may optionally be aggregated into entity spans, providing the predicted label, confidence, and positional information for each recognized entity in the text.\", \"examples\": [{\"user_intent\": \"Extract the company names mentioned in the sentence 'I recently started working at OpenAI and Microsoft.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"I recently started working at OpenAI and Microsoft.\"}}}, {\"user_intent\": \"Identify entities representing companies in the text 'Tesla, SpaceX, and Neuralink are all led by Elon Musk.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"Tesla, SpaceX, and Neuralink are all led by Elon Musk.\"}}}, {\"user_intent\": \"Find all company names in the paragraph: 'Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.\"}}}]}\n\n# User Query\nWrite a welcome email to a new employee joining the company.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a welcoming email for a new employee joining the company.",
                    "tool_description": "Generate a personalized email based on company-specific information and the new employee's role.",
                    "expected_response": "A text-based email draft with a professional and welcoming tone, including the employee's name and role."
                }
            ],
            "extracted_arguments": {
                "employee_name": "The name of the new employee.",
                "employee_role": "The job title or position of the new employee.",
                "company_name": "The name of the company where the new employee is joining."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_8",
        "query": "We received a customer feedback document which is very lengthy. We need a summarization of it.",
        "instruction": "Given a `text summarization` task, retrieve tools that process lengthy textual content to generate concise summaries, effectively condensing the main points and maintaining the essence of the original document.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text-to-Text Generation",
                    "api_call": "pipeline('summarization', model='philschmid/bart-large-cnn-samsum')",
                    "api_arguments": {
                        "model": "philschmid/bart-large-cnn-samsum"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest"
                    },
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\nconversation = '''Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \nJeff: ok.\nJeff: and how can I get started? \nJeff: where can I find documentation? \nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\n'''\nsummarizer(conversation)",
                    "performance": {
                        "dataset": "samsum",
                        "accuracy": {
                            "eval_rouge1": 42.621,
                            "eval_rouge2": 21.9825,
                            "eval_rougeL": 33.034,
                            "eval_rougeLsum": 39.6783,
                            "test_rouge1": 41.3174,
                            "test_rouge2": 20.8716,
                            "test_rougeL": 32.1337,
                            "test_rougeLsum": 38.4149
                        }
                    },
                    "description": "philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.",
                    "id": "huggingface_api_549",
                    "name": "philschmid/bart-large-cnn-samsum"
                },
                "id": "gorilla_huggingface_tool_545",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: The \"google/bigbird-pegasus-large-arxiv\" tool is a text summarization model based on the BigBird architecture, which uses sparse attention to handle longer sequences more efficiently than traditional transformers like BERT. It is designed for summarizing long documents, such as scientific papers, with a lower compute cost while maintaining high performance. This model is ideal for tasks involving very long text sequences, achieving state-of-the-art results in summarization and question-answering tasks.",
            "Tool 2: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.",
            "Tool 3: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 4: A Spanish text summarization tool based on BERT2BERT (BETO) fine-tuned on the MLSUM dataset, which generates concise summaries of input text using a Hugging Face Transformers sequence-to-sequence model.",
            "Tool 5: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 6: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 7: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 8: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 9: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 11: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 12: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 13: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.",
            "Tool 14: The \"microsoft/resnet-18\" is a pre-trained ResNet model for image classification, built on the Hugging Face Transformers framework. It is trained on the ImageNet-1k dataset and utilizes residual connections to enable the training of very deep networks. ResNet-18 won the 2015 ILSVRC & COCO competition and is capable of classifying images into categories, making it suitable for computer vision tasks where image recognition is required.",
            "Tool 15: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 16: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 17: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 18: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 19: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: The \"google/bigbird-pegasus-large-arxiv\" tool is a text summarization model based on the BigBird architecture, which uses sparse attention to handle longer sequences more efficiently than traditional transformers like BERT. It is designed for summarizing long documents, such as scientific papers, with a lower compute cost while maintaining high performance. This model is ideal for tasks involving very long text sequences, achieving state-of-the-art results in summarization and question-answering tasks.\nTool 2: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.\nTool 3: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 4: A Spanish text summarization tool based on BERT2BERT (BETO) fine-tuned on the MLSUM dataset, which generates concise summaries of input text using a Hugging Face Transformers sequence-to-sequence model.\nTool 5: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 6: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 7: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 8: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 9: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 11: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 12: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 13: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.\nTool 14: The \"microsoft/resnet-18\" is a pre-trained ResNet model for image classification, built on the Hugging Face Transformers framework. It is trained on the ImageNet-1k dataset and utilizes residual connections to enable the training of very deep networks. ResNet-18 won the 2015 ILSVRC & COCO competition and is capable of classifying images into categories, making it suitable for computer vision tasks where image recognition is required.\nTool 15: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 16: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 17: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 18: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 19: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nWe received a customer feedback document which is very lengthy. We need a summarization of it.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Summarize a long customer feedback document to extract the key points.",
                    "tool_description": "Generate a concise summary of a lengthy text document while preserving the main information and insights, suitable for understanding customer feedback quickly.",
                    "expected_response": "A summarized version of the input text highlighting the key points and essential information from the document."
                }
            ],
            "extracted_arguments": {
                "document_text": "The full text content of the customer feedback document that needs to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_158",
        "query": "I have a website text about technology and I want to know if it represents a positive sentiment or a negative one.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that process text inputs to determine the sentiment expressed, whether positive or negative, to align with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')",
                    "api_arguments": "text, candidate_labels",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nnlp = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\nresult = nlp('The movie was great!', ['positive', 'negative'])\nprint(result)",
                    "performance": {
                        "dataset": "MNLI",
                        "accuracy": {
                            "matched_acc": "89.19",
                            "mismatched_acc": "89.01"
                        }
                    },
                    "description": "distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.",
                    "id": "huggingface_api_511",
                    "name": "valhalla/distilbart-mnli-12-6"
                },
                "id": "gorilla_huggingface_tool_507",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 1: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 3: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 4: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 5: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 6: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 7: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 8: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 9: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 10: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 11: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 12: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 13: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.",
            "Tool 14: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 15: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 16: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 17: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 18: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 19: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy."
        ],
        "tool_view": {
            "description": "A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "parameters": {
                "data": {
                    "type": "dataframe",
                    "description": "A pandas DataFrame containing the input features for the model. Each column should correspond to a feature used during training, and the columns must be preprocessed as expected by the model.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_0': [0.1, 0.5], 'feat_1': [1.2, 3.4]})"
                }
            },
            "response": "The tool response is expected to contain the following elements:\n\n1. **Predictions**: This will be the main output of the model's inference. It is expected to include the predicted labels for the input data provided in the `data` argument. Each label corresponds to a binary classification result (e.g., 0 or 1, representing categories such as \"negative\" or \"positive\" sentiment in the context of sentiment analysis).\n\n2. **Confidence Scores (optional)**: While not explicitly required by the documentation, a typical binary classification model may return confidence scores or probabilities associated with each prediction. These would reflect the model's certainty about each classification decision, usually in the form of probabilities between 0 and 1 for each class (e.g., the likelihood that a given review is positive or negative).\n\n3. **Metadata (optional)**: The response may include additional metadata about the prediction process. This could encompass things like the model version, processing time, or other relevant information that may assist with interpreting the results. However, this is not explicitly mentioned in the documentation, so it would be inferred as a common practice.\n\n4. **Model Information (optional)**: Although not required, there might be an inclusion of model-related information like the model's name (`desertdev/autotrain-imdb-sentiment-analysis-44994113085`), accuracy (`0.565`), and possibly any other relevant details about the training dataset (e.g., `desertdev/autotrain-data-imdb-sentiment-analysis`) to provide context for the predictions made.\n\nIn summary, the response would contain the binary classification results (predicted labels for the input data), potentially accompanied by confidence scores and additional metadata for the user to understand the predictions and their context.",
            "examples": [
                {
                    "user_intent": "I want to predict whether the movie reviews in my CSV file are positive or negative.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "movie_reviews.csv"
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of a batch of new reviews we just collected from our website.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "new_reviews_batch.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV file with user comments, and I need the model to classify each as positive or negative.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "user_comments.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 1: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 3: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 4: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 5: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 6: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 7: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 8: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 9: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 10: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 11: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 12: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 13: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.\nTool 14: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 15: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 16: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 17: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 18: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 19: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\", \"parameters\": {\"data\": {\"type\": \"dataframe\", \"description\": \"A pandas DataFrame containing the input features for the model. Each column should correspond to a feature used during training, and the columns must be preprocessed as expected by the model.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_0': [0.1, 0.5], 'feat_1': [1.2, 3.4]})\"}}, \"response\": \"The tool response is expected to contain the following elements:\\n\\n1. **Predictions**: This will be the main output of the model's inference. It is expected to include the predicted labels for the input data provided in the `data` argument. Each label corresponds to a binary classification result (e.g., 0 or 1, representing categories such as \\\"negative\\\" or \\\"positive\\\" sentiment in the context of sentiment analysis).\\n\\n2. **Confidence Scores (optional)**: While not explicitly required by the documentation, a typical binary classification model may return confidence scores or probabilities associated with each prediction. These would reflect the model's certainty about each classification decision, usually in the form of probabilities between 0 and 1 for each class (e.g., the likelihood that a given review is positive or negative).\\n\\n3. **Metadata (optional)**: The response may include additional metadata about the prediction process. This could encompass things like the model version, processing time, or other relevant information that may assist with interpreting the results. However, this is not explicitly mentioned in the documentation, so it would be inferred as a common practice.\\n\\n4. **Model Information (optional)**: Although not required, there might be an inclusion of model-related information like the model's name (`desertdev/autotrain-imdb-sentiment-analysis-44994113085`), accuracy (`0.565`), and possibly any other relevant details about the training dataset (e.g., `desertdev/autotrain-data-imdb-sentiment-analysis`) to provide context for the predictions made.\\n\\nIn summary, the response would contain the binary classification results (predicted labels for the input data), potentially accompanied by confidence scores and additional metadata for the user to understand the predictions and their context.\", \"examples\": [{\"user_intent\": \"I want to predict whether the movie reviews in my CSV file are positive or negative.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"movie_reviews.csv\"}}}, {\"user_intent\": \"Check the sentiment of a batch of new reviews we just collected from our website.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"new_reviews_batch.csv\"}}}, {\"user_intent\": \"I have a CSV file with user comments, and I need the model to classify each as positive or negative.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"user_comments.csv\"}}}]}\n\n# User Query\nI have a website text about technology and I want to know if it represents a positive sentiment or a negative one.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify the sentiment of website text to see if it is positive or negative.",
                    "tool_description": "A binary classification model that predicts whether a given text expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in online content.",
                    "expected_response": "Predicted sentiment labels for the input text, indicating either positive or negative sentiment, optionally with confidence scores and metadata about the prediction."
                }
            ],
            "extracted_arguments": {
                "text_data": "The body of website text to analyze, represented as a string or structured text input compatible with the sentiment analysis model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_372",
        "query": "Our user would like to develop an audiobook using a Text-to-Speech API. Convert the text of a book into an audio file.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that transform textual content into audio outputs by utilizing models designed for synthesizing speech in various languages from text inputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')",
                    "api_arguments": null,
                    "python_environment_requirements": "huggingface_hub, fairseq",
                    "example_code": null,
                    "performance": {
                        "dataset": "covost2",
                        "accuracy": null
                    },
                    "description": "A text-to-speech model trained on multiple datasets including mtedx, covost2, europarl_st, and voxpopuli. Supports English, Spanish, French, and Italian languages.",
                    "id": "huggingface_api_728",
                    "name": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10"
                },
                "id": "gorilla_huggingface_tool_723",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 1: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 2: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 3: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 4: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 5: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 6: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 7: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 8: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 9: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 10: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 11: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.",
            "Tool 12: The \"mio/Artoria\" is a text-to-speech model trained using the ESPnet framework and the Fate recipe, capable of converting text input into speech output. It can be accessed through the ESPnet pipeline API, specifically for text-to-speech tasks, and requires the Transformers library in a Python environment.",
            "Tool 13: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.",
            "Tool 14: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.",
            "Tool 15: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 16: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.",
            "Tool 17: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.",
            "Tool 18: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 19: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech."
        ],
        "tool_view": {
            "description": "A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in Korean that will be converted into speech by the text-to-speech model.",
                    "required": true,
                    "example": "안녕하세요. 오늘 기분은 어떠신가요?"
                }
            },
            "response": "The tool produces synthesized speech audio from input text. The response typically consists of an audio waveform or audio data object representing the spoken version of the provided text. Key aspects of the response include:\n\n- **Audio waveform or tensor**: The primary content is the generated speech, usually represented as a multi-dimensional array (tensor) or waveform that can be converted to standard audio formats (e.g., WAV, MP3). This contains the amplitude values of the sound over time.  \n- **Sampling rate (optional but common)**: Some implementations may include metadata specifying the sampling rate used for the audio, which indicates how many samples per second the waveform contains.  \n- **Length/duration metadata (optional)**: The duration of the generated audio may also be returned or inferable from the waveform length and sampling rate.  \n- **Format compatibility**: The response is structured so it can be saved or played using standard audio libraries once converted to a suitable format.  \n\nIn principle, the response encapsulates all the information necessary to reconstruct or play the generated Korean speech corresponding to the input text.",
            "examples": [
                {
                    "user_intent": "Convert this Korean paragraph into speech for my audiobook project.",
                    "tool_call": {
                        "name": "lakahaga/novel_reading_tts",
                        "arguments": {
                            "text": "오늘 아침, 나는 창문을 열고 상쾌한 공기를 마셨다."
                        }
                    }
                },
                {
                    "user_intent": "I want to generate a TTS version of a Korean dialogue between two characters.",
                    "tool_call": {
                        "name": "lakahaga/novel_reading_tts",
                        "arguments": {
                            "text": "철수: 안녕, 영희! 오늘 기분 어때?\n영희: 안녕하세요, 철수! 저는 아주 좋아요."
                        }
                    }
                },
                {
                    "user_intent": "Produce a spoken version of a short Korean poem for language learning.",
                    "tool_call": {
                        "name": "lakahaga/novel_reading_tts",
                        "arguments": {
                            "text": "달빛 아래서 나는 조용히 노래를 부른다."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 1: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 2: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 3: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 4: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 5: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 6: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 7: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 8: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 9: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 10: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 11: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.\nTool 12: The \"mio/Artoria\" is a text-to-speech model trained using the ESPnet framework and the Fate recipe, capable of converting text input into speech output. It can be accessed through the ESPnet pipeline API, specifically for text-to-speech tasks, and requires the Transformers library in a Python environment.\nTool 13: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.\nTool 14: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.\nTool 15: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 16: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.\nTool 17: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\nTool 18: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 19: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in Korean that will be converted into speech by the text-to-speech model.\", \"required\": true, \"example\": \"\\uc548\\ub155\\ud558\\uc138\\uc694. \\uc624\\ub298 \\uae30\\ubd84\\uc740 \\uc5b4\\ub5a0\\uc2e0\\uac00\\uc694?\"}}, \"response\": \"The tool produces synthesized speech audio from input text. The response typically consists of an audio waveform or audio data object representing the spoken version of the provided text. Key aspects of the response include:\\n\\n- **Audio waveform or tensor**: The primary content is the generated speech, usually represented as a multi-dimensional array (tensor) or waveform that can be converted to standard audio formats (e.g., WAV, MP3). This contains the amplitude values of the sound over time.  \\n- **Sampling rate (optional but common)**: Some implementations may include metadata specifying the sampling rate used for the audio, which indicates how many samples per second the waveform contains.  \\n- **Length/duration metadata (optional)**: The duration of the generated audio may also be returned or inferable from the waveform length and sampling rate.  \\n- **Format compatibility**: The response is structured so it can be saved or played using standard audio libraries once converted to a suitable format.  \\n\\nIn principle, the response encapsulates all the information necessary to reconstruct or play the generated Korean speech corresponding to the input text.\", \"examples\": [{\"user_intent\": \"Convert this Korean paragraph into speech for my audiobook project.\", \"tool_call\": {\"name\": \"lakahaga/novel_reading_tts\", \"arguments\": {\"text\": \"\\uc624\\ub298 \\uc544\\uce68, \\ub098\\ub294 \\ucc3d\\ubb38\\uc744 \\uc5f4\\uace0 \\uc0c1\\ucf8c\\ud55c \\uacf5\\uae30\\ub97c \\ub9c8\\uc168\\ub2e4.\"}}}, {\"user_intent\": \"I want to generate a TTS version of a Korean dialogue between two characters.\", \"tool_call\": {\"name\": \"lakahaga/novel_reading_tts\", \"arguments\": {\"text\": \"\\ucca0\\uc218: \\uc548\\ub155, \\uc601\\ud76c! \\uc624\\ub298 \\uae30\\ubd84 \\uc5b4\\ub54c?\\n\\uc601\\ud76c: \\uc548\\ub155\\ud558\\uc138\\uc694, \\ucca0\\uc218! \\uc800\\ub294 \\uc544\\uc8fc \\uc88b\\uc544\\uc694.\"}}}, {\"user_intent\": \"Produce a spoken version of a short Korean poem for language learning.\", \"tool_call\": {\"name\": \"lakahaga/novel_reading_tts\", \"arguments\": {\"text\": \"\\ub2ec\\ube5b \\uc544\\ub798\\uc11c \\ub098\\ub294 \\uc870\\uc6a9\\ud788 \\ub178\\ub798\\ub97c \\ubd80\\ub978\\ub2e4.\"}}}]}\n\n# User Query\nOur user would like to develop an audiobook using a Text-to-Speech API. Convert the text of a book into an audio file.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert the text of my book into spoken Korean audio for an audiobook.",
                    "tool_description": "Use a Korean text-to-speech model to transform written Korean text into natural-sounding speech audio.",
                    "expected_response": "An audio waveform or file representing the spoken version of the input text, suitable for playback or saving as an audiobook."
                }
            ],
            "extracted_arguments": {
                "text": "The full body of written Korean text from the book that will be converted into speech."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_42",
        "query": "Develop an AI character that can play the SoccerTwos game with advanced strategies.",
        "instruction": "Given a `AI character development` task for a specific game, retrieve tools that harness machine learning and reinforcement learning frameworks to design and implement advanced strategies for gameplay, ensuring effective interaction within the game's environment.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "ML-Agents",
                    "functionality": "SoccerTwos",
                    "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'",
                    "api_arguments": [
                        "your_configuration_file_path.yaml",
                        "run_id"
                    ],
                    "python_environment_requirements": [
                        "unity-ml-agents",
                        "deep-reinforcement-learning",
                        "ML-Agents-SoccerTwos"
                    ],
                    "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀",
                    "performance": {
                        "dataset": "SoccerTwos",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.",
                    "id": "huggingface_api_907",
                    "name": "Raiden-1001/poca-Soccerv7"
                },
                "id": "gorilla_huggingface_tool_900",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 2: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 3: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 4: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 5: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 6: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 7: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.",
            "Tool 8: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.",
            "Tool 9: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 10: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 11: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 12: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 13: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 14: Provides a baseline regression model for predicting tips using the tips9y0jvt5q dataset, implemented with Ridge regression via the dabl library. It is intended to quickly establish a reference performance for tabular regression tasks before applying more advanced training methods.",
            "Tool 15: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.",
            "Tool 16: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.",
            "Tool 17: This tool provides a pretrained TrOCR model for converting images of handwritten text into machine-readable text. It uses a transformer-based encoder-decoder architecture, with an image Transformer as the encoder and a text Transformer as the decoder, allowing accurate transcription of handwritten documents. It is designed for optical character recognition tasks and can be applied to scanned or photographed handwritten material.",
            "Tool 18: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 19: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332."
        ],
        "tool_view": {
            "description": "A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "parameters": {
                "your_configuration_file_path.yaml": {
                    "type": "string",
                    "description": "The file path to your configuration YAML file, which contains the setup and hyperparameters for training the model.",
                    "required": true,
                    "example": "./configurations/soccer_twos_config.yaml"
                },
                "run_id": {
                    "type": "string",
                    "description": "A unique identifier for the training run. This ID is used to distinguish between different runs of the model.",
                    "required": true,
                    "example": "soccer_twos_run_001"
                }
            },
            "response": "The tool’s response is not explicitly defined in the JSON, but based on its functionality, a reasonable and typical response would include information related to the trained model and its usage in the Unity ML-Agents environment. The response would likely have the following components:\n\n1. **Model Metadata**  \n   - **Name/Identifier**: The unique identifier of the trained model (e.g., `poca-SoccerTwosv2`).  \n   - **Version/Run ID**: The specific training run identifier used to track the model instance.  \n   - **Configuration**: Reference to the YAML configuration file that specifies training parameters.\n\n2. **Training Information**  \n   - **Environment Details**: Description of the SoccerTwos environment used for training.  \n   - **Training Status**: Indication of whether training is complete, in progress, or resumed.  \n   - **Performance Metrics**: Any available performance measures, such as rewards, scores, or success rates (though accuracy is not provided in this case).\n\n3. **Model Access/Usage**  \n   - **Local Path**: File system location where the model has been downloaded or saved.  \n   - **Commands for Use**: Instructions or scripts to load, resume, or evaluate the model (e.g., `mlagents-learn` with appropriate arguments).  \n   - **API Call Confirmation**: Acknowledgment that the `mlagents-load-from-hf` command successfully fetched the model.\n\n4. **Dependencies**  \n   - **Required Python Packages**: List of necessary Python environment packages (e.g., `ml-agents`) to run or interact with the model.\n\n5. **Optional Artifacts**  \n   - **Logs or Checkpoints**: References to training logs, checkpoints, or replay files that can be used to monitor performance or resume training.  \n   - **Evaluation Results**: If the model is evaluated post-training, a summary of scores or metrics achieved in test episodes.\n\nOverall, the response conveys both metadata about the trained agent and actionable information for loading, running, or further training the model in the Unity ML-Agents framework.",
            "examples": [
                {
                    "user_intent": "I want to train a SoccerTwos model using the Unity ML-Agents Library with my custom configuration file.",
                    "tool_call": {
                        "name": "mlagents-load-from-hf",
                        "arguments": {
                            "repo-id": "Raiden-1001/poca-SoccerTwosv2",
                            "local-dir": "./downloads",
                            "your_configuration_file_path.yaml": "configs/soccer_twos_config.yaml",
                            "run_id": "soccer_twos_run_01"
                        }
                    }
                },
                {
                    "user_intent": "How can I resume a previous SoccerTwos training run in Unity ML-Agents?",
                    "tool_call": {
                        "name": "mlagents-learn",
                        "arguments": {
                            "your_configuration_file_path.yaml": "configs/soccer_twos_config.yaml",
                            "run-id": "soccer_twos_run_01",
                            "resume": true
                        }
                    }
                },
                {
                    "user_intent": "I need to load the 'poca-SoccerTwosv2' model to start testing in Unity ML-Agents.",
                    "tool_call": {
                        "name": "mlagents-load-from-hf",
                        "arguments": {
                            "repo-id": "Raiden-1001/poca-SoccerTwosv2",
                            "local-dir": "./downloads",
                            "your_configuration_file_path.yaml": "configs/soccer_twos_test_config.yaml",
                            "run_id": "soccer_twos_test_01"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 2: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 3: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 4: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 5: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 6: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 7: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.\nTool 8: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.\nTool 9: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 10: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 11: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 12: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 13: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 14: Provides a baseline regression model for predicting tips using the tips9y0jvt5q dataset, implemented with Ridge regression via the dabl library. It is intended to quickly establish a reference performance for tabular regression tasks before applying more advanced training methods.\nTool 15: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.\nTool 16: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.\nTool 17: This tool provides a pretrained TrOCR model for converting images of handwritten text into machine-readable text. It uses a transformer-based encoder-decoder architecture, with an image Transformer as the encoder and a text Transformer as the decoder, allowing accurate transcription of handwritten documents. It is designed for optical character recognition tasks and can be applied to scanned or photographed handwritten material.\nTool 18: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 19: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\", \"parameters\": {\"your_configuration_file_path.yaml\": {\"type\": \"string\", \"description\": \"The file path to your configuration YAML file, which contains the setup and hyperparameters for training the model.\", \"required\": true, \"example\": \"./configurations/soccer_twos_config.yaml\"}, \"run_id\": {\"type\": \"string\", \"description\": \"A unique identifier for the training run. This ID is used to distinguish between different runs of the model.\", \"required\": true, \"example\": \"soccer_twos_run_001\"}}, \"response\": \"The tool\\u2019s response is not explicitly defined in the JSON, but based on its functionality, a reasonable and typical response would include information related to the trained model and its usage in the Unity ML-Agents environment. The response would likely have the following components:\\n\\n1. **Model Metadata**  \\n   - **Name/Identifier**: The unique identifier of the trained model (e.g., `poca-SoccerTwosv2`).  \\n   - **Version/Run ID**: The specific training run identifier used to track the model instance.  \\n   - **Configuration**: Reference to the YAML configuration file that specifies training parameters.\\n\\n2. **Training Information**  \\n   - **Environment Details**: Description of the SoccerTwos environment used for training.  \\n   - **Training Status**: Indication of whether training is complete, in progress, or resumed.  \\n   - **Performance Metrics**: Any available performance measures, such as rewards, scores, or success rates (though accuracy is not provided in this case).\\n\\n3. **Model Access/Usage**  \\n   - **Local Path**: File system location where the model has been downloaded or saved.  \\n   - **Commands for Use**: Instructions or scripts to load, resume, or evaluate the model (e.g., `mlagents-learn` with appropriate arguments).  \\n   - **API Call Confirmation**: Acknowledgment that the `mlagents-load-from-hf` command successfully fetched the model.\\n\\n4. **Dependencies**  \\n   - **Required Python Packages**: List of necessary Python environment packages (e.g., `ml-agents`) to run or interact with the model.\\n\\n5. **Optional Artifacts**  \\n   - **Logs or Checkpoints**: References to training logs, checkpoints, or replay files that can be used to monitor performance or resume training.  \\n   - **Evaluation Results**: If the model is evaluated post-training, a summary of scores or metrics achieved in test episodes.\\n\\nOverall, the response conveys both metadata about the trained agent and actionable information for loading, running, or further training the model in the Unity ML-Agents framework.\", \"examples\": [{\"user_intent\": \"I want to train a SoccerTwos model using the Unity ML-Agents Library with my custom configuration file.\", \"tool_call\": {\"name\": \"mlagents-load-from-hf\", \"arguments\": {\"repo-id\": \"Raiden-1001/poca-SoccerTwosv2\", \"local-dir\": \"./downloads\", \"your_configuration_file_path.yaml\": \"configs/soccer_twos_config.yaml\", \"run_id\": \"soccer_twos_run_01\"}}}, {\"user_intent\": \"How can I resume a previous SoccerTwos training run in Unity ML-Agents?\", \"tool_call\": {\"name\": \"mlagents-learn\", \"arguments\": {\"your_configuration_file_path.yaml\": \"configs/soccer_twos_config.yaml\", \"run-id\": \"soccer_twos_run_01\", \"resume\": true}}}, {\"user_intent\": \"I need to load the 'poca-SoccerTwosv2' model to start testing in Unity ML-Agents.\", \"tool_call\": {\"name\": \"mlagents-load-from-hf\", \"arguments\": {\"repo-id\": \"Raiden-1001/poca-SoccerTwosv2\", \"local-dir\": \"./downloads\", \"your_configuration_file_path.yaml\": \"configs/soccer_twos_test_config.yaml\", \"run_id\": \"soccer_twos_test_01\"}}}]}\n\n# User Query\nDevelop an AI character that can play the SoccerTwos game with advanced strategies.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained Poca agent model for SoccerTwos to start developing an AI player.",
                    "tool_description": "Load a trained reinforcement learning model of a Poca agent for the SoccerTwos environment using the Unity ML-Agents Library, allowing the model to be used for training, evaluation, or experimentation.",
                    "expected_response": "Metadata and usage information about the loaded model, including its configuration, run ID, environment details, and instructions for running or further training."
                },
                {
                    "user_intent": "Train or fine-tune the AI agent to implement advanced strategies in SoccerTwos.",
                    "tool_description": "Use the Unity ML-Agents framework to train or fine-tune the loaded Poca agent model on SoccerTwos, adjusting hyperparameters and environment settings to improve gameplay strategies.",
                    "expected_response": "Training logs, performance metrics such as scores or rewards, and updated model files reflecting improved strategic behavior."
                },
                {
                    "user_intent": "Evaluate the trained AI character's performance in SoccerTwos.",
                    "tool_description": "Run simulations using the trained Poca agent in the SoccerTwos environment to assess its strategic performance and behavior.",
                    "expected_response": "Evaluation results including gameplay outcomes, reward statistics, and observations on strategic effectiveness."
                }
            ],
            "extracted_arguments": {
                "configuration_file_path": "The file path to a YAML configuration file specifying training parameters and environment setup.",
                "run_id": "A unique identifier for the training or evaluation session of the AI agent.",
                "model_repo_id": "The repository identifier for the pre-trained Poca agent model to load."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_447",
        "query": "My lecture has been recorded, and I would like to transcribe the audio to create a transcript of my presentation.",
        "instruction": "Given an `audio transcription` task, retrieve tools that specialize in automatic speech recognition by processing recorded audio inputs to generate an accurate and coherent textual transcript of the presentation.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Automatic Speech Recognition and Speech Translation",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')",
                    "api_arguments": {
                        "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets"
                    ],
                    "example_code": [
                        "from transformers import WhisperProcessor, WhisperForConditionalGeneration",
                        "from datasets import load_dataset",
                        "processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')",
                        "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')",
                        "model.config.forced_decoder_ids = None",
                        "ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')",
                        "sample = ds[0]['audio']",
                        "input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features",
                        "predicted_ids = model.generate(input_features)",
                        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
                    ],
                    "performance": {
                        "dataset": "LibriSpeech test-clean",
                        "accuracy": 3.0003583080317573
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.",
                    "id": "huggingface_api_757",
                    "name": "openai/whisper-large-v2"
                },
                "id": "gorilla_huggingface_tool_752",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 1: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 2: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 3: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.",
            "Tool 4: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 5: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 6: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.",
            "Tool 7: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 8: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.",
            "Tool 9: Whisper is a pre-trained automatic speech recognition (ASR) and speech translation model developed by OpenAI. Based on a Transformer architecture, it can transcribe and translate audio in multiple languages. Trained on 680,000 hours of labeled data, it excels in generalizing across various datasets and domains without requiring fine-tuning. The tool can be used for accurate transcription tasks and supports integration within Python environments using libraries like Hugging Face Transformers and Datasets.",
            "Tool 10: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 11: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 12: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.",
            "Tool 13: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 14: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.",
            "Tool 15: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.",
            "Tool 16: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 17: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.",
            "Tool 18: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 19: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy."
        ],
        "tool_view": {
            "description": "A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "parameters": {
                "audio file or recording": {
                    "type": "string",
                    "description": "The path to an audio file or a recording input that will be used as input for the speech-to-speech translation model.",
                    "required": true,
                    "example": "path/to/input_audio.wav"
                }
            },
            "response": "The tool’s response is expected to be an audio representation of the input speech translated from Romanian to English. Specifically, the response would typically include:  \n\n1. **Translated audio content**: A waveform or audio file containing the spoken translation in English, preserving natural prosody and intonation.  \n2. **Metadata (optional, depending on implementation)**: This may include information about the audio format (e.g., sample rate, encoding), duration of the generated audio, and possibly confidence scores or timing alignment if the system provides them.  \n\nIn essence, the primary output is a playable audio signal representing the speech-to-speech translation, produced from the input audio in Romanian.",
            "examples": [
                {
                    "user_intent": "Translate this Romanian audio recording of a short story into English speech.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "story_ro.wav"
                        }
                    }
                },
                {
                    "user_intent": "I have a 2-minute interview in Romanian. Convert it to English audio.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "interview_ro.mp3"
                        }
                    }
                },
                {
                    "user_intent": "Translate this Romanian podcast episode into English for my listeners.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "podcast_episode_ro.flac"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 1: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 2: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 3: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.\nTool 4: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 5: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 6: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.\nTool 7: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 8: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.\nTool 9: Whisper is a pre-trained automatic speech recognition (ASR) and speech translation model developed by OpenAI. Based on a Transformer architecture, it can transcribe and translate audio in multiple languages. Trained on 680,000 hours of labeled data, it excels in generalizing across various datasets and domains without requiring fine-tuning. The tool can be used for accurate transcription tasks and supports integration within Python environments using libraries like Hugging Face Transformers and Datasets.\nTool 10: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 11: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 12: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.\nTool 13: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 14: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.\nTool 15: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.\nTool 16: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 17: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.\nTool 18: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 19: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\", \"parameters\": {\"audio file or recording\": {\"type\": \"string\", \"description\": \"The path to an audio file or a recording input that will be used as input for the speech-to-speech translation model.\", \"required\": true, \"example\": \"path/to/input_audio.wav\"}}, \"response\": \"The tool\\u2019s response is expected to be an audio representation of the input speech translated from Romanian to English. Specifically, the response would typically include:  \\n\\n1. **Translated audio content**: A waveform or audio file containing the spoken translation in English, preserving natural prosody and intonation.  \\n2. **Metadata (optional, depending on implementation)**: This may include information about the audio format (e.g., sample rate, encoding), duration of the generated audio, and possibly confidence scores or timing alignment if the system provides them.  \\n\\nIn essence, the primary output is a playable audio signal representing the speech-to-speech translation, produced from the input audio in Romanian.\", \"examples\": [{\"user_intent\": \"Translate this Romanian audio recording of a short story into English speech.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"story_ro.wav\"}}}, {\"user_intent\": \"I have a 2-minute interview in Romanian. Convert it to English audio.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"interview_ro.mp3\"}}}, {\"user_intent\": \"Translate this Romanian podcast episode into English for my listeners.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"podcast_episode_ro.flac\"}}}]}\n\n# User Query\nMy lecture has been recorded, and I would like to transcribe the audio to create a transcript of my presentation.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Transcribe my recorded lecture audio into text to create a transcript.",
                    "tool_description": "Process an audio file containing speech and generate an accurate textual transcription using an automatic speech recognition (ASR) model.",
                    "expected_response": "A text transcript of the spoken content in the audio, including punctuation and sentence structure where possible."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The path to the recorded lecture audio file to be transcribed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_463",
        "query": "Provide a short summary of an article about cryptocurrency investment risks.",
        "instruction": "Given a `summary generation` task, retrieve tools that process textual content to produce concise and coherent summaries aligned with the query's subject matter and key details.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='decapoda-research/llama-13b-hf')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "generator('Once upon a time')",
                    "performance": {
                        "dataset": [
                            {
                                "name": "BoolQ",
                                "accuracy": "85.3"
                            },
                            {
                                "name": "PIQA",
                                "accuracy": "82.8"
                            },
                            {
                                "name": "SIQA",
                                "accuracy": "52.3"
                            },
                            {
                                "name": "HellaSwag",
                                "accuracy": "84.2"
                            },
                            {
                                "name": "WinoGrande",
                                "accuracy": "77"
                            },
                            {
                                "name": "ARC-e",
                                "accuracy": "81.5"
                            },
                            {
                                "name": "ARC-c",
                                "accuracy": "56"
                            },
                            {
                                "name": "OBQACOPA",
                                "accuracy": "60.2"
                            }
                        ]
                    },
                    "description": "LLaMA-13B is an auto-regressive language model based on the transformer architecture developed by the FAIR team of Meta AI. It is designed for research purposes, such as question answering, natural language understanding, and reading comprehension. The model has been trained on a variety of sources, including web data, GitHub, Wikipedia, and books in 20 languages. It has been evaluated on several benchmarks, including BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, and OpenBookQA.",
                    "id": "huggingface_api_623",
                    "name": "decapoda-research/llama-13b-hf"
                },
                "id": "gorilla_huggingface_tool_618",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 1: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 2: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 3: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 4: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 5: A YOLOv8-based model for detecting and segmenting defects on printed circuit boards (PCBs), including issues like dry joints, incorrect installation, PCB damage, and short circuits, allowing precise localization and mask generation for quality control and inspection purposes.",
            "Tool 6: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 7: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 8: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 9: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 10: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 11: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 12: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 13: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 14: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 15: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.",
            "Tool 16: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 17: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 18: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.",
            "Tool 19: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model."
        ],
        "tool_view": {
            "description": "A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path or URL of the image on which the visual question will be asked.",
                    "required": true,
                    "example": "images/dog.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The question to be answered about the given image.",
                    "required": true,
                    "example": "What breed is the dog in the image?"
                }
            },
            "response": "The tool produces a response that provides an answer to a user-specified question about a given image. The response is typically structured as a dictionary or object containing the following components:\n\n1. **`answer`** – A string representing the model’s textual answer to the question posed about the image. This is the primary output of the tool.  \n2. **`score`** (optional) – A numeric confidence score indicating the model’s certainty in the provided answer. This may be a probability or other relevance measure.  \n3. **`question`** (optional) – The text of the input question, often echoed in the response for clarity.  \n4. **`image`** (optional) – A reference or identifier for the input image, included for traceability when multiple queries are processed.  \n\nIn essence, the response conveys the model’s interpretation of the visual content in relation to the user’s question, potentially accompanied by metadata that quantifies confidence or links back to the original inputs.",
            "examples": [
                {
                    "user_intent": "I have a photo of my living room. Can you tell me how many chairs are visible in it?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/living_room.jpg",
                            "question": "How many chairs are in this room?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this image of a street scene. What color is the car in the foreground?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/street_scene.png",
                            "question": "What is the color of the car in the foreground?"
                        }
                    }
                },
                {
                    "user_intent": "Here's a picture of a group of people at a party. Can you tell me how many people are wearing hats?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/party_group.jpeg",
                            "question": "How many people are wearing hats in this image?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 1: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 2: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 3: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 4: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 5: A YOLOv8-based model for detecting and segmenting defects on printed circuit boards (PCBs), including issues like dry joints, incorrect installation, PCB damage, and short circuits, allowing precise localization and mask generation for quality control and inspection purposes.\nTool 6: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 7: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 8: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 9: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 10: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 11: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 12: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 13: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 14: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 15: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.\nTool 16: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 17: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 18: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.\nTool 19: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab\\u2019s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path or URL of the image on which the visual question will be asked.\", \"required\": true, \"example\": \"images/dog.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The question to be answered about the given image.\", \"required\": true, \"example\": \"What breed is the dog in the image?\"}}, \"response\": \"The tool produces a response that provides an answer to a user-specified question about a given image. The response is typically structured as a dictionary or object containing the following components:\\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s textual answer to the question posed about the image. This is the primary output of the tool.  \\n2. **`score`** (optional) \\u2013 A numeric confidence score indicating the model\\u2019s certainty in the provided answer. This may be a probability or other relevance measure.  \\n3. **`question`** (optional) \\u2013 The text of the input question, often echoed in the response for clarity.  \\n4. **`image`** (optional) \\u2013 A reference or identifier for the input image, included for traceability when multiple queries are processed.  \\n\\nIn essence, the response conveys the model\\u2019s interpretation of the visual content in relation to the user\\u2019s question, potentially accompanied by metadata that quantifies confidence or links back to the original inputs.\", \"examples\": [{\"user_intent\": \"I have a photo of my living room. Can you tell me how many chairs are visible in it?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/living_room.jpg\", \"question\": \"How many chairs are in this room?\"}}}, {\"user_intent\": \"Look at this image of a street scene. What color is the car in the foreground?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/street_scene.png\", \"question\": \"What is the color of the car in the foreground?\"}}}, {\"user_intent\": \"Here's a picture of a group of people at a party. Can you tell me how many people are wearing hats?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/party_group.jpeg\", \"question\": \"How many people are wearing hats in this image?\"}}}]}\n\n# User Query\nProvide a short summary of an article about cryptocurrency investment risks.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a concise summary of a Chinese text article.",
                    "tool_description": "Use a Pegasus-based Chinese text summarization model to produce an abstractive, concise summary from the input text, capturing key points and essential information.",
                    "expected_response": "A short textual summary highlighting the main ideas and important details of the provided article."
                }
            ],
            "extracted_arguments": {
                "article_text": "The full text content of the article about cryptocurrency investment risks, as a string input."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_289",
        "query": "We are tasked to analyze text for a Russian newspaper to help understand general sentiment and trends in the text.",
        "instruction": "Given a `text analysis` task for sentiment and trend detection in a Russian newspaper, retrieve tools that leverage language models capable of processing Russian text to extract meaningful features, embeddings, and insights aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')",
                    "api_arguments": [
                        "sentences",
                        "padding",
                        "truncation",
                        "max_length",
                        "return_tensors"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\n\n# Sentences we want sentence embeddings for sentences = ['?']\n\n# Load AutoModel from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])",
                    "performance": {
                        "dataset": "Russian SuperGLUE",
                        "accuracy": "Not provided"
                    },
                    "description": "BERT large model multitask (cased) for Sentence Embeddings in Russian language.",
                    "id": "huggingface_api_24",
                    "name": "sberbank-ai/sbert_large_mt_nlu_ru"
                },
                "id": "gorilla_huggingface_tool_24",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 1: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 2: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 3: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 4: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 5: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 6: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 7: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 8: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 9: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 10: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.",
            "Tool 11: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.",
            "Tool 12: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 13: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 14: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.",
            "Tool 15: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 16: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 17: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 18: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 19: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian."
        ],
        "tool_view": {
            "description": "A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text containing a masked token, indicated by <mask>, that the model will attempt to predict.",
                    "required": true,
                    "example": "Hello I'm a <mask> model."
                }
            },
            "response": "The tool returns a list of predictions for the masked token(s) in the input text. Each prediction in the response typically includes the following components:\n\n1. **`sequence`** – The input text with the mask replaced by the predicted token, showing the full reconstructed sentence. This allows the user to see how the model fills in the masked position in context.  \n\n2. **`score`** – A numerical value representing the model’s confidence or probability for this particular token prediction. Higher scores indicate higher likelihood according to the model.  \n\n3. **`token`** – The integer ID of the predicted token in the model’s vocabulary. This is useful for internal processing or mapping to embeddings but is generally less important for casual interpretation.  \n\n4. **`token_str`** – The string representation of the predicted token, which is the actual text that replaces the mask. This is the most interpretable part of the prediction for users.  \n\nThe response is typically ordered by descending confidence, so the first entry is the model’s top prediction for the masked token, followed by alternative plausible completions. The number of predictions returned can usually be configured via parameters but defaults to a small set (e.g., top 5).  \n\nThis response allows users to understand the model’s likely substitutions for a masked word in context, along with a measure of confidence for each option.",
            "examples": [
                {
                    "user_intent": "I want to predict the missing word in this sentence: 'The weather today is <mask>.'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "The weather today is <mask>."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for this sentence about technology: 'Artificial intelligence is transforming <mask>.'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "Artificial intelligence is transforming <mask>."
                        }
                    }
                },
                {
                    "user_intent": "Guess the missing word in this casual sentence: 'I can't wait to eat some <mask> tonight!'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "I can't wait to eat some <mask> tonight!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 1: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 2: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 3: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 4: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 5: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 6: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 7: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 8: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 9: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 10: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.\nTool 11: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.\nTool 12: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 13: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 14: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.\nTool 15: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 16: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 17: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 18: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 19: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text containing a masked token, indicated by <mask>, that the model will attempt to predict.\", \"required\": true, \"example\": \"Hello I'm a <mask> model.\"}}, \"response\": \"The tool returns a list of predictions for the masked token(s) in the input text. Each prediction in the response typically includes the following components:\\n\\n1. **`sequence`** \\u2013 The input text with the mask replaced by the predicted token, showing the full reconstructed sentence. This allows the user to see how the model fills in the masked position in context.  \\n\\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence or probability for this particular token prediction. Higher scores indicate higher likelihood according to the model.  \\n\\n3. **`token`** \\u2013 The integer ID of the predicted token in the model\\u2019s vocabulary. This is useful for internal processing or mapping to embeddings but is generally less important for casual interpretation.  \\n\\n4. **`token_str`** \\u2013 The string representation of the predicted token, which is the actual text that replaces the mask. This is the most interpretable part of the prediction for users.  \\n\\nThe response is typically ordered by descending confidence, so the first entry is the model\\u2019s top prediction for the masked token, followed by alternative plausible completions. The number of predictions returned can usually be configured via parameters but defaults to a small set (e.g., top 5).  \\n\\nThis response allows users to understand the model\\u2019s likely substitutions for a masked word in context, along with a measure of confidence for each option.\", \"examples\": [{\"user_intent\": \"I want to predict the missing word in this sentence: 'The weather today is <mask>.'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"The weather today is <mask>.\"}}}, {\"user_intent\": \"Fill in the blank for this sentence about technology: 'Artificial intelligence is transforming <mask>.'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"Artificial intelligence is transforming <mask>.\"}}}, {\"user_intent\": \"Guess the missing word in this casual sentence: 'I can't wait to eat some <mask> tonight!'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"I can't wait to eat some <mask> tonight!\"}}}]}\n\n# User Query\nWe are tasked to analyze text for a Russian newspaper to help understand general sentiment and trends in the text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze the sentiment of a text from a Russian newspaper.",
                    "tool_description": "Perform sentiment analysis on a given Russian text to classify the sentiment as positive, negative, or neutral.",
                    "expected_response": "A sentiment classification result indicating whether the sentiment of the provided text is positive, negative, or neutral."
                }
            ],
            "extracted_arguments": {
                "text": "The Russian text to be analyzed for sentiment."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_352",
        "query": "The school wants a tool to teach foreign students Chinese grammar. They want you to develop a part-of-speech tagging system to detect the words' grammatical roles.",
        "instruction": "Given a `part-of-speech tagging` task, retrieve tools that specialize in natural language processing using token classification. These tools should process text input to identify and label the grammatical roles of words, specifically focusing on Chinese language grammar to assist in educational settings.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Part-of-speech tagging",
                    "api_call": "AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')",
                    "api_arguments": {
                        "tokenizer": "BertTokenizerFast.from_pretrained('bert-base-chinese')"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).",
                    "id": "huggingface_api_423",
                    "name": "ckiplab/bert-base-chinese-pos"
                },
                "id": "gorilla_huggingface_tool_419",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 1: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.",
            "Tool 2: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "Tool 3: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.",
            "Tool 4: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "Tool 5: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 6: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.",
            "Tool 7: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 8: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.",
            "Tool 9: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 10: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 11: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 12: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 13: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.",
            "Tool 14: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 15: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 16: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 17: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 18: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 19: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output."
        ],
        "tool_view": {
            "description": "A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "parameters": {},
            "response": "The tool produces a token classification output for a given English text input. Its response is typically structured as a sequence of entries, one for each token in the input text, where each entry contains the following components:\n\n1. **Token** – The individual word or subword unit from the input text that the model processed. This may include subword tokens if the tokenizer splits words into smaller units.  \n\n2. **Label** – The predicted Part-of-Speech (POS) tag assigned to the corresponding token. This is usually a string representing standard POS categories (e.g., noun, verb, adjective).  \n\n3. **Score / Confidence (optional)** – A numerical value indicating the model’s confidence or probability for the predicted label. Not all frameworks include this, but it is common in transformer-based token classification outputs.  \n\n4. **Index (optional)** – The position of the token in the input sequence, which can help align predictions with the original text.  \n\nIn practice, the overall response is a list or sequence where each element corresponds to one token from the input, providing the model’s POS classification for that token along with any associated metadata like confidence or token index. This allows users to map the predictions back to the original text for downstream NLP tasks.",
            "examples": [
                {
                    "user_intent": "Classify the part-of-speech tags for the sentence 'The quick brown fox jumps over the lazy dog'.",
                    "tool_call": {
                        "name": "vblagoje/bert-english-uncased-finetuned-pos",
                        "arguments": {
                            "model": "vblagoje/bert-english-uncased-finetuned-pos"
                        }
                    }
                },
                {
                    "user_intent": "I want to extract the POS tags from the sentence 'She sells sea shells by the sea shore'.",
                    "tool_call": {
                        "name": "vblagoje/bert-english-uncased-finetuned-pos",
                        "arguments": {
                            "model": "vblagoje/bert-english-uncased-finetuned-pos"
                        }
                    }
                },
                {
                    "user_intent": "Run part-of-speech tagging on the sentence 'I love programming in Python'.",
                    "tool_call": {
                        "name": "vblagoje/bert-english-uncased-finetuned-pos",
                        "arguments": {
                            "model": "vblagoje/bert-english-uncased-finetuned-pos"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 1: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.\nTool 2: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\nTool 3: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.\nTool 4: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\nTool 5: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 6: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.\nTool 7: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 8: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.\nTool 9: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 10: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 11: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 12: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 13: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.\nTool 14: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 15: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 16: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 17: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 18: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 19: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\", \"parameters\": {}, \"response\": \"The tool produces a token classification output for a given English text input. Its response is typically structured as a sequence of entries, one for each token in the input text, where each entry contains the following components:\\n\\n1. **Token** \\u2013 The individual word or subword unit from the input text that the model processed. This may include subword tokens if the tokenizer splits words into smaller units.  \\n\\n2. **Label** \\u2013 The predicted Part-of-Speech (POS) tag assigned to the corresponding token. This is usually a string representing standard POS categories (e.g., noun, verb, adjective).  \\n\\n3. **Score / Confidence (optional)** \\u2013 A numerical value indicating the model\\u2019s confidence or probability for the predicted label. Not all frameworks include this, but it is common in transformer-based token classification outputs.  \\n\\n4. **Index (optional)** \\u2013 The position of the token in the input sequence, which can help align predictions with the original text.  \\n\\nIn practice, the overall response is a list or sequence where each element corresponds to one token from the input, providing the model\\u2019s POS classification for that token along with any associated metadata like confidence or token index. This allows users to map the predictions back to the original text for downstream NLP tasks.\", \"examples\": [{\"user_intent\": \"Classify the part-of-speech tags for the sentence 'The quick brown fox jumps over the lazy dog'.\", \"tool_call\": {\"name\": \"vblagoje/bert-english-uncased-finetuned-pos\", \"arguments\": {\"model\": \"vblagoje/bert-english-uncased-finetuned-pos\"}}}, {\"user_intent\": \"I want to extract the POS tags from the sentence 'She sells sea shells by the sea shore'.\", \"tool_call\": {\"name\": \"vblagoje/bert-english-uncased-finetuned-pos\", \"arguments\": {\"model\": \"vblagoje/bert-english-uncased-finetuned-pos\"}}}, {\"user_intent\": \"Run part-of-speech tagging on the sentence 'I love programming in Python'.\", \"tool_call\": {\"name\": \"vblagoje/bert-english-uncased-finetuned-pos\", \"arguments\": {\"model\": \"vblagoje/bert-english-uncased-finetuned-pos\"}}}]}\n\n# User Query\nThe school wants a tool to teach foreign students Chinese grammar. They want you to develop a part-of-speech tagging system to detect the words' grammatical roles.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to develop a part-of-speech tagging system for Chinese grammar detection.",
                    "tool_description": "Develop a system using a pre-trained model to classify words in a Chinese text according to their grammatical roles (POS tagging).",
                    "expected_response": "A list of token classifications, where each entry includes the token, the POS tag, and optional metadata like confidence or token index."
                }
            ],
            "extracted_arguments": {
                "language": "The target language for part-of-speech tagging (Chinese).",
                "task": "The specific task of part-of-speech tagging in Chinese text."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_212",
        "query": "We have serveral articles translated in different languages, and we want to group the articles discussing the specific topic.",
        "instruction": "Given a `topic-based article grouping` task, retrieve tools that utilize sentence embeddings to identify and cluster related content across multiple languages, enabling efficient organization of articles by specific topics.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "N/A"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_701",
                    "name": "sentence-transformers/distiluse-base-multilingual-cased-v1"
                },
                "id": "gorilla_huggingface_tool_696",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 1: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 2: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 3: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 4: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 5: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 6: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 7: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 8: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 9: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 10: mT5 is a multilingual text-to-text model based on the T5 architecture, pre-trained on a large Common Crawl dataset covering 101 languages. It uses a unified text-to-text framework to perform a wide range of multilingual NLP tasks, such as translation, summarization, and text generation, enabling consistent input-to-output processing across different languages.",
            "Tool 11: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.",
            "Tool 12: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 13: The \"vicgalle/xlm-roberta-large-xnli-anli\" tool is a pre-trained XLM-RoBERTa model fine-tuned for zero-shot classification. It enables classifying text sequences into predefined categories without needing task-specific training data. This model, based on multiple natural language inference datasets, can be used to classify text in different languages, with high accuracy on datasets like XNLI. It is integrated into the Hugging Face Transformers framework and requires the \"transformers\" library in a Python environment.",
            "Tool 14: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 15: bigscience/bloom-7b1 is a large, multilingual transformer-based language model designed for generating text. It supports 48 natural languages and multiple programming languages, making it suitable for producing coherent and contextually relevant text or serving as a pretrained base for fine-tuning on specific NLP tasks.",
            "Tool 16: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 17: This tool uses the MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli model to perform zero-shot text classification, allowing users to assign a text sequence to one or more candidate labels without requiring task-specific training. It leverages a DeBERTa-v3-base model fine-tuned on MultiNLI, Fever-NLI, and Adversarial-NLI datasets, achieving strong performance on natural language inference benchmarks, and is suitable for quickly categorizing text into predefined topics in applications like content labeling, topic detection, or automated tagging.",
            "Tool 18: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 19: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F."
        ],
        "tool_view": {
            "description": "A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text or article that you want the model to summarize.",
                    "required": true,
                    "example": "The stock market saw significant gains today as tech companies reported higher than expected earnings."
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens for the generated summary. Controls the length of the output.",
                    "required": false,
                    "default": 150,
                    "example": 100
                }
            },
            "response": "The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \n\nKey aspects of the response:  \n\n- **Type:** String (text)  \n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \n\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.",
            "examples": [
                {
                    "user_intent": "Summarize a recent news article about the global climate summit in under 100 words.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.",
                            "max_length": 100
                        }
                    }
                },
                {
                    "user_intent": "Create a concise summary of a long article about advancements in AI and robotics.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.",
                            "max_length": 120
                        }
                    }
                },
                {
                    "user_intent": "Provide a short summary for a news article about a major sports event.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 1: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 2: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 3: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 4: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 5: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 6: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 7: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 8: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 9: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 10: mT5 is a multilingual text-to-text model based on the T5 architecture, pre-trained on a large Common Crawl dataset covering 101 languages. It uses a unified text-to-text framework to perform a wide range of multilingual NLP tasks, such as translation, summarization, and text generation, enabling consistent input-to-output processing across different languages.\nTool 11: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.\nTool 12: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 13: The \"vicgalle/xlm-roberta-large-xnli-anli\" tool is a pre-trained XLM-RoBERTa model fine-tuned for zero-shot classification. It enables classifying text sequences into predefined categories without needing task-specific training data. This model, based on multiple natural language inference datasets, can be used to classify text in different languages, with high accuracy on datasets like XNLI. It is integrated into the Hugging Face Transformers framework and requires the \"transformers\" library in a Python environment.\nTool 14: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 15: bigscience/bloom-7b1 is a large, multilingual transformer-based language model designed for generating text. It supports 48 natural languages and multiple programming languages, making it suitable for producing coherent and contextually relevant text or serving as a pretrained base for fine-tuning on specific NLP tasks.\nTool 16: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 17: This tool uses the MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli model to perform zero-shot text classification, allowing users to assign a text sequence to one or more candidate labels without requiring task-specific training. It leverages a DeBERTa-v3-base model fine-tuned on MultiNLI, Fever-NLI, and Adversarial-NLI datasets, achieving strong performance on natural language inference benchmarks, and is suitable for quickly categorizing text into predefined topics in applications like content labeling, topic detection, or automated tagging.\nTool 18: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 19: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text or article that you want the model to summarize.\", \"required\": true, \"example\": \"The stock market saw significant gains today as tech companies reported higher than expected earnings.\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens for the generated summary. Controls the length of the output.\", \"required\": false, \"default\": 150, \"example\": 100}}, \"response\": \"The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \\n\\nKey aspects of the response:  \\n\\n- **Type:** String (text)  \\n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \\n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \\n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \\n\\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.\", \"examples\": [{\"user_intent\": \"Summarize a recent news article about the global climate summit in under 100 words.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.\", \"max_length\": 100}}}, {\"user_intent\": \"Create a concise summary of a long article about advancements in AI and robotics.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.\", \"max_length\": 120}}}, {\"user_intent\": \"Provide a short summary for a news article about a major sports event.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.\", \"max_length\": 80}}}]}\n\n# User Query\nWe have serveral articles translated in different languages, and we want to group the articles discussing the specific topic.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate all articles into a single common language for uniform processing.",
                    "tool_description": "Use a multilingual machine translation model to convert text from various source languages into a specified target language, ensuring consistent language representation across all documents.",
                    "expected_response": "A collection of articles translated into the target language, preserving the original meaning and context."
                },
                {
                    "user_intent": "Classify and group articles based on their topic or content.",
                    "tool_description": "Apply a zero-shot or supervised text classification model to assign topic labels to each article and group them according to shared topics or themes.",
                    "expected_response": "A structured grouping of articles where each group contains articles discussing the same topic."
                }
            ],
            "extracted_arguments": {
                "articles": "A collection of text documents or articles, each potentially in different languages.",
                "target_language": "The language into which all articles should be translated for uniform processing.",
                "topic_labels": "Optional set of predefined topics or categories to guide classification and grouping."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_489",
        "query": "I am an artist who wants to create a new piece of artwork based on the prompt \"A futuristic city under the ocean\".",
        "instruction": "Given an `art generation` task, retrieve tools capable of transforming text prompts into visual artwork by using text-to-image models to create imaginative and high-quality images aligned with the given creative concept.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Generation",
                    "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')",
                    "api_arguments": [
                        "prompt"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "scipy"
                    ],
                    "example_code": "import torch\nfrom diffusers import StableDiffusionPipeline\nmodel_id = CompVis/stable-diffusion-v1-4\ndevice = cuda\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)",
                    "performance": {
                        "dataset": "COCO2017 validation set",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.",
                    "id": "huggingface_api_30",
                    "name": "CompVis/stable-diffusion-v1-4"
                },
                "id": "gorilla_huggingface_tool_30",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 1: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 2: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 3: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 4: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 5: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 6: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 7: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.",
            "Tool 8: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 9: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.",
            "Tool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 11: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 12: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 13: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 14: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.",
            "Tool 15: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 17: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 18: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.",
            "Tool 19: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation."
        ],
        "tool_view": {
            "description": "Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A descriptive text string that specifies the content, style, and details of the image to be generated by the model.",
                    "required": true,
                    "example": "A grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes, in the style of Jordan Grimmer and Greg Rutkowski"
                }
            },
            "response": "The tool is designed to generate images from textual descriptions using a pre-trained model. Specifically, it utilizes the Dreamlike Diffusion 1.0 model from Hugging Face’s `diffusers` library, built on the Stable Diffusion framework.\n\n### Expected Response Description:\nThe response from this tool will contain an image generated from the given text prompt. The key components of the response are:\n\n1. **Images**: \n   - The core content of the response is a set of images generated based on the provided textual prompt. Typically, there will be a list of images (often a single image in most use cases), represented as image objects.\n   - The format and resolution of the image depend on the model’s configuration, but the model is designed to generate high-quality artistic images, as indicated by its fine-tuning on a high-quality art dataset.\n\n2. **Metadata** (if applicable):\n   - The response might include additional metadata associated with the generated image, such as the image resolution, the time taken to generate the image, or other processing parameters, though this is not strictly required or detailed in the tool documentation.\n  \nThe specific content of the response will be the visual output of the model in the form of an image (or images), potentially saved as files or returned as image objects depending on the API’s implementation.",
            "examples": [
                {
                    "user_intent": "Generate an image of a futuristic city skyline at sunset with flying cars and neon lights in a cyberpunk style.",
                    "tool_call": {
                        "name": "dreamlike-art/dreamlike-diffusion-1.0",
                        "arguments": {
                            "prompt": "Futuristic city skyline at sunset, flying cars, neon lights, cyberpunk style, cinematic lighting, highly detailed, in the style of Syd Mead and Beeple"
                        }
                    }
                },
                {
                    "user_intent": "Create a whimsical illustration of a small dragon reading a book under a giant mushroom in a magical forest.",
                    "tool_call": {
                        "name": "dreamlike-art/dreamlike-diffusion-1.0",
                        "arguments": {
                            "prompt": "Whimsical illustration of a small dragon reading a book under a giant mushroom, magical forest, soft lighting, vivid colors, in the style of Studio Ghibli and Mary Blair"
                        }
                    }
                },
                {
                    "user_intent": "Produce concept art of a grungy female warrior with rainbow hair, in a dynamic pose, standing in a ruined city.",
                    "tool_call": {
                        "name": "dreamlike-art/dreamlike-diffusion-1.0",
                        "arguments": {
                            "prompt": "Grungy female warrior with rainbow hair, dynamic pose, standing in a ruined city, torn clothes, soft eyes, long straight hair, concept art, in the style of Jordan Grimmer and Greg Rutkowski, crisp lines, complex background, particles and wind effects"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 1: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 2: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 3: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 4: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 5: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 6: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 7: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.\nTool 8: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 9: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\nTool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 11: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 12: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 13: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 14: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.\nTool 15: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 17: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 18: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.\nTool 19: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A descriptive text string that specifies the content, style, and details of the image to be generated by the model.\", \"required\": true, \"example\": \"A grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes, in the style of Jordan Grimmer and Greg Rutkowski\"}}, \"response\": \"The tool is designed to generate images from textual descriptions using a pre-trained model. Specifically, it utilizes the Dreamlike Diffusion 1.0 model from Hugging Face\\u2019s `diffusers` library, built on the Stable Diffusion framework.\\n\\n### Expected Response Description:\\nThe response from this tool will contain an image generated from the given text prompt. The key components of the response are:\\n\\n1. **Images**: \\n   - The core content of the response is a set of images generated based on the provided textual prompt. Typically, there will be a list of images (often a single image in most use cases), represented as image objects.\\n   - The format and resolution of the image depend on the model\\u2019s configuration, but the model is designed to generate high-quality artistic images, as indicated by its fine-tuning on a high-quality art dataset.\\n\\n2. **Metadata** (if applicable):\\n   - The response might include additional metadata associated with the generated image, such as the image resolution, the time taken to generate the image, or other processing parameters, though this is not strictly required or detailed in the tool documentation.\\n  \\nThe specific content of the response will be the visual output of the model in the form of an image (or images), potentially saved as files or returned as image objects depending on the API\\u2019s implementation.\", \"examples\": [{\"user_intent\": \"Generate an image of a futuristic city skyline at sunset with flying cars and neon lights in a cyberpunk style.\", \"tool_call\": {\"name\": \"dreamlike-art/dreamlike-diffusion-1.0\", \"arguments\": {\"prompt\": \"Futuristic city skyline at sunset, flying cars, neon lights, cyberpunk style, cinematic lighting, highly detailed, in the style of Syd Mead and Beeple\"}}}, {\"user_intent\": \"Create a whimsical illustration of a small dragon reading a book under a giant mushroom in a magical forest.\", \"tool_call\": {\"name\": \"dreamlike-art/dreamlike-diffusion-1.0\", \"arguments\": {\"prompt\": \"Whimsical illustration of a small dragon reading a book under a giant mushroom, magical forest, soft lighting, vivid colors, in the style of Studio Ghibli and Mary Blair\"}}}, {\"user_intent\": \"Produce concept art of a grungy female warrior with rainbow hair, in a dynamic pose, standing in a ruined city.\", \"tool_call\": {\"name\": \"dreamlike-art/dreamlike-diffusion-1.0\", \"arguments\": {\"prompt\": \"Grungy female warrior with rainbow hair, dynamic pose, standing in a ruined city, torn clothes, soft eyes, long straight hair, concept art, in the style of Jordan Grimmer and Greg Rutkowski, crisp lines, complex background, particles and wind effects\"}}}]}\n\n# User Query\nI am an artist who wants to create a new piece of artwork based on the prompt \"A futuristic city under the ocean\".\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate an image of a futuristic city under the ocean based on my descriptive prompt.",
                    "tool_description": "Use a text-to-image diffusion model to generate a high-quality, stylistic image from a textual prompt, producing visually rich and imaginative artwork.",
                    "expected_response": "A generated image depicting the scene described in the prompt, including metadata such as resolution or generation parameters if available."
                }
            ],
            "extracted_arguments": {
                "prompt": "A descriptive text string specifying the content, setting, and style of the artwork to generate."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_277",
        "query": "Develop a fashion app that segments and identifies clothing items in an uploaded image.",
        "instruction": "Given a `fashion image segmentation` task, retrieve tools that can process image inputs to identify and segment clothing items, providing visual output aligned with the task requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')",
                    "api_arguments": [
                        "image"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests",
                        "matplotlib",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = extractor(images=image, return_tensors='pt')\noutputs = model(**inputs)\nlogits = outputs.logits.cpu()\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\npred_seg = upsampled_logits.argmax(dim=1)[0]\nplt.imshow(pred_seg)",
                    "performance": {
                        "dataset": "mattmdjaga/human_parsing_dataset",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on ATR dataset for clothes segmentation.",
                    "id": "huggingface_api_239",
                    "name": "mattmdjaga/segformer_b2_clothes"
                },
                "id": "gorilla_huggingface_tool_235",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 1: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 2: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 3: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 4: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 5: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 6: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 8: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 9: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 10: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 11: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 12: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.",
            "Tool 13: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 14: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 15: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 16: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 17: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 18: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 19: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German."
        ],
        "tool_view": {
            "description": "A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "parameters": {
                "image": {
                    "type": "object",
                    "description": "The input image to be segmented. This can be a PIL Image object, typically loaded from a file or URL.",
                    "required": true,
                    "example": "Image.open('path_to_image.jpg')"
                }
            },
            "response": "The tool produces a semantic segmentation output for a given input image. The response can be described as follows:\n\n- **logits**: A multi-dimensional tensor representing the raw, unnormalized class scores for each pixel in the input image. Its shape typically corresponds to `(batch_size, num_classes, height, width)` before any resizing or post-processing. Each channel represents the likelihood of a particular class at each pixel location.\n\n- **upsampled_logits**: A tensor derived from `logits` that has been resized (interpolated) to match the spatial dimensions of the original input image. This allows for pixel-wise alignment between the model predictions and the original image.\n\n- **pred_seg**: A 2D tensor (or array) representing the final predicted segmentation map. Each element corresponds to a pixel in the input image and contains an integer indicating the predicted class label for that pixel. This is obtained by taking the `argmax` over the class dimension of the `upsampled_logits`.\n\n- **Visual representation**: While not strictly part of the model output, the predicted segmentation map (`pred_seg`) is often visualized using a plotting library, where different class labels are rendered with distinct colors to indicate segmented regions.\n\nIn essence, the response encodes, for each pixel in the input image, which clothing or body-related class the model predicts it belongs to, optionally providing both raw logits and a finalized segmentation map suitable for visualization or further processing.",
            "examples": [
                {
                    "user_intent": "Segment the clothing items in this portrait image to identify shirts, pants, and jackets.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a group photo to separate and highlight different clothing types for fashion research.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e"
                        }
                    }
                },
                {
                    "user_intent": "Process a full-body street style image to extract the clothing regions for a style recommendation app.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1595152772835-219674b2a8a2"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 1: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 2: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 3: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 4: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 5: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 6: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 8: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 9: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 10: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 11: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 12: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.\nTool 13: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 14: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 15: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 16: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 17: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 18: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 19: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\", \"parameters\": {\"image\": {\"type\": \"object\", \"description\": \"The input image to be segmented. This can be a PIL Image object, typically loaded from a file or URL.\", \"required\": true, \"example\": \"Image.open('path_to_image.jpg')\"}}, \"response\": \"The tool produces a semantic segmentation output for a given input image. The response can be described as follows:\\n\\n- **logits**: A multi-dimensional tensor representing the raw, unnormalized class scores for each pixel in the input image. Its shape typically corresponds to `(batch_size, num_classes, height, width)` before any resizing or post-processing. Each channel represents the likelihood of a particular class at each pixel location.\\n\\n- **upsampled_logits**: A tensor derived from `logits` that has been resized (interpolated) to match the spatial dimensions of the original input image. This allows for pixel-wise alignment between the model predictions and the original image.\\n\\n- **pred_seg**: A 2D tensor (or array) representing the final predicted segmentation map. Each element corresponds to a pixel in the input image and contains an integer indicating the predicted class label for that pixel. This is obtained by taking the `argmax` over the class dimension of the `upsampled_logits`.\\n\\n- **Visual representation**: While not strictly part of the model output, the predicted segmentation map (`pred_seg`) is often visualized using a plotting library, where different class labels are rendered with distinct colors to indicate segmented regions.\\n\\nIn essence, the response encodes, for each pixel in the input image, which clothing or body-related class the model predicts it belongs to, optionally providing both raw logits and a finalized segmentation map suitable for visualization or further processing.\", \"examples\": [{\"user_intent\": \"Segment the clothing items in this portrait image to identify shirts, pants, and jackets.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d\"}}}, {\"user_intent\": \"Analyze a group photo to separate and highlight different clothing types for fashion research.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e\"}}}, {\"user_intent\": \"Process a full-body street style image to extract the clothing regions for a style recommendation app.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1595152772835-219674b2a8a2\"}}}]}\n\n# User Query\nDevelop a fashion app that segments and identifies clothing items in an uploaded image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to segment clothing items in an uploaded image for a fashion app.",
                    "tool_description": "Segment the clothing regions in an input image to identify different clothing items such as shirts, pants, and jackets. This can be useful for applications like fashion analysis or virtual try-on.",
                    "expected_response": "A segmented output for the clothing regions in the image, with class labels for each pixel corresponding to specific clothing items."
                }
            ],
            "extracted_arguments": {
                "image": "The input image to be segmented, typically provided as a URL or file."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_293",
        "query": "A healthcare professional wants to get quick answers to COVID-19 related questions from the latest research articles.",
        "instruction": "Given a `question answering` task related to COVID-19, retrieve tools that utilize natural language processing models to process text inputs and provide accurate responses based on the latest research articles.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model=RobertaForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2-covid'), tokenizer=RobertaTokenizer.from_pretrained('deepset/roberta-base-squad2-covid'))",
                    "api_arguments": {
                        "model_name": "deepset/roberta-base-squad2-covid",
                        "tokenizer": "deepset/roberta-base-squad2-covid"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "QA_input": {
                            "question": "Why is model conversion important?",
                            "context": "The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks."
                        },
                        "res": "nlp(QA_input)"
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "XVAL_EM": 0.17890995260663506,
                            "XVAL_f1": 0.49925444207319924,
                            "XVAL_top_3_recall": 0.8021327014218009
                        }
                    },
                    "description": "This model is a Roberta-based model fine-tuned on SQuAD-style CORD-19 annotations for the task of extractive question answering in the context of COVID-19. It can be used with the Hugging Face Transformers library for question answering tasks.",
                    "id": "huggingface_api_472",
                    "name": "deepset/roberta-base-squad2-covid"
                },
                "id": "gorilla_huggingface_tool_468",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.",
            "Tool 1: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 2: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.",
            "Tool 3: This tool provides a question-answering model based on Roberta Base, specifically trained on the SQuAD task, allowing it to accurately answer questions from text. It uses the Hugging Face Transformers framework and is compatible with the latest version of the `transformers` library. The model has demonstrated strong performance, with high accuracy on datasets like SQuADv1 and MoviesQA.",
            "Tool 4: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 5: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.",
            "Tool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 7: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 8: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.",
            "Tool 9: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 10: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 11: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.",
            "Tool 12: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 13: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 14: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 15: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 16: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 17: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 18: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 19: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library."
        ],
        "tool_view": {
            "description": "A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "Why is model conversion important?"
                },
                "context": {
                    "type": "string",
                    "description": "The text passage or context in which the model should find the answer to the question.",
                    "required": true,
                    "example": "The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks."
                }
            },
            "response": "The tool's response will contain the answer to a given question, based on the context provided. This response will be structured to include the following components:\n\n1. **Answer**: The specific answer to the question posed, extracted from the context. This will be a text string, representing the portion of the context that best addresses the question.\n\n2. **Score**: A numerical value representing the confidence level or likelihood that the answer provided is correct, based on the model’s internal computations. This score is typically expressed as a floating-point value.\n\n3. **Start and End Positions**: These are the indices (in the provided context text) that indicate where the answer begins and ends. This allows users to trace the exact segment of the context from which the answer is derived.\n\nIn principle, the response will contain:\n- A text string as the **answer** to the user’s question.\n- A **score** representing the model's confidence in the answer.\n- **Start and end positions** within the context text indicating where the model identified the answer.\n\nThis structure is typical for extractive question-answering tasks, where the goal is to extract a relevant portion of the provided context as the answer to the question.",
            "examples": [
                {
                    "user_intent": "I want to know the recommended isolation period for COVID-19 patients according to recent guidelines.",
                    "tool_call": {
                        "name": "deepset/roberta-base-squad2-covid",
                        "arguments": {
                            "question": "What is the recommended isolation period for COVID-19 patients?",
                            "context": "According to CDC guidelines, individuals who test positive for COVID-19 should isolate for at least 5 days from the onset of symptoms, followed by 5 days of mask-wearing around others if symptoms are improving."
                        }
                    }
                },
                {
                    "user_intent": "What symptoms indicate severe COVID-19 that require immediate medical attention?",
                    "tool_call": {
                        "name": "deepset/roberta-base-squad2-covid",
                        "arguments": {
                            "question": "Which symptoms indicate severe COVID-19 requiring hospitalization?",
                            "context": "Severe COVID-19 symptoms include difficulty breathing, persistent chest pain, confusion, inability to stay awake, and bluish lips or face. Immediate medical attention is advised in such cases."
                        }
                    }
                },
                {
                    "user_intent": "Can COVID-19 be transmitted through surfaces?",
                    "tool_call": {
                        "name": "deepset/roberta-base-squad2-covid",
                        "arguments": {
                            "question": "Is surface transmission of COVID-19 possible?",
                            "context": "Studies suggest that while SARS-CoV-2 can survive on surfaces for hours to days, transmission through surface contact is considered less common compared to respiratory droplets and aerosols."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.\nTool 1: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 2: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.\nTool 3: This tool provides a question-answering model based on Roberta Base, specifically trained on the SQuAD task, allowing it to accurately answer questions from text. It uses the Hugging Face Transformers framework and is compatible with the latest version of the `transformers` library. The model has demonstrated strong performance, with high accuracy on datasets like SQuADv1 and MoviesQA.\nTool 4: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 5: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.\nTool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 7: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 8: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.\nTool 9: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 10: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 11: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.\nTool 12: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 13: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 14: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 15: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 16: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 17: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 18: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 19: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"Why is model conversion important?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text passage or context in which the model should find the answer to the question.\", \"required\": true, \"example\": \"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\"}}, \"response\": \"The tool's response will contain the answer to a given question, based on the context provided. This response will be structured to include the following components:\\n\\n1. **Answer**: The specific answer to the question posed, extracted from the context. This will be a text string, representing the portion of the context that best addresses the question.\\n\\n2. **Score**: A numerical value representing the confidence level or likelihood that the answer provided is correct, based on the model\\u2019s internal computations. This score is typically expressed as a floating-point value.\\n\\n3. **Start and End Positions**: These are the indices (in the provided context text) that indicate where the answer begins and ends. This allows users to trace the exact segment of the context from which the answer is derived.\\n\\nIn principle, the response will contain:\\n- A text string as the **answer** to the user\\u2019s question.\\n- A **score** representing the model's confidence in the answer.\\n- **Start and end positions** within the context text indicating where the model identified the answer.\\n\\nThis structure is typical for extractive question-answering tasks, where the goal is to extract a relevant portion of the provided context as the answer to the question.\", \"examples\": [{\"user_intent\": \"I want to know the recommended isolation period for COVID-19 patients according to recent guidelines.\", \"tool_call\": {\"name\": \"deepset/roberta-base-squad2-covid\", \"arguments\": {\"question\": \"What is the recommended isolation period for COVID-19 patients?\", \"context\": \"According to CDC guidelines, individuals who test positive for COVID-19 should isolate for at least 5 days from the onset of symptoms, followed by 5 days of mask-wearing around others if symptoms are improving.\"}}}, {\"user_intent\": \"What symptoms indicate severe COVID-19 that require immediate medical attention?\", \"tool_call\": {\"name\": \"deepset/roberta-base-squad2-covid\", \"arguments\": {\"question\": \"Which symptoms indicate severe COVID-19 requiring hospitalization?\", \"context\": \"Severe COVID-19 symptoms include difficulty breathing, persistent chest pain, confusion, inability to stay awake, and bluish lips or face. Immediate medical attention is advised in such cases.\"}}}, {\"user_intent\": \"Can COVID-19 be transmitted through surfaces?\", \"tool_call\": {\"name\": \"deepset/roberta-base-squad2-covid\", \"arguments\": {\"question\": \"Is surface transmission of COVID-19 possible?\", \"context\": \"Studies suggest that while SARS-CoV-2 can survive on surfaces for hours to days, transmission through surface contact is considered less common compared to respiratory droplets and aerosols.\"}}}]}\n\n# User Query\nA healthcare professional wants to get quick answers to COVID-19 related questions from the latest research articles.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to get precise answers to COVID-19 related questions from research articles.",
                    "tool_description": "Use a RoBERTa-based extractive question-answering model fine-tuned on SQuAD-style CORD-19 data to identify exact answers within a given text context related to COVID-19.",
                    "expected_response": "A structured response containing the extracted answer text, a confidence score, and start and end positions within the context."
                }
            ],
            "extracted_arguments": {
                "question": "The natural language question about COVID-19 that needs an answer.",
                "context": "The text of the research article or passage from which the answer should be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_230",
        "query": "We need to classify the type of an image for an inventory.",
        "instruction": "Given an `image classification` task, retrieve tools that process visual inputs to identify and categorize images based on predefined classes, utilizing computer vision and machine learning techniques to align with inventory needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "zuppif/regnet-y-040"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoFeatureExtractor, RegNetForImageClassification",
                        "torch": "torch",
                        "datasets": "load_dataset"
                    },
                    "example_code": "from transformers import AutoFeatureExtractor, RegNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(huggingface/cats-image)\nimage = dataset[test][image][0]\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\ninputs = feature_extractor(image, return_tensors=pt)\nwith torch.no_grad():\n... logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not provided"
                    },
                    "description": "RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.",
                    "id": "huggingface_api_199",
                    "name": "facebook/regnet-y-008"
                },
                "id": "gorilla_huggingface_tool_195",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 1: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 2: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 3: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 6: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.",
            "Tool 7: A Hugging Face Transformers model for image classification that uses the SegFormer encoder fine-tuned on ImageNet-1k, enabling accurate prediction of image classes from input images. It is designed for efficient and effective image classification leveraging a hierarchical Transformer architecture and can be used to classify a wide variety of images in computer vision applications.",
            "Tool 8: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.",
            "Tool 9: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.",
            "Tool 10: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 11: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.",
            "Tool 12: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 13: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 14: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.",
            "Tool 15: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 16: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.",
            "Tool 17: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 18: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.",
            "Tool 19: A fine-tuned version of the Microsoft Swin-Tiny model for image classification, trained on the ImageFolder dataset, achieving high accuracy. It is designed for computer vision tasks using the Hugging Face Transformers framework and can be used to classify images with configurable training parameters."
        ],
        "tool_view": {
            "description": "A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "parameters": {
                "tokenizer": {
                    "type": "object",
                    "description": "A tokenizer object from Hugging Face Transformers used to preprocess input text for the sequence classification model. It converts raw text into token IDs suitable for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                }
            },
            "response": "The tool described is a text classification model designed for distinguishing between question queries and statement queries. The response from this tool would likely consist of:\n\n1. **Predicted Label**: The primary output of the tool will be the predicted label for the input text. This label will correspond to whether the input text is classified as a \"Question Query\" or a \"Statement Query.\" \n\n2. **Model Confidence**: The tool may also provide a confidence score or probability associated with the prediction. This indicates the model’s certainty about the classification.\n\n3. **Model Metadata**: There could be metadata related to the model, such as the name of the model (\"shahrukhx01/question-vs-statement-classifier\") and the tokenizer used (which is specified in the API arguments). However, this information is more likely to be used for technical debugging or auditing rather than part of the core response.\n\n4. **Model Output Format**: The output could follow a typical classification result format, which would include:\n   - **Class Label**: Identifies whether the input text is classified as a \"Question Query\" or a \"Statement Query.\"\n   - **Confidence Score**: A numerical value (if available) representing how confident the model is about the classification.\n  \n5. **Error or Exception Handling**: If there’s an issue with the input or the processing, the response might include an error message or exception details explaining what went wrong (e.g., invalid input, processing error, or model failure). \n\nIn summary, the response from the tool will primarily include the classification label (whether the input text is a question or a statement) and possibly a confidence score, alongside any necessary error information if applicable.",
            "examples": [
                {
                    "user_intent": "I want to automatically detect if a user input is a question or just a statement.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "What time does the meeting start?",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                },
                {
                    "user_intent": "Classify this customer feedback to see if it contains questions or general statements.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "I really like the new layout of your website.",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                },
                {
                    "user_intent": "Analyze multiple chatbot messages to separate questions from statements.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "Can you help me reset my password?",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 1: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 2: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 3: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 6: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.\nTool 7: A Hugging Face Transformers model for image classification that uses the SegFormer encoder fine-tuned on ImageNet-1k, enabling accurate prediction of image classes from input images. It is designed for efficient and effective image classification leveraging a hierarchical Transformer architecture and can be used to classify a wide variety of images in computer vision applications.\nTool 8: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.\nTool 9: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\nTool 10: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 11: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.\nTool 12: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 13: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 14: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.\nTool 15: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 16: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.\nTool 17: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 18: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.\nTool 19: A fine-tuned version of the Microsoft Swin-Tiny model for image classification, trained on the ImageFolder dataset, achieving high accuracy. It is designed for computer vision tasks using the Hugging Face Transformers framework and can be used to classify images with configurable training parameters.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\", \"parameters\": {\"tokenizer\": {\"type\": \"object\", \"description\": \"A tokenizer object from Hugging Face Transformers used to preprocess input text for the sequence classification model. It converts raw text into token IDs suitable for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}, \"response\": \"The tool described is a text classification model designed for distinguishing between question queries and statement queries. The response from this tool would likely consist of:\\n\\n1. **Predicted Label**: The primary output of the tool will be the predicted label for the input text. This label will correspond to whether the input text is classified as a \\\"Question Query\\\" or a \\\"Statement Query.\\\" \\n\\n2. **Model Confidence**: The tool may also provide a confidence score or probability associated with the prediction. This indicates the model\\u2019s certainty about the classification.\\n\\n3. **Model Metadata**: There could be metadata related to the model, such as the name of the model (\\\"shahrukhx01/question-vs-statement-classifier\\\") and the tokenizer used (which is specified in the API arguments). However, this information is more likely to be used for technical debugging or auditing rather than part of the core response.\\n\\n4. **Model Output Format**: The output could follow a typical classification result format, which would include:\\n   - **Class Label**: Identifies whether the input text is classified as a \\\"Question Query\\\" or a \\\"Statement Query.\\\"\\n   - **Confidence Score**: A numerical value (if available) representing how confident the model is about the classification.\\n  \\n5. **Error or Exception Handling**: If there\\u2019s an issue with the input or the processing, the response might include an error message or exception details explaining what went wrong (e.g., invalid input, processing error, or model failure). \\n\\nIn summary, the response from the tool will primarily include the classification label (whether the input text is a question or a statement) and possibly a confidence score, alongside any necessary error information if applicable.\", \"examples\": [{\"user_intent\": \"I want to automatically detect if a user input is a question or just a statement.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"What time does the meeting start?\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}, {\"user_intent\": \"Classify this customer feedback to see if it contains questions or general statements.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"I really like the new layout of your website.\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}, {\"user_intent\": \"Analyze multiple chatbot messages to separate questions from statements.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"Can you help me reset my password?\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}]}\n\n# User Query\nWe need to classify the type of an image for an inventory.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify an image to determine its type for inventory purposes.",
                    "tool_description": "Perform image classification using a pre-trained or zero-shot model to categorize the image into predefined or user-provided classes without requiring additional training.",
                    "expected_response": "A classification result containing the predicted category or label of the image, along with an optional confidence score for each class."
                }
            ],
            "extracted_arguments": {
                "image": "The input image to be classified, provided as a file path, URL, or image object.",
                "class_labels": "An optional list of possible categories or labels to classify the image into, if using zero-shot classification."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_145",
        "query": "Give me a solution to find out the license plate in the given car images.",
        "instruction": "Given a `license plate detection` task, retrieve tools that utilize computer vision object detection to identify and extract license plate information from provided car images, ensuring the output aligns with the query's requirements for accurate recognition.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "License Plate Detection",
                    "api_call": "yolov5.load('keremberke/yolov5m-license-plate')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic": false,
                        "multi_label": false,
                        "max_det": 1000,
                        "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                        "size": 640,
                        "augment": true
                    },
                    "python_environment_requirements": "pip install -U yolov5",
                    "example_code": [
                        "import yolov5",
                        "model = yolov5.load('keremberke/yolov5m-license-plate')",
                        "model.conf = 0.25",
                        "model.iou = 0.45",
                        "model.agnostic = False",
                        "model.multi_label = False",
                        "model.max_det = 1000",
                        "img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'",
                        "results = model(img, size=640)",
                        "results = model(img, augment=True)",
                        "predictions = results.pred[0]",
                        "boxes = predictions[:, :4]",
                        "scores = predictions[:, 4]",
                        "categories = predictions[:, 5]",
                        "results.show()",
                        "results.save(save_dir='results/')"
                    ],
                    "performance": {
                        "dataset": "keremberke/license-plate-object-detection",
                        "accuracy": 0.988
                    },
                    "description": "A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.",
                    "id": "huggingface_api_214",
                    "name": "keremberke/yolov5m-license-plate"
                },
                "id": "gorilla_huggingface_tool_210",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.",
            "Tool 1: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "Tool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 3: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 4: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 5: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 6: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 7: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.",
            "Tool 8: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 9: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 10: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 11: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 12: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 13: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.",
            "Tool 14: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.",
            "Tool 15: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.",
            "Tool 16: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.",
            "Tool 17: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 18: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 19: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text."
        ],
        "tool_view": {
            "description": "A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.",
            "parameters": {
                "img": {
                    "type": "string",
                    "description": "The input image for license plate detection. Can be a URL or a local file path pointing to the image.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                },
                "size": {
                    "type": "number",
                    "description": "The dimension to which the input image should be resized before detection. This controls the input resolution for the model.",
                    "required": true,
                    "example": 640
                },
                "augment": {
                    "type": "boolean",
                    "description": "Optional parameter to enable test-time augmentation for potentially improved detection results.",
                    "required": false,
                    "example": true
                }
            },
            "response": "The tool returns an object containing the results of license plate detection on the input image. The response is typically structured as follows:\n\n1. **`pred`** – A collection of detection predictions for each image processed. Each prediction entry represents a detected object and is usually a multidimensional array where each row corresponds to a single detected license plate. Each row contains:\n   - **Bounding box coordinates** – The first four elements define the detected license plate’s location in the image (typically `[x_min, y_min, x_max, y_max]`).\n   - **Confidence score** – A numerical value indicating the model’s confidence in the detection.\n   - **Class index/category** – An integer representing the object class (for this model, likely always the license plate class).\n\n2. **`boxes`** – Extracted bounding box coordinates from the predictions, representing the spatial extent of each detected license plate.\n\n3. **`scores`** – Extracted confidence scores corresponding to each detected bounding box.\n\n4. **`categories`** – Extracted class indices for each detection, which indicate the object category detected (here, the license plate class).\n\n5. **Visualization methods**:\n   - **`results.show()`** – Displays the input image with predicted bounding boxes overlaid.\n   - **`results.save(save_dir)`** – Saves annotated images to a specified directory.\n\n6. **Optional arguments**:\n   - **`size`** – Controls image resizing before inference.\n   - **`augment`** – Enables test-time augmentation to potentially improve detection accuracy.\n\nOverall, the response provides both raw detection data (boxes, scores, categories) and utilities for visualizing or saving the annotated images. Each component allows downstream processing, such as extracting bounding boxes or filtering predictions by confidence.",
            "examples": [
                {
                    "user_intent": "I need to detect license plates from an image of a car using a YOLOv5 model.",
                    "tool_call": {
                        "name": "keremberke/yolov5s-license-plate",
                        "arguments": {
                            "img": "https://example.com/car_image.jpg",
                            "size": 640
                        }
                    }
                },
                {
                    "user_intent": "Can I run license plate detection with augmented images for better accuracy?",
                    "tool_call": {
                        "name": "keremberke/yolov5s-license-plate",
                        "arguments": {
                            "img": "path/to/local/image.jpg",
                            "size": 640,
                            "augment": true
                        }
                    }
                },
                {
                    "user_intent": "I want to detect license plates from an image and save the results.",
                    "tool_call": {
                        "name": "keremberke/yolov5s-license-plate",
                        "arguments": {
                            "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                            "size": 640
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.\nTool 1: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\nTool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 3: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 4: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 5: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 6: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 7: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.\nTool 8: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 9: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 10: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 11: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 12: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 13: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.\nTool 14: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.\nTool 15: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\nTool 16: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.\nTool 17: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 18: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 19: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.\", \"parameters\": {\"img\": {\"type\": \"string\", \"description\": \"The input image for license plate detection. Can be a URL or a local file path pointing to the image.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"size\": {\"type\": \"number\", \"description\": \"The dimension to which the input image should be resized before detection. This controls the input resolution for the model.\", \"required\": true, \"example\": 640}, \"augment\": {\"type\": \"boolean\", \"description\": \"Optional parameter to enable test-time augmentation for potentially improved detection results.\", \"required\": false, \"example\": true}}, \"response\": \"The tool returns an object containing the results of license plate detection on the input image. The response is typically structured as follows:\\n\\n1. **`pred`** \\u2013 A collection of detection predictions for each image processed. Each prediction entry represents a detected object and is usually a multidimensional array where each row corresponds to a single detected license plate. Each row contains:\\n   - **Bounding box coordinates** \\u2013 The first four elements define the detected license plate\\u2019s location in the image (typically `[x_min, y_min, x_max, y_max]`).\\n   - **Confidence score** \\u2013 A numerical value indicating the model\\u2019s confidence in the detection.\\n   - **Class index/category** \\u2013 An integer representing the object class (for this model, likely always the license plate class).\\n\\n2. **`boxes`** \\u2013 Extracted bounding box coordinates from the predictions, representing the spatial extent of each detected license plate.\\n\\n3. **`scores`** \\u2013 Extracted confidence scores corresponding to each detected bounding box.\\n\\n4. **`categories`** \\u2013 Extracted class indices for each detection, which indicate the object category detected (here, the license plate class).\\n\\n5. **Visualization methods**:\\n   - **`results.show()`** \\u2013 Displays the input image with predicted bounding boxes overlaid.\\n   - **`results.save(save_dir)`** \\u2013 Saves annotated images to a specified directory.\\n\\n6. **Optional arguments**:\\n   - **`size`** \\u2013 Controls image resizing before inference.\\n   - **`augment`** \\u2013 Enables test-time augmentation to potentially improve detection accuracy.\\n\\nOverall, the response provides both raw detection data (boxes, scores, categories) and utilities for visualizing or saving the annotated images. Each component allows downstream processing, such as extracting bounding boxes or filtering predictions by confidence.\", \"examples\": [{\"user_intent\": \"I need to detect license plates from an image of a car using a YOLOv5 model.\", \"tool_call\": {\"name\": \"keremberke/yolov5s-license-plate\", \"arguments\": {\"img\": \"https://example.com/car_image.jpg\", \"size\": 640}}}, {\"user_intent\": \"Can I run license plate detection with augmented images for better accuracy?\", \"tool_call\": {\"name\": \"keremberke/yolov5s-license-plate\", \"arguments\": {\"img\": \"path/to/local/image.jpg\", \"size\": 640, \"augment\": true}}}, {\"user_intent\": \"I want to detect license plates from an image and save the results.\", \"tool_call\": {\"name\": \"keremberke/yolov5s-license-plate\", \"arguments\": {\"img\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", \"size\": 640}}}]}\n\n# User Query\nGive me a solution to find out the license plate in the given car images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I need to detect license plates from an image of a car using a YOLOv5 model.",
                    "tool_description": "Use a YOLOv5-based computer vision model to automatically detect and localize license plates in input car images.",
                    "expected_response": "An object containing detected license plate bounding boxes, confidence scores, and class indices, along with visualization options."
                }
            ],
            "extracted_arguments": {
                "img": "The input image or images of the car(s) to analyze, provided as a URL or local file path.",
                "size": "The resolution dimension to which the input image should be resized before detection, specified as a numeric value.",
                "augment": "Optional boolean flag to enable test-time augmentation for potentially improved detection accuracy."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_44",
        "query": "In order to have a better understanding of our clients, I'd like to identify the names of people and organizations mentioned in the following customer review.",
        "instruction": "Given a `named entity recognition` task, retrieve tools proficient in natural language processing to identify and classify entities such as names of people and organizations within text data by leveraging advanced models designed for token classification and entity recognition.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')",
                        "tokenizer": "AutoTokenizer.from_pretrained('dslim/bert-large-NER')"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.0.1"
                    },
                    "example_code": {
                        "example": "My name is Wolfgang and I live in Berlin",
                        "ner_results": "nlp(example)"
                    },
                    "performance": {
                        "dataset": "conll2003",
                        "accuracy": {
                            "f1": 0.92,
                            "precision": 0.92,
                            "recall": 0.919
                        }
                    },
                    "description": "bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).",
                    "id": "huggingface_api_412",
                    "name": "dslim/bert-large-NER"
                },
                "id": "gorilla_huggingface_tool_408",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 2: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 3: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 4: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 7: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 8: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 10: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 11: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 12: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 13: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 14: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 15: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 16: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 17: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 18: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.",
            "Tool 19: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on."
        ],
        "tool_view": {
            "description": "The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "parameters": {
                "sentence": {
                    "type": "string",
                    "description": "The input text sentence on which the Named Entity Recognition (NER) model will perform entity tagging.",
                    "required": true,
                    "example": "George Washington went to Washington"
                }
            },
            "response": "The tool’s response consists of an annotated sentence object that includes recognized named entities identified within the input text. Specifically, after the model processes the input sentence, the response encapsulates the sentence along with labeled entity spans corresponding to one of four predefined categories: PER (person names), LOC (location names), ORG (organization names), and MISC (miscellaneous names).\n\nEach entity span in the response includes at least the following components:\n\n- The text segment from the sentence identified as an entity.\n- The entity label indicating the type of named entity (PER, LOC, ORG, or MISC).\n- The position or span indices within the sentence marking where the entity occurs.\n- A confidence score or probability reflecting the model’s certainty about the prediction (typically included in Flair’s entity spans).\n\nOverall, the response structure is a richly annotated sentence object that integrates the original text with its token-level classification into named entity categories, enabling downstream use such as extraction, display, or further analysis of recognized entities.",
            "examples": [
                {
                    "user_intent": "Identify all the named entities in the sentence 'Barack Obama visited Paris last summer.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Barack Obama visited Paris last summer."
                        }
                    }
                },
                {
                    "user_intent": "I want to extract organizations and locations mentioned in 'Apple announced a new product in New York today.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Apple announced a new product in New York today."
                        }
                    }
                },
                {
                    "user_intent": "Find all the people, places, and miscellaneous entities in 'Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 2: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 3: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 4: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 7: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 8: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 10: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 11: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 12: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 13: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 14: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 15: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 16: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 17: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 18: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\nTool 19: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"flair/ner-english\\\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\", \"parameters\": {\"sentence\": {\"type\": \"string\", \"description\": \"The input text sentence on which the Named Entity Recognition (NER) model will perform entity tagging.\", \"required\": true, \"example\": \"George Washington went to Washington\"}}, \"response\": \"The tool\\u2019s response consists of an annotated sentence object that includes recognized named entities identified within the input text. Specifically, after the model processes the input sentence, the response encapsulates the sentence along with labeled entity spans corresponding to one of four predefined categories: PER (person names), LOC (location names), ORG (organization names), and MISC (miscellaneous names).\\n\\nEach entity span in the response includes at least the following components:\\n\\n- The text segment from the sentence identified as an entity.\\n- The entity label indicating the type of named entity (PER, LOC, ORG, or MISC).\\n- The position or span indices within the sentence marking where the entity occurs.\\n- A confidence score or probability reflecting the model\\u2019s certainty about the prediction (typically included in Flair\\u2019s entity spans).\\n\\nOverall, the response structure is a richly annotated sentence object that integrates the original text with its token-level classification into named entity categories, enabling downstream use such as extraction, display, or further analysis of recognized entities.\", \"examples\": [{\"user_intent\": \"Identify all the named entities in the sentence 'Barack Obama visited Paris last summer.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Barack Obama visited Paris last summer.\"}}}, {\"user_intent\": \"I want to extract organizations and locations mentioned in 'Apple announced a new product in New York today.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Apple announced a new product in New York today.\"}}}, {\"user_intent\": \"Find all the people, places, and miscellaneous entities in 'Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.\"}}}]}\n\n# User Query\nIn order to have a better understanding of our clients, I'd like to identify the names of people and organizations mentioned in the following customer review.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract the names of people and organizations mentioned in the customer review.",
                    "tool_description": "Use a Named Entity Recognition (NER) model to identify person names (PER) and organization names (ORG) in the input text.",
                    "expected_response": "An annotated sentence object containing the recognized person names (PER) and organization names (ORG), along with their positions in the text and associated confidence scores."
                }
            ],
            "extracted_arguments": {
                "sentence": "The customer review text in which named entities (persons and organizations) will be identified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_235",
        "query": "As a community manager, I would like to monitor my forum's comment section for toxic or harmful content. I want to find a solution that can flag these types of comments automatically, so I can address them promptly.",
        "instruction": "Given a `content moderation` task, retrieve tools that can automatically analyze text inputs for harmful or toxic language, enabling community managers to swiftly identify and address inappropriate content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline(model='martin-ha/toxic-comment-model')",
                    "api_arguments": {
                        "model_path": "martin-ha/toxic-comment-model"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\nmodel_path = martin-ha/toxic-comment-model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))",
                    "performance": {
                        "dataset": "held-out test set",
                        "accuracy": 0.94,
                        "f1-score": 0.59
                    },
                    "description": "This model is a fine-tuned version of the DistilBERT model to classify toxic comments.",
                    "id": "huggingface_api_392",
                    "name": "martin-ha/toxic-comment-model"
                },
                "id": "gorilla_huggingface_tool_388",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 1: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 2: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 3: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 4: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 5: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 6: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 7: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 8: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 9: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 10: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 11: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 12: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 13: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 14: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.",
            "Tool 15: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.",
            "Tool 16: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 17: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 18: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 19: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research."
        ],
        "tool_view": {
            "description": "A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "parameters": {
                "model_path": {
                    "type": "string",
                    "description": "The identifier or path of the pre-trained model to load for toxic comment classification, typically from Hugging Face model hub.",
                    "required": true,
                    "example": "martin-ha/toxic-comment-model"
                }
            },
            "response": "The tool returns a structured classification of input text with respect to toxicity. The typical response is a list of objects, each representing a predicted label along with an associated confidence score. The structure and meaning of each component are as follows:\n\n- **List of classification results**: The response is an array where each element corresponds to one possible label predicted by the model for the given input text. The array may contain one or multiple elements depending on whether the pipeline is configured to return the top label only or multiple possible labels.\n\n- **`label`**: A string indicating the category assigned to the input text. In this context, the labels represent toxicity classes, such as \"toxic,\" \"severe_toxic,\" \"obscene,\" or other similar categories depending on the model’s fine-tuning.\n\n- **`score`**: A numeric value (typically a float between 0 and 1) representing the model’s confidence in the prediction of that label. Higher values indicate greater confidence that the input text belongs to the corresponding category.\n\nOverall, the response communicates which toxicity categories the model associates with the input text and how confident it is in each classification. The information is suitable for downstream tasks such as filtering, moderation, or further text analysis.",
            "examples": [
                {
                    "user_intent": "Check if this comment is toxic: 'You're so stupid, I can't believe you wrote that.'",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "You're so stupid, I can't believe you wrote that."
                        }
                    }
                },
                {
                    "user_intent": "Analyze this customer review and see if it contains any toxic language: 'I hate this product, it's the worst thing I've ever bought!'",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "I hate this product, it's the worst thing I've ever bought!"
                        }
                    }
                },
                {
                    "user_intent": "I want to scan multiple forum posts for toxicity and flag any harmful comments.",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "I can't stand anyone who disagrees with me. They're all idiots."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 1: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 2: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 3: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 4: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 5: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 6: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 7: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 8: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 9: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 10: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 11: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 12: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 13: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 14: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.\nTool 15: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.\nTool 16: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 17: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 18: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 19: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\", \"parameters\": {\"model_path\": {\"type\": \"string\", \"description\": \"The identifier or path of the pre-trained model to load for toxic comment classification, typically from Hugging Face model hub.\", \"required\": true, \"example\": \"martin-ha/toxic-comment-model\"}}, \"response\": \"The tool returns a structured classification of input text with respect to toxicity. The typical response is a list of objects, each representing a predicted label along with an associated confidence score. The structure and meaning of each component are as follows:\\n\\n- **List of classification results**: The response is an array where each element corresponds to one possible label predicted by the model for the given input text. The array may contain one or multiple elements depending on whether the pipeline is configured to return the top label only or multiple possible labels.\\n\\n- **`label`**: A string indicating the category assigned to the input text. In this context, the labels represent toxicity classes, such as \\\"toxic,\\\" \\\"severe_toxic,\\\" \\\"obscene,\\\" or other similar categories depending on the model\\u2019s fine-tuning.\\n\\n- **`score`**: A numeric value (typically a float between 0 and 1) representing the model\\u2019s confidence in the prediction of that label. Higher values indicate greater confidence that the input text belongs to the corresponding category.\\n\\nOverall, the response communicates which toxicity categories the model associates with the input text and how confident it is in each classification. The information is suitable for downstream tasks such as filtering, moderation, or further text analysis.\", \"examples\": [{\"user_intent\": \"Check if this comment is toxic: 'You're so stupid, I can't believe you wrote that.'\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"You're so stupid, I can't believe you wrote that.\"}}}, {\"user_intent\": \"Analyze this customer review and see if it contains any toxic language: 'I hate this product, it's the worst thing I've ever bought!'\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"I hate this product, it's the worst thing I've ever bought!\"}}}, {\"user_intent\": \"I want to scan multiple forum posts for toxicity and flag any harmful comments.\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"I can't stand anyone who disagrees with me. They're all idiots.\"}}}]}\n\n# User Query\nAs a community manager, I would like to monitor my forum's comment section for toxic or harmful content. I want to find a solution that can flag these types of comments automatically, so I can address them promptly.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Scan forum comments to detect toxic or harmful language automatically.",
                    "tool_description": "Use a fine-tuned DistilBERT model to classify comments for toxicity, providing labels and confidence scores for each input text.",
                    "expected_response": "A list of classification results for each comment, including a predicted toxicity label and a confidence score indicating the likelihood of the comment being harmful."
                }
            ],
            "extracted_arguments": {
                "model_path": "The identifier or path of the pre-trained toxic comment classification model to load, typically from Hugging Face."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_51",
        "query": "We are creating an analysis platform for Counter-Strike: Global Offensive. Detect and locate players in the given image.",
        "instruction": "Given an `object detection` task, retrieve tools that process image inputs to identify and locate specific objects, such as players, within the content and provide precise detection results aligned with the requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8n-csgo-player-detection')",
                    "api_arguments": {
                        "image": "URL or local path to image"
                    },
                    "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21",
                    "example_code": "from ultralyticsplus import YOLO, render_result\n\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\n\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\n\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\nresults = model.predict(image)\n\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "csgo-object-detection",
                        "accuracy": 0.844
                    },
                    "description": "A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].",
                    "id": "huggingface_api_229",
                    "name": "keremberke/yolov8n-csgo-player-detection"
                },
                "id": "gorilla_huggingface_tool_225",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 1: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 2: This tool uses a YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players in images, supporting labels such as 'ct', 'cthead', 't', and 'thead'. It is designed for object detection tasks and can be utilized through the Hugging Face Transformers framework. The model provides high accuracy, with a performance score of 0.886 on the CS:GO object detection dataset.",
            "Tool 3: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 4: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 5: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 6: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.",
            "Tool 7: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 8: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 9: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.",
            "Tool 10: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 11: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.",
            "Tool 12: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 13: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 14: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 15: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 16: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 17: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 18: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 19: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image."
        ],
        "tool_view": {
            "description": "A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The URL or file path of the image on which to perform object detection. This image will be processed by the YOLOv8 CS:GO player detection model.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns an object detection result for the given input image. The response is structured as a list of detection outputs, typically with one entry per image processed. Each detection output contains the following components:  \n\n1. **`boxes`** – A collection of bounding boxes representing detected objects in the image. Each bounding box includes:  \n   - **Coordinates**: The position and size of the box, usually expressed as `[x_min, y_min, x_max, y_max]` or in a similar format.  \n   - **Confidence score**: A numeric value representing the model’s confidence that the detected object corresponds to the predicted class.  \n   - **Class label**: The predicted category of the detected object, such as `'ct'`, `'cthead'`, `'t'`, or `'thead'`.  \n\n2. **`masks` (optional)** – If segmentation is enabled, this may contain pixel-level masks for each detected object.  \n\n3. **`keypoints` (optional)** – If keypoint detection is enabled, this may include locations of relevant points for each detected object.  \n\n4. **`metadata` (optional)** – Additional information about the detection process, such as inference time, image size, or model configuration overrides.  \n\nThe response is designed to allow post-processing, visualization, or rendering. For example, it can be used with a rendering function to draw bounding boxes and labels directly on the input image. Each detection entry provides both the spatial location and classification information necessary to identify CS:GO players in the image.  \n\n",
            "examples": [
                {
                    "user_intent": "I want to detect CS:GO players in an image using the YOLOv8 model.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-csgo-player-detection",
                        "arguments": {
                            "image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Can you detect CS:GO players in this image and show the results?",
                    "tool_call": {
                        "name": "keremberke/yolov8m-csgo-player-detection",
                        "arguments": {
                            "image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Run an object detection on a CS:GO image and output the boxes for the detected players.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-csgo-player-detection",
                        "arguments": {
                            "image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 1: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 2: This tool uses a YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players in images, supporting labels such as 'ct', 'cthead', 't', and 'thead'. It is designed for object detection tasks and can be utilized through the Hugging Face Transformers framework. The model provides high accuracy, with a performance score of 0.886 on the CS:GO object detection dataset.\nTool 3: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 4: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 5: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 6: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.\nTool 7: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 8: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 9: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.\nTool 10: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 11: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.\nTool 12: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 13: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 14: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 15: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 16: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 17: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 18: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 19: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The URL or file path of the image on which to perform object detection. This image will be processed by the YOLOv8 CS:GO player detection model.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns an object detection result for the given input image. The response is structured as a list of detection outputs, typically with one entry per image processed. Each detection output contains the following components:  \\n\\n1. **`boxes`** \\u2013 A collection of bounding boxes representing detected objects in the image. Each bounding box includes:  \\n   - **Coordinates**: The position and size of the box, usually expressed as `[x_min, y_min, x_max, y_max]` or in a similar format.  \\n   - **Confidence score**: A numeric value representing the model\\u2019s confidence that the detected object corresponds to the predicted class.  \\n   - **Class label**: The predicted category of the detected object, such as `'ct'`, `'cthead'`, `'t'`, or `'thead'`.  \\n\\n2. **`masks` (optional)** \\u2013 If segmentation is enabled, this may contain pixel-level masks for each detected object.  \\n\\n3. **`keypoints` (optional)** \\u2013 If keypoint detection is enabled, this may include locations of relevant points for each detected object.  \\n\\n4. **`metadata` (optional)** \\u2013 Additional information about the detection process, such as inference time, image size, or model configuration overrides.  \\n\\nThe response is designed to allow post-processing, visualization, or rendering. For example, it can be used with a rendering function to draw bounding boxes and labels directly on the input image. Each detection entry provides both the spatial location and classification information necessary to identify CS:GO players in the image.  \\n\\n\", \"examples\": [{\"user_intent\": \"I want to detect CS:GO players in an image using the YOLOv8 model.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-csgo-player-detection\", \"arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}}, {\"user_intent\": \"Can you detect CS:GO players in this image and show the results?\", \"tool_call\": {\"name\": \"keremberke/yolov8m-csgo-player-detection\", \"arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}}, {\"user_intent\": \"Run an object detection on a CS:GO image and output the boxes for the detected players.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-csgo-player-detection\", \"arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}}]}\n\n# User Query\nWe are creating an analysis platform for Counter-Strike: Global Offensive. Detect and locate players in the given image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect and locate CS:GO players in an image.",
                    "tool_description": "Use a YOLOv8-based object detection model to detect and classify CS:GO players in an image, identifying labels such as 'ct', 'cthead', 't', and 'thead'.",
                    "expected_response": "A collection of detection results, including bounding boxes, confidence scores, and class labels (e.g., 'ct', 'cthead', 't', 'thead')."
                }
            ],
            "extracted_arguments": {
                "image": "The URL or file path of the image on which to perform object detection."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_461",
        "query": "As a specialist in computer vision, we need to use the OwlViT model to identify objects in an image described by specific text phrases like \"a photo of a cat\" and \"a photo of a dog.\"",
        "instruction": "Given an `object detection` task using a specific model, retrieve tools that support zero-shot object detection by processing image inputs and text phrases to accurately identify objects described by the query, utilizing computer vision techniques and pre-trained model frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "zero-shot-object-detection",
                    "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')",
                    "api_arguments": {
                        "model_name": "google/owlvit-large-patch14"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": [
                        "import requests",
                        "from PIL import Image",
                        "import torch",
                        "from transformers import OwlViTProcessor, OwlViTForObjectDetection",
                        "processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)",
                        "model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)",
                        "url = http://images.cocodataset.org/val2017/000000039769.jpg",
                        "image = Image.open(requests.get(url, stream=True).raw)",
                        "texts = [[a photo of a cat, a photo of a dog]",
                        "inputs = processor(text=texts, images=image, return_tensors=pt)",
                        "outputs = model(**inputs)",
                        "target_sizes = torch.Tensor([image.size[::-1]])",
                        "results = processor.post_process(outputs=outputs, target_sizes=target_sizes)",
                        "i = 0",
                        "text = texts[i]",
                        "boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]",
                        "score_threshold = 0.1",
                        "for box, score, label in zip(boxes, scores, labels):",
                        " box = [round(i, 2) for i in box.tolist()]",
                        " if score >= score_threshold:",
                        " print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})"
                    ],
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not specified"
                    },
                    "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.",
                    "id": "huggingface_api_218",
                    "name": "google/owlvit-large-patch14"
                },
                "id": "gorilla_huggingface_tool_214",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.",
            "Tool 1: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 2: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 3: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 4: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 5: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 6: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 7: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 8: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 9: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 10: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 11: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.",
            "Tool 12: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 13: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 14: A zero-shot image classification tool using OpenAI's CLIP model, allowing images to be classified based on arbitrary text descriptions without task-specific training. It leverages the CLIP transformer architecture to generalize across a wide range of computer vision datasets, making it useful for evaluating model robustness and performing flexible image recognition directly from text prompts.",
            "Tool 15: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 16: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 17: The `google/vit-base-patch16-224-in21k` tool provides a pretrained Vision Transformer (ViT) model for extracting image features using a transformer-based encoder. It processes images to generate rich representations suitable for tasks like image classification or downstream computer vision applications, leveraging a model trained on the large-scale ImageNet-21k dataset.",
            "Tool 18: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 19: Performs semantic image segmentation using the UperNet framework with a ConvNeXt backbone, enabling detailed scene understanding in computer vision tasks."
        ],
        "tool_view": {
            "description": "Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "The text description or caption that the model will use to generate a corresponding image.",
                    "required": true,
                    "example": "a photo of an astronaut riding a horse on Mars"
                }
            },
            "response": "The tool is designed for generating images from textual descriptions using the **Stable Diffusion model**. The expected response from this tool will contain a **generated image** based on the provided **text prompt**. Below is the description of the response structure and its components:\n\n### Response Structure Description:\n\n1. **Generated Image**:\n   - The core output of the tool is an image, which is a result of the model processing the provided textual description (prompt). The image is expected to be a **photo-realistic** visual representation of the described scene or concept.\n   - The image will be in a format that can be rendered or saved (such as PNG, JPEG, etc.), and will represent the best match to the textual input based on the model’s understanding.\n\n2. **Response Fields**:\n   - **images** (list): A list of generated images. Since the tool may generate multiple images in response to a single prompt (depending on the configuration of the pipeline), this list may contain one or more image objects.\n     - Each entry in the list represents an individual generated image, with at least one image being returned. The images are typically PIL Image objects or similar format depending on the implementation.\n   \n3. **Image Format**:\n   - The image(s) returned are **visual outputs**, not textual. Depending on the specific configuration, the image may be returned in a format compatible with Python image libraries (e.g., PIL, NumPy array, or directly as a file).\n   - The format allows for easy post-processing or saving to disk by the user (e.g., saving it as a PNG or JPEG file).\n   \n4. **Additional Metadata** (if available):\n   - The tool may include additional metadata regarding the image generation process. This could involve timing or status information (e.g., how long the image generation took or error messages if something went wrong). However, this is not explicitly defined in the provided tool documentation, so it's not guaranteed to appear in every case.\n\n### Overall, the response from the tool will primarily be a generated image (or a set of images), with each image corresponding to the input text prompt, reflecting the model's attempt to generate a visual representation of the described scenario.",
            "examples": [
                {
                    "user_intent": "Generate a realistic image of a futuristic city skyline at sunset.",
                    "tool_call": {
                        "name": "runwayml/stable-diffusion-v1-5",
                        "arguments": {
                            "prompt": "a futuristic city skyline at sunset, highly detailed, cinematic lighting"
                        }
                    }
                },
                {
                    "user_intent": "Create an artistic image of a fantasy dragon flying over a mountain range.",
                    "tool_call": {
                        "name": "runwayml/stable-diffusion-v1-5",
                        "arguments": {
                            "prompt": "a majestic fantasy dragon flying over snow-covered mountains, epic and colorful"
                        }
                    }
                },
                {
                    "user_intent": "Generate a whimsical illustration of a cat wearing a wizard hat in a magical library.",
                    "tool_call": {
                        "name": "runwayml/stable-diffusion-v1-5",
                        "arguments": {
                            "prompt": "a cute cat wearing a wizard hat in a magical library, detailed and whimsical illustration"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.\nTool 1: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 2: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 3: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 4: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 5: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 6: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 7: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 8: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 9: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 10: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 11: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.\nTool 12: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 13: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 14: A zero-shot image classification tool using OpenAI's CLIP model, allowing images to be classified based on arbitrary text descriptions without task-specific training. It leverages the CLIP transformer architecture to generalize across a wide range of computer vision datasets, making it useful for evaluating model robustness and performing flexible image recognition directly from text prompts.\nTool 15: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 16: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 17: The `google/vit-base-patch16-224-in21k` tool provides a pretrained Vision Transformer (ViT) model for extracting image features using a transformer-based encoder. It processes images to generate rich representations suitable for tasks like image classification or downstream computer vision applications, leveraging a model trained on the large-scale ImageNet-21k dataset.\nTool 18: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 19: Performs semantic image segmentation using the UperNet framework with a ConvNeXt backbone, enabling detailed scene understanding in computer vision tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"The text description or caption that the model will use to generate a corresponding image.\", \"required\": true, \"example\": \"a photo of an astronaut riding a horse on Mars\"}}, \"response\": \"The tool is designed for generating images from textual descriptions using the **Stable Diffusion model**. The expected response from this tool will contain a **generated image** based on the provided **text prompt**. Below is the description of the response structure and its components:\\n\\n### Response Structure Description:\\n\\n1. **Generated Image**:\\n   - The core output of the tool is an image, which is a result of the model processing the provided textual description (prompt). The image is expected to be a **photo-realistic** visual representation of the described scene or concept.\\n   - The image will be in a format that can be rendered or saved (such as PNG, JPEG, etc.), and will represent the best match to the textual input based on the model\\u2019s understanding.\\n\\n2. **Response Fields**:\\n   - **images** (list): A list of generated images. Since the tool may generate multiple images in response to a single prompt (depending on the configuration of the pipeline), this list may contain one or more image objects.\\n     - Each entry in the list represents an individual generated image, with at least one image being returned. The images are typically PIL Image objects or similar format depending on the implementation.\\n   \\n3. **Image Format**:\\n   - The image(s) returned are **visual outputs**, not textual. Depending on the specific configuration, the image may be returned in a format compatible with Python image libraries (e.g., PIL, NumPy array, or directly as a file).\\n   - The format allows for easy post-processing or saving to disk by the user (e.g., saving it as a PNG or JPEG file).\\n   \\n4. **Additional Metadata** (if available):\\n   - The tool may include additional metadata regarding the image generation process. This could involve timing or status information (e.g., how long the image generation took or error messages if something went wrong). However, this is not explicitly defined in the provided tool documentation, so it's not guaranteed to appear in every case.\\n\\n### Overall, the response from the tool will primarily be a generated image (or a set of images), with each image corresponding to the input text prompt, reflecting the model's attempt to generate a visual representation of the described scenario.\", \"examples\": [{\"user_intent\": \"Generate a realistic image of a futuristic city skyline at sunset.\", \"tool_call\": {\"name\": \"runwayml/stable-diffusion-v1-5\", \"arguments\": {\"prompt\": \"a futuristic city skyline at sunset, highly detailed, cinematic lighting\"}}}, {\"user_intent\": \"Create an artistic image of a fantasy dragon flying over a mountain range.\", \"tool_call\": {\"name\": \"runwayml/stable-diffusion-v1-5\", \"arguments\": {\"prompt\": \"a majestic fantasy dragon flying over snow-covered mountains, epic and colorful\"}}}, {\"user_intent\": \"Generate a whimsical illustration of a cat wearing a wizard hat in a magical library.\", \"tool_call\": {\"name\": \"runwayml/stable-diffusion-v1-5\", \"arguments\": {\"prompt\": \"a cute cat wearing a wizard hat in a magical library, detailed and whimsical illustration\"}}}]}\n\n# User Query\nAs a specialist in computer vision, we need to use the OwlViT model to identify objects in an image described by specific text phrases like \"a photo of a cat\" and \"a photo of a dog.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect objects in an image based on specific text descriptions.",
                    "tool_description": "Use the OWL-ViT model to detect objects in an image by providing text queries, allowing identification and localization of objects without task-specific training.",
                    "expected_response": "A set of detected objects with their corresponding locations (bounding boxes) and labels based on the provided text descriptions."
                }
            ],
            "extracted_arguments": {
                "text_queries": "The descriptive text phrases used to identify objects in the image, such as 'a photo of a cat' or 'a photo of a dog.'",
                "image_input": "The image to be analyzed for object detection, either in URL or file format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_262",
        "query": "Analyze an image of an urban scene to identify and separate regions with different semantics, such as streets, pedestrians, buildings, and vehicles.",
        "instruction": "Given an `image analysis` task, retrieve tools that perform semantic segmentation by processing image inputs to identify and delineate regions with distinct semantics such as streets, pedestrians, buildings, and vehicles, aligning with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Semantic Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')",
                    "api_arguments": {
                        "images": "image",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "PIL": "latest",
                        "requests": "latest"
                    },
                    "example_code": "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors='pt')\noutputs = model(**inputs)\nlogits = outputs.logits",
                    "performance": {
                        "dataset": "Cityscapes",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.",
                    "id": "huggingface_api_235",
                    "name": "nvidia/segformer-b2-finetuned-cityscapes-1024-1024"
                },
                "id": "gorilla_huggingface_tool_231",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 1: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.",
            "Tool 2: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 3: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 4: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 5: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.",
            "Tool 6: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.",
            "Tool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 8: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 9: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 10: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 11: A Latent Diffusion Model that generates and modifies images based on text prompts, allowing users to inpaint or alter specific regions of an image using a mask. It leverages a pretrained text encoder to interpret prompts and produce high-quality, customizable visual content.",
            "Tool 12: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 13: Performs semantic image segmentation using the UperNet framework with a ConvNeXt backbone, enabling detailed scene understanding in computer vision tasks.",
            "Tool 14: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 15: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 16: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 17: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 18: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 19: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image."
        ],
        "tool_view": {
            "description": "A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "parameters": {
                "images": {
                    "type": "image",
                    "description": "The input image or list of images to be segmented by the model. Each image should be in a format compatible with PIL or a similar image-handling library.",
                    "required": true,
                    "example": "Image.open('example.jpg')"
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors to return after preprocessing. Typically set to 'pt' for PyTorch tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool performs semantic image segmentation using the SegFormer model, specifically the fine-tuned version of SegFormer (model name: \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\") trained on the CityScapes dataset.\n\n### Expected Response Structure:\n\n1. **Segmentation Output (Logits)**:\n   - The response includes the raw output logits of the model after processing the input image. These logits represent the predicted segmentation classes for each pixel in the input image.\n   - The logits are typically returned as a tensor, with dimensions corresponding to the number of pixels in the image and the number of segmentation classes.\n   - The logits can be used to derive the final segmentation map, which assigns a class to each pixel.\n\n2. **Tensor Type**:\n   - The logits tensor is returned in PyTorch format (`pt`), as indicated by the `return_tensors` argument set to `\"pt\"` in the API call.\n   - This means the output tensor is in a format compatible with PyTorch-based processing and further model handling.\n\n3. **Image and Segmentation Context**:\n   - The response does not directly provide the input image or a visual representation of the segmented image. However, the logits can be processed to generate a segmentation map that can be visualized.\n   - The response would focus on the model's output in terms of logits, which represent probabilities for each pixel belonging to various predefined classes (such as buildings, roads, cars, etc.).\n\n4. **Performance Metrics (Optional)**:\n   - While not explicitly provided in the response, performance metrics such as accuracy may be available as part of model documentation or evaluation, but are not part of the segmentation response itself.\n\n### General Meaning of Components:\n- **Logits**: These are the raw, unnormalized predictions made by the model for each pixel, which can later be passed through a softmax function or thresholding technique to assign class labels.\n- **Tensor**: The logits are in tensor form, which is a multi-dimensional array used in PyTorch, representing the model's predictions for the segmentation task.\n",
            "examples": [
                {
                    "user_intent": "I want to segment the streets, cars, and pedestrians in a cityscape image I downloaded.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "city_street.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this traffic camera snapshot to identify roads, vehicles, and sidewalks.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "traffic_snapshot.png",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Segment a drone image of a city to detect buildings and roads for urban planning.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "drone_city_view.jpeg",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 1: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.\nTool 2: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 3: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 4: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 5: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.\nTool 6: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.\nTool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 8: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 9: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 10: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 11: A Latent Diffusion Model that generates and modifies images based on text prompts, allowing users to inpaint or alter specific regions of an image using a mask. It leverages a pretrained text encoder to interpret prompts and produce high-quality, customizable visual content.\nTool 12: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 13: Performs semantic image segmentation using the UperNet framework with a ConvNeXt backbone, enabling detailed scene understanding in computer vision tasks.\nTool 14: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 15: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 16: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 17: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 18: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 19: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024\\u00d71024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\", \"parameters\": {\"images\": {\"type\": \"image\", \"description\": \"The input image or list of images to be segmented by the model. Each image should be in a format compatible with PIL or a similar image-handling library.\", \"required\": true, \"example\": \"Image.open('example.jpg')\"}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors to return after preprocessing. Typically set to 'pt' for PyTorch tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool performs semantic image segmentation using the SegFormer model, specifically the fine-tuned version of SegFormer (model name: \\\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\\\") trained on the CityScapes dataset.\\n\\n### Expected Response Structure:\\n\\n1. **Segmentation Output (Logits)**:\\n   - The response includes the raw output logits of the model after processing the input image. These logits represent the predicted segmentation classes for each pixel in the input image.\\n   - The logits are typically returned as a tensor, with dimensions corresponding to the number of pixels in the image and the number of segmentation classes.\\n   - The logits can be used to derive the final segmentation map, which assigns a class to each pixel.\\n\\n2. **Tensor Type**:\\n   - The logits tensor is returned in PyTorch format (`pt`), as indicated by the `return_tensors` argument set to `\\\"pt\\\"` in the API call.\\n   - This means the output tensor is in a format compatible with PyTorch-based processing and further model handling.\\n\\n3. **Image and Segmentation Context**:\\n   - The response does not directly provide the input image or a visual representation of the segmented image. However, the logits can be processed to generate a segmentation map that can be visualized.\\n   - The response would focus on the model's output in terms of logits, which represent probabilities for each pixel belonging to various predefined classes (such as buildings, roads, cars, etc.).\\n\\n4. **Performance Metrics (Optional)**:\\n   - While not explicitly provided in the response, performance metrics such as accuracy may be available as part of model documentation or evaluation, but are not part of the segmentation response itself.\\n\\n### General Meaning of Components:\\n- **Logits**: These are the raw, unnormalized predictions made by the model for each pixel, which can later be passed through a softmax function or thresholding technique to assign class labels.\\n- **Tensor**: The logits are in tensor form, which is a multi-dimensional array used in PyTorch, representing the model's predictions for the segmentation task.\\n\", \"examples\": [{\"user_intent\": \"I want to segment the streets, cars, and pedestrians in a cityscape image I downloaded.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"city_street.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Analyze this traffic camera snapshot to identify roads, vehicles, and sidewalks.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"traffic_snapshot.png\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Segment a drone image of a city to detect buildings and roads for urban planning.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"drone_city_view.jpeg\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nAnalyze an image of an urban scene to identify and separate regions with different semantics, such as streets, pedestrians, buildings, and vehicles.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Segment an urban scene image to identify streets, pedestrians, buildings, and vehicles.",
                    "tool_description": "Perform semantic segmentation on a cityscape image using a SegFormer model fine-tuned on the CityScapes dataset, labeling each pixel according to predefined urban scene classes.",
                    "expected_response": "A tensor containing per-pixel logits for each semantic class, which can be processed to generate a segmentation map of streets, pedestrians, buildings, and vehicles."
                }
            ],
            "extracted_arguments": {
                "images": "The input urban scene image to be segmented, compatible with PIL or similar image-handling libraries.",
                "return_tensors": "The format of the output tensor, typically 'pt' for PyTorch tensors."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_200",
        "query": "Our team is working on a recommendation system for a news article app. We should understand the semantic similarity of various texts.",
        "instruction": "Given a `semantic similarity analysis` task, retrieve tools that utilize feature extraction techniques to evaluate and compare the semantic content of various texts, supporting tasks that involve understanding and recommending based on textual similarities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')",
                    "api_arguments": [
                        "AutoTokenizer",
                        "AutoModel"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)",
                    "performance": {
                        "dataset": "STS tasks",
                        "accuracy": "Spearman's correlation (See associated paper Appendix B)"
                    },
                    "description": "A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.",
                    "id": "huggingface_api_13",
                    "name": "sup-simcse-roberta-large"
                },
                "id": "gorilla_huggingface_tool_13",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "Tool 1: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.",
            "Tool 2: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 3: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.",
            "Tool 4: This tool uses the `sentence-transformers/paraphrase-distilroberta-base-v2` model to convert sentences or paragraphs into 768-dimensional dense vector embeddings, enabling applications such as semantic similarity comparison, clustering, and semantic search.",
            "Tool 5: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.",
            "Tool 6: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.",
            "Tool 7: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 8: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 9: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 10: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 11: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 12: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 13: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.",
            "Tool 14: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 15: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 16: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.",
            "Tool 17: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 18: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 19: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications."
        ],
        "tool_view": {
            "description": "SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "parameters": {
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer instance used to convert input text (titles and abstracts of research papers) into token IDs compatible with the SciNCL model.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('malteos/scincl')"
                },
                "model": {
                    "type": "string",
                    "description": "The pre-trained SciNCL model used to generate document-level embeddings for research papers.",
                    "required": true,
                    "example": "AutoModel.from_pretrained('malteos/scincl')"
                }
            },
            "response": "The tool returns document-level embeddings of research papers. The response would likely consist of the following components:\n\n1. **Embeddings Data**: A tensor or matrix representation of the research papers, specifically the document-level embeddings. These embeddings capture the semantic information of the input papers, derived from their titles and abstracts. The response would contain one embedding vector per paper, where each vector corresponds to the semantic features of the paper as learned by the model.\n\n2. **Shape of the Embeddings**: The embeddings are typically shaped as a 2D tensor with dimensions corresponding to the number of papers and the embedding size (the length of the vector). For example, if the model produces embeddings of size 768, each research paper would have a 768-dimensional vector.\n\n3. **Metadata (Optional)**: The response may include metadata related to the embedding generation process, such as:\n   - The model used for embedding generation (`malteos/scincl`).\n   - The input parameters (e.g., whether truncation or padding was applied).\n   - Information about whether additional processing steps like normalization or scaling were applied.\n\nThe embeddings themselves represent a condensed, high-dimensional feature vector for each paper, summarizing its content (title and abstract) in a form that can be used for downstream tasks like document similarity, clustering, or retrieval.",
            "examples": [
                {
                    "user_intent": "I want to generate embeddings for a batch of recent AI research papers to use in a similarity search.",
                    "tool_call": {
                        "name": "malteos/scincl",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('malteos/scincl')",
                            "model": "AutoModel.from_pretrained('malteos/scincl')",
                            "papers": [
                                {
                                    "title": "GPT-4 Technical Report",
                                    "abstract": "We present GPT-4, a large-scale multimodal model..."
                                },
                                {
                                    "title": "Transformers in Computer Vision",
                                    "abstract": "This paper explores transformer architectures applied to image recognition tasks..."
                                }
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I need document embeddings for a set of biomedical papers to cluster similar research topics.",
                    "tool_call": {
                        "name": "malteos/scincl",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('malteos/scincl')",
                            "model": "AutoModel.from_pretrained('malteos/scincl')",
                            "papers": [
                                {
                                    "title": "CRISPR-Cas9 Gene Editing",
                                    "abstract": "We investigate the efficiency and specificity of CRISPR-Cas9 in genome editing."
                                },
                                {
                                    "title": "mRNA Vaccine Development",
                                    "abstract": "This study analyzes the mechanisms behind mRNA vaccine efficacy against COVID-19."
                                }
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Create embeddings for a mixed set of physics and chemistry papers to feed into a recommendation system.",
                    "tool_call": {
                        "name": "malteos/scincl",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('malteos/scincl')",
                            "model": "AutoModel.from_pretrained('malteos/scincl')",
                            "papers": [
                                {
                                    "title": "Quantum Entanglement in Particle Physics",
                                    "abstract": "We explore entanglement phenomena in high-energy particle collisions."
                                },
                                {
                                    "title": "Catalysis in Green Chemistry",
                                    "abstract": "This paper presents novel catalysts for environmentally friendly chemical reactions."
                                }
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\nTool 1: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.\nTool 2: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 3: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.\nTool 4: This tool uses the `sentence-transformers/paraphrase-distilroberta-base-v2` model to convert sentences or paragraphs into 768-dimensional dense vector embeddings, enabling applications such as semantic similarity comparison, clustering, and semantic search.\nTool 5: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.\nTool 6: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.\nTool 7: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 8: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 9: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 10: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 11: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 12: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 13: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\nTool 14: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 15: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 16: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\nTool 17: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 18: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 19: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\", \"parameters\": {\"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer instance used to convert input text (titles and abstracts of research papers) into token IDs compatible with the SciNCL model.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('malteos/scincl')\"}, \"model\": {\"type\": \"string\", \"description\": \"The pre-trained SciNCL model used to generate document-level embeddings for research papers.\", \"required\": true, \"example\": \"AutoModel.from_pretrained('malteos/scincl')\"}}, \"response\": \"The tool returns document-level embeddings of research papers. The response would likely consist of the following components:\\n\\n1. **Embeddings Data**: A tensor or matrix representation of the research papers, specifically the document-level embeddings. These embeddings capture the semantic information of the input papers, derived from their titles and abstracts. The response would contain one embedding vector per paper, where each vector corresponds to the semantic features of the paper as learned by the model.\\n\\n2. **Shape of the Embeddings**: The embeddings are typically shaped as a 2D tensor with dimensions corresponding to the number of papers and the embedding size (the length of the vector). For example, if the model produces embeddings of size 768, each research paper would have a 768-dimensional vector.\\n\\n3. **Metadata (Optional)**: The response may include metadata related to the embedding generation process, such as:\\n   - The model used for embedding generation (`malteos/scincl`).\\n   - The input parameters (e.g., whether truncation or padding was applied).\\n   - Information about whether additional processing steps like normalization or scaling were applied.\\n\\nThe embeddings themselves represent a condensed, high-dimensional feature vector for each paper, summarizing its content (title and abstract) in a form that can be used for downstream tasks like document similarity, clustering, or retrieval.\", \"examples\": [{\"user_intent\": \"I want to generate embeddings for a batch of recent AI research papers to use in a similarity search.\", \"tool_call\": {\"name\": \"malteos/scincl\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('malteos/scincl')\", \"model\": \"AutoModel.from_pretrained('malteos/scincl')\", \"papers\": [{\"title\": \"GPT-4 Technical Report\", \"abstract\": \"We present GPT-4, a large-scale multimodal model...\"}, {\"title\": \"Transformers in Computer Vision\", \"abstract\": \"This paper explores transformer architectures applied to image recognition tasks...\"}]}}}, {\"user_intent\": \"I need document embeddings for a set of biomedical papers to cluster similar research topics.\", \"tool_call\": {\"name\": \"malteos/scincl\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('malteos/scincl')\", \"model\": \"AutoModel.from_pretrained('malteos/scincl')\", \"papers\": [{\"title\": \"CRISPR-Cas9 Gene Editing\", \"abstract\": \"We investigate the efficiency and specificity of CRISPR-Cas9 in genome editing.\"}, {\"title\": \"mRNA Vaccine Development\", \"abstract\": \"This study analyzes the mechanisms behind mRNA vaccine efficacy against COVID-19.\"}]}}}, {\"user_intent\": \"Create embeddings for a mixed set of physics and chemistry papers to feed into a recommendation system.\", \"tool_call\": {\"name\": \"malteos/scincl\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('malteos/scincl')\", \"model\": \"AutoModel.from_pretrained('malteos/scincl')\", \"papers\": [{\"title\": \"Quantum Entanglement in Particle Physics\", \"abstract\": \"We explore entanglement phenomena in high-energy particle collisions.\"}, {\"title\": \"Catalysis in Green Chemistry\", \"abstract\": \"This paper presents novel catalysts for environmentally friendly chemical reactions.\"}]}}}]}\n\n# User Query\nOur team is working on a recommendation system for a news article app. We should understand the semantic similarity of various texts.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate embeddings for a collection of news articles to analyze their semantic similarity.",
                    "tool_description": "Convert the text of news articles into dense vector embeddings using a pre-trained sentence or document embedding model to capture their semantic content.",
                    "expected_response": "A set of high-dimensional embeddings representing each news article, which can be used to measure semantic similarity or for clustering and recommendation tasks."
                }
            ],
            "extracted_arguments": {
                "text_corpus": "A list of news articles, where each article contains textual content such as title and body, used as input for embedding generation.",
                "embedding_model": "The pre-trained sentence or document embedding model used to generate semantic vectors for the articles.",
                "tokenizer": "The tokenizer associated with the embedding model, used to convert text into model-compatible token IDs."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_279",
        "query": "Bob is designing an app for his city. He needs to quickly identify if the provided image has a bike or a car. Design a model that recognizes a bike or car and provides classification output for the given image.",
        "instruction": "Given an `image classification` task, retrieve tools that use visual inputs to determine the presence of specific objects, such as bikes or cars, by analyzing and classifying image data to produce accurate recognition outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')",
                    "api_arguments": "image, class_names",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline; clip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'); clip(image, class_names=['cat', 'dog', 'fish'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "79.1 - 79.4"
                    },
                    "description": "A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.",
                    "id": "huggingface_api_364",
                    "name": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind"
                },
                "id": "gorilla_huggingface_tool_360",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "Tool 1: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.",
            "Tool 2: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 3: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.",
            "Tool 4: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 5: A lightweight Vision Transformer (ViT) model for image classification that uses Hugging Face Transformers, suitable for quickly classifying images with a pretrained tiny-vit-random model.",
            "Tool 6: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 7: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 8: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 9: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.",
            "Tool 10: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 11: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 12: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 13: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 14: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 15: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.",
            "Tool 16: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.",
            "Tool 17: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 18: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 19: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable."
        ],
        "tool_view": {
            "description": "A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "parameters": {},
            "response": "The tool produces a text-generation response in a conversational format. Its response structure is typically a string representing the model’s generated continuation or reply to the provided input message.  \n\nKey components of the response:  \n\n- **Generated Text**: The main content of the response is a natural language string that represents the model’s reply in a conversational style, intended to mimic Elon Musk’s tweeting or speaking style. This text may include informal language, emojis, or humorous/non-factual content.  \n- **Optional Metadata (framework-dependent)**: Depending on the Hugging Face Transformers pipeline configuration, the response may be returned as a list of dictionaries, where each dictionary contains a key like `\"generated_text\"` mapping to the generated string. Additional metadata, such as token usage or generation parameters, might also be included if explicitly enabled, though it is not required by default.  \n\nSemantically, the response conveys the AI’s attempt to continue the conversation coherently based on the input prompt, reflecting the stylistic characteristics learned from Elon Musk’s tweets. It may vary in relevance and factual accuracy, with occasional nonsensical output.",
            "examples": [
                {
                    "user_intent": "I want to chat with a version of Elon Musk. Can you start a conversation?",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "Hello, Elon!"
                        }
                    }
                },
                {
                    "user_intent": "Let's see if this AI can make a joke like Elon Musk would.",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "Tell me a joke, Elon!"
                        }
                    }
                },
                {
                    "user_intent": "I need to test how Elon Musk would respond to a random question. Ask him about space.",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "What do you think about Mars colonization?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\nTool 1: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.\nTool 2: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 3: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.\nTool 4: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 5: A lightweight Vision Transformer (ViT) model for image classification that uses Hugging Face Transformers, suitable for quickly classifying images with a pretrained tiny-vit-random model.\nTool 6: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 7: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 8: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 9: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.\nTool 10: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 11: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 12: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 13: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 14: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 15: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.\nTool 16: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.\nTool 17: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 18: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 19: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A conversational AI model that mimics Elon Musk\\u2019s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\", \"parameters\": {}, \"response\": \"The tool produces a text-generation response in a conversational format. Its response structure is typically a string representing the model\\u2019s generated continuation or reply to the provided input message.  \\n\\nKey components of the response:  \\n\\n- **Generated Text**: The main content of the response is a natural language string that represents the model\\u2019s reply in a conversational style, intended to mimic Elon Musk\\u2019s tweeting or speaking style. This text may include informal language, emojis, or humorous/non-factual content.  \\n- **Optional Metadata (framework-dependent)**: Depending on the Hugging Face Transformers pipeline configuration, the response may be returned as a list of dictionaries, where each dictionary contains a key like `\\\"generated_text\\\"` mapping to the generated string. Additional metadata, such as token usage or generation parameters, might also be included if explicitly enabled, though it is not required by default.  \\n\\nSemantically, the response conveys the AI\\u2019s attempt to continue the conversation coherently based on the input prompt, reflecting the stylistic characteristics learned from Elon Musk\\u2019s tweets. It may vary in relevance and factual accuracy, with occasional nonsensical output.\", \"examples\": [{\"user_intent\": \"I want to chat with a version of Elon Musk. Can you start a conversation?\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"Hello, Elon!\"}}}, {\"user_intent\": \"Let's see if this AI can make a joke like Elon Musk would.\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"Tell me a joke, Elon!\"}}}, {\"user_intent\": \"I need to test how Elon Musk would respond to a random question. Ask him about space.\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"What do you think about Mars colonization?\"}}}]}\n\n# User Query\nBob is designing an app for his city. He needs to quickly identify if the provided image has a bike or a car. Design a model that recognizes a bike or car and provides classification output for the given image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify an image to determine whether it contains a bike or a car.",
                    "tool_description": "Use a pretrained image classification model to identify whether an input image contains a bike or a car. The model analyzes the visual content and predicts the class label from the predefined categories 'bike' and 'car'.",
                    "expected_response": "A classification result indicating whether the image depicts a bike or a car, optionally with confidence scores for each class."
                }
            ],
            "extracted_arguments": {
                "input_image": "An image file containing visual content to be classified as either a bike or a car."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_201",
        "query": "We are developing a voice-controlled drone. Please identify the spoken command in the audio clip provided.",
        "instruction": "Given a `spoken command recognition` task, retrieve tools that classify and interpret audio inputs to accurately identify spoken commands for integration with voice-controlled devices.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers==4.27.1, pytorch==1.11.0, datasets==2.10.1, tokenizers==0.12.1",
                    "example_code": "",
                    "performance": {
                        "dataset": "mazkooleg/0-9up_google_speech_commands_augmented_raw",
                        "accuracy": 0.9979
                    },
                    "description": "This model is a fine-tuned version of microsoft/unispeech-sat-base on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0123, Accuracy: 0.9979.",
                    "id": "huggingface_api_823",
                    "name": "mazkooleg/0-9up-unispeech-sat-base-ft"
                },
                "id": "gorilla_huggingface_tool_818",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.",
            "Tool 1: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.",
            "Tool 2: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 3: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 4: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 5: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 6: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 7: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 8: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 9: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 10: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 11: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 12: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.",
            "Tool 13: This tool uses a fine-tuned Hugging Face Transformers model to perform audio classification, specifically recognizing speech commands. It is highly accurate, achieving 99.79% accuracy on a benchmark speech commands dataset, making it suitable for applications that require reliable automated speech recognition and command detection.",
            "Tool 14: A compact wav2vec2 model for automatic speech recognition that converts spoken audio into text using the Hugging Face Transformers framework.",
            "Tool 15: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 16: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 17: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 18: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 19: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings."
        ],
        "tool_view": {
            "description": "This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.",
            "parameters": {},
            "response": "The tool produces a response that is a structured collection of audio classification predictions for a given audio input. Each element in the response represents a single predicted class for the input audio and typically contains the following components:\n\n1. **`label`** – A string indicating the predicted category or class of the audio. This corresponds to one of the classes the model was trained to recognize.\n\n2. **`score`** – A numerical value (usually a float between 0 and 1) representing the model’s confidence in the prediction for the corresponding label. Higher values indicate greater confidence.\n\nThe overall response is an array (or list) of such predictions, typically ordered from the most confident to the least confident. In practice, for a single audio input, the response usually contains one or several top predictions, depending on how the pipeline is configured. \n\nEach prediction in the response reflects the model’s assessment of the likelihood that the input audio belongs to a specific class from the model’s training set.",
            "examples": [
                {
                    "user_intent": "I want to classify a short audio clip to detect which spoken command it contains.",
                    "tool_call": {
                        "name": "mazkooleg/0-9up-wavlm-base-plus-ft",
                        "arguments": {
                            "audio_file_path": "commands/turn_on_lights.wav"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this audio snippet to see if it contains the word 'stop'.",
                    "tool_call": {
                        "name": "mazkooleg/0-9up-wavlm-base-plus-ft",
                        "arguments": {
                            "audio_file_path": "samples/stop_signal.wav"
                        }
                    }
                },
                {
                    "user_intent": "Classify multiple audio recordings from my dataset to identify spoken keywords.",
                    "tool_call": {
                        "name": "mazkooleg/0-9up-wavlm-base-plus-ft",
                        "arguments": {
                            "audio_file_path": [
                                "dataset/audio1.wav",
                                "dataset/audio2.wav",
                                "dataset/audio3.wav"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.\nTool 1: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.\nTool 2: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 3: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 4: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 5: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 6: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 7: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 8: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 9: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 10: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 11: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 12: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.\nTool 13: This tool uses a fine-tuned Hugging Face Transformers model to perform audio classification, specifically recognizing speech commands. It is highly accurate, achieving 99.79% accuracy on a benchmark speech commands dataset, making it suitable for applications that require reliable automated speech recognition and command detection.\nTool 14: A compact wav2vec2 model for automatic speech recognition that converts spoken audio into text using the Hugging Face Transformers framework.\nTool 15: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 16: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 17: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 18: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 19: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.\", \"parameters\": {}, \"response\": \"The tool produces a response that is a structured collection of audio classification predictions for a given audio input. Each element in the response represents a single predicted class for the input audio and typically contains the following components:\\n\\n1. **`label`** \\u2013 A string indicating the predicted category or class of the audio. This corresponds to one of the classes the model was trained to recognize.\\n\\n2. **`score`** \\u2013 A numerical value (usually a float between 0 and 1) representing the model\\u2019s confidence in the prediction for the corresponding label. Higher values indicate greater confidence.\\n\\nThe overall response is an array (or list) of such predictions, typically ordered from the most confident to the least confident. In practice, for a single audio input, the response usually contains one or several top predictions, depending on how the pipeline is configured. \\n\\nEach prediction in the response reflects the model\\u2019s assessment of the likelihood that the input audio belongs to a specific class from the model\\u2019s training set.\", \"examples\": [{\"user_intent\": \"I want to classify a short audio clip to detect which spoken command it contains.\", \"tool_call\": {\"name\": \"mazkooleg/0-9up-wavlm-base-plus-ft\", \"arguments\": {\"audio_file_path\": \"commands/turn_on_lights.wav\"}}}, {\"user_intent\": \"Analyze this audio snippet to see if it contains the word 'stop'.\", \"tool_call\": {\"name\": \"mazkooleg/0-9up-wavlm-base-plus-ft\", \"arguments\": {\"audio_file_path\": \"samples/stop_signal.wav\"}}}, {\"user_intent\": \"Classify multiple audio recordings from my dataset to identify spoken keywords.\", \"tool_call\": {\"name\": \"mazkooleg/0-9up-wavlm-base-plus-ft\", \"arguments\": {\"audio_file_path\": [\"dataset/audio1.wav\", \"dataset/audio2.wav\", \"dataset/audio3.wav\"]}}}]}\n\n# User Query\nWe are developing a voice-controlled drone. Please identify the spoken command in the audio clip provided.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify a short audio clip to detect which spoken command it contains.",
                    "tool_description": "Use a Hugging Face Transformers audio classification pipeline to identify spoken commands in an audio input, leveraging a fine-tuned model trained for high-accuracy speech command recognition.",
                    "expected_response": "A list of predicted command labels with associated confidence scores, indicating the model's assessment of which command was spoken in the audio."
                }
            ],
            "extracted_arguments": {
                "audio_file_path": "The file path or location of the audio clip containing the spoken command."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_417",
        "query": "We are a consulting firm, and we want to easily identify company names from texts.",
        "instruction": "Given a `company name extraction` task, retrieve tools that perform entity extraction by processing text inputs to identify and extract company names from the content accurately.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Entity Extraction",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548')",
                    "api_arguments": {
                        "inputs": "I love AutoTrain"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTokenClassification, AutoTokenizer"
                    },
                    "example_code": "from transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "ismail-lucifer011/autotrain-data-company_all",
                        "accuracy": 0.9979930566588805
                    },
                    "description": "A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.",
                    "id": "huggingface_api_414",
                    "name": "903429548"
                },
                "id": "gorilla_huggingface_tool_410",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 1: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 2: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 3: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 4: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 5: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 6: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 7: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 8: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 9: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 10: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 11: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 12: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 13: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 14: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 15: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 16: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.",
            "Tool 17: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 18: The \"mio/tokiwa_midori\" is a Text-to-Speech model developed using the ESPnet framework, trained on the Amadeus dataset. It enables converting text into speech, utilizing pre-trained models from the Hugging Face Hub. This tool is useful for speech synthesis applications and is easily invoked through a simple API call with specific configuration and model paths.",
            "Tool 19: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks."
        ],
        "tool_view": {
            "description": "A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "parameters": {
                "inputs": {
                    "type": "string",
                    "description": "The text input that will be processed by the model to perform entity extraction. This is typically the raw text containing entities (like company names) that the model will extract.",
                    "required": true,
                    "example": "I love AutoTrain"
                }
            },
            "response": "The tool produces a **token classification response** typical of Hugging Face Transformers models for entity extraction. The response structure can be described as follows:\n\n1. **Token-level outputs**:  \n   The model returns a list of predictions corresponding to each token in the input text. Each element in this list represents a single token and contains information about the model’s classification of that token.\n\n2. **Typical fields per token**:  \n   - **`word` or `token`**: The actual text of the token from the input sentence.  \n   - **`entity` or `label`**: The predicted entity label assigned to this token (e.g., company name, organization, or non-entity). Labels usually follow the IOB or BIO tagging scheme.  \n   - **`score` or `confidence`**: A probability or confidence score indicating how confident the model is in its predicted label.  \n   - **`start` and `end`** (optional depending on implementation): Character offsets in the original input string that indicate the span of text covered by the token.\n\n3. **Aggregated entities (optional)**:  \n   Some implementations may post-process token-level outputs to group consecutive tokens with the same entity label into a single entity mention, returning:  \n   - **`entity_group` or `entity`**: The combined label for the entity span.  \n   - **`text`**: The reconstructed text of the entity from consecutive tokens.  \n   - **`start` and `end`**: Character offsets for the span of the entity in the original text.  \n   - **`score`**: Confidence score for the grouped entity, often computed as an aggregate of token-level scores.\n\n4. **Overall semantics**:  \n   The response allows a user to identify **which parts of the input text correspond to company names**, along with model confidence and positional information, enabling downstream tasks such as extraction, highlighting, or database population.  \n\nIn summary, the response is a **structured token-level prediction** that may optionally be aggregated into entity spans, providing the predicted label, confidence, and positional information for each recognized entity in the text.",
            "examples": [
                {
                    "user_intent": "Extract the company names mentioned in the sentence 'I recently started working at OpenAI and Microsoft.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "I recently started working at OpenAI and Microsoft."
                        }
                    }
                },
                {
                    "user_intent": "Identify entities representing companies in the text 'Tesla, SpaceX, and Neuralink are all led by Elon Musk.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "Tesla, SpaceX, and Neuralink are all led by Elon Musk."
                        }
                    }
                },
                {
                    "user_intent": "Find all company names in the paragraph: 'Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 1: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 2: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 3: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 4: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 5: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 6: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 7: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 8: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 9: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 10: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 11: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 12: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 13: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 14: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 15: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 16: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\nTool 17: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 18: The \"mio/tokiwa_midori\" is a Text-to-Speech model developed using the ESPnet framework, trained on the Amadeus dataset. It enables converting text into speech, utilizing pre-trained models from the Hugging Face Hub. This tool is useful for speech synthesis applications and is easily invoked through a simple API call with specific configuration and model paths.\nTool 19: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\", \"parameters\": {\"inputs\": {\"type\": \"string\", \"description\": \"The text input that will be processed by the model to perform entity extraction. This is typically the raw text containing entities (like company names) that the model will extract.\", \"required\": true, \"example\": \"I love AutoTrain\"}}, \"response\": \"The tool produces a **token classification response** typical of Hugging Face Transformers models for entity extraction. The response structure can be described as follows:\\n\\n1. **Token-level outputs**:  \\n   The model returns a list of predictions corresponding to each token in the input text. Each element in this list represents a single token and contains information about the model\\u2019s classification of that token.\\n\\n2. **Typical fields per token**:  \\n   - **`word` or `token`**: The actual text of the token from the input sentence.  \\n   - **`entity` or `label`**: The predicted entity label assigned to this token (e.g., company name, organization, or non-entity). Labels usually follow the IOB or BIO tagging scheme.  \\n   - **`score` or `confidence`**: A probability or confidence score indicating how confident the model is in its predicted label.  \\n   - **`start` and `end`** (optional depending on implementation): Character offsets in the original input string that indicate the span of text covered by the token.\\n\\n3. **Aggregated entities (optional)**:  \\n   Some implementations may post-process token-level outputs to group consecutive tokens with the same entity label into a single entity mention, returning:  \\n   - **`entity_group` or `entity`**: The combined label for the entity span.  \\n   - **`text`**: The reconstructed text of the entity from consecutive tokens.  \\n   - **`start` and `end`**: Character offsets for the span of the entity in the original text.  \\n   - **`score`**: Confidence score for the grouped entity, often computed as an aggregate of token-level scores.\\n\\n4. **Overall semantics**:  \\n   The response allows a user to identify **which parts of the input text correspond to company names**, along with model confidence and positional information, enabling downstream tasks such as extraction, highlighting, or database population.  \\n\\nIn summary, the response is a **structured token-level prediction** that may optionally be aggregated into entity spans, providing the predicted label, confidence, and positional information for each recognized entity in the text.\", \"examples\": [{\"user_intent\": \"Extract the company names mentioned in the sentence 'I recently started working at OpenAI and Microsoft.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"I recently started working at OpenAI and Microsoft.\"}}}, {\"user_intent\": \"Identify entities representing companies in the text 'Tesla, SpaceX, and Neuralink are all led by Elon Musk.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"Tesla, SpaceX, and Neuralink are all led by Elon Musk.\"}}}, {\"user_intent\": \"Find all company names in the paragraph: 'Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.\"}}}]}\n\n# User Query\nWe are a consulting firm, and we want to easily identify company names from texts.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract all company names mentioned in a given text.",
                    "tool_description": "A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. It identifies and extracts company name entities efficiently using a distilBERT-based architecture.",
                    "expected_response": "A structured list of token-level predictions indicating company names, including text spans, entity labels, and confidence scores. Optionally, consecutive tokens with the same label may be aggregated into single entity mentions."
                }
            ],
            "extracted_arguments": {
                "text_input": "The raw text in which company names need to be identified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_68",
        "query": "I am writing an article on the history of technology companies, and I want to extract the names of companies and people mentioned in the text.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that analyze text to identify and extract entities such as names of companies and people, based on the content and requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/roberta-large-ner-english')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/roberta-large-ner-english')",
                        "tokenizer": "AutoTokenizer.from_pretrained('Jean-Baptiste/roberta-large-ner-english')",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": ">=4.0.0"
                    },
                    "example_code": "nlp(Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer)",
                    "performance": {
                        "dataset": "conll2003",
                        "accuracy": {
                            "PER": {
                                "precision": 0.9914,
                                "recall": 0.9927,
                                "f1": 0.992
                            },
                            "ORG": {
                                "precision": 0.9627,
                                "recall": 0.9661,
                                "f1": 0.9644
                            },
                            "LOC": {
                                "precision": 0.9795,
                                "recall": 0.9862,
                                "f1": 0.9828
                            },
                            "MISC": {
                                "precision": 0.9292,
                                "recall": 0.9262,
                                "f1": 0.9277
                            },
                            "Overall": {
                                "precision": 0.974,
                                "recall": 0.9766,
                                "f1": 0.9753
                            }
                        }
                    },
                    "description": "roberta-large-ner-english is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular, the model seems to work better on entities that don't start with an upper case.",
                    "id": "huggingface_api_416",
                    "name": "Jean-Baptiste/roberta-large-ner-english"
                },
                "id": "gorilla_huggingface_tool_412",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 1: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 2: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.",
            "Tool 3: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 4: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 5: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 6: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 8: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 9: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 10: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 11: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 12: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.",
            "Tool 13: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 14: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 15: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 16: EleutherAI/gpt-j-6B is a 6-billion-parameter transformer model designed for natural language text generation. Trained on the large-scale Pile dataset, it leverages a 28-layer architecture with 16 attention heads and Rotary Position Embeddings, supporting high-quality generation across diverse tasks. It can be used via the Hugging Face Transformers library to generate coherent, context-aware text for applications such as creative writing, dialogue systems, and general-purpose language modeling.",
            "Tool 17: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 18: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 19: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation."
        ],
        "tool_view": {
            "description": "The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "parameters": {
                "sentence": {
                    "type": "string",
                    "description": "The input text sentence on which the Named Entity Recognition (NER) model will perform entity tagging.",
                    "required": true,
                    "example": "George Washington went to Washington"
                }
            },
            "response": "The tool’s response consists of an annotated sentence object that includes recognized named entities identified within the input text. Specifically, after the model processes the input sentence, the response encapsulates the sentence along with labeled entity spans corresponding to one of four predefined categories: PER (person names), LOC (location names), ORG (organization names), and MISC (miscellaneous names).\n\nEach entity span in the response includes at least the following components:\n\n- The text segment from the sentence identified as an entity.\n- The entity label indicating the type of named entity (PER, LOC, ORG, or MISC).\n- The position or span indices within the sentence marking where the entity occurs.\n- A confidence score or probability reflecting the model’s certainty about the prediction (typically included in Flair’s entity spans).\n\nOverall, the response structure is a richly annotated sentence object that integrates the original text with its token-level classification into named entity categories, enabling downstream use such as extraction, display, or further analysis of recognized entities.",
            "examples": [
                {
                    "user_intent": "Identify all the named entities in the sentence 'Barack Obama visited Paris last summer.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Barack Obama visited Paris last summer."
                        }
                    }
                },
                {
                    "user_intent": "I want to extract organizations and locations mentioned in 'Apple announced a new product in New York today.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Apple announced a new product in New York today."
                        }
                    }
                },
                {
                    "user_intent": "Find all the people, places, and miscellaneous entities in 'Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 1: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 2: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.\nTool 3: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 4: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 5: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 6: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 8: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 9: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 10: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 11: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 12: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.\nTool 13: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 14: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 15: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 16: EleutherAI/gpt-j-6B is a 6-billion-parameter transformer model designed for natural language text generation. Trained on the large-scale Pile dataset, it leverages a 28-layer architecture with 16 attention heads and Rotary Position Embeddings, supporting high-quality generation across diverse tasks. It can be used via the Hugging Face Transformers library to generate coherent, context-aware text for applications such as creative writing, dialogue systems, and general-purpose language modeling.\nTool 17: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 18: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 19: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"flair/ner-english\\\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\", \"parameters\": {\"sentence\": {\"type\": \"string\", \"description\": \"The input text sentence on which the Named Entity Recognition (NER) model will perform entity tagging.\", \"required\": true, \"example\": \"George Washington went to Washington\"}}, \"response\": \"The tool\\u2019s response consists of an annotated sentence object that includes recognized named entities identified within the input text. Specifically, after the model processes the input sentence, the response encapsulates the sentence along with labeled entity spans corresponding to one of four predefined categories: PER (person names), LOC (location names), ORG (organization names), and MISC (miscellaneous names).\\n\\nEach entity span in the response includes at least the following components:\\n\\n- The text segment from the sentence identified as an entity.\\n- The entity label indicating the type of named entity (PER, LOC, ORG, or MISC).\\n- The position or span indices within the sentence marking where the entity occurs.\\n- A confidence score or probability reflecting the model\\u2019s certainty about the prediction (typically included in Flair\\u2019s entity spans).\\n\\nOverall, the response structure is a richly annotated sentence object that integrates the original text with its token-level classification into named entity categories, enabling downstream use such as extraction, display, or further analysis of recognized entities.\", \"examples\": [{\"user_intent\": \"Identify all the named entities in the sentence 'Barack Obama visited Paris last summer.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Barack Obama visited Paris last summer.\"}}}, {\"user_intent\": \"I want to extract organizations and locations mentioned in 'Apple announced a new product in New York today.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Apple announced a new product in New York today.\"}}}, {\"user_intent\": \"Find all the people, places, and miscellaneous entities in 'Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.\"}}}]}\n\n# User Query\nI am writing an article on the history of technology companies, and I want to extract the names of companies and people mentioned in the text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract all company names mentioned in my text about technology history.",
                    "tool_description": "A token classification model for identifying and extracting company names from English text using a distilBERT-based architecture trained on a company dataset.",
                    "expected_response": "A list of identified company name entities, including their text spans, positions in the text, and confidence scores."
                },
                {
                    "user_intent": "Extract all people mentioned in my text about technology history.",
                    "tool_description": "A Named Entity Recognition (NER) model designed to identify person names (PER) in English text using Flair embeddings and an LSTM-CRF architecture.",
                    "expected_response": "A list of identified person name entities, including their text spans, positions in the text, and confidence scores."
                }
            ],
            "extracted_arguments": {
                "text": "The full English-language text content containing mentions of people and company names."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_432",
        "query": "A soccer simulation company wants to use a reinforcement learning agent that can play SoccerTwos effectively.",
        "instruction": "Given a `reinforcement learning agent deployment` task, retrieve tools that facilitate training and deploying reinforcement learning agents using Unity ML-Agents, specifically for playing games like SoccerTwos, by utilizing configuration files and managing the execution environment to ensure smooth functioning.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Unity ML-Agents",
                    "functionality": "Train and play SoccerTwos",
                    "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'",
                    "api_arguments": [
                        "your_configuration_file_path.yaml",
                        "run_id"
                    ],
                    "python_environment_requirements": [
                        "ml-agents"
                    ],
                    "example_code": "mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume",
                    "performance": {
                        "dataset": "SoccerTwos",
                        "accuracy": "Not provided"
                    },
                    "description": "A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.",
                    "id": "huggingface_api_900",
                    "name": "Raiden-1001/poca-Soccerv7.1"
                },
                "id": "gorilla_huggingface_tool_893",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 1: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 2: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.",
            "Tool 3: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.",
            "Tool 4: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.",
            "Tool 5: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 6: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "Tool 7: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 8: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.",
            "Tool 9: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.",
            "Tool 10: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.",
            "Tool 11: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.",
            "Tool 12: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.",
            "Tool 13: This tool provides a pre-trained TD3 reinforcement learning agent for the Ant-v3 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load, evaluate, or continue training a high-performing agent without starting from scratch, leveraging a framework that includes optimized hyperparameters and pre-trained models for efficient experimentation and benchmarking in continuous control tasks.",
            "Tool 14: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.",
            "Tool 15: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.",
            "Tool 16: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.",
            "Tool 17: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.",
            "Tool 18: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.",
            "Tool 19: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset."
        ],
        "tool_view": {
            "description": "A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "parameters": {
                "your_configuration_file_path.yaml": {
                    "type": "string",
                    "description": "The file path to your YAML configuration file that defines the training parameters for the SoccerTwos environment.",
                    "required": true,
                    "example": "./configs/soccer_config.yaml"
                },
                "run_id": {
                    "type": "string",
                    "description": "A unique identifier for this specific training run or session, used to track and resume experiments.",
                    "required": true,
                    "example": "run_01"
                }
            },
            "response": "The tool does not explicitly define a structured response format in the provided documentation. Based on its functionality, a reasonable and typical response would include information related to the loading, training, or execution of the SoccerTwos agent using Unity ML-Agents. The response can be described as follows:  \n\n- **Status/Confirmation**: Indicates whether the model was successfully loaded from the specified repository or whether training/play execution has started successfully. It may include success or error messages.  \n\n- **Model Information**: Summarizes details about the loaded agent, such as the model name, run ID, configuration used, and possibly metadata from the repository.  \n\n- **Training/Simulation Metrics** (if training is invoked): Reports performance indicators such as episode rewards, cumulative reward curves, number of steps, episode lengths, or other reinforcement learning statistics.  \n\n- **Logs/Progress Updates**: Provides sequential logging of events during model execution, including environment initialization, agent interactions, and any warnings or errors encountered.  \n\n- **Output Artifacts** (optional): References to files or directories created during training or simulation, such as checkpoints, summaries, or TensorBoard logs.  \n\n- **Error Messages** (if applicable): Describes issues encountered during loading, configuration parsing, or execution, including missing dependencies or misconfigured parameters.  \n\nOverall, the response serves as feedback on the status and progress of using the specified trained model for SoccerTwos, including operational, performance, and diagnostic information.",
            "examples": [
                {
                    "user_intent": "I want to train the SoccerTwos agent using my custom configuration and track this training session.",
                    "tool_call": {
                        "name": "Raiden-1001/poca-Soccerv7.1",
                        "arguments": {
                            "your_configuration_file_path.yaml": "./configs/soccertwos_train.yaml",
                            "run_id": "session_001"
                        }
                    }
                },
                {
                    "user_intent": "Load the pre-trained SoccerTwos model to test it locally without retraining.",
                    "tool_call": {
                        "name": "Raiden-1001/poca-Soccerv7.1",
                        "arguments": {
                            "your_configuration_file_path.yaml": "./configs/soccertwos_test.yaml",
                            "run_id": "eval_run"
                        }
                    }
                },
                {
                    "user_intent": "Resume a previously interrupted training of the SoccerTwos agent using my existing run ID.",
                    "tool_call": {
                        "name": "Raiden-1001/poca-Soccerv7.1",
                        "arguments": {
                            "your_configuration_file_path.yaml": "./configs/soccertwos_resume.yaml",
                            "run_id": "session_001"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 1: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 2: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.\nTool 3: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.\nTool 4: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.\nTool 5: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 6: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\nTool 7: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 8: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.\nTool 9: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.\nTool 10: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.\nTool 11: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.\nTool 12: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.\nTool 13: This tool provides a pre-trained TD3 reinforcement learning agent for the Ant-v3 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load, evaluate, or continue training a high-performing agent without starting from scratch, leveraging a framework that includes optimized hyperparameters and pre-trained models for efficient experimentation and benchmarking in continuous control tasks.\nTool 14: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.\nTool 15: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.\nTool 16: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.\nTool 17: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.\nTool 18: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.\nTool 19: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\", \"parameters\": {\"your_configuration_file_path.yaml\": {\"type\": \"string\", \"description\": \"The file path to your YAML configuration file that defines the training parameters for the SoccerTwos environment.\", \"required\": true, \"example\": \"./configs/soccer_config.yaml\"}, \"run_id\": {\"type\": \"string\", \"description\": \"A unique identifier for this specific training run or session, used to track and resume experiments.\", \"required\": true, \"example\": \"run_01\"}}, \"response\": \"The tool does not explicitly define a structured response format in the provided documentation. Based on its functionality, a reasonable and typical response would include information related to the loading, training, or execution of the SoccerTwos agent using Unity ML-Agents. The response can be described as follows:  \\n\\n- **Status/Confirmation**: Indicates whether the model was successfully loaded from the specified repository or whether training/play execution has started successfully. It may include success or error messages.  \\n\\n- **Model Information**: Summarizes details about the loaded agent, such as the model name, run ID, configuration used, and possibly metadata from the repository.  \\n\\n- **Training/Simulation Metrics** (if training is invoked): Reports performance indicators such as episode rewards, cumulative reward curves, number of steps, episode lengths, or other reinforcement learning statistics.  \\n\\n- **Logs/Progress Updates**: Provides sequential logging of events during model execution, including environment initialization, agent interactions, and any warnings or errors encountered.  \\n\\n- **Output Artifacts** (optional): References to files or directories created during training or simulation, such as checkpoints, summaries, or TensorBoard logs.  \\n\\n- **Error Messages** (if applicable): Describes issues encountered during loading, configuration parsing, or execution, including missing dependencies or misconfigured parameters.  \\n\\nOverall, the response serves as feedback on the status and progress of using the specified trained model for SoccerTwos, including operational, performance, and diagnostic information.\", \"examples\": [{\"user_intent\": \"I want to train the SoccerTwos agent using my custom configuration and track this training session.\", \"tool_call\": {\"name\": \"Raiden-1001/poca-Soccerv7.1\", \"arguments\": {\"your_configuration_file_path.yaml\": \"./configs/soccertwos_train.yaml\", \"run_id\": \"session_001\"}}}, {\"user_intent\": \"Load the pre-trained SoccerTwos model to test it locally without retraining.\", \"tool_call\": {\"name\": \"Raiden-1001/poca-Soccerv7.1\", \"arguments\": {\"your_configuration_file_path.yaml\": \"./configs/soccertwos_test.yaml\", \"run_id\": \"eval_run\"}}}, {\"user_intent\": \"Resume a previously interrupted training of the SoccerTwos agent using my existing run ID.\", \"tool_call\": {\"name\": \"Raiden-1001/poca-Soccerv7.1\", \"arguments\": {\"your_configuration_file_path.yaml\": \"./configs/soccertwos_resume.yaml\", \"run_id\": \"session_001\"}}}]}\n\n# User Query\nA soccer simulation company wants to use a reinforcement learning agent that can play SoccerTwos effectively.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained SoccerTwos agent to evaluate its performance in a soccer simulation.",
                    "tool_description": "Load and run a pre-trained POCA reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, enabling testing or evaluation of its gameplay without additional training.",
                    "expected_response": "Status confirmation of the model load, metadata about the agent, and logs of the simulation performance."
                },
                {
                    "user_intent": "Train or fine-tune the SoccerTwos agent using custom parameters to optimize its performance.",
                    "tool_description": "Initiate training of a SoccerTwos agent using a specified YAML configuration file and run ID, allowing the reinforcement learning model to learn and improve through simulated gameplay.",
                    "expected_response": "Training metrics including episode rewards, cumulative reward curves, episode lengths, logs of environment interactions, and optional output artifacts such as checkpoints or summaries."
                }
            ],
            "extracted_arguments": {
                "your_configuration_file_path.yaml": "The file path to a YAML configuration file defining training parameters and environment settings for the SoccerTwos agent.",
                "run_id": "A unique identifier for this training or evaluation session to track progress and resume experiments if needed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_313",
        "query": "Provide the code to enhance a single audio track, possibly containing dialogue, music and background noise, extracted from a video game.",
        "instruction": "Given an `audio enhancement` task, retrieve tools that enhance audio tracks by processing audio inputs and improving sound quality, specifically for dialogue, music, and background noise scenarios.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "AutoModelForAudioToAudio.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')",
                    "api_arguments": "pretrained_model_name_or_path",
                    "python_environment_requirements": [
                        "transformers",
                        "asteroid"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri1Mix",
                        "accuracy": {
                            "si_sdr": 13.329767398333798,
                            "si_sdr_imp": 9.879986092474098,
                            "sdr": 13.87279932997016,
                            "sdr_imp": 10.370136530757103,
                            "sir": "Infinity",
                            "sir_imp": "NaN",
                            "sar": 13.87279932997016,
                            "sar_imp": 10.370136530757103,
                            "stoi": 0.9140907015623948,
                            "stoi_imp": 0.11817087802185405
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.",
                    "id": "huggingface_api_776",
                    "name": "DCCRNet_Libri1Mix_enhsingle_16k"
                },
                "id": "gorilla_huggingface_tool_771",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "Tool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 2: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.",
            "Tool 3: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 4: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 5: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 7: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 8: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.",
            "Tool 9: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 10: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 11: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 12: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 13: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.",
            "Tool 14: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 15: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes.",
            "Tool 16: DCCRNet_Libri1Mix_enhsingle_16k is an audio-to-audio model built with Hugging Face Transformers and Asteroid, designed for speech enhancement. Trained on the Libri1Mix dataset using the enh_single task, it improves speech quality by separating and enhancing single-speaker audio, achieving high performance on standard metrics such as SI-SDR and STOI.",
            "Tool 17: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 18: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 19: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency."
        ],
        "tool_view": {
            "description": "DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "parameters": {
                "audio": {
                    "type": "array",
                    "description": "An array representing the input audio waveform to be processed by the model. The audio should be preprocessed to match the model's expected sample rate of 16 kHz.",
                    "required": true,
                    "example": [
                        0.0,
                        0.1,
                        -0.1,
                        0.05,
                        -0.05
                    ]
                }
            },
            "response": "The tool produces an audio-to-audio output representing the enhanced or separated version of the input audio. Specifically, given an input audio signal, the model applies source separation or speech enhancement to produce a corresponding output audio signal with reduced interference or noise.  \n\nThe response can be described as follows:  \n\n- **Type**: Audio waveform or array  \n- **Format**: Typically a numeric array or tensor representing the waveform samples of the processed audio. The sampling rate matches the model’s training configuration (e.g., 16 kHz for this model).  \n- **Content**:  \n  - The same temporal length as the input audio (or slightly adjusted due to internal processing).  \n  - Enhanced or separated audio where target sources (e.g., speech) are more isolated, and noise or interfering sources are reduced.  \n- **Semantic role**: Each element of the array corresponds to an audio sample amplitude at a specific time point, representing the model’s attempt to reconstruct clean speech from the input mixture.  \n\nNo additional metadata is explicitly returned by the tool; the primary output is the processed audio waveform itself, suitable for playback or further audio processing tasks.",
            "examples": [
                {
                    "user_intent": "I want to use a pre-trained model to enhance audio. Can I try the DCUNet model for that?",
                    "tool_call": {
                        "name": "DCUNet_Libri1Mix_enhsingle_16k",
                        "arguments": {
                            "audio": "path_to_audio_file.wav"
                        }
                    }
                },
                {
                    "user_intent": "How can I apply audio enhancement using the DCUNet model from the Hugging Face Transformers framework?",
                    "tool_call": {
                        "name": "DCUNet_Libri1Mix_enhsingle_16k",
                        "arguments": {
                            "audio": "audio_input_data"
                        }
                    }
                },
                {
                    "user_intent": "I need to enhance a speech recording using a pre-trained model. Can I use the DCUNet model for this task?",
                    "tool_call": {
                        "name": "DCUNet_Libri1Mix_enhsingle_16k",
                        "arguments": {
                            "audio": "speech_recording.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\nTool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 2: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.\nTool 3: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 4: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 5: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 7: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 8: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.\nTool 9: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 10: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 11: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 12: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 13: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.\nTool 14: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 15: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes.\nTool 16: DCCRNet_Libri1Mix_enhsingle_16k is an audio-to-audio model built with Hugging Face Transformers and Asteroid, designed for speech enhancement. Trained on the Libri1Mix dataset using the enh_single task, it improves speech quality by separating and enhancing single-speaker audio, achieving high performance on standard metrics such as SI-SDR and STOI.\nTool 17: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 18: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 19: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\", \"parameters\": {\"audio\": {\"type\": \"array\", \"description\": \"An array representing the input audio waveform to be processed by the model. The audio should be preprocessed to match the model's expected sample rate of 16 kHz.\", \"required\": true, \"example\": [0.0, 0.1, -0.1, 0.05, -0.05]}}, \"response\": \"The tool produces an audio-to-audio output representing the enhanced or separated version of the input audio. Specifically, given an input audio signal, the model applies source separation or speech enhancement to produce a corresponding output audio signal with reduced interference or noise.  \\n\\nThe response can be described as follows:  \\n\\n- **Type**: Audio waveform or array  \\n- **Format**: Typically a numeric array or tensor representing the waveform samples of the processed audio. The sampling rate matches the model\\u2019s training configuration (e.g., 16 kHz for this model).  \\n- **Content**:  \\n  - The same temporal length as the input audio (or slightly adjusted due to internal processing).  \\n  - Enhanced or separated audio where target sources (e.g., speech) are more isolated, and noise or interfering sources are reduced.  \\n- **Semantic role**: Each element of the array corresponds to an audio sample amplitude at a specific time point, representing the model\\u2019s attempt to reconstruct clean speech from the input mixture.  \\n\\nNo additional metadata is explicitly returned by the tool; the primary output is the processed audio waveform itself, suitable for playback or further audio processing tasks.\", \"examples\": [{\"user_intent\": \"I want to use a pre-trained model to enhance audio. Can I try the DCUNet model for that?\", \"tool_call\": {\"name\": \"DCUNet_Libri1Mix_enhsingle_16k\", \"arguments\": {\"audio\": \"path_to_audio_file.wav\"}}}, {\"user_intent\": \"How can I apply audio enhancement using the DCUNet model from the Hugging Face Transformers framework?\", \"tool_call\": {\"name\": \"DCUNet_Libri1Mix_enhsingle_16k\", \"arguments\": {\"audio\": \"audio_input_data\"}}}, {\"user_intent\": \"I need to enhance a speech recording using a pre-trained model. Can I use the DCUNet model for this task?\", \"tool_call\": {\"name\": \"DCUNet_Libri1Mix_enhsingle_16k\", \"arguments\": {\"audio\": \"speech_recording.wav\"}}}]}\n\n# User Query\nProvide the code to enhance a single audio track, possibly containing dialogue, music and background noise, extracted from a video game.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to enhance the quality of a mixed audio track containing dialogue, music, and background noise.",
                    "tool_description": "Enhance a single audio track containing multiple sources (e.g., dialogue, music, and background noise) using a pre-trained model for speech enhancement or source separation.",
                    "expected_response": "An enhanced version of the input audio with improved clarity and separation of dialogue from other sources, such as music and background noise."
                }
            ],
            "extracted_arguments": {
                "audio": "The audio waveform representing the mixed audio track, typically preprocessed to match the model's expected sample rate."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_177",
        "query": "Assist us in creating a question answering system to provide quick answers for customer inquiries.",
        "instruction": "Given a `question answering system development` task, retrieve tools that leverage natural language processing to create systems capable of providing quick and accurate answers to customer inquiries by utilizing models fine-tuned on relevant datasets.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='philschmid/distilbert-onnx')",
                    "api_arguments": {
                        "model": "philschmid/distilbert-onnx"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "onnx"
                    ],
                    "example_code": {
                        "Compute": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})"
                    },
                    "performance": {
                        "dataset": "squad",
                        "accuracy": "F1 score: 87.1"
                    },
                    "description": "This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.",
                    "id": "huggingface_api_475",
                    "name": "philschmid/distilbert-onnx"
                },
                "id": "gorilla_huggingface_tool_471",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.",
            "Tool 1: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 2: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 4: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 5: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 6: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 7: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 8: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 9: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 10: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 11: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 12: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 13: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 14: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 15: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 17: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 18: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information."
        ],
        "tool_view": {
            "description": "A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path to the input image that will be used for visual question answering.",
                    "required": true,
                    "example": "path/to/image.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The natural language question related to the input image that the model should answer.",
                    "required": true,
                    "example": "What is the color of the car in the image?"
                }
            },
            "response": "The tool is designed for Visual Question Answering (VQA) using the VILT framework, which combines vision and language tasks. Given an input image and a question, the model processes the visual content of the image along with the textual query to provide an answer.\n\nSince the specific response format is not defined in the provided documentation, a reasonable and typical response from this tool would include:\n\n1. **Answer**: A textual string representing the answer to the question based on the analysis of the image and the provided query.\n\n2. **Confidence/Probability (optional)**: An optional field indicating the model's confidence in the generated answer. This could be a score between 0 and 1, reflecting how likely the answer is correct according to the model.\n\n3. **Question**: The original question posed to the model, for context and validation of the response.\n\n4. **Image ID (optional)**: An identifier or reference to the input image, especially if the model processes multiple images in one batch or if additional context is needed for verification.\n\nThe primary role of this tool is to provide an answer based on both visual and textual information, and the response would typically include the most relevant answer to the query, possibly along with confidence or contextual data.",
            "examples": [
                {
                    "user_intent": "I have a photo of a cat sitting on a sofa. Can you tell me what animal is in the picture?",
                    "tool_call": {
                        "name": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering",
                        "arguments": {
                            "image": "images/cat_sofa.jpg",
                            "question": "What animal is in this picture?"
                        }
                    }
                },
                {
                    "user_intent": "I took a picture of my breakfast. I want to know if it contains any fruits.",
                    "tool_call": {
                        "name": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering",
                        "arguments": {
                            "image": "images/breakfast_plate.png",
                            "question": "Does this meal contain any fruits?"
                        }
                    }
                },
                {
                    "user_intent": "I have a photo of a busy street. Can you tell me how many vehicles are visible?",
                    "tool_call": {
                        "name": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering",
                        "arguments": {
                            "image": "images/busy_street.jpeg",
                            "question": "How many vehicles are in this image?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.\nTool 1: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 2: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 4: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 5: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 6: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 7: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 8: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 9: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 10: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 11: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 12: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 13: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 14: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 15: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 17: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 18: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path to the input image that will be used for visual question answering.\", \"required\": true, \"example\": \"path/to/image.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The natural language question related to the input image that the model should answer.\", \"required\": true, \"example\": \"What is the color of the car in the image?\"}}, \"response\": \"The tool is designed for Visual Question Answering (VQA) using the VILT framework, which combines vision and language tasks. Given an input image and a question, the model processes the visual content of the image along with the textual query to provide an answer.\\n\\nSince the specific response format is not defined in the provided documentation, a reasonable and typical response from this tool would include:\\n\\n1. **Answer**: A textual string representing the answer to the question based on the analysis of the image and the provided query.\\n\\n2. **Confidence/Probability (optional)**: An optional field indicating the model's confidence in the generated answer. This could be a score between 0 and 1, reflecting how likely the answer is correct according to the model.\\n\\n3. **Question**: The original question posed to the model, for context and validation of the response.\\n\\n4. **Image ID (optional)**: An identifier or reference to the input image, especially if the model processes multiple images in one batch or if additional context is needed for verification.\\n\\nThe primary role of this tool is to provide an answer based on both visual and textual information, and the response would typically include the most relevant answer to the query, possibly along with confidence or contextual data.\", \"examples\": [{\"user_intent\": \"I have a photo of a cat sitting on a sofa. Can you tell me what animal is in the picture?\", \"tool_call\": {\"name\": \"hf-tiny-model-private/tiny-random-ViltForQuestionAnswering\", \"arguments\": {\"image\": \"images/cat_sofa.jpg\", \"question\": \"What animal is in this picture?\"}}}, {\"user_intent\": \"I took a picture of my breakfast. I want to know if it contains any fruits.\", \"tool_call\": {\"name\": \"hf-tiny-model-private/tiny-random-ViltForQuestionAnswering\", \"arguments\": {\"image\": \"images/breakfast_plate.png\", \"question\": \"Does this meal contain any fruits?\"}}}, {\"user_intent\": \"I have a photo of a busy street. Can you tell me how many vehicles are visible?\", \"tool_call\": {\"name\": \"hf-tiny-model-private/tiny-random-ViltForQuestionAnswering\", \"arguments\": {\"image\": \"images/busy_street.jpeg\", \"question\": \"How many vehicles are in this image?\"}}}]}\n\n# User Query\nAssist us in creating a question answering system to provide quick answers for customer inquiries.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to build a system that can answer customer inquiries quickly using text context.",
                    "tool_description": "Deploy a transformer-based question answering model that takes a natural language question and a text context (such as FAQs, manuals, or knowledge base entries) to generate accurate answers.",
                    "expected_response": "A textual answer extracted or inferred from the provided context, optionally including a confidence score or reference to the source context."
                },
                {
                    "user_intent": "Optionally, enhance the system to answer questions from images or documents submitted by customers.",
                    "tool_description": "Use a Visual Question Answering (VQA) model or document question answering model to interpret images or scanned documents and provide answers to user questions based on visual content or document layout.",
                    "expected_response": "A textual answer derived from the image or document content, potentially with context metadata such as page number, section, or confidence score."
                }
            ],
            "extracted_arguments": {
                "question": "The natural language question or inquiry submitted by the customer.",
                "context_text": "The textual content (FAQ, knowledge base, or documentation) that provides information to answer the question.",
                "context_image": "Optional image input containing visual information relevant to the question.",
                "context_document": "Optional document input (scanned or digital) containing structured or unstructured content relevant to the question."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_308",
        "query": "We are a city planning department and want to evaluate the city layout. Analyze the image we provide to segment and understand the various urban elements.",
        "instruction": "Given an `urban layout analysis` task, retrieve tools that perform image segmentation to process and analyze urban elements by utilizing techniques like semantic segmentation to identify and categorize different components within a cityscape image.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Semantic Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')",
                    "api_arguments": {
                        "images": "image",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits",
                    "performance": {
                        "dataset": "CityScapes",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.",
                    "id": "huggingface_api_249",
                    "name": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024"
                },
                "id": "gorilla_huggingface_tool_245",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 1: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 2: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 3: A Table Transformer (DETR) model designed for detecting structural elements in tables, such as rows and columns, using the Hugging Face Transformers framework. It is trained on the PubTables1M dataset and can be used via the object-detection pipeline to analyze and extract table layouts from images.",
            "Tool 4: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 5: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 6: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 7: This tool provides a ControlNet model conditioned on image segmentation, enabling precise control of Stable Diffusion outputs using segmentation maps. It is designed for generating images that adhere to specific structural layouts and is trained on a large dataset of segmentation-image pairs, making it useful for tasks that require structured or layout-aware image synthesis.",
            "Tool 8: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 9: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.",
            "Tool 10: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 11: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 12: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 13: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 14: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 15: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 16: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 17: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 18: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.",
            "Tool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information."
        ],
        "tool_view": {
            "description": "A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "parameters": {},
            "response": "The tool produces answers to questions posed about the content of a document, using both textual and layout information from the document. The typical response structure of the tool would include the following components:\n\n1. **Answer Text** – A string representing the predicted answer to the input question based on the document content. This is the primary output of the model.\n\n2. **Score / Confidence (optional)** – A numeric value or probability indicating the model’s confidence in the predicted answer. While not always explicitly returned, many implementations of `LayoutLMv2ForQuestionAnswering` provide a confidence score associated with the answer span.\n\n3. **Answer Span (optional)** – Information about the position of the answer in the document, such as start and end indices corresponding to tokens in the processed input. This allows tracing back the answer to the exact location in the document.\n\n4. **Supporting Metadata (optional)** – Additional fields may include token-level predictions, bounding boxes for text regions in the document, or other layout-related annotations used to derive the answer.\n\nIn essence, the response conveys the predicted answer, optionally accompanied by confidence metrics and positional or layout information linking the answer back to the document structure.",
            "examples": [
                {
                    "user_intent": "I have a scanned invoice PDF and I want to know the total amount billed.",
                    "tool_call": {
                        "name": "dperales/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "document_path": "documents/invoice_2025_01.pdf",
                            "question": "What is the total amount billed?"
                        }
                    }
                },
                {
                    "user_intent": "Check the employee handbook and tell me the company's leave policy.",
                    "tool_call": {
                        "name": "dperales/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "document_path": "documents/employee_handbook.pdf",
                            "question": "What is the company's leave policy?"
                        }
                    }
                },
                {
                    "user_intent": "I want to extract the expiration date from a scanned passport document.",
                    "tool_call": {
                        "name": "dperales/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "document_path": "documents/passport_scan.jpg",
                            "question": "What is the expiration date of the passport?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 1: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 2: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 3: A Table Transformer (DETR) model designed for detecting structural elements in tables, such as rows and columns, using the Hugging Face Transformers framework. It is trained on the PubTables1M dataset and can be used via the object-detection pipeline to analyze and extract table layouts from images.\nTool 4: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 5: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 6: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 7: This tool provides a ControlNet model conditioned on image segmentation, enabling precise control of Stable Diffusion outputs using segmentation maps. It is designed for generating images that adhere to specific structural layouts and is trained on a large dataset of segmentation-image pairs, making it useful for tasks that require structured or layout-aware image synthesis.\nTool 8: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 9: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.\nTool 10: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 11: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 12: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 13: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 14: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 15: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 16: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 17: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 18: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.\nTool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\", \"parameters\": {}, \"response\": \"The tool produces answers to questions posed about the content of a document, using both textual and layout information from the document. The typical response structure of the tool would include the following components:\\n\\n1. **Answer Text** \\u2013 A string representing the predicted answer to the input question based on the document content. This is the primary output of the model.\\n\\n2. **Score / Confidence (optional)** \\u2013 A numeric value or probability indicating the model\\u2019s confidence in the predicted answer. While not always explicitly returned, many implementations of `LayoutLMv2ForQuestionAnswering` provide a confidence score associated with the answer span.\\n\\n3. **Answer Span (optional)** \\u2013 Information about the position of the answer in the document, such as start and end indices corresponding to tokens in the processed input. This allows tracing back the answer to the exact location in the document.\\n\\n4. **Supporting Metadata (optional)** \\u2013 Additional fields may include token-level predictions, bounding boxes for text regions in the document, or other layout-related annotations used to derive the answer.\\n\\nIn essence, the response conveys the predicted answer, optionally accompanied by confidence metrics and positional or layout information linking the answer back to the document structure.\", \"examples\": [{\"user_intent\": \"I have a scanned invoice PDF and I want to know the total amount billed.\", \"tool_call\": {\"name\": \"dperales/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"document_path\": \"documents/invoice_2025_01.pdf\", \"question\": \"What is the total amount billed?\"}}}, {\"user_intent\": \"Check the employee handbook and tell me the company's leave policy.\", \"tool_call\": {\"name\": \"dperales/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"document_path\": \"documents/employee_handbook.pdf\", \"question\": \"What is the company's leave policy?\"}}}, {\"user_intent\": \"I want to extract the expiration date from a scanned passport document.\", \"tool_call\": {\"name\": \"dperales/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"document_path\": \"documents/passport_scan.jpg\", \"question\": \"What is the expiration date of the passport?\"}}}]}\n\n# User Query\nWe are a city planning department and want to evaluate the city layout. Analyze the image we provide to segment and understand the various urban elements.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze an urban scene image to identify and segment different city elements such as roads, buildings, and pedestrians.",
                    "tool_description": "Perform semantic image segmentation on urban scenes using a transformer-based model fine-tuned on cityscape datasets, labeling each pixel according to its class to extract structured scene information.",
                    "expected_response": "A segmented image or mask where each pixel is labeled with its corresponding urban element, along with a mapping of labels to element classes."
                }
            ],
            "extracted_arguments": {
                "image_input": "The image file of the urban area to be analyzed, used as input for semantic segmentation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_6",
        "query": "You are an Indian in Telugu pandit teaching kids how to pronounce conventional holy prayers.. Teach me mantras in Telugu synthesized by human like voice pronunciation.",
        "instruction": "Given a `text-to-speech synthesis` task, retrieve tools that convert textual representations of Telugu mantras into audio outputs with natural and human-like pronunciation, tailored to the query's language and cultural context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "pipeline('text-to-speech', model='SYSPIN/Telugu_Male_TTS')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.",
                    "id": "huggingface_api_734",
                    "name": "SYSPIN/Telugu_Male_TTS"
                },
                "id": "gorilla_huggingface_tool_729",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 1: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 2: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 3: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 4: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 5: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 6: A Text-to-Speech tool using the ESPnet framework, trained on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.",
            "Tool 7: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 8: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 9: A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.",
            "Tool 10: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 11: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 12: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 13: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.",
            "Tool 14: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.",
            "Tool 15: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 16: Generate human-like text using the Facebook OPT-1.3B model, a pre-trained transformer designed for natural language generation tasks, suitable for creating coherent continuations of input text or evaluating downstream NLP tasks.",
            "Tool 17: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 18: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 19: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue."
        ],
        "tool_view": {
            "description": "Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in Telugu that you want the model to convert into speech audio.",
                    "required": true,
                    "example": "హలో, మీరు ఎలా ఉన్నారు?"
                }
            },
            "response": "The tool produces a synthesized audio output corresponding to the input text in Telugu, using a male voice. The response is typically a structured object containing at least the following components:  \n\n1. **Audio Data**: This is the primary output representing the spoken version of the input text. It may be encoded as raw waveform data, a byte stream, or a file reference (e.g., WAV or MP3 format).  \n\n2. **Sampling Rate / Audio Metadata** (optional but common): Information about the audio such as sampling rate, number of channels, duration, or format, which is necessary for correctly playing or processing the generated audio.  \n\n3. **Input Text Echo** (optional): Some implementations may include the original input text for reference or verification purposes.  \n\nIn summary, the response encapsulates the conversion of the provided Telugu text into audible speech, typically providing both the audio data and, optionally, metadata about the generated audio.",
            "examples": [
                {
                    "user_intent": "I want to convert some text into Telugu speech using a male voice.",
                    "tool_call": {
                        "name": "SYSPIN/Telugu_Male_TTS",
                        "arguments": {
                            "text": "Hello, how are you today?"
                        }
                    }
                },
                {
                    "user_intent": "Convert the following sentence into speech with a Telugu male voice: 'Welcome to our service!'",
                    "tool_call": {
                        "name": "SYSPIN/Telugu_Male_TTS",
                        "arguments": {
                            "text": "Welcome to our service!"
                        }
                    }
                },
                {
                    "user_intent": "Can you convert this Telugu sentence into speech? 'మీరు ఎలా ఉన్నారు?'",
                    "tool_call": {
                        "name": "SYSPIN/Telugu_Male_TTS",
                        "arguments": {
                            "text": "మీరు ఎలా ఉన్నారు?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 1: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 2: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 3: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 4: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 5: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 6: A Text-to-Speech tool using the ESPnet framework, trained on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.\nTool 7: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 8: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 9: A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.\nTool 10: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 11: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 12: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 13: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.\nTool 14: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.\nTool 15: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 16: Generate human-like text using the Facebook OPT-1.3B model, a pre-trained transformer designed for natural language generation tasks, suitable for creating coherent continuations of input text or evaluating downstream NLP tasks.\nTool 17: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 18: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 19: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in Telugu that you want the model to convert into speech audio.\", \"required\": true, \"example\": \"\\u0c39\\u0c32\\u0c4b, \\u0c2e\\u0c40\\u0c30\\u0c41 \\u0c0e\\u0c32\\u0c3e \\u0c09\\u0c28\\u0c4d\\u0c28\\u0c3e\\u0c30\\u0c41?\"}}, \"response\": \"The tool produces a synthesized audio output corresponding to the input text in Telugu, using a male voice. The response is typically a structured object containing at least the following components:  \\n\\n1. **Audio Data**: This is the primary output representing the spoken version of the input text. It may be encoded as raw waveform data, a byte stream, or a file reference (e.g., WAV or MP3 format).  \\n\\n2. **Sampling Rate / Audio Metadata** (optional but common): Information about the audio such as sampling rate, number of channels, duration, or format, which is necessary for correctly playing or processing the generated audio.  \\n\\n3. **Input Text Echo** (optional): Some implementations may include the original input text for reference or verification purposes.  \\n\\nIn summary, the response encapsulates the conversion of the provided Telugu text into audible speech, typically providing both the audio data and, optionally, metadata about the generated audio.\", \"examples\": [{\"user_intent\": \"I want to convert some text into Telugu speech using a male voice.\", \"tool_call\": {\"name\": \"SYSPIN/Telugu_Male_TTS\", \"arguments\": {\"text\": \"Hello, how are you today?\"}}}, {\"user_intent\": \"Convert the following sentence into speech with a Telugu male voice: 'Welcome to our service!'\", \"tool_call\": {\"name\": \"SYSPIN/Telugu_Male_TTS\", \"arguments\": {\"text\": \"Welcome to our service!\"}}}, {\"user_intent\": \"Can you convert this Telugu sentence into speech? '\\u0c2e\\u0c40\\u0c30\\u0c41 \\u0c0e\\u0c32\\u0c3e \\u0c09\\u0c28\\u0c4d\\u0c28\\u0c3e\\u0c30\\u0c41?'\", \"tool_call\": {\"name\": \"SYSPIN/Telugu_Male_TTS\", \"arguments\": {\"text\": \"\\u0c2e\\u0c40\\u0c30\\u0c41 \\u0c0e\\u0c32\\u0c3e \\u0c09\\u0c28\\u0c4d\\u0c28\\u0c3e\\u0c30\\u0c41?\"}}}]}\n\n# User Query\nYou are an Indian in Telugu pandit teaching kids how to pronounce conventional holy prayers.. Teach me mantras in Telugu synthesized by human like voice pronunciation.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to learn how to pronounce Telugu mantras with human-like synthesized speech.",
                    "tool_description": "Convert written Telugu text of traditional mantras into natural-sounding speech using a male voice, leveraging a high-quality text-to-speech synthesis framework.",
                    "expected_response": "A synthesized audio output of the Telugu mantras with a human-like male voice, optionally including audio metadata such as sampling rate and duration."
                }
            ],
            "extracted_arguments": {
                "text": "The Telugu mantra or prayer text that needs to be converted into speech."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_105",
        "query": "There is a news article stating, \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU.\" We need to determine which category this article should fall under.",
        "instruction": "Given a `category classification` task, retrieve tools that perform text classification by analyzing the content and matching it to predefined categories using natural language processing techniques, specifically zero-shot classification models.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')",
                    "api_arguments": {
                        "sequence_to_classify": "Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU",
                        "candidate_labels": [
                            "politics",
                            "economy",
                            "entertainment",
                            "environment"
                        ],
                        "multi_label": false
                    },
                    "python_environment_requirements": [
                        "transformers==4.13"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)\nsequence_to_classify = Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\ncandidate_labels = [politics, economy, entertainment, environment]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "MultiNLI-matched",
                                "accuracy": 0.857
                            },
                            {
                                "name": "MultiNLI-mismatched",
                                "accuracy": 0.856
                            },
                            {
                                "name": "ANLI-all",
                                "accuracy": 0.537
                            },
                            {
                                "name": "ANLI-r3",
                                "accuracy": 0.497
                            },
                            {
                                "name": "WANLI",
                                "accuracy": 0.732
                            },
                            {
                                "name": "LingNLI",
                                "accuracy": 0.788
                            },
                            {
                                "name": "fever-nli",
                                "accuracy": 0.761
                            }
                        ]
                    },
                    "description": "This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people.",
                    "id": "huggingface_api_512",
                    "name": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"
                },
                "id": "gorilla_huggingface_tool_508",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 1: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 2: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 3: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 4: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 5: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 6: This tool uses the ConvNeXT large model from Hugging Face to perform image classification, allowing users to predict the category of an input image. Built as a modernized convolutional neural network inspired by Vision Transformers, it is designed for high-performance visual recognition tasks and can be applied to datasets like ImageNet or custom image collections.",
            "Tool 7: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 9: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 10: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 11: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 12: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 13: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 14: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 15: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 16: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 17: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 18: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 19: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt."
        ],
        "tool_view": {
            "description": "A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in Italian that you want the model to summarize. This should be a coherent piece of text such as a news article or a paragraph.",
                    "required": true,
                    "example": "Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di una sorta di Netflix della cultura."
                }
            },
            "response": "The tool produces a concise summary of the input Italian text provided in the `text` argument. The response is a string representing the condensed version of the original content, preserving the main points and essential information while omitting less critical details.  \n\nSpecifically:  \n\n- **Type**: Text string.  \n- **Content**: A shortened version of the input text, maintaining semantic coherence, grammatical correctness, and the key information or narrative of the original.  \n- **Purpose**: To enable quick understanding of long or detailed Italian news articles by distilling them into a few sentences or a brief paragraph.  \n- **Constraints**: The summary is generated in Italian, reflecting the source language, and is typically aligned with the style and factual content of the original input.  \n\nNo additional metadata or structured fields are returned; the output is solely the generated summarized text.",
            "examples": [
                {
                    "user_intent": "Can you provide a concise summary of this Italian news article about the launch of ITsART?",
                    "tool_call": {
                        "name": "it5-base-news-summarization",
                        "arguments": {
                            "text": "Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente."
                        }
                    }
                },
                {
                    "user_intent": "Summarize this Italian article about recent cultural events for a newsletter.",
                    "tool_call": {
                        "name": "it5-base-news-summarization",
                        "arguments": {
                            "text": "Il ministero della Cultura ha annunciato una nuova serie di eventi culturali in tutta Italia, inclusi spettacoli teatrali e mostre d’arte contemporanea. L’iniziativa mira a promuovere la partecipazione dei giovani e a valorizzare le produzioni locali, con particolare attenzione alle regioni meno servite dai grandi eventi. Gli organizzatori sperano che la rassegna stimoli un aumento del turismo culturale durante i prossimi mesi."
                        }
                    }
                },
                {
                    "user_intent": "Create a brief summary of this Italian news text for a mobile app notification.",
                    "tool_call": {
                        "name": "it5-base-news-summarization",
                        "arguments": {
                            "text": "Secondo le ultime statistiche, la piattaforma ITsART ha superato i 100.000 abbonati nel primo mese di attività. Il catalogo comprende film, documentari e spettacoli teatrali, molti dei quali accessibili solo a pagamento, mentre alcune produzioni sono disponibili gratuitamente su altre piattaforme. L’iniziativa vuole offrire una vetrina internazionale per la cultura italiana."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 1: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 2: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 3: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 4: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 5: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 6: This tool uses the ConvNeXT large model from Hugging Face to perform image classification, allowing users to predict the category of an input image. Built as a modernized convolutional neural network inspired by Vision Transformers, it is designed for high-performance visual recognition tasks and can be applied to datasets like ImageNet or custom image collections.\nTool 7: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 9: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 10: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 11: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 12: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 13: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 14: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 15: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 16: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 17: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 18: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 19: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in Italian that you want the model to summarize. This should be a coherent piece of text such as a news article or a paragraph.\", \"required\": true, \"example\": \"Dal 31 maggio \\u00e8 infine partita la piattaforma ITsART, a pi\\u00f9 di un anno da quando \\u2013 durante il primo lockdown \\u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di una sorta di Netflix della cultura.\"}}, \"response\": \"The tool produces a concise summary of the input Italian text provided in the `text` argument. The response is a string representing the condensed version of the original content, preserving the main points and essential information while omitting less critical details.  \\n\\nSpecifically:  \\n\\n- **Type**: Text string.  \\n- **Content**: A shortened version of the input text, maintaining semantic coherence, grammatical correctness, and the key information or narrative of the original.  \\n- **Purpose**: To enable quick understanding of long or detailed Italian news articles by distilling them into a few sentences or a brief paragraph.  \\n- **Constraints**: The summary is generated in Italian, reflecting the source language, and is typically aligned with the style and factual content of the original input.  \\n\\nNo additional metadata or structured fields are returned; the output is solely the generated summarized text.\", \"examples\": [{\"user_intent\": \"Can you provide a concise summary of this Italian news article about the launch of ITsART?\", \"tool_call\": {\"name\": \"it5-base-news-summarization\", \"arguments\": {\"text\": \"Dal 31 maggio \\u00e8 infine partita la piattaforma ITsART, a pi\\u00f9 di un anno da quando \\u2013 durante il primo lockdown \\u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \\u00abuna sorta di Netflix della cultura\\u00bb, pensata per \\u00aboffrire a tutto il mondo la cultura italiana a pagamento\\u00bb. \\u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\\u00e0 difficile farlo anche pi\\u00f9 avanti senza numeri precisi. Al momento, l\\u2019unica cosa che si pu\\u00f2 fare \\u00e8 guardare com\\u2019\\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \\u201ctitoli\\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\\u00e0. Intanto, una cosa notata da pi\\u00f9 parti \\u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.\"}}}, {\"user_intent\": \"Summarize this Italian article about recent cultural events for a newsletter.\", \"tool_call\": {\"name\": \"it5-base-news-summarization\", \"arguments\": {\"text\": \"Il ministero della Cultura ha annunciato una nuova serie di eventi culturali in tutta Italia, inclusi spettacoli teatrali e mostre d\\u2019arte contemporanea. L\\u2019iniziativa mira a promuovere la partecipazione dei giovani e a valorizzare le produzioni locali, con particolare attenzione alle regioni meno servite dai grandi eventi. Gli organizzatori sperano che la rassegna stimoli un aumento del turismo culturale durante i prossimi mesi.\"}}}, {\"user_intent\": \"Create a brief summary of this Italian news text for a mobile app notification.\", \"tool_call\": {\"name\": \"it5-base-news-summarization\", \"arguments\": {\"text\": \"Secondo le ultime statistiche, la piattaforma ITsART ha superato i 100.000 abbonati nel primo mese di attivit\\u00e0. Il catalogo comprende film, documentari e spettacoli teatrali, molti dei quali accessibili solo a pagamento, mentre alcune produzioni sono disponibili gratuitamente su altre piattaforme. L\\u2019iniziativa vuole offrire una vetrina internazionale per la cultura italiana.\"}}}]}\n\n# User Query\nThere is a news article stating, \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU.\" We need to determine which category this article should fall under.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify a short news article about Angela Merkel into a predefined category.",
                    "tool_description": "Perform text classification on the input news article to determine its relevant topic or category, such as politics, sports, entertainment, etc.",
                    "expected_response": "A categorical label indicating the topic of the article, e.g., 'politics'."
                }
            ],
            "extracted_arguments": {
                "text": "The content of the news article to be classified, provided as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_45",
        "query": "We are a content marketing agency and we are focusing on promoting our clients' products. We want to use a tool to generate interesting marketing messages.",
        "instruction": "Given a `marketing message generation` task, retrieve tools that support natural language processing to generate compelling and creative marketing content by processing text inputs, utilizing advanced models such as BART for effective text generation and manipulation aligned with the query's goals.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BartModel.from_pretrained('facebook/bart-large')",
                    "api_arguments": {
                        "pretrained_model_name": "facebook/bart-large"
                    },
                    "python_environment_requirements": {
                        "library": "transformers",
                        "version": "latest"
                    },
                    "example_code": "from transformers import BartTokenizer, BartModel\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\nmodel = BartModel.from_pretrained('facebook/bart-large')\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "arxiv",
                        "accuracy": "Not provided"
                    },
                    "description": "BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).",
                    "id": "huggingface_api_5",
                    "name": "facebook/bart-large"
                },
                "id": "gorilla_huggingface_tool_5",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 1: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 2: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.",
            "Tool 3: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.",
            "Tool 4: A natural language processing tool that uses a fine-tuned PEGASUS model to generate paraphrases of input text, allowing multiple alternative phrasings to be produced for tasks like rewording, content variation, or text simplification.",
            "Tool 5: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 6: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 7: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 8: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 9: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 10: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.",
            "Tool 11: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 12: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.",
            "Tool 13: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 14: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.",
            "Tool 15: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.",
            "Tool 16: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.",
            "Tool 17: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.",
            "Tool 18: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.",
            "Tool 19: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language."
        ],
        "tool_view": {
            "description": "This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "parameters": {
                "sentence": {
                    "type": "string",
                    "description": "The input sentence or text that will be processed by the Named Entity Recognition model to identify named entities.",
                    "required": true,
                    "example": "On September 1st George won 1 dollar while watching Game of Thrones."
                }
            },
            "response": "The tool returns a **sequence of named entity predictions** for a given input sentence. The response is typically structured around **spans of text identified as entities**, with each span annotated with its **predicted entity type**, **position in the text**, and **confidence score**. Specifically, the response contains the following components:\n\n1. **Sentence object** – Represents the input text after processing. It contains the original text and all annotations made by the model.\n\n2. **Entity spans** – A collection of identified entities within the sentence. Each entity span typically includes:\n   - **Text span**: The substring of the sentence corresponding to the entity.\n   - **Entity type**: The predicted named entity category, one of the 18 Ontonotes classes (e.g., PERSON, ORGANIZATION, LOCATION, DATE, MONEY, EVENT, WORK_OF_ART, etc.).\n   - **Start and end positions**: Character or token indices indicating where the entity occurs in the sentence.\n   - **Confidence score**: A numeric value representing the model’s confidence in the predicted entity label.\n\n3. **Additional metadata** (optional) – Depending on the implementation, the sentence object may also include token-level embeddings, token-level labels, or other linguistic annotations, but the primary structured output focuses on the entity spans.\n\nThe response is designed for easy iteration over entities, allowing downstream applications to extract, filter, or analyze entities by type, span, or confidence.",
            "examples": [
                {
                    "user_intent": "Identify all named entities in the sentence about George winning money while watching a TV show.",
                    "tool_call": {
                        "name": "flair/ner-english-ontonotes-large",
                        "arguments": {
                            "sentence": "On September 1st George won 1 dollar while watching Game of Thrones."
                        }
                    }
                },
                {
                    "user_intent": "Extract people, organizations, and locations from a news article snippet.",
                    "tool_call": {
                        "name": "flair/ner-english-ontonotes-large",
                        "arguments": {
                            "sentence": "Apple Inc. announced a partnership with the University of Cambridge to develop AI technologies in London."
                        }
                    }
                },
                {
                    "user_intent": "Find dates, amounts, and event names in a financial report sentence.",
                    "tool_call": {
                        "name": "flair/ner-english-ontonotes-large",
                        "arguments": {
                            "sentence": "On March 15th, the company paid $5 million as a settlement for the 2024 Tech Innovation Awards."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 1: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 2: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.\nTool 3: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.\nTool 4: A natural language processing tool that uses a fine-tuned PEGASUS model to generate paraphrases of input text, allowing multiple alternative phrasings to be produced for tasks like rewording, content variation, or text simplification.\nTool 5: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 6: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 7: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 8: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 9: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 10: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\nTool 11: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 12: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.\nTool 13: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 14: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.\nTool 15: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.\nTool 16: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\nTool 17: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.\nTool 18: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.\nTool 19: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities\\u2014including dates, monetary amounts, organizations, locations, persons, events, products, and works of art\\u2014within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\", \"parameters\": {\"sentence\": {\"type\": \"string\", \"description\": \"The input sentence or text that will be processed by the Named Entity Recognition model to identify named entities.\", \"required\": true, \"example\": \"On September 1st George won 1 dollar while watching Game of Thrones.\"}}, \"response\": \"The tool returns a **sequence of named entity predictions** for a given input sentence. The response is typically structured around **spans of text identified as entities**, with each span annotated with its **predicted entity type**, **position in the text**, and **confidence score**. Specifically, the response contains the following components:\\n\\n1. **Sentence object** \\u2013 Represents the input text after processing. It contains the original text and all annotations made by the model.\\n\\n2. **Entity spans** \\u2013 A collection of identified entities within the sentence. Each entity span typically includes:\\n   - **Text span**: The substring of the sentence corresponding to the entity.\\n   - **Entity type**: The predicted named entity category, one of the 18 Ontonotes classes (e.g., PERSON, ORGANIZATION, LOCATION, DATE, MONEY, EVENT, WORK_OF_ART, etc.).\\n   - **Start and end positions**: Character or token indices indicating where the entity occurs in the sentence.\\n   - **Confidence score**: A numeric value representing the model\\u2019s confidence in the predicted entity label.\\n\\n3. **Additional metadata** (optional) \\u2013 Depending on the implementation, the sentence object may also include token-level embeddings, token-level labels, or other linguistic annotations, but the primary structured output focuses on the entity spans.\\n\\nThe response is designed for easy iteration over entities, allowing downstream applications to extract, filter, or analyze entities by type, span, or confidence.\", \"examples\": [{\"user_intent\": \"Identify all named entities in the sentence about George winning money while watching a TV show.\", \"tool_call\": {\"name\": \"flair/ner-english-ontonotes-large\", \"arguments\": {\"sentence\": \"On September 1st George won 1 dollar while watching Game of Thrones.\"}}}, {\"user_intent\": \"Extract people, organizations, and locations from a news article snippet.\", \"tool_call\": {\"name\": \"flair/ner-english-ontonotes-large\", \"arguments\": {\"sentence\": \"Apple Inc. announced a partnership with the University of Cambridge to develop AI technologies in London.\"}}}, {\"user_intent\": \"Find dates, amounts, and event names in a financial report sentence.\", \"tool_call\": {\"name\": \"flair/ner-english-ontonotes-large\", \"arguments\": {\"sentence\": \"On March 15th, the company paid $5 million as a settlement for the 2024 Tech Innovation Awards.\"}}}]}\n\n# User Query\nWe are a content marketing agency and we are focusing on promoting our clients' products. We want to use a tool to generate interesting marketing messages.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate engaging marketing messages for promoting client products.",
                    "tool_description": "Produce contextually relevant and creative text content based on input prompts describing client products, using a pre-trained text generation model.",
                    "expected_response": "A set of generated text outputs, each containing a complete marketing message or copy suitable for promotion."
                }
            ],
            "extracted_arguments": {
                "product_description": "A textual description of the client's product or service to be promoted, serving as the input prompt for text generation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_318",
        "query": "Help me to translate a Spanish text to English. The Spanish text is: \"Lo siento, pero no puedo ir a la reunión debido a una emergencia personal. Avisaré al equipo y nos pondremos en contacto para reprogramar la reunión.\"",
        "instruction": "Given a `language translation` task, retrieve tools that perform text translation by processing Spanish text inputs and providing accurate English translations aligned with the query's content and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\ntranslation = pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')('Hola, ¿cómo estás?')",
                    "performance": {
                        "dataset": [
                            {
                                "name": "newssyscomb2009-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 30.6,
                                    "chr-F": 0.57
                                }
                            },
                            {
                                "name": "news-test2008-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 27.9,
                                    "chr-F": 0.553
                                }
                            },
                            {
                                "name": "newstest2009-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 30.4,
                                    "chr-F": 0.572
                                }
                            },
                            {
                                "name": "newstest2010-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 36.1,
                                    "chr-F": 0.614
                                }
                            },
                            {
                                "name": "newstest2011-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 34.2,
                                    "chr-F": 0.599
                                }
                            },
                            {
                                "name": "newstest2012-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 37.9,
                                    "chr-F": 0.624
                                }
                            },
                            {
                                "name": "newstest2013-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 35.3,
                                    "chr-F": 0.609
                                }
                            },
                            {
                                "name": "Tatoeba-test.spa.eng",
                                "accuracy": {
                                    "BLEU": 59.6,
                                    "chr-F": 0.739
                                }
                            }
                        ]
                    },
                    "description": "Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset.",
                    "id": "huggingface_api_530",
                    "name": "Helsinki-NLP/opus-mt-es-en"
                },
                "id": "gorilla_huggingface_tool_526",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.",
            "Tool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 2: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 3: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 4: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 5: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 6: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 7: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 8: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 9: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 10: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 11: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 12: A Spanish text summarization tool based on BERT2BERT (BETO) fine-tuned on the MLSUM dataset, which generates concise summaries of input text using a Hugging Face Transformers sequence-to-sequence model.",
            "Tool 13: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 15: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.",
            "Tool 16: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 17: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 18: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 19: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing."
        ],
        "tool_view": {
            "description": "Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The Spanish text string that you want to translate into English using the Helsinki-NLP/opus-mt-es-en model.",
                    "required": true,
                    "example": "Hola, ¿cómo estás?"
                }
            },
            "response": "The tool’s response is a translation output produced by the `Helsinki-NLP/opus-mt-es-en` model when given Spanish text as input. The response is typically structured as a list of dictionaries, where each dictionary corresponds to a translation of an input text segment. Each dictionary generally contains the following component:  \n\n- **`translation_text`**: A string representing the translated text in English. This is the primary output of the model, containing the semantic and grammatical rendering of the original Spanish input.  \n\nSince the pipeline is designed for single-text or batch translation, the response may contain one dictionary for a single input string or multiple dictionaries if a list of texts is provided. There are no additional fields by default; the response focuses solely on the translated content.",
            "examples": [
                {
                    "user_intent": "Translate a friendly Spanish greeting into English.",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-es-en",
                        "arguments": {
                            "text": "Hola, ¿cómo estás?"
                        }
                    }
                },
                {
                    "user_intent": "Convert this Spanish news headline into English.",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-es-en",
                        "arguments": {
                            "text": "El presidente firmó un nuevo acuerdo comercial con Europa."
                        }
                    }
                },
                {
                    "user_intent": "I have a Spanish paragraph from a novel; translate it to English.",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-es-en",
                        "arguments": {
                            "text": "Era una noche oscura y tormentosa, y el viento azotaba las ventanas con fuerza."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.\nTool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 2: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 3: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 4: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 5: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 6: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 7: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 8: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 9: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 10: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 11: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 12: A Spanish text summarization tool based on BERT2BERT (BETO) fine-tuned on the MLSUM dataset, which generates concise summaries of input text using a Hugging Face Transformers sequence-to-sequence model.\nTool 13: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 15: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.\nTool 16: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 17: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 18: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 19: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The Spanish text string that you want to translate into English using the Helsinki-NLP/opus-mt-es-en model.\", \"required\": true, \"example\": \"Hola, \\u00bfc\\u00f3mo est\\u00e1s?\"}}, \"response\": \"The tool\\u2019s response is a translation output produced by the `Helsinki-NLP/opus-mt-es-en` model when given Spanish text as input. The response is typically structured as a list of dictionaries, where each dictionary corresponds to a translation of an input text segment. Each dictionary generally contains the following component:  \\n\\n- **`translation_text`**: A string representing the translated text in English. This is the primary output of the model, containing the semantic and grammatical rendering of the original Spanish input.  \\n\\nSince the pipeline is designed for single-text or batch translation, the response may contain one dictionary for a single input string or multiple dictionaries if a list of texts is provided. There are no additional fields by default; the response focuses solely on the translated content.\", \"examples\": [{\"user_intent\": \"Translate a friendly Spanish greeting into English.\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-es-en\", \"arguments\": {\"text\": \"Hola, \\u00bfc\\u00f3mo est\\u00e1s?\"}}}, {\"user_intent\": \"Convert this Spanish news headline into English.\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-es-en\", \"arguments\": {\"text\": \"El presidente firm\\u00f3 un nuevo acuerdo comercial con Europa.\"}}}, {\"user_intent\": \"I have a Spanish paragraph from a novel; translate it to English.\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-es-en\", \"arguments\": {\"text\": \"Era una noche oscura y tormentosa, y el viento azotaba las ventanas con fuerza.\"}}}]}\n\n# User Query\nHelp me to translate a Spanish text to English. The Spanish text is: \"Lo siento, pero no puedo ir a la reunión debido a una emergencia personal. Avisaré al equipo y nos pondremos en contacto para reprogramar la reunión.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate a Spanish message about a personal emergency into English.",
                    "tool_description": "Convert Spanish text into English using a machine translation model based on the Helsinki-NLP/opus-mt-es-en Marian framework trained on the OPUS dataset.",
                    "expected_response": "A translated English version of the original Spanish text."
                }
            ],
            "extracted_arguments": {
                "text": "The Spanish text string that needs to be translated into English."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_60",
        "query": "I'm developing a game that needs more Minecraft skins for some characters. How can I generete them with this model?",
        "instruction": "Given an `image generation` task for character customization, retrieve tools that utilize diffusion models to generate Minecraft skin images by processing model-specific parameters and techniques to create unique visual content aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Diffusers",
                    "api_call": "DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\nimage = pipeline().images[0].convert('RGBA')\nimage",
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "An unconditional image generation model for generating Minecraft skin images using the diffusion model.",
                    "id": "huggingface_api_302",
                    "name": "WiNE-iNEFF/Minecraft-Skin-Diffusion-V2"
                },
                "id": "gorilla_huggingface_tool_298",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 1: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 2: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 3: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 5: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 6: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 7: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 8: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 9: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 10: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 11: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 12: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.",
            "Tool 13: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 14: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 15: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 16: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 17: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.",
            "Tool 18: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 19: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content."
        ],
        "tool_view": {
            "description": "A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "parameters": {
                "api_arguments": {
                    "type": "array",
                    "description": "A list of arguments to pass to the model pipeline. This tool does not require any arguments for execution.",
                    "required": true,
                    "example": "[]"
                }
            },
            "response": "The tool produces an image generated by an unconditional diffusion model specifically trained to create Minecraft skin images. The typical response structure is as follows:\n\n- **images**: A list or collection containing one or more generated images. Each element in this list represents a single image output from the model.  \n  - Each image is usually represented as an image object compatible with standard Python image libraries (e.g., a PIL `Image` object).  \n  - The images can be in various modes such as RGB or RGBA, depending on post-processing.  \n\n- **additional metadata (optional, depending on the implementation)**: While not explicitly defined in the JSON, the pipeline may include optional fields such as generation parameters, model configuration, or inference details. However, the core content is the generated image itself.  \n\nIn essence, the response primarily delivers the visual content (Minecraft skin images) as a standard image object, suitable for immediate display, saving, or further manipulation in Python.",
            "examples": [
                {
                    "user_intent": "Generate a Minecraft skin using the WiNE-iNEFF model.",
                    "tool_call": {
                        "name": "WiNE-iNEFF/Minecraft-Skin-Diffusion-V2",
                        "arguments": []
                    }
                },
                {
                    "user_intent": "I want to create a random Minecraft character skin using a diffusion model.",
                    "tool_call": {
                        "name": "WiNE-iNEFF/Minecraft-Skin-Diffusion-V2",
                        "arguments": []
                    }
                },
                {
                    "user_intent": "Use the Minecraft-Skin-Diffusion-V2 model to generate a new Minecraft skin.",
                    "tool_call": {
                        "name": "WiNE-iNEFF/Minecraft-Skin-Diffusion-V2",
                        "arguments": []
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 1: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 2: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 3: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 5: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 6: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 7: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 8: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 9: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 10: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 11: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 12: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.\nTool 13: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 14: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 15: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 16: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 17: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.\nTool 18: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 19: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\", \"parameters\": {\"api_arguments\": {\"type\": \"array\", \"description\": \"A list of arguments to pass to the model pipeline. This tool does not require any arguments for execution.\", \"required\": true, \"example\": \"[]\"}}, \"response\": \"The tool produces an image generated by an unconditional diffusion model specifically trained to create Minecraft skin images. The typical response structure is as follows:\\n\\n- **images**: A list or collection containing one or more generated images. Each element in this list represents a single image output from the model.  \\n  - Each image is usually represented as an image object compatible with standard Python image libraries (e.g., a PIL `Image` object).  \\n  - The images can be in various modes such as RGB or RGBA, depending on post-processing.  \\n\\n- **additional metadata (optional, depending on the implementation)**: While not explicitly defined in the JSON, the pipeline may include optional fields such as generation parameters, model configuration, or inference details. However, the core content is the generated image itself.  \\n\\nIn essence, the response primarily delivers the visual content (Minecraft skin images) as a standard image object, suitable for immediate display, saving, or further manipulation in Python.\", \"examples\": [{\"user_intent\": \"Generate a Minecraft skin using the WiNE-iNEFF model.\", \"tool_call\": {\"name\": \"WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\", \"arguments\": []}}, {\"user_intent\": \"I want to create a random Minecraft character skin using a diffusion model.\", \"tool_call\": {\"name\": \"WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\", \"arguments\": []}}, {\"user_intent\": \"Use the Minecraft-Skin-Diffusion-V2 model to generate a new Minecraft skin.\", \"tool_call\": {\"name\": \"WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\", \"arguments\": []}}]}\n\n# User Query\nI'm developing a game that needs more Minecraft skins for some characters. How can I generete them with this model?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate Minecraft skins for characters in my game.",
                    "tool_description": "Generate unique Minecraft skin images using a diffusion-based model that doesn't require any input prompts.",
                    "expected_response": "A list of generated Minecraft skin images, each represented as an image object."
                }
            ],
            "extracted_arguments": {
                "api_arguments": "An empty list of arguments, as the model does not require any input prompts for generating the Minecraft skins."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_198",
        "query": "I have a picture of a room demonstrating a mixture of objects. The model needs to seperate the objects and label them accordingly.",
        "instruction": "Given an `image segmentation and labeling` task, retrieve tools that utilize computer vision algorithms to process image inputs, separate objects within the image, and assign appropriate labels to each object based on their characteristics.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')",
                    "api_arguments": {
                        "from_pretrained": "facebook/maskformer-swin-large-ade"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": "from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\nurl = 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-large-ade')\ninputs = processor(images=image, return_tensors='pt')\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\noutputs = model(**inputs)\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]",
                    "performance": {
                        "dataset": "ADE20k",
                        "accuracy": "Not provided"
                    },
                    "description": "MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.",
                    "id": "huggingface_api_243",
                    "name": "facebook/maskformer-swin-large-ade"
                },
                "id": "gorilla_huggingface_tool_239",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 1: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 2: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 3: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 4: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 5: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 6: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 7: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 8: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.",
            "Tool 9: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 10: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 11: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.",
            "Tool 12: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 13: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.",
            "Tool 14: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 15: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 16: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 17: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 18: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.",
            "Tool 19: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data."
        ],
        "tool_view": {
            "description": "A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "parameters": {
                "images": {
                    "type": "image",
                    "description": "The input image or list of images to be segmented by the model. Each image should be in a format compatible with PIL or a similar image-handling library.",
                    "required": true,
                    "example": "Image.open('example.jpg')"
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors to return after preprocessing. Typically set to 'pt' for PyTorch tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool performs semantic image segmentation using the SegFormer model, specifically the fine-tuned version of SegFormer (model name: \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\") trained on the CityScapes dataset.\n\n### Expected Response Structure:\n\n1. **Segmentation Output (Logits)**:\n   - The response includes the raw output logits of the model after processing the input image. These logits represent the predicted segmentation classes for each pixel in the input image.\n   - The logits are typically returned as a tensor, with dimensions corresponding to the number of pixels in the image and the number of segmentation classes.\n   - The logits can be used to derive the final segmentation map, which assigns a class to each pixel.\n\n2. **Tensor Type**:\n   - The logits tensor is returned in PyTorch format (`pt`), as indicated by the `return_tensors` argument set to `\"pt\"` in the API call.\n   - This means the output tensor is in a format compatible with PyTorch-based processing and further model handling.\n\n3. **Image and Segmentation Context**:\n   - The response does not directly provide the input image or a visual representation of the segmented image. However, the logits can be processed to generate a segmentation map that can be visualized.\n   - The response would focus on the model's output in terms of logits, which represent probabilities for each pixel belonging to various predefined classes (such as buildings, roads, cars, etc.).\n\n4. **Performance Metrics (Optional)**:\n   - While not explicitly provided in the response, performance metrics such as accuracy may be available as part of model documentation or evaluation, but are not part of the segmentation response itself.\n\n### General Meaning of Components:\n- **Logits**: These are the raw, unnormalized predictions made by the model for each pixel, which can later be passed through a softmax function or thresholding technique to assign class labels.\n- **Tensor**: The logits are in tensor form, which is a multi-dimensional array used in PyTorch, representing the model's predictions for the segmentation task.\n",
            "examples": [
                {
                    "user_intent": "I want to segment the streets, cars, and pedestrians in a cityscape image I downloaded.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "city_street.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this traffic camera snapshot to identify roads, vehicles, and sidewalks.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "traffic_snapshot.png",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Segment a drone image of a city to detect buildings and roads for urban planning.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "drone_city_view.jpeg",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 1: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 2: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 3: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 4: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 5: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 6: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 7: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 8: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.\nTool 9: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 10: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 11: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.\nTool 12: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 13: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.\nTool 14: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 15: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 16: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 17: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 18: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.\nTool 19: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024\\u00d71024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\", \"parameters\": {\"images\": {\"type\": \"image\", \"description\": \"The input image or list of images to be segmented by the model. Each image should be in a format compatible with PIL or a similar image-handling library.\", \"required\": true, \"example\": \"Image.open('example.jpg')\"}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors to return after preprocessing. Typically set to 'pt' for PyTorch tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool performs semantic image segmentation using the SegFormer model, specifically the fine-tuned version of SegFormer (model name: \\\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\\\") trained on the CityScapes dataset.\\n\\n### Expected Response Structure:\\n\\n1. **Segmentation Output (Logits)**:\\n   - The response includes the raw output logits of the model after processing the input image. These logits represent the predicted segmentation classes for each pixel in the input image.\\n   - The logits are typically returned as a tensor, with dimensions corresponding to the number of pixels in the image and the number of segmentation classes.\\n   - The logits can be used to derive the final segmentation map, which assigns a class to each pixel.\\n\\n2. **Tensor Type**:\\n   - The logits tensor is returned in PyTorch format (`pt`), as indicated by the `return_tensors` argument set to `\\\"pt\\\"` in the API call.\\n   - This means the output tensor is in a format compatible with PyTorch-based processing and further model handling.\\n\\n3. **Image and Segmentation Context**:\\n   - The response does not directly provide the input image or a visual representation of the segmented image. However, the logits can be processed to generate a segmentation map that can be visualized.\\n   - The response would focus on the model's output in terms of logits, which represent probabilities for each pixel belonging to various predefined classes (such as buildings, roads, cars, etc.).\\n\\n4. **Performance Metrics (Optional)**:\\n   - While not explicitly provided in the response, performance metrics such as accuracy may be available as part of model documentation or evaluation, but are not part of the segmentation response itself.\\n\\n### General Meaning of Components:\\n- **Logits**: These are the raw, unnormalized predictions made by the model for each pixel, which can later be passed through a softmax function or thresholding technique to assign class labels.\\n- **Tensor**: The logits are in tensor form, which is a multi-dimensional array used in PyTorch, representing the model's predictions for the segmentation task.\\n\", \"examples\": [{\"user_intent\": \"I want to segment the streets, cars, and pedestrians in a cityscape image I downloaded.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"city_street.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Analyze this traffic camera snapshot to identify roads, vehicles, and sidewalks.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"traffic_snapshot.png\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Segment a drone image of a city to detect buildings and roads for urban planning.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"drone_city_view.jpeg\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nI have a picture of a room demonstrating a mixture of objects. The model needs to seperate the objects and label them accordingly.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to segment all individual objects in a room image and label each one.",
                    "tool_description": "Perform instance or panoptic segmentation on the input image to identify, separate, and label each object using a transformer-based segmentation model.",
                    "expected_response": "A set of per-object masks with corresponding class labels, allowing visualization or further processing of each detected object."
                }
            ],
            "extracted_arguments": {
                "images": "The input image of the room containing multiple objects, compatible with standard image libraries like PIL.",
                "return_tensors": "The format for the output tensors, e.g., 'pt' for PyTorch tensors."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_104",
        "query": "Extract a conclusion from the following text: \"Studies have been shown that owning a dog is good for you. Having a dog can help decrease stress levels, improve your mood, and increase physical activity.\"",
        "instruction": "Given a `text summarization` task, retrieve tools that can process text inputs to generate concise conclusions, capturing the essence of the provided content in alignment with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Translation, Summarization, Question Answering, Text Classification",
                    "api_call": "T5Model.from_pretrained('t5-base')",
                    "api_arguments": [
                        "input_ids",
                        "decoder_input_ids"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import T5Tokenizer, T5Model\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\nmodel = T5Model.from_pretrained('t5-base')\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14"
                    },
                    "description": "T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library.",
                    "id": "huggingface_api_519",
                    "name": "t5-base"
                },
                "id": "gorilla_huggingface_tool_515",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 1: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 2: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 3: `google/pegasus-large` is a pre-trained model for abstractive text summarization that leverages the PEGASUS architecture. It is designed to generate concise summaries from longer text inputs, having been trained on large datasets like C4 and HugeNews and fine-tuned across multiple summarization benchmarks. The model supports a wide range of summarization tasks and delivers strong performance on datasets such as XSum, CNN/DailyMail, and BigPatent, making it useful for efficiently condensing documents while preserving key information.",
            "Tool 4: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 5: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 7: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 8: Cerebras-GPT-111M is a 111-million-parameter transformer-based language model designed for text generation and natural language processing research. Trained on the Pile dataset using a GPT-3 style architecture, it can serve as a foundation model for NLP applications, fine-tuning, and studies in ethics and alignment.",
            "Tool 9: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.",
            "Tool 10: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 11: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 12: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 13: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 14: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 15: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 16: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 17: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.",
            "Tool 18: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 19: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output."
        ],
        "tool_view": {
            "description": "A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path or URL of the image to be classified by the model.",
                    "required": true,
                    "example": "images/dog.jpg"
                }
            },
            "response": "The tool returns a structured classification result for a given input image. Each response typically includes the following components:\n\n1. **Label / Class Name**: A string indicating the predicted category of the image (e.g., \"dog\" or \"food\"). This represents the model’s best guess for the image class.\n\n2. **Score / Confidence**: A numerical value, usually between 0 and 1, representing the model's confidence in the predicted label. Higher values indicate greater certainty in the classification.\n\n3. **Optional Additional Metadata**: Depending on the Hugging Face pipeline implementation, the response may include multiple predictions sorted by confidence, allowing inspection of secondary likely classes along with their respective scores.\n\nOverall, the response is a list of one or more objects, each containing a label and its associated confidence score, providing both the classification result and the model's certainty about that result.",
            "examples": [
                {
                    "user_intent": "I have a photo of my pet and I want to check if it's a dog or something else.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/photos/pet1.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Classify this picture to see if it's a meal or a dog.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/images/dinner_plate.png"
                        }
                    }
                },
                {
                    "user_intent": "I want to automatically sort my images folder into dogs and food photos.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/photos/vacation/photo_2025.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 1: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 2: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 3: `google/pegasus-large` is a pre-trained model for abstractive text summarization that leverages the PEGASUS architecture. It is designed to generate concise summaries from longer text inputs, having been trained on large datasets like C4 and HugeNews and fine-tuned across multiple summarization benchmarks. The model supports a wide range of summarization tasks and delivers strong performance on datasets such as XSum, CNN/DailyMail, and BigPatent, making it useful for efficiently condensing documents while preserving key information.\nTool 4: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 5: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 7: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 8: Cerebras-GPT-111M is a 111-million-parameter transformer-based language model designed for text generation and natural language processing research. Trained on the Pile dataset using a GPT-3 style architecture, it can serve as a foundation model for NLP applications, fine-tuning, and studies in ethics and alignment.\nTool 9: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.\nTool 10: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 11: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 12: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 13: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 14: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 15: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 16: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 17: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.\nTool 18: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 19: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path or URL of the image to be classified by the model.\", \"required\": true, \"example\": \"images/dog.jpg\"}}, \"response\": \"The tool returns a structured classification result for a given input image. Each response typically includes the following components:\\n\\n1. **Label / Class Name**: A string indicating the predicted category of the image (e.g., \\\"dog\\\" or \\\"food\\\"). This represents the model\\u2019s best guess for the image class.\\n\\n2. **Score / Confidence**: A numerical value, usually between 0 and 1, representing the model's confidence in the predicted label. Higher values indicate greater certainty in the classification.\\n\\n3. **Optional Additional Metadata**: Depending on the Hugging Face pipeline implementation, the response may include multiple predictions sorted by confidence, allowing inspection of secondary likely classes along with their respective scores.\\n\\nOverall, the response is a list of one or more objects, each containing a label and its associated confidence score, providing both the classification result and the model's certainty about that result.\", \"examples\": [{\"user_intent\": \"I have a photo of my pet and I want to check if it's a dog or something else.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/photos/pet1.jpg\"}}}, {\"user_intent\": \"Classify this picture to see if it's a meal or a dog.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/images/dinner_plate.png\"}}}, {\"user_intent\": \"I want to automatically sort my images folder into dogs and food photos.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/photos/vacation/photo_2025.jpg\"}}}]}\n\n# User Query\nExtract a conclusion from the following text: \"Studies have been shown that owning a dog is good for you. Having a dog can help decrease stress levels, improve your mood, and increase physical activity.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to summarize this text to extract the main conclusion.",
                    "tool_description": "Generate a concise summary or conclusion from a given text input, highlighting the key findings or statements.",
                    "expected_response": "A short text output capturing the main conclusion or insight from the input passage."
                }
            ],
            "extracted_arguments": {
                "input_text": "The body of text from which the conclusion should be extracted, provided as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_218",
        "query": "A movie studio needs to estimate the human pose of an actor from an image for an upcoming film project.",
        "instruction": "Given a `human pose estimation` task, retrieve tools that analyze image inputs to estimate and provide details of human poses, applicable for projects such as film production requiring accurate pose predictions from visual data.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Human Pose Estimation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                    "api_arguments": {
                        "text": "chef in the kitchen",
                        "image": "image",
                        "num_inference_steps": 20
                    },
                    "python_environment_requirements": {
                        "diffusers": "pip install diffusers",
                        "transformers": "pip install transformers",
                        "accelerate": "pip install accelerate",
                        "controlnet_aux": "pip install controlnet_aux"
                    },
                    "example_code": "from PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers.utils import load_image\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\nimage = openpose(image)\ncontrolnet = ControlNetModel.from_pretrained(\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\nimage.save('images/chef_pose_out.png')",
                    "performance": {
                        "dataset": "200k pose-image, caption pairs",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_262",
                    "name": "lllyasviel/sd-controlnet-openpose"
                },
                "id": "gorilla_huggingface_tool_258",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 1: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 2: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 4: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 5: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 6: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 7: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 8: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 9: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 10: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 11: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 12: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 13: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.",
            "Tool 14: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.",
            "Tool 15: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.",
            "Tool 16: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 17: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 18: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 19: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text."
        ],
        "tool_view": {
            "description": "This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The text prompt describing the scene or subject that guides the image generation process.",
                    "required": true,
                    "example": "chef in the kitchen"
                },
                "image": {
                    "type": "string",
                    "description": "The input image used as a reference or condition for the pose-guided image generation.",
                    "required": true,
                    "example": "image.png"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of inference steps the model should perform during image generation. Higher numbers generally improve quality but take longer.",
                    "required": true,
                    "example": 20
                }
            },
            "response": "The tool described in the raw JSON is designed for human pose estimation using a ControlNet model in combination with Stable Diffusion. The expected response from the tool would likely consist of the following components:\n\n1. **Generated Image**: \n   - The main output would be a newly generated image that incorporates the human pose specified in the input. This image is generated based on the given text prompt (e.g., \"chef in the kitchen\") and the pose derived from the provided input image.\n   - The image would typically have the human subject in the pose captured by the pose estimation model, modified or integrated into the scene described by the text prompt.\n\n2. **Inference Details**: \n   - The response might include metadata or information about the inference process. This could include the number of inference steps performed (in this case, specified as 20) and any relevant configurations or parameters used during generation (e.g., model details, device used for computation).\n\n3. **Performance Information (Optional)**: \n   - Though accuracy isn't explicitly provided, the response could potentially include the dataset used for training (e.g., \"200k pose-image, caption pairs\") or other performance-related details if available or relevant to the particular response.\n\n4. **Image Format and Saving**:\n   - The generated image would be provided in a format suitable for saving or further use (e.g., a file path or image object), as indicated by the example code where the output is saved as 'images/chef_pose_out.png'.\n\nOverall, the response will primarily focus on delivering the resulting image that reflects the input pose and text prompt, along with any relevant metadata or parameters used during the generation process.",
            "examples": [
                {
                    "user_intent": "Generate an image of a yoga instructor in a studio while keeping the exact human pose from a reference image.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "yoga instructor in a studio",
                            "image": "yoga_pose_reference.png",
                            "num_inference_steps": 25
                        }
                    }
                },
                {
                    "user_intent": "Create an illustration of a basketball player in mid-air jump, preserving the pose from my uploaded photo.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "basketball player jumping in a stadium",
                            "image": "player_jump.png",
                            "num_inference_steps": 30
                        }
                    }
                },
                {
                    "user_intent": "Produce an image of a fashion model on a runway while keeping the pose from an existing fashion shoot photo.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "fashion model walking on a runway",
                            "image": "model_pose_reference.jpg",
                            "num_inference_steps": 20
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 1: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 2: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 4: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 5: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 6: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 7: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 8: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 9: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 10: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 11: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 12: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 13: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.\nTool 14: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.\nTool 15: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.\nTool 16: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 17: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 18: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 19: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The text prompt describing the scene or subject that guides the image generation process.\", \"required\": true, \"example\": \"chef in the kitchen\"}, \"image\": {\"type\": \"string\", \"description\": \"The input image used as a reference or condition for the pose-guided image generation.\", \"required\": true, \"example\": \"image.png\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of inference steps the model should perform during image generation. Higher numbers generally improve quality but take longer.\", \"required\": true, \"example\": 20}}, \"response\": \"The tool described in the raw JSON is designed for human pose estimation using a ControlNet model in combination with Stable Diffusion. The expected response from the tool would likely consist of the following components:\\n\\n1. **Generated Image**: \\n   - The main output would be a newly generated image that incorporates the human pose specified in the input. This image is generated based on the given text prompt (e.g., \\\"chef in the kitchen\\\") and the pose derived from the provided input image.\\n   - The image would typically have the human subject in the pose captured by the pose estimation model, modified or integrated into the scene described by the text prompt.\\n\\n2. **Inference Details**: \\n   - The response might include metadata or information about the inference process. This could include the number of inference steps performed (in this case, specified as 20) and any relevant configurations or parameters used during generation (e.g., model details, device used for computation).\\n\\n3. **Performance Information (Optional)**: \\n   - Though accuracy isn't explicitly provided, the response could potentially include the dataset used for training (e.g., \\\"200k pose-image, caption pairs\\\") or other performance-related details if available or relevant to the particular response.\\n\\n4. **Image Format and Saving**:\\n   - The generated image would be provided in a format suitable for saving or further use (e.g., a file path or image object), as indicated by the example code where the output is saved as 'images/chef_pose_out.png'.\\n\\nOverall, the response will primarily focus on delivering the resulting image that reflects the input pose and text prompt, along with any relevant metadata or parameters used during the generation process.\", \"examples\": [{\"user_intent\": \"Generate an image of a yoga instructor in a studio while keeping the exact human pose from a reference image.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"yoga instructor in a studio\", \"image\": \"yoga_pose_reference.png\", \"num_inference_steps\": 25}}}, {\"user_intent\": \"Create an illustration of a basketball player in mid-air jump, preserving the pose from my uploaded photo.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"basketball player jumping in a stadium\", \"image\": \"player_jump.png\", \"num_inference_steps\": 30}}}, {\"user_intent\": \"Produce an image of a fashion model on a runway while keeping the pose from an existing fashion shoot photo.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"fashion model walking on a runway\", \"image\": \"model_pose_reference.jpg\", \"num_inference_steps\": 20}}}]}\n\n# User Query\nA movie studio needs to estimate the human pose of an actor from an image for an upcoming film project.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate the human pose of an actor from an image for a film project.",
                    "tool_description": "Use a ControlNet model specialized for human pose estimation to derive the human pose from an input image.",
                    "expected_response": "A set of pose data representing the estimated human pose, such as joint locations and body structure."
                }
            ],
            "extracted_arguments": {
                "image": "The input image used for pose estimation, typically containing the actor whose pose needs to be analyzed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_90",
        "query": "The company director needs a summary of a recent financial report. You should provide the answer of following question: What were the total revenues for the last quarter?",
        "instruction": "Given a `financial report analysis` task, retrieve tools that can extract and summarize key financial data by processing document inputs and addressing specific queries regarding financial metrics.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')",
                    "api_arguments": {
                        "question": "string",
                        "context": "string"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A LayoutLM model for document question answering.",
                    "id": "huggingface_api_135",
                    "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023"
                },
                "id": "gorilla_huggingface_tool_133",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 1: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 2: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 3: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 4: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.",
            "Tool 5: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 6: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 7: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 8: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 9: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 10: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 11: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 12: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 13: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 14: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 15: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 16: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 17: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 18: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 19: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application."
        ],
        "tool_view": {
            "description": "A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path or URL of the image on which the visual question will be asked.",
                    "required": true,
                    "example": "images/dog.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The question to be answered about the given image.",
                    "required": true,
                    "example": "What breed is the dog in the image?"
                }
            },
            "response": "The tool produces a response that provides an answer to a user-specified question about a given image. The response is typically structured as a dictionary or object containing the following components:\n\n1. **`answer`** – A string representing the model’s textual answer to the question posed about the image. This is the primary output of the tool.  \n2. **`score`** (optional) – A numeric confidence score indicating the model’s certainty in the provided answer. This may be a probability or other relevance measure.  \n3. **`question`** (optional) – The text of the input question, often echoed in the response for clarity.  \n4. **`image`** (optional) – A reference or identifier for the input image, included for traceability when multiple queries are processed.  \n\nIn essence, the response conveys the model’s interpretation of the visual content in relation to the user’s question, potentially accompanied by metadata that quantifies confidence or links back to the original inputs.",
            "examples": [
                {
                    "user_intent": "I have a photo of my living room. Can you tell me how many chairs are visible in it?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/living_room.jpg",
                            "question": "How many chairs are in this room?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this image of a street scene. What color is the car in the foreground?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/street_scene.png",
                            "question": "What is the color of the car in the foreground?"
                        }
                    }
                },
                {
                    "user_intent": "Here's a picture of a group of people at a party. Can you tell me how many people are wearing hats?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/party_group.jpeg",
                            "question": "How many people are wearing hats in this image?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 1: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 2: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 3: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 4: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.\nTool 5: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 6: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 7: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 8: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 9: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 10: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 11: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 12: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 13: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 14: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 15: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 16: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 17: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 18: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 19: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab\\u2019s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path or URL of the image on which the visual question will be asked.\", \"required\": true, \"example\": \"images/dog.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The question to be answered about the given image.\", \"required\": true, \"example\": \"What breed is the dog in the image?\"}}, \"response\": \"The tool produces a response that provides an answer to a user-specified question about a given image. The response is typically structured as a dictionary or object containing the following components:\\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s textual answer to the question posed about the image. This is the primary output of the tool.  \\n2. **`score`** (optional) \\u2013 A numeric confidence score indicating the model\\u2019s certainty in the provided answer. This may be a probability or other relevance measure.  \\n3. **`question`** (optional) \\u2013 The text of the input question, often echoed in the response for clarity.  \\n4. **`image`** (optional) \\u2013 A reference or identifier for the input image, included for traceability when multiple queries are processed.  \\n\\nIn essence, the response conveys the model\\u2019s interpretation of the visual content in relation to the user\\u2019s question, potentially accompanied by metadata that quantifies confidence or links back to the original inputs.\", \"examples\": [{\"user_intent\": \"I have a photo of my living room. Can you tell me how many chairs are visible in it?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/living_room.jpg\", \"question\": \"How many chairs are in this room?\"}}}, {\"user_intent\": \"Look at this image of a street scene. What color is the car in the foreground?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/street_scene.png\", \"question\": \"What is the color of the car in the foreground?\"}}}, {\"user_intent\": \"Here's a picture of a group of people at a party. Can you tell me how many people are wearing hats?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/party_group.jpeg\", \"question\": \"How many people are wearing hats in this image?\"}}}]}\n\n# User Query\nThe company director needs a summary of a recent financial report. You should provide the answer of following question: What were the total revenues for the last quarter?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Summarize the recent financial report to extract key information.",
                    "tool_description": "Generate a concise abstractive summary of a financial document, highlighting important metrics and insights from the text.",
                    "expected_response": "A textual summary of the financial report containing the most relevant information, including key figures and performance highlights."
                },
                {
                    "user_intent": "Extract the total revenues for the last quarter from the summarized financial report.",
                    "tool_description": "Analyze financial text or summarized content to identify and extract the total revenue figure for the specified period.",
                    "expected_response": "A numeric value or textual representation indicating the total revenue for the last quarter."
                }
            ],
            "extracted_arguments": {
                "financial_report_text": "The full text of the financial report or its digital content.",
                "report_period": "The specific time frame of the report, e.g., last quarter, represented as a date range or label."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_291",
        "query": "Write a script for an online forum moderator that will help them to detect gibberish text in a post.",
        "instruction": "Given a `gibberish text detection` task, retrieve tools that utilize natural language processing models for text classification, specifically designed to analyze textual inputs and identify gibberish content within them.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457')",
                    "api_arguments": {
                        "inputs": "I love AutoNLP"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForSequenceClassification",
                        "AutoTokenizer": "from_pretrained"
                    },
                    "example_code": "from transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "madhurjindal/autonlp-data-Gibberish-Detector",
                        "accuracy": 0.9735624586913417
                    },
                    "description": "A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.",
                    "id": "huggingface_api_398",
                    "name": "madhurjindal/autonlp-Gibberish-Detector-492513457"
                },
                "id": "gorilla_huggingface_tool_394",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a multi-class text classification model designed to detect gibberish text. It is built using AutoNLP and DistilBERT, and offers high performance, with an accuracy of 97.36% on the \"madhurjindal/autonlp-data-Gibberish-Detector\" dataset. The model can be accessed through the Hugging Face Transformers framework and is useful for identifying nonsensical or irrelevant text in natural language processing tasks.",
            "Tool 1: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 2: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 3: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 4: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 5: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 7: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 8: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 9: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 10: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 11: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 12: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 13: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 14: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 15: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 16: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 17: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 18: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 19: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding."
        ],
        "tool_view": {
            "description": "This tool is a multi-class text classification model designed to detect gibberish text. It is built using AutoNLP and DistilBERT, and offers high performance, with an accuracy of 97.36% on the \"madhurjindal/autonlp-data-Gibberish-Detector\" dataset. The model can be accessed through the Hugging Face Transformers framework and is useful for identifying nonsensical or irrelevant text in natural language processing tasks.",
            "parameters": {
                "inputs": {
                    "type": "string",
                    "description": "The text input to be classified by the gibberish detection model.",
                    "required": true,
                    "example": "I love AutoNLP"
                }
            },
            "response": "The tool is a pre-trained natural language processing (NLP) model designed for text classification. Its specific task is to detect gibberish text. Below is a description of the expected response structure and the meaning of each component:\n\n### Response Structure:\n\n1. **Predicted Class**: \n   - This field contains the predicted classification label for the given input text (e.g., whether the text is considered gibberish or not). This label is derived based on the model's output probabilities.\n\n2. **Class Probabilities**: \n   - A set of probabilities corresponding to the model's confidence in each class. Each probability represents the likelihood that the input text belongs to a particular class based on the model's classification.\n\n3. **Model Name**:\n   - The name or identifier of the model being used, which is `\"madhurjindal/autonlp-Gibberish-Detector-492513457\"` in this case. This helps to track the specific model that produced the result.\n\n4. **Inputs**:\n   - The original input text that was fed into the model (e.g., `\"I love AutoNLP\"`). This is useful for reference and to validate the classification.\n\n5. **Additional Metadata** (optional):\n   - If applicable, the response may contain additional details related to the model's performance or specific configurations used during the inference, like `tokenizer` settings or `model` settings.\n\n### General Purpose:\n\nThe expected output will help a user understand whether a given text is classified as gibberish or a valid piece of text, along with the model's confidence level for each potential classification. It provides not only the final classification but also insight into how confident the model is in making that prediction, based on its underlying training data and architecture (DistilBERT in this case).",
            "examples": [
                {
                    "user_intent": "I want to check if this sentence is gibberish: 'Hfjdksl qwoeir lkj!'.",
                    "tool_call": {
                        "name": "madhurjindal/autonlp-Gibberish-Detector-492513457",
                        "arguments": {
                            "inputs": "Hfjdksl qwoeir lkj!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this customer review and tell me if it contains any meaningless or gibberish text.",
                    "tool_call": {
                        "name": "madhurjindal/autonlp-Gibberish-Detector-492513457",
                        "arguments": {
                            "inputs": "I just love this product!!! Asdfghjkl."
                        }
                    }
                },
                {
                    "user_intent": "Detect whether the following chat message is gibberish: 'The quick brown fox jumps over the lazy dog.'",
                    "tool_call": {
                        "name": "madhurjindal/autonlp-Gibberish-Detector-492513457",
                        "arguments": {
                            "inputs": "The quick brown fox jumps over the lazy dog."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a multi-class text classification model designed to detect gibberish text. It is built using AutoNLP and DistilBERT, and offers high performance, with an accuracy of 97.36% on the \"madhurjindal/autonlp-data-Gibberish-Detector\" dataset. The model can be accessed through the Hugging Face Transformers framework and is useful for identifying nonsensical or irrelevant text in natural language processing tasks.\nTool 1: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 2: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 3: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 4: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 5: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 7: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 8: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 9: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 10: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 11: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 12: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 13: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 14: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 15: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 16: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 17: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 18: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 19: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a multi-class text classification model designed to detect gibberish text. It is built using AutoNLP and DistilBERT, and offers high performance, with an accuracy of 97.36% on the \\\"madhurjindal/autonlp-data-Gibberish-Detector\\\" dataset. The model can be accessed through the Hugging Face Transformers framework and is useful for identifying nonsensical or irrelevant text in natural language processing tasks.\", \"parameters\": {\"inputs\": {\"type\": \"string\", \"description\": \"The text input to be classified by the gibberish detection model.\", \"required\": true, \"example\": \"I love AutoNLP\"}}, \"response\": \"The tool is a pre-trained natural language processing (NLP) model designed for text classification. Its specific task is to detect gibberish text. Below is a description of the expected response structure and the meaning of each component:\\n\\n### Response Structure:\\n\\n1. **Predicted Class**: \\n   - This field contains the predicted classification label for the given input text (e.g., whether the text is considered gibberish or not). This label is derived based on the model's output probabilities.\\n\\n2. **Class Probabilities**: \\n   - A set of probabilities corresponding to the model's confidence in each class. Each probability represents the likelihood that the input text belongs to a particular class based on the model's classification.\\n\\n3. **Model Name**:\\n   - The name or identifier of the model being used, which is `\\\"madhurjindal/autonlp-Gibberish-Detector-492513457\\\"` in this case. This helps to track the specific model that produced the result.\\n\\n4. **Inputs**:\\n   - The original input text that was fed into the model (e.g., `\\\"I love AutoNLP\\\"`). This is useful for reference and to validate the classification.\\n\\n5. **Additional Metadata** (optional):\\n   - If applicable, the response may contain additional details related to the model's performance or specific configurations used during the inference, like `tokenizer` settings or `model` settings.\\n\\n### General Purpose:\\n\\nThe expected output will help a user understand whether a given text is classified as gibberish or a valid piece of text, along with the model's confidence level for each potential classification. It provides not only the final classification but also insight into how confident the model is in making that prediction, based on its underlying training data and architecture (DistilBERT in this case).\", \"examples\": [{\"user_intent\": \"I want to check if this sentence is gibberish: 'Hfjdksl qwoeir lkj!'.\", \"tool_call\": {\"name\": \"madhurjindal/autonlp-Gibberish-Detector-492513457\", \"arguments\": {\"inputs\": \"Hfjdksl qwoeir lkj!\"}}}, {\"user_intent\": \"Analyze this customer review and tell me if it contains any meaningless or gibberish text.\", \"tool_call\": {\"name\": \"madhurjindal/autonlp-Gibberish-Detector-492513457\", \"arguments\": {\"inputs\": \"I just love this product!!! Asdfghjkl.\"}}}, {\"user_intent\": \"Detect whether the following chat message is gibberish: 'The quick brown fox jumps over the lazy dog.'\", \"tool_call\": {\"name\": \"madhurjindal/autonlp-Gibberish-Detector-492513457\", \"arguments\": {\"inputs\": \"The quick brown fox jumps over the lazy dog.\"}}}]}\n\n# User Query\nWrite a script for an online forum moderator that will help them to detect gibberish text in a post.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to automatically check forum posts to see if they contain gibberish text.",
                    "tool_description": "Use a pre-trained multi-class text classification model to analyze input text and determine whether it is gibberish or meaningful content.",
                    "expected_response": "A structured response indicating the predicted class (gibberish or not), the model's confidence probabilities for each class, the model identifier, and the original input text."
                }
            ],
            "extracted_arguments": {
                "post_text": "The textual content of a forum post that needs to be evaluated for gibberish."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_63",
        "query": "A company is having an ad campaign with a superhero theme. They want to generate a video of Spiderman surfing to showcase as a social media ad.",
        "instruction": "Given a `video creation` task with a thematic and character-based focus, retrieve tools that leverage text-to-video synthesis capabilities by processing descriptive text inputs to generate video content consistent with the campaign's superhero theme.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Video Synthesis",
                    "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy')",
                    "api_arguments": [
                        "prompt",
                        "num_inference_steps"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate"
                    ],
                    "example_code": "import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)",
                    "performance": {
                        "dataset": [
                            "LAION5B",
                            "ImageNet",
                            "Webvid"
                        ],
                        "accuracy": "Not provided"
                    },
                    "description": "This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.",
                    "id": "huggingface_api_94",
                    "name": "damo-vilab/text-to-video-ms-1.7b-legacy"
                },
                "id": "gorilla_huggingface_tool_94",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 1: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 4: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 5: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 6: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 7: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 8: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 9: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 10: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 11: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 12: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 13: `google/pegasus-large` is a pre-trained model for abstractive text summarization that leverages the PEGASUS architecture. It is designed to generate concise summaries from longer text inputs, having been trained on large datasets like C4 and HugeNews and fine-tuned across multiple summarization benchmarks. The model supports a wide range of summarization tasks and delivers strong performance on datasets such as XSum, CNN/DailyMail, and BigPatent, making it useful for efficiently condensing documents while preserving key information.",
            "Tool 14: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 15: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 16: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 17: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 18: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently."
        ],
        "tool_view": {
            "description": "A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The path to the pre-trained model or model identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "cardiffnlp/twitter-roberta-base-sentiment-latest"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The path to the pre-trained tokenizer or tokenizer identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "cardiffnlp/twitter-roberta-base-sentiment-latest"
                }
            },
            "response": "The tool is designed to perform sentiment analysis on text inputs, specifically leveraging a RoBERTa-based model trained on a substantial dataset of tweets. The expected response from the tool would typically include the following components:\n\n1. **Label**: This component indicates the sentiment classification assigned to the input text. It may represent categories such as positive, negative, or neutral, reflecting the emotional tone of the analyzed text.\n\n2. **Score**: This component provides a numerical value representing the confidence level of the sentiment classification. It quantifies the model's certainty regarding the assigned label, often expressed as a probability or percentage.\n\n3. **Input Text**: This component may echo the original text that was analyzed, providing context for the sentiment classification and allowing users to verify the input against the output.\n\nOverall, the response encapsulates the sentiment analysis results, offering insights into the emotional content of the provided text along with the model's confidence in its assessment.",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this tweet: 'I love the new features in the latest smartphone update!'",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": "I love the new features in the latest smartphone update!"
                        }
                    }
                },
                {
                    "user_intent": "Check whether this tweet about the stock market is positive, negative, or neutral: 'Markets crashed today after unexpected news.'",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": "Markets crashed today after unexpected news."
                        }
                    }
                },
                {
                    "user_intent": "Determine the sentiment of multiple tweets regarding a recent movie release.",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": [
                                "Absolutely loved the new movie! Best I've seen this year.",
                                "The movie was okay, not really memorable.",
                                "Terrible film, wasted two hours of my life."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 1: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 4: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 5: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 6: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 7: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 8: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 9: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 10: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 11: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 12: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 13: `google/pegasus-large` is a pre-trained model for abstractive text summarization that leverages the PEGASUS architecture. It is designed to generate concise summaries from longer text inputs, having been trained on large datasets like C4 and HugeNews and fine-tuned across multiple summarization benchmarks. The model supports a wide range of summarization tasks and delivers strong performance on datasets such as XSum, CNN/DailyMail, and BigPatent, making it useful for efficiently condensing documents while preserving key information.\nTool 14: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 15: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 16: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 17: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 18: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The path to the pre-trained model or model identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The path to the pre-trained tokenizer or tokenizer identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}}, \"response\": \"The tool is designed to perform sentiment analysis on text inputs, specifically leveraging a RoBERTa-based model trained on a substantial dataset of tweets. The expected response from the tool would typically include the following components:\\n\\n1. **Label**: This component indicates the sentiment classification assigned to the input text. It may represent categories such as positive, negative, or neutral, reflecting the emotional tone of the analyzed text.\\n\\n2. **Score**: This component provides a numerical value representing the confidence level of the sentiment classification. It quantifies the model's certainty regarding the assigned label, often expressed as a probability or percentage.\\n\\n3. **Input Text**: This component may echo the original text that was analyzed, providing context for the sentiment classification and allowing users to verify the input against the output.\\n\\nOverall, the response encapsulates the sentiment analysis results, offering insights into the emotional content of the provided text along with the model's confidence in its assessment.\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this tweet: 'I love the new features in the latest smartphone update!'\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": \"I love the new features in the latest smartphone update!\"}}}, {\"user_intent\": \"Check whether this tweet about the stock market is positive, negative, or neutral: 'Markets crashed today after unexpected news.'\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": \"Markets crashed today after unexpected news.\"}}}, {\"user_intent\": \"Determine the sentiment of multiple tweets regarding a recent movie release.\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": [\"Absolutely loved the new movie! Best I've seen this year.\", \"The movie was okay, not really memorable.\", \"Terrible film, wasted two hours of my life.\"]}}}]}\n\n# User Query\nA company is having an ad campaign with a superhero theme. They want to generate a video of Spiderman surfing to showcase as a social media ad.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a video from a text description with a superhero theme.",
                    "tool_description": "Generate a video based on a text prompt describing a specific scene or character action.",
                    "expected_response": "A video file or a link to the generated video showcasing the scene as described in the prompt."
                }
            ],
            "extracted_arguments": {
                "text_description": "A textual description of the scene or action to be visualized in the video, including specific details like character and action.",
                "theme": "A theme that helps define the context of the video, such as superhero genre."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_288",
        "query": "I am learning English literature. I plan to build a system that has a huge database of English sentences and keeps the important ones.",
        "instruction": "Given a `sentence database` construction task, retrieve tools that assist in building and managing large collections of English sentences, with functionalities focusing on storing, indexing, and retrieving significant sentences as per specified criteria or importance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/all-distilroberta-v1')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "s2orc",
                                "accuracy": "Not provided"
                            },
                            {
                                "name": "MS Marco",
                                "accuracy": "Not provided"
                            },
                            {
                                "name": "yahoo_answers_topics",
                                "accuracy": "Not provided"
                            }
                        ]
                    },
                    "description": "This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_689",
                    "name": "sentence-transformers/all-distilroberta-v1"
                },
                "id": "gorilla_huggingface_tool_684",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 1: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "Tool 2: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 3: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 4: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 5: A pretrained BERT large (cased) model for English that performs masked language modeling, allowing users to predict missing words in text. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336 million parameters, making it suitable for tasks like text completion, language understanding, and NLP evaluation.",
            "Tool 6: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 7: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 8: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 9: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.",
            "Tool 10: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 11: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 12: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 13: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 14: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 15: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 16: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "Tool 17: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.",
            "Tool 18: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 19: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation."
        ],
        "tool_view": {
            "description": "The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "parameters": {},
            "response": "The tool’s response is not explicitly defined in the JSON, but based on its functionality as an abstractive summarization model using the Hugging Face Transformers framework, a typical response would have the following structure:\n\n- **summary_text**: A string containing the generated abstractive summary of the input text. This is the main output produced by the PEGASUS model. The summary condenses the input content while attempting to preserve the most important information and meaning.\n\nOptional or framework-specific components (depending on how the model is called) may include:\n\n- **input_tokens / tokenized_input**: A representation of the input text after tokenization, often as a list or array of token IDs used internally by the model.\n- **output_tokens / tokenized_output**: A list or array of token IDs representing the generated summary before decoding back into text.\n- **metadata / model_info**: Information about the generation process, such as attention scores, generation length, or decoding parameters (e.g., beam size, sampling temperature) if explicitly returned.\n- **confidence_scores / log_probs**: Optional numerical scores indicating the model’s confidence for each generated token or overall sequence likelihood.\n\nSemantically, the core of the response is the **summary_text**, while the additional fields provide internal or diagnostic information useful for analysis or further processing. The response is intended to allow downstream applications to obtain a concise, coherent summary of the original input text.",
            "examples": [
                {
                    "user_intent": "Summarize a long PubMed research article into a concise abstract.",
                    "tool_call": {
                        "name": "google/pegasus-pubmed",
                        "arguments": {
                            "input_text": "The study investigates the effects of a new anti-cancer drug on tumor growth in mice over a 12-week period. Various biomarkers were monitored, and significant reductions in tumor size were observed...",
                            "max_length": 150,
                            "min_length": 50,
                            "do_sample": true
                        }
                    }
                },
                {
                    "user_intent": "Generate a brief summary of a collection of biomedical case reports for quick reading.",
                    "tool_call": {
                        "name": "google/pegasus-pubmed",
                        "arguments": {
                            "input_text": "Case report 1: Patient presented with acute liver failure following drug exposure. Case report 2: A rare genetic mutation caused sudden cardiac arrest in a teenager...",
                            "max_length": 120,
                            "min_length": 40,
                            "do_sample": true
                        }
                    }
                },
                {
                    "user_intent": "Condense a PubMed review paper into key sentences highlighting main findings.",
                    "tool_call": {
                        "name": "google/pegasus-pubmed",
                        "arguments": {
                            "input_text": "This review summarizes recent advances in CRISPR-Cas9 gene editing technologies, including applications in disease modeling, therapeutic interventions, and ethical considerations in clinical trials...",
                            "max_length": 200,
                            "min_length": 60,
                            "do_sample": false
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 1: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\nTool 2: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 3: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 4: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 5: A pretrained BERT large (cased) model for English that performs masked language modeling, allowing users to predict missing words in text. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336 million parameters, making it suitable for tasks like text completion, language understanding, and NLP evaluation.\nTool 6: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 7: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 8: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 9: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.\nTool 10: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 11: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 12: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 13: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 14: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 15: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 16: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\nTool 17: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.\nTool 18: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 19: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"google/pegasus-pubmed\\\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\", \"parameters\": {}, \"response\": \"The tool\\u2019s response is not explicitly defined in the JSON, but based on its functionality as an abstractive summarization model using the Hugging Face Transformers framework, a typical response would have the following structure:\\n\\n- **summary_text**: A string containing the generated abstractive summary of the input text. This is the main output produced by the PEGASUS model. The summary condenses the input content while attempting to preserve the most important information and meaning.\\n\\nOptional or framework-specific components (depending on how the model is called) may include:\\n\\n- **input_tokens / tokenized_input**: A representation of the input text after tokenization, often as a list or array of token IDs used internally by the model.\\n- **output_tokens / tokenized_output**: A list or array of token IDs representing the generated summary before decoding back into text.\\n- **metadata / model_info**: Information about the generation process, such as attention scores, generation length, or decoding parameters (e.g., beam size, sampling temperature) if explicitly returned.\\n- **confidence_scores / log_probs**: Optional numerical scores indicating the model\\u2019s confidence for each generated token or overall sequence likelihood.\\n\\nSemantically, the core of the response is the **summary_text**, while the additional fields provide internal or diagnostic information useful for analysis or further processing. The response is intended to allow downstream applications to obtain a concise, coherent summary of the original input text.\", \"examples\": [{\"user_intent\": \"Summarize a long PubMed research article into a concise abstract.\", \"tool_call\": {\"name\": \"google/pegasus-pubmed\", \"arguments\": {\"input_text\": \"The study investigates the effects of a new anti-cancer drug on tumor growth in mice over a 12-week period. Various biomarkers were monitored, and significant reductions in tumor size were observed...\", \"max_length\": 150, \"min_length\": 50, \"do_sample\": true}}}, {\"user_intent\": \"Generate a brief summary of a collection of biomedical case reports for quick reading.\", \"tool_call\": {\"name\": \"google/pegasus-pubmed\", \"arguments\": {\"input_text\": \"Case report 1: Patient presented with acute liver failure following drug exposure. Case report 2: A rare genetic mutation caused sudden cardiac arrest in a teenager...\", \"max_length\": 120, \"min_length\": 40, \"do_sample\": true}}}, {\"user_intent\": \"Condense a PubMed review paper into key sentences highlighting main findings.\", \"tool_call\": {\"name\": \"google/pegasus-pubmed\", \"arguments\": {\"input_text\": \"This review summarizes recent advances in CRISPR-Cas9 gene editing technologies, including applications in disease modeling, therapeutic interventions, and ethical considerations in clinical trials...\", \"max_length\": 200, \"min_length\": 60, \"do_sample\": false}}}]}\n\n# User Query\nI am learning English literature. I plan to build a system that has a huge database of English sentences and keeps the important ones.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert English sentences into vector embeddings for semantic storage and retrieval.",
                    "tool_description": "Use a sentence-transformers model to transform sentences into dense vector representations, enabling semantic search, clustering, and similarity comparisons.",
                    "expected_response": "A numerical vector (embedding) representing the semantic content of each input sentence."
                },
                {
                    "user_intent": "Identify the most important sentences in the collection for prioritization or summarization.",
                    "tool_description": "Apply an abstractive summarization model to extract and condense key sentences from English text, highlighting the most important content.",
                    "expected_response": "A concise summary or a selection of key sentences that capture the essential information of the input text."
                }
            ],
            "extracted_arguments": {
                "input_sentences": "A collection of English sentences intended for storage and analysis.",
                "embedding_model": "The pre-trained model used to convert sentences into dense vector embeddings.",
                "summarization_model": "The pre-trained abstractive summarization model used to select or condense important sentences."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_422",
        "query": "We're creating a chatbot that will detect the user's emotion. I want to start by implementing the basic functionality of emotion detection from the user's responses.",
        "instruction": "Given an `emotion detection` task, retrieve tools that leverage natural language processing to analyze text inputs and identify emotions or sentiments based on user responses, facilitating the development of chatbots with emotion recognition capabilities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('sentiment-analysis', model='michellejieli/emotion_text_classifier')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "classifier(I love this!)",
                    "performance": {
                        "dataset": [
                            "Crowdflower (2016)",
                            "Emotion Dataset, Elvis et al. (2018)",
                            "GoEmotions, Demszky et al. (2020)",
                            "ISEAR, Vikash (2018)",
                            "MELD, Poria et al. (2019)",
                            "SemEval-2018, EI-reg, Mohammad et al. (2018)",
                            "Emotion Lines (Friends)"
                        ],
                        "accuracy": "Not provided"
                    },
                    "description": "DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.",
                    "id": "huggingface_api_400",
                    "name": "michellejieli/emotion_text_classifier"
                },
                "id": "gorilla_huggingface_tool_396",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 1: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 2: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 3: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.",
            "Tool 4: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 5: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 6: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 7: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.",
            "Tool 8: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.",
            "Tool 9: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.",
            "Tool 10: This tool performs emotion recognition on audio data using the wav2vec2 model fine-tuned for the SUPERB Emotion Recognition task. It requires a 16kHz sampled speech input and provides classification results based on the IEMOCAP dataset. The model is implemented using PyTorch and Transformers, and it can be accessed via the `pipeline('audio-classification', model='superb/wav2vec2-base-superb-er')` API call.",
            "Tool 11: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.",
            "Tool 12: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 13: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 14: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 15: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 16: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 17: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.",
            "Tool 18: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 19: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents."
        ],
        "tool_view": {
            "description": "This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text that will be classified by the emotion classification model. The model predicts the emotion expressed in the text based on Ekman's 6 basic emotions and a neutral class.",
                    "required": true,
                    "example": "I love this!"
                }
            },
            "response": "The tool returns a structured list of emotion predictions for a given input text. Each element in the response corresponds to one possible emotion label and includes the following components:\n\n1. **`label`** – A string representing the predicted emotion class. Valid labels include the six basic Ekman emotions plus neutral: `anger`, `disgust`, `fear`, `joy`, `neutral`, `sadness`, and `surprise`.\n\n2. **`score`** – A numerical value representing the model’s confidence or probability that the input text expresses the corresponding emotion. The score is typically a floating-point number between 0 and 1.\n\nBecause the tool is configured with `return_all_scores=True`, the response includes **all emotion labels** for the input text, each paired with its corresponding confidence score, rather than only the single highest-probability label. The overall structure is a list of dictionaries, with one dictionary per emotion. This allows downstream applications to interpret the full distribution of emotional predictions for the text.",
            "examples": [
                {
                    "user_intent": "I want to analyze the emotions in this tweet: 'Just got a promotion at work, feeling on top of the world!'",
                    "tool_call": {
                        "name": "j-hartmann/emotion-english-distilroberta-base",
                        "arguments": {
                            "text": "Just got a promotion at work, feeling on top of the world!"
                        }
                    }
                },
                {
                    "user_intent": "Classify the emotion expressed in this customer review: 'The product stopped working after a week, really disappointed.'",
                    "tool_call": {
                        "name": "j-hartmann/emotion-english-distilroberta-base",
                        "arguments": {
                            "text": "The product stopped working after a week, really disappointed."
                        }
                    }
                },
                {
                    "user_intent": "Detect the emotional tone of this sentence: 'Wow, I can't believe this surprise party, I'm speechless!'",
                    "tool_call": {
                        "name": "j-hartmann/emotion-english-distilroberta-base",
                        "arguments": {
                            "text": "Wow, I can't believe this surprise party, I'm speechless!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 1: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 2: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 3: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\nTool 4: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 5: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 6: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 7: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.\nTool 8: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\nTool 9: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.\nTool 10: This tool performs emotion recognition on audio data using the wav2vec2 model fine-tuned for the SUPERB Emotion Recognition task. It requires a 16kHz sampled speech input and provides classification results based on the IEMOCAP dataset. The model is implemented using PyTorch and Transformers, and it can be accessed via the `pipeline('audio-classification', model='superb/wav2vec2-base-superb-er')` API call.\nTool 11: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.\nTool 12: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 13: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 14: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 15: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 16: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 17: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.\nTool 18: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 19: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text that will be classified by the emotion classification model. The model predicts the emotion expressed in the text based on Ekman's 6 basic emotions and a neutral class.\", \"required\": true, \"example\": \"I love this!\"}}, \"response\": \"The tool returns a structured list of emotion predictions for a given input text. Each element in the response corresponds to one possible emotion label and includes the following components:\\n\\n1. **`label`** \\u2013 A string representing the predicted emotion class. Valid labels include the six basic Ekman emotions plus neutral: `anger`, `disgust`, `fear`, `joy`, `neutral`, `sadness`, and `surprise`.\\n\\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence or probability that the input text expresses the corresponding emotion. The score is typically a floating-point number between 0 and 1.\\n\\nBecause the tool is configured with `return_all_scores=True`, the response includes **all emotion labels** for the input text, each paired with its corresponding confidence score, rather than only the single highest-probability label. The overall structure is a list of dictionaries, with one dictionary per emotion. This allows downstream applications to interpret the full distribution of emotional predictions for the text.\", \"examples\": [{\"user_intent\": \"I want to analyze the emotions in this tweet: 'Just got a promotion at work, feeling on top of the world!'\", \"tool_call\": {\"name\": \"j-hartmann/emotion-english-distilroberta-base\", \"arguments\": {\"text\": \"Just got a promotion at work, feeling on top of the world!\"}}}, {\"user_intent\": \"Classify the emotion expressed in this customer review: 'The product stopped working after a week, really disappointed.'\", \"tool_call\": {\"name\": \"j-hartmann/emotion-english-distilroberta-base\", \"arguments\": {\"text\": \"The product stopped working after a week, really disappointed.\"}}}, {\"user_intent\": \"Detect the emotional tone of this sentence: 'Wow, I can't believe this surprise party, I'm speechless!'\", \"tool_call\": {\"name\": \"j-hartmann/emotion-english-distilroberta-base\", \"arguments\": {\"text\": \"Wow, I can't believe this surprise party, I'm speechless!\"}}}]}\n\n# User Query\nWe're creating a chatbot that will detect the user's emotion. I want to start by implementing the basic functionality of emotion detection from the user's responses.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze the emotions in user messages to detect their emotional state.",
                    "tool_description": "Classify emotions in English text based on Ekman's six basic emotions plus a neutral category, providing confidence scores for each emotion.",
                    "expected_response": "A structured list of predicted emotions for the input text, each with a label and confidence score."
                }
            ],
            "extracted_arguments": {
                "text": "The user's input message in natural language to be analyzed for emotional content."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_424",
        "query": "The development team is trying to create a function in Python to print \"Hello, World!\" but they're not sure how to proceed. Generate this function for them.",
        "instruction": "Given a `function generation` task, retrieve tools that assist in the creation of simple programming functions, particularly in Python, by generating code snippets aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)",
                    "api_arguments": [
                        "inputs"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = bigcode/santacoder\ndevice = cuda # for GPU usage or cpu for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\ninputs = tokenizer.encode(def print_hello_world():, return_tensors=pt).to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))",
                    "performance": {
                        "dataset": "bigcode/the-stack",
                        "accuracy": {
                            "pass@1 on MultiPL HumanEval (Python)": 0.18,
                            "pass@10 on MultiPL HumanEval (Python)": 0.29,
                            "pass@100 on MultiPL HumanEval (Python)": 0.49,
                            "pass@1 on MultiPL MBPP (Python)": 0.35,
                            "pass@10 on MultiPL MBPP (Python)": 0.58,
                            "pass@100 on MultiPL MBPP (Python)": 0.77,
                            "pass@1 on MultiPL HumanEval (JavaScript)": 0.16,
                            "pass@10 on MultiPL HumanEval (JavaScript)": 0.27,
                            "pass@100 on MultiPL HumanEval (JavaScript)": 0.47,
                            "pass@1 on MultiPL MBPP (Javascript)": 0.28,
                            "pass@10 on MultiPL MBPP (Javascript)": 0.51,
                            "pass@100 on MultiPL MBPP (Javascript)": 0.7,
                            "pass@1 on MultiPL HumanEval (Java)": 0.15,
                            "pass@10 on MultiPL HumanEval (Java)": 0.26,
                            "pass@100 on MultiPL HumanEval (Java)": 0.41,
                            "pass@1 on MultiPL MBPP (Java)": 0.28,
                            "pass@10 on MultiPL MBPP (Java)": 0.44,
                            "pass@100 on MultiPL MBPP (Java)": 0.59,
                            "single_line on HumanEval FIM (Python)": 0.44,
                            "single_line on MultiPL HumanEval FIM (Java)": 0.62,
                            "single_line on MultiPL HumanEval FIM (JavaScript)": 0.6,
                            "BLEU on CodeXGLUE code-to-text (Python)": 18.13
                        }
                    },
                    "description": "The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). The main model uses Multi Query Attention, was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective. In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations.",
                    "id": "huggingface_api_610",
                    "name": "bigcode/santacoder"
                },
                "id": "gorilla_huggingface_tool_605",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.",
            "Tool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 2: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 3: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 4: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 6: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 7: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 8: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 9: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 10: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 11: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 12: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 13: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 14: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 15: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 16: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 17: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 18: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 19: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency."
        ],
        "tool_view": {
            "description": "The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.",
            "parameters": {
                "context": {
                    "type": "string",
                    "description": "The context or passage of text in which the answer to the question is expected to be found.",
                    "required": true,
                    "example": "The human genome consists of 23 chromosomes."
                },
                "question": {
                    "type": "string",
                    "description": "The question for which an answer is to be extracted from the given context.",
                    "required": true,
                    "example": "How many chromosomes are in the human genome?"
                }
            },
            "response": "The tool returns a structured answer to a question based on a given context passage. The response is typically a dictionary containing the following components:\n\n- **`answer`**: A string representing the extracted or generated answer from the context that best responds to the input question.  \n- **`score`**: A numeric value indicating the confidence or probability assigned by the model to the provided answer. Higher values indicate greater confidence.  \n- **`start`**: An integer representing the character index in the context where the answer begins. This allows precise localization of the answer within the context text.  \n- **`end`**: An integer representing the character index in the context where the answer ends. Together with `start`, it defines the span of text corresponding to the answer.  \n\nThis response structure allows both the retrieval of the answer text and the exact position in the context, along with a confidence metric to assess reliability.",
            "examples": [
                {
                    "user_intent": "I want to find out what the main treatment for Type 2 Diabetes is according to recent biomedical research.",
                    "tool_call": {
                        "name": "sultan/BioM-ELECTRA-Large-SQuAD2",
                        "arguments": {
                            "context": "Type 2 Diabetes is a chronic condition that affects the way the body processes blood sugar (glucose). Current treatments include lifestyle changes, oral medications like metformin, and sometimes insulin therapy for advanced cases.",
                            "question": "What is the main treatment for Type 2 Diabetes?"
                        }
                    }
                },
                {
                    "user_intent": "Check which gene is commonly associated with breast cancer in biomedical studies.",
                    "tool_call": {
                        "name": "sultan/BioM-ELECTRA-Large-SQuAD2",
                        "arguments": {
                            "context": "BRCA1 and BRCA2 are tumor suppressor genes. Mutations in these genes have been strongly linked to an increased risk of developing breast and ovarian cancer in numerous biomedical studies.",
                            "question": "Which gene is commonly associated with breast cancer?"
                        }
                    }
                },
                {
                    "user_intent": "I need to know the recommended first-line antibiotic for treating bacterial pneumonia according to medical guidelines.",
                    "tool_call": {
                        "name": "sultan/BioM-ELECTRA-Large-SQuAD2",
                        "arguments": {
                            "context": "Community-acquired bacterial pneumonia is often treated with antibiotics. Guidelines recommend amoxicillin as the first-line therapy for adults without comorbidities, while macrolides or doxycycline may be alternatives.",
                            "question": "What is the recommended first-line antibiotic for bacterial pneumonia?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.\nTool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 2: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 3: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 4: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 6: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 7: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 8: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 9: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 10: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 11: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 12: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 13: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 14: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 15: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 16: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 17: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 18: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 19: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"sultan/BioM-ELECTRA-Large-SQuAD2\\\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.\", \"parameters\": {\"context\": {\"type\": \"string\", \"description\": \"The context or passage of text in which the answer to the question is expected to be found.\", \"required\": true, \"example\": \"The human genome consists of 23 chromosomes.\"}, \"question\": {\"type\": \"string\", \"description\": \"The question for which an answer is to be extracted from the given context.\", \"required\": true, \"example\": \"How many chromosomes are in the human genome?\"}}, \"response\": \"The tool returns a structured answer to a question based on a given context passage. The response is typically a dictionary containing the following components:\\n\\n- **`answer`**: A string representing the extracted or generated answer from the context that best responds to the input question.  \\n- **`score`**: A numeric value indicating the confidence or probability assigned by the model to the provided answer. Higher values indicate greater confidence.  \\n- **`start`**: An integer representing the character index in the context where the answer begins. This allows precise localization of the answer within the context text.  \\n- **`end`**: An integer representing the character index in the context where the answer ends. Together with `start`, it defines the span of text corresponding to the answer.  \\n\\nThis response structure allows both the retrieval of the answer text and the exact position in the context, along with a confidence metric to assess reliability.\", \"examples\": [{\"user_intent\": \"I want to find out what the main treatment for Type 2 Diabetes is according to recent biomedical research.\", \"tool_call\": {\"name\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"arguments\": {\"context\": \"Type 2 Diabetes is a chronic condition that affects the way the body processes blood sugar (glucose). Current treatments include lifestyle changes, oral medications like metformin, and sometimes insulin therapy for advanced cases.\", \"question\": \"What is the main treatment for Type 2 Diabetes?\"}}}, {\"user_intent\": \"Check which gene is commonly associated with breast cancer in biomedical studies.\", \"tool_call\": {\"name\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"arguments\": {\"context\": \"BRCA1 and BRCA2 are tumor suppressor genes. Mutations in these genes have been strongly linked to an increased risk of developing breast and ovarian cancer in numerous biomedical studies.\", \"question\": \"Which gene is commonly associated with breast cancer?\"}}}, {\"user_intent\": \"I need to know the recommended first-line antibiotic for treating bacterial pneumonia according to medical guidelines.\", \"tool_call\": {\"name\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"arguments\": {\"context\": \"Community-acquired bacterial pneumonia is often treated with antibiotics. Guidelines recommend amoxicillin as the first-line therapy for adults without comorbidities, while macrolides or doxycycline may be alternatives.\", \"question\": \"What is the recommended first-line antibiotic for bacterial pneumonia?\"}}}]}\n\n# User Query\nThe development team is trying to create a function in Python to print \"Hello, World!\" but they're not sure how to proceed. Generate this function for them.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a Python function that prints 'Hello, World!'.",
                    "tool_description": "Create a Python function that outputs the string 'Hello, World!' when called.",
                    "expected_response": "A code snippet defining a Python function that prints 'Hello, World!' to the console."
                }
            ],
            "extracted_arguments": {
                "function_name": "The name of the Python function to be defined as a string.",
                "output_string": "The text string that the function should print when called."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_309",
        "query": "I’m putting together a dating site where users can submit questions they'd like the matching algorithm to ask. I want to suggest questions like the ones they have already submitted. Can you provide me with a model to do that?",
        "instruction": "Given a `question similarity` task, retrieve tools that utilize sentence embedding techniques to suggest questions similar to already submitted ones by processing text inputs and mapping them to a vector space for clustering or semantic similarity assessment.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "snli, multi_nli, ms_marco",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_707",
                    "name": "sentence-transformers/paraphrase-MiniLM-L3-v2"
                },
                "id": "gorilla_huggingface_tool_702",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 1: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 2: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 3: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 4: The tool uses a fine-tuned version of DistilBERT (distilbert-base-uncased-distilled-squad) for extractive question answering, where it identifies answers to questions based on a provided context. It is designed to be faster and more efficient than the original BERT model, with 40% fewer parameters and 60% faster execution, while maintaining over 95% of BERT's performance on language understanding tasks. This model is particularly suited for tasks like answering questions from datasets such as SQuAD.",
            "Tool 5: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 6: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 8: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 9: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.",
            "Tool 10: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 11: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 12: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 13: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 14: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 15: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 16: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 17: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 18: The `facebook/dpr-question_encoder-single-nq-base` is a Dense Passage Retrieval (DPR) question encoder that converts natural language questions into dense vector embeddings for open-domain question answering. Trained on the Natural Questions (NQ) dataset, it is designed to capture semantic meaning of queries, enabling efficient retrieval of relevant passages from large text corpora. This model is used in NLP systems to improve the accuracy and speed of information retrieval in applications like search engines and QA pipelines.",
            "Tool 19: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way."
        ],
        "tool_view": {
            "description": "The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want to ask about the table.",
                    "required": true,
                    "example": "How many goals did player A score?"
                },
                "table": {
                    "type": "array",
                    "description": "A two-dimensional array representing the table data, where the first row contains column headers and subsequent rows contain the data entries.",
                    "required": true,
                    "example": [
                        [
                            "Player",
                            "Goals"
                        ],
                        [
                            "Player A",
                            5
                        ],
                        [
                            "Player B",
                            3
                        ]
                    ]
                }
            },
            "response": "The tool’s response is a structured output representing the answer to a natural language question posed over a tabular dataset. The typical response structure includes the following components:  \n\n1. **`answer`** – A string or scalar value representing the model’s predicted answer to the input question. This could be a single value, a numeric count, or a textual summary derived from the table.  \n\n2. **`coordinates`** (optional) – A list of table cell locations or indices that the model used to compute or extract the answer. Each coordinate identifies a specific row and column in the input table.  \n\n3. **`aggregation`** (optional) – A string indicating the type of aggregation applied to the table data, such as “sum,” “average,” “count,” or “none,” if the answer was extracted directly without aggregation.  \n\n4. **`logits`** (optional) – Numerical scores or confidence values associated with potential answer cells or aggregation choices, representing the model’s internal confidence in its prediction.  \n\n5. **`units`** (optional) – If the table contains measurable quantities, this field may indicate the units associated with the predicted numeric answer.  \n\nIn practice, the response is typically a dictionary containing at least the `answer` key, with optional fields providing additional interpretability about how the answer was derived from the table.",
            "examples": [
                {
                    "user_intent": "I want to know how many goals Player B scored in the season.",
                    "tool_call": {
                        "name": "google/tapas-base-finetuned-sqa",
                        "arguments": {
                            "question": "How many goals did Player B score?",
                            "table": [
                                [
                                    "Player",
                                    "Goals"
                                ],
                                [
                                    "Player A",
                                    5
                                ],
                                [
                                    "Player B",
                                    3
                                ],
                                [
                                    "Player C",
                                    7
                                ]
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Which department has the highest number of employees?",
                    "tool_call": {
                        "name": "google/tapas-base-finetuned-sqa",
                        "arguments": {
                            "question": "Which department has the most employees?",
                            "table": [
                                [
                                    "Department",
                                    "Employees"
                                ],
                                [
                                    "HR",
                                    12
                                ],
                                [
                                    "Engineering",
                                    45
                                ],
                                [
                                    "Sales",
                                    30
                                ]
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Find the total revenue for Q2 from the financial table.",
                    "tool_call": {
                        "name": "google/tapas-base-finetuned-sqa",
                        "arguments": {
                            "question": "What is the total revenue for Q2?",
                            "table": [
                                [
                                    "Quarter",
                                    "Revenue"
                                ],
                                [
                                    "Q1",
                                    120000
                                ],
                                [
                                    "Q2",
                                    150000
                                ],
                                [
                                    "Q3",
                                    130000
                                ],
                                [
                                    "Q4",
                                    160000
                                ]
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 1: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 2: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 3: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 4: The tool uses a fine-tuned version of DistilBERT (distilbert-base-uncased-distilled-squad) for extractive question answering, where it identifies answers to questions based on a provided context. It is designed to be faster and more efficient than the original BERT model, with 40% fewer parameters and 60% faster execution, while maintaining over 95% of BERT's performance on language understanding tasks. This model is particularly suited for tasks like answering questions from datasets such as SQuAD.\nTool 5: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 6: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 8: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 9: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.\nTool 10: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 11: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 12: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 13: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 14: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 15: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 16: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 17: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 18: The `facebook/dpr-question_encoder-single-nq-base` is a Dense Passage Retrieval (DPR) question encoder that converts natural language questions into dense vector embeddings for open-domain question answering. Trained on the Natural Questions (NQ) dataset, it is designed to capture semantic meaning of queries, enabling efficient retrieval of relevant passages from large text corpora. This model is used in NLP systems to improve the accuracy and speed of information retrieval in applications like search engines and QA pipelines.\nTool 19: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"google/tapas-base-finetuned-sqa\\\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want to ask about the table.\", \"required\": true, \"example\": \"How many goals did player A score?\"}, \"table\": {\"type\": \"array\", \"description\": \"A two-dimensional array representing the table data, where the first row contains column headers and subsequent rows contain the data entries.\", \"required\": true, \"example\": [[\"Player\", \"Goals\"], [\"Player A\", 5], [\"Player B\", 3]]}}, \"response\": \"The tool\\u2019s response is a structured output representing the answer to a natural language question posed over a tabular dataset. The typical response structure includes the following components:  \\n\\n1. **`answer`** \\u2013 A string or scalar value representing the model\\u2019s predicted answer to the input question. This could be a single value, a numeric count, or a textual summary derived from the table.  \\n\\n2. **`coordinates`** (optional) \\u2013 A list of table cell locations or indices that the model used to compute or extract the answer. Each coordinate identifies a specific row and column in the input table.  \\n\\n3. **`aggregation`** (optional) \\u2013 A string indicating the type of aggregation applied to the table data, such as \\u201csum,\\u201d \\u201caverage,\\u201d \\u201ccount,\\u201d or \\u201cnone,\\u201d if the answer was extracted directly without aggregation.  \\n\\n4. **`logits`** (optional) \\u2013 Numerical scores or confidence values associated with potential answer cells or aggregation choices, representing the model\\u2019s internal confidence in its prediction.  \\n\\n5. **`units`** (optional) \\u2013 If the table contains measurable quantities, this field may indicate the units associated with the predicted numeric answer.  \\n\\nIn practice, the response is typically a dictionary containing at least the `answer` key, with optional fields providing additional interpretability about how the answer was derived from the table.\", \"examples\": [{\"user_intent\": \"I want to know how many goals Player B scored in the season.\", \"tool_call\": {\"name\": \"google/tapas-base-finetuned-sqa\", \"arguments\": {\"question\": \"How many goals did Player B score?\", \"table\": [[\"Player\", \"Goals\"], [\"Player A\", 5], [\"Player B\", 3], [\"Player C\", 7]]}}}, {\"user_intent\": \"Which department has the highest number of employees?\", \"tool_call\": {\"name\": \"google/tapas-base-finetuned-sqa\", \"arguments\": {\"question\": \"Which department has the most employees?\", \"table\": [[\"Department\", \"Employees\"], [\"HR\", 12], [\"Engineering\", 45], [\"Sales\", 30]]}}}, {\"user_intent\": \"Find the total revenue for Q2 from the financial table.\", \"tool_call\": {\"name\": \"google/tapas-base-finetuned-sqa\", \"arguments\": {\"question\": \"What is the total revenue for Q2?\", \"table\": [[\"Quarter\", \"Revenue\"], [\"Q1\", 120000], [\"Q2\", 150000], [\"Q3\", 130000], [\"Q4\", 160000]]}}}]}\n\n# User Query\nI’m putting together a dating site where users can submit questions they'd like the matching algorithm to ask. I want to suggest questions like the ones they have already submitted. Can you provide me with a model to do that?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to suggest new questions similar to the ones users have already submitted.",
                    "tool_description": "Generate semantically similar question suggestions based on a set of input questions using a natural language embedding or similarity model.",
                    "expected_response": "A list of recommended questions ranked by relevance or similarity to the input questions."
                }
            ],
            "extracted_arguments": {
                "user_questions": "A list of questions previously submitted by users, represented as strings, to be used as the basis for generating similar questions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_413",
        "query": "Use this API to get a suggestion on how to respond to a customer's complaint about the late delivery of their package.",
        "instruction": "Given a `customer response generation` task, retrieve tools capable of generating conversational responses to customer complaints by leveraging external knowledge and contextual dialogue processing to craft appropriate and helpful replies.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Conversational",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')",
                    "api_arguments": [
                        "instruction",
                        "knowledge",
                        "dialog"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\ndef generate(instruction, knowledge, dialog):\n if knowledge != '':\n knowledge = '[KNOWLEDGE] ' + knowledge\n dialog = ' EOS '.join(dialog)\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n return output",
                    "performance": {
                        "dataset": "Reddit discussion thread, instruction and knowledge grounded dialogs",
                        "accuracy": "N/A"
                    },
                    "description": "GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.",
                    "id": "huggingface_api_593",
                    "name": "microsoft/GODEL-v1_1-base-seq2seq"
                },
                "id": "gorilla_huggingface_tool_588",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.",
            "Tool 1: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 2: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.",
            "Tool 3: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 4: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 5: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 6: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.",
            "Tool 7: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 8: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 9: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 10: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 11: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 12: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 13: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 14: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 15: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 16: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.",
            "Tool 17: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 18: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.",
            "Tool 19: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs."
        ],
        "tool_view": {
            "description": "This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.",
            "parameters": {},
            "response": "The tool loads a pre-trained depth estimation model using the Hugging Face Transformers framework. The response of the tool is an instance of a **transformers model object** (specifically a `PreTrainedModel`), which represents the loaded neural network configured for depth estimation.  \n\nKey aspects of the response:  \n\n- **Model object**: This is the main component returned. It encapsulates the architecture, learned weights, and configuration of the depth estimation model.  \n- **Methods for inference**: The model object includes methods for performing forward passes, allowing users to input images and obtain corresponding depth maps.  \n- **Configuration and metadata**: The response contains model configuration details such as input shape, preprocessing requirements, and other parameters necessary for correct usage.  \n- **Integration with PyTorch**: Since the model is a PyTorch-based Transformers model, the response can be used directly with PyTorch tensors and supports GPU acceleration if available.  \n\nThe response does **not directly return depth maps**; instead, it provides the model instance that can be applied to input images to generate depth estimations.",
            "examples": [
                {
                    "user_intent": "I need a pre-trained model for depth estimation using Hugging Face Transformers.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-093747",
                        "arguments": []
                    }
                },
                {
                    "user_intent": "Load the GLPN model fine-tuned on the DIODE dataset for depth estimation.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-093747",
                        "arguments": []
                    }
                },
                {
                    "user_intent": "Can you provide the Hugging Face model for depth estimation with the DIODE dataset?",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-093747",
                        "arguments": []
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.\nTool 1: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 2: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.\nTool 3: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 4: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 5: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 6: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.\nTool 7: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 8: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 9: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 10: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 11: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 12: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 13: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 14: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 15: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 16: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\nTool 17: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 18: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.\nTool 19: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.\", \"parameters\": {}, \"response\": \"The tool loads a pre-trained depth estimation model using the Hugging Face Transformers framework. The response of the tool is an instance of a **transformers model object** (specifically a `PreTrainedModel`), which represents the loaded neural network configured for depth estimation.  \\n\\nKey aspects of the response:  \\n\\n- **Model object**: This is the main component returned. It encapsulates the architecture, learned weights, and configuration of the depth estimation model.  \\n- **Methods for inference**: The model object includes methods for performing forward passes, allowing users to input images and obtain corresponding depth maps.  \\n- **Configuration and metadata**: The response contains model configuration details such as input shape, preprocessing requirements, and other parameters necessary for correct usage.  \\n- **Integration with PyTorch**: Since the model is a PyTorch-based Transformers model, the response can be used directly with PyTorch tensors and supports GPU acceleration if available.  \\n\\nThe response does **not directly return depth maps**; instead, it provides the model instance that can be applied to input images to generate depth estimations.\", \"examples\": [{\"user_intent\": \"I need a pre-trained model for depth estimation using Hugging Face Transformers.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-093747\", \"arguments\": []}}, {\"user_intent\": \"Load the GLPN model fine-tuned on the DIODE dataset for depth estimation.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-093747\", \"arguments\": []}}, {\"user_intent\": \"Can you provide the Hugging Face model for depth estimation with the DIODE dataset?\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-093747\", \"arguments\": []}}]}\n\n# User Query\nUse this API to get a suggestion on how to respond to a customer's complaint about the late delivery of their package.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate an appropriate response to a customer's complaint about a late package delivery.",
                    "tool_description": "Generate a response suggestion based on a customer complaint regarding late delivery of a package.",
                    "expected_response": "A text-based suggestion for a response to the customer's complaint."
                }
            ],
            "extracted_arguments": {
                "complaint_type": "The nature of the complaint (in this case, late delivery of a package).",
                "customer_feedback": "The content or tone of the complaint from the customer."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_358",
        "query": "I work for a Japanese company, and my manager needs me to take a look at a request from a client. I can understand fluent Japanese, but I need a little help filling in missing words from the text.",
        "instruction": "Given a `text completion` task, retrieve tools that assist in completing or filling in missing words in Japanese text by leveraging pre-trained language models to process the input text and provide contextually appropriate suggestions.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "fill_mask('[MASK]')",
                    "performance": {
                        "dataset": "wikipedia",
                        "accuracy": "N/A"
                    },
                    "description": "This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.",
                    "id": "huggingface_api_684",
                    "name": "cl-tohoku/bert-base-japanese"
                },
                "id": "gorilla_huggingface_tool_679",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 1: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 2: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 3: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 4: A specialized OCR tool designed for recognizing Japanese text in manga, capable of accurately reading vertical and horizontal text, text with furigana, text overlaid on images, and a wide variety of fonts and styles, even in low-quality images. It uses a Vision Encoder-Decoder framework and can also serve as a general-purpose printed Japanese OCR, providing robust text extraction in diverse manga-specific scenarios.",
            "Tool 5: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 6: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 7: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 8: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.",
            "Tool 9: This tool provides speech recognition for Japanese using a fine-tuned version of the XLSR-53 large model, based on Hugging Face Transformers. It transcribes audio files (e.g., MP3, WAV) into text and is designed to work with audio sampled at 16kHz. The model, trained on datasets like Common Voice, CSS10, and JSUT, achieves an accuracy of 81.8% word error rate (WER) and 20.16% character error rate (CER). It requires libraries such as `huggingsound`, `torch`, `librosa`, `datasets`, and `transformers` in a Python environment.",
            "Tool 10: The `bert-base-multilingual-cased` model is a case-sensitive BERT model pretrained on Wikipedia content from 104 languages using masked language modeling. It can predict missing words in text and serve as a foundation for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 11: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 12: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 13: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 14: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 15: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 16: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 17: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 18: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently."
        ],
        "tool_view": {
            "description": "A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name of the pretrained BERT model to be used. In this case, it specifies the model trained for Japanese text processing with word-level tokenization.",
                    "required": true,
                    "example": "cl-tohoku/bert-base-japanese-whole-word-masking"
                }
            },
            "response": "The tool is designed to perform the \"Fill-Mask\" task using a BERT model pretrained on Japanese text. The expected response from this tool would be a set of predictions for the masked token in a given input text.\n\n### Structure and Meaning of the Response:\n1. **Predictions**: \n   - The response will likely include a list or collection of possible words or phrases that can replace the masked token in the input text.\n   - Each prediction would typically have a score, representing the model's confidence or likelihood of that particular replacement.\n\n2. **Fields in the Response**:\n   - **sequence**: The completed text after the masked token has been filled.\n     - This represents the original input text with the masked word replaced by one of the model's predictions.\n   - **score**: The confidence score for the prediction, typically in the form of a probability.\n     - This score shows how likely the model thinks that the predicted word or phrase is the correct replacement for the masked token.\n   - **token**: The predicted token itself (i.e., the word or phrase that the model suggests to fill in the mask).\n     - This will be a text string representing the candidate word that the model considers a good fit for the masked position.\n   - **token_id**: A numerical ID corresponding to the predicted token in the model’s vocabulary.\n     - This ID refers to the index of the token within the model's vocabulary.\n\n3. **Optional Fields**:\n   - Depending on the implementation, additional metadata might be returned, such as:\n     - **probabilities**: A list of scores for all possible tokens considered by the model for filling the mask, ordered by likelihood.\n     - **top_k**: The number of top predictions returned, which could be limited based on the `k` parameter set in the request.\n\nThe tool will return a set of predictions with associated confidence scores, and possibly a ranked list of multiple candidates for the masked token.",
            "examples": [
                {
                    "user_intent": "I want to fill in a missing word in a Japanese sentence.",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-whole-word-masking",
                        "arguments": {
                            "model": "cl-tohoku/bert-base-japanese-whole-word-masking",
                            "input": "[MASK] は日本で人気があります。"
                        }
                    }
                },
                {
                    "user_intent": "Can you help me predict the masked word in this Japanese sentence: '今日は[MASK]に行きます'?",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-whole-word-masking",
                        "arguments": {
                            "model": "cl-tohoku/bert-base-japanese-whole-word-masking",
                            "input": "今日は[MASK]に行きます"
                        }
                    }
                },
                {
                    "user_intent": "Fill in the missing word for a Japanese phrase using a pretrained model.",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-whole-word-masking",
                        "arguments": {
                            "model": "cl-tohoku/bert-base-japanese-whole-word-masking",
                            "input": "[MASK]を食べるのが好きです。"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 1: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 2: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 3: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 4: A specialized OCR tool designed for recognizing Japanese text in manga, capable of accurately reading vertical and horizontal text, text with furigana, text overlaid on images, and a wide variety of fonts and styles, even in low-quality images. It uses a Vision Encoder-Decoder framework and can also serve as a general-purpose printed Japanese OCR, providing robust text extraction in diverse manga-specific scenarios.\nTool 5: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 6: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 7: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 8: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.\nTool 9: This tool provides speech recognition for Japanese using a fine-tuned version of the XLSR-53 large model, based on Hugging Face Transformers. It transcribes audio files (e.g., MP3, WAV) into text and is designed to work with audio sampled at 16kHz. The model, trained on datasets like Common Voice, CSS10, and JSUT, achieves an accuracy of 81.8% word error rate (WER) and 20.16% character error rate (CER). It requires libraries such as `huggingsound`, `torch`, `librosa`, `datasets`, and `transformers` in a Python environment.\nTool 10: The `bert-base-multilingual-cased` model is a case-sensitive BERT model pretrained on Wikipedia content from 104 languages using masked language modeling. It can predict missing words in text and serve as a foundation for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 11: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 12: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 13: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 14: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 15: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 16: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 17: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 18: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name of the pretrained BERT model to be used. In this case, it specifies the model trained for Japanese text processing with word-level tokenization.\", \"required\": true, \"example\": \"cl-tohoku/bert-base-japanese-whole-word-masking\"}}, \"response\": \"The tool is designed to perform the \\\"Fill-Mask\\\" task using a BERT model pretrained on Japanese text. The expected response from this tool would be a set of predictions for the masked token in a given input text.\\n\\n### Structure and Meaning of the Response:\\n1. **Predictions**: \\n   - The response will likely include a list or collection of possible words or phrases that can replace the masked token in the input text.\\n   - Each prediction would typically have a score, representing the model's confidence or likelihood of that particular replacement.\\n\\n2. **Fields in the Response**:\\n   - **sequence**: The completed text after the masked token has been filled.\\n     - This represents the original input text with the masked word replaced by one of the model's predictions.\\n   - **score**: The confidence score for the prediction, typically in the form of a probability.\\n     - This score shows how likely the model thinks that the predicted word or phrase is the correct replacement for the masked token.\\n   - **token**: The predicted token itself (i.e., the word or phrase that the model suggests to fill in the mask).\\n     - This will be a text string representing the candidate word that the model considers a good fit for the masked position.\\n   - **token_id**: A numerical ID corresponding to the predicted token in the model\\u2019s vocabulary.\\n     - This ID refers to the index of the token within the model's vocabulary.\\n\\n3. **Optional Fields**:\\n   - Depending on the implementation, additional metadata might be returned, such as:\\n     - **probabilities**: A list of scores for all possible tokens considered by the model for filling the mask, ordered by likelihood.\\n     - **top_k**: The number of top predictions returned, which could be limited based on the `k` parameter set in the request.\\n\\nThe tool will return a set of predictions with associated confidence scores, and possibly a ranked list of multiple candidates for the masked token.\", \"examples\": [{\"user_intent\": \"I want to fill in a missing word in a Japanese sentence.\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"arguments\": {\"model\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"input\": \"[MASK] \\u306f\\u65e5\\u672c\\u3067\\u4eba\\u6c17\\u304c\\u3042\\u308a\\u307e\\u3059\\u3002\"}}}, {\"user_intent\": \"Can you help me predict the masked word in this Japanese sentence: '\\u4eca\\u65e5\\u306f[MASK]\\u306b\\u884c\\u304d\\u307e\\u3059'?\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"arguments\": {\"model\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"input\": \"\\u4eca\\u65e5\\u306f[MASK]\\u306b\\u884c\\u304d\\u307e\\u3059\"}}}, {\"user_intent\": \"Fill in the missing word for a Japanese phrase using a pretrained model.\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"arguments\": {\"model\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"input\": \"[MASK]\\u3092\\u98df\\u3079\\u308b\\u306e\\u304c\\u597d\\u304d\\u3067\\u3059\\u3002\"}}}]}\n\n# User Query\nI work for a Japanese company, and my manager needs me to take a look at a request from a client. I can understand fluent Japanese, but I need a little help filling in missing words from the text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to fill in missing words in Japanese text to better understand a client's request.",
                    "tool_description": "Use a pretrained Japanese BERT model to perform masked language modeling, predicting missing words in sentences based on context.",
                    "expected_response": "A set of predictions for each masked token, including the completed text sequences and confidence scores for each suggested word."
                }
            ],
            "extracted_arguments": {
                "model": "The name of a pretrained Japanese BERT model suitable for masked language modeling",
                "input_text": "The Japanese sentence or passage containing masked tokens that need to be predicted"
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_360",
        "query": "In our language app, we want to include text to speech functionality for Hokkien, a dialect of Chinese, using the TAT-TTS dataset.",
        "instruction": "Given a `text-to-speech` task, retrieve tools that enable the conversion of text into spoken language for dialects like Hokkien by utilizing specified datasets such as TAT-TTS to ensure accurate pronunciation and delivery according to the app's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TT')",
                    "api_arguments": {
                        "unit": "Text input for the TTS model"
                    },
                    "python_environment_requirements": [
                        "fairseq",
                        "huggingface_hub",
                        "torchaudio"
                    ],
                    "example_code": "import json\nimport os\nfrom pathlib import Path\nimport IPython.display as ipd\nfrom fairseq import hub_utils\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\nfrom huggingface_hub import snapshot_download\nimport torchaudio\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\nlibrary_name = fairseq\ncache_dir = (\n cache_dir or (Path.home() / .cache / library_name).as_posix()\n)\ncache_dir = snapshot_download(\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\n)\nx = hub_utils.from_pretrained(\n cache_dir,\n model.pt,\n .,\n archive_map=CodeHiFiGANVocoder.hub_models(),\n config_yaml=config.json,\n fp16=False,\n is_vocoder=True,\n)\nwith open(f{x['args']['data']}/config.json) as f:\n vocoder_cfg = json.load(f)\nassert (\n len(x[args][model_path]) == 1\n), Too many vocoder models in the input\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\ntts_sample = tts_model.get_model_input(unit)\nwav, sr = tts_model.get_prediction(tts_sample)\nipd.Audio(wav, rate=sr)",
                    "performance": {
                        "dataset": "TAT-TTS",
                        "accuracy": "Not provided"
                    },
                    "description": "Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.",
                    "id": "huggingface_api_740",
                    "name": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS"
                },
                "id": "gorilla_huggingface_tool_735",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 1: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.",
            "Tool 2: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.",
            "Tool 3: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 4: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "Tool 5: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.",
            "Tool 6: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 7: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 8: A Text-to-Speech (TTS) tool that converts text into spoken audio using the ESPnet JETS model exported to ONNX. It can be integrated with the txtai pipeline or used directly with ONNX to generate high-quality speech from text.",
            "Tool 9: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.",
            "Tool 10: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 11: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 12: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 13: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 14: This tool provides a pre-trained HiFIGAN vocoder model for converting spectrograms into audio waveforms. It is typically used after a Text-to-Speech (TTS) model to transform the spectrogram output into natural-sounding speech. The model is trained on the LJSpeech dataset and operates at a sampling frequency of 22,050 Hz.",
            "Tool 15: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.",
            "Tool 16: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 17: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 18: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.",
            "Tool 19: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction."
        ],
        "tool_view": {
            "description": "This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "parameters": {
                "unit": {
                    "type": "string",
                    "description": "Text input for the Text-to-Speech (TTS) model, which will be converted to speech.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces an audio waveform generated from the input text using a Hokkien Text-to-Speech (TTS) model. The response can be described as follows:\n\n- **Audio waveform (`wav`)**: A numerical array representing the synthesized speech signal corresponding to the input text. This array contains the amplitude values of the audio over time.\n\n- **Sample rate (`sr`)**: An integer indicating the number of audio samples per second. This defines how the waveform should be interpreted or played back.\n\nTogether, these components allow the user to reconstruct and play the synthesized speech. In practice, the waveform and sample rate can be used with standard audio libraries to save to a file or play in an interactive environment. The response may also be wrapped in an object or tuple containing these two elements, depending on the interface used.  \n\nNo additional metadata, confidence scores, or textual output is provided by the tool itself; the primary output is the generated audio corresponding to the TTS input.",
            "examples": [
                {
                    "user_intent": "Convert a short Taiwanese Hokkien greeting into speech.",
                    "tool_call": {
                        "name": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS",
                        "arguments": {
                            "unit": "Li ho bo? (How are you?)"
                        }
                    }
                },
                {
                    "user_intent": "Generate an audio file reading a paragraph of text in Hokkien with natural intonation.",
                    "tool_call": {
                        "name": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS",
                        "arguments": {
                            "unit": "Wa chia̍h-pn̄g--ah, guá beh khì chhī-á tī tiàm-piáⁿ, tī hîⁿ ê lâng chia̍h pn̄g."
                        }
                    }
                },
                {
                    "user_intent": "Test the Hokkien TTS model by synthesizing a tongue-twister sentence.",
                    "tool_call": {
                        "name": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS",
                        "arguments": {
                            "unit": "Chhit-chhīⁿ chhē chhit-chhīⁿ, chhit-chhīⁿ chhē chhit-chhīⁿ, chhit-chhīⁿ chhē chhit-chhīⁿ."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 1: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\nTool 2: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.\nTool 3: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 4: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\nTool 5: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.\nTool 6: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 7: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 8: A Text-to-Speech (TTS) tool that converts text into spoken audio using the ESPnet JETS model exported to ONNX. It can be integrated with the txtai pipeline or used directly with ONNX to generate high-quality speech from text.\nTool 9: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.\nTool 10: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 11: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 12: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 13: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 14: This tool provides a pre-trained HiFIGAN vocoder model for converting spectrograms into audio waveforms. It is typically used after a Text-to-Speech (TTS) model to transform the spectrogram output into natural-sounding speech. The model is trained on the LJSpeech dataset and operates at a sampling frequency of 22,050 Hz.\nTool 15: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.\nTool 16: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 17: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 18: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.\nTool 19: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\", \"parameters\": {\"unit\": {\"type\": \"string\", \"description\": \"Text input for the Text-to-Speech (TTS) model, which will be converted to speech.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces an audio waveform generated from the input text using a Hokkien Text-to-Speech (TTS) model. The response can be described as follows:\\n\\n- **Audio waveform (`wav`)**: A numerical array representing the synthesized speech signal corresponding to the input text. This array contains the amplitude values of the audio over time.\\n\\n- **Sample rate (`sr`)**: An integer indicating the number of audio samples per second. This defines how the waveform should be interpreted or played back.\\n\\nTogether, these components allow the user to reconstruct and play the synthesized speech. In practice, the waveform and sample rate can be used with standard audio libraries to save to a file or play in an interactive environment. The response may also be wrapped in an object or tuple containing these two elements, depending on the interface used.  \\n\\nNo additional metadata, confidence scores, or textual output is provided by the tool itself; the primary output is the generated audio corresponding to the TTS input.\", \"examples\": [{\"user_intent\": \"Convert a short Taiwanese Hokkien greeting into speech.\", \"tool_call\": {\"name\": \"unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\", \"arguments\": {\"unit\": \"Li ho bo? (How are you?)\"}}}, {\"user_intent\": \"Generate an audio file reading a paragraph of text in Hokkien with natural intonation.\", \"tool_call\": {\"name\": \"unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\", \"arguments\": {\"unit\": \"Wa chia\\u030dh-pn\\u0304g--ah, gu\\u00e1 beh kh\\u00ec chh\\u012b-\\u00e1 t\\u012b ti\\u00e0m-pi\\u00e1\\u207f, t\\u012b h\\u00ee\\u207f \\u00ea l\\u00e2ng chia\\u030dh pn\\u0304g.\"}}}, {\"user_intent\": \"Test the Hokkien TTS model by synthesizing a tongue-twister sentence.\", \"tool_call\": {\"name\": \"unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\", \"arguments\": {\"unit\": \"Chhit-chh\\u012b\\u207f chh\\u0113 chhit-chh\\u012b\\u207f, chhit-chh\\u012b\\u207f chh\\u0113 chhit-chh\\u012b\\u207f, chhit-chh\\u012b\\u207f chh\\u0113 chhit-chh\\u012b\\u207f.\"}}}]}\n\n# User Query\nIn our language app, we want to include text to speech functionality for Hokkien, a dialect of Chinese, using the TAT-TTS dataset.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert written Hokkien text into natural-sounding speech for our language app.",
                    "tool_description": "Generate high-quality audio from Hokkien text input using a Text-to-Speech model trained on the TAT-TTS dataset and based on the Unit HiFiGAN vocoder.",
                    "expected_response": "An audio waveform representing the synthesized Hokkien speech along with the corresponding sample rate."
                }
            ],
            "extracted_arguments": {
                "text_input": "The written Hokkien text that needs to be converted into speech, represented as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_110",
        "query": "I need a text analysis tool that can automatically predict the most plausible missing text in a given sentence.",
        "instruction": "Given a `text prediction` task, retrieve tools that leverage natural language processing capabilities to automatically predict and fill in missing text within a sentence, ensuring the predictions are contextually appropriate and coherent with the existing text.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='albert-base-v2')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='albert-base-v2')\nunmasker(Hello I'm a [MASK] model.)",
                    "performance": {
                        "dataset": {
                            "SQuAD1.1": "90.2/83.2",
                            "SQuAD2.0": "82.1/79.3",
                            "MNLI": "84.6",
                            "SST-2": "92.9",
                            "RACE": "66.8"
                        },
                        "accuracy": "82.3"
                    },
                    "description": "ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.",
                    "id": "huggingface_api_666",
                    "name": "albert-base-v2"
                },
                "id": "gorilla_huggingface_tool_661",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 1: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 2: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 3: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 4: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "Tool 5: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 6: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 7: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 8: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 9: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 10: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 12: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 13: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 14: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 15: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 16: The `bert-base-multilingual-cased` model is a case-sensitive BERT model pretrained on Wikipedia content from 104 languages using masked language modeling. It can predict missing words in text and serve as a foundation for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 17: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 18: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 19: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis."
        ],
        "tool_view": {
            "description": "CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.",
                    "required": true,
                    "example": "camembert-base"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.",
                    "required": true,
                    "example": "camembert-base"
                }
            },
            "response": "The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \n\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \n- **score**: A numerical value representing the model’s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \n- **token**: The identifier of the predicted token according to the model’s vocabulary.  \n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \n\nThe response is usually ordered by descending score, so the first entry is the model’s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.",
            "examples": [
                {
                    "user_intent": "Predict the missing word in the French sentence about cheese.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Le camembert est <mask> :)"
                        }
                    }
                },
                {
                    "user_intent": "Complete this French sentence describing the weather.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Aujourd'hui, il fait très <mask> dehors."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for a French sentence about happiness.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Je me sens vraiment <mask> aujourd'hui."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 1: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 2: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 3: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 4: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\nTool 5: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 6: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 7: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 8: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 9: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 10: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 12: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 13: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 14: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 15: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 16: The `bert-base-multilingual-cased` model is a case-sensitive BERT model pretrained on Wikipedia content from 104 languages using masked language modeling. It can predict missing words in text and serve as a foundation for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 17: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 18: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 19: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.\", \"required\": true, \"example\": \"camembert-base\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.\", \"required\": true, \"example\": \"camembert-base\"}}, \"response\": \"The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \\n\\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \\n- **score**: A numerical value representing the model\\u2019s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \\n- **token**: The identifier of the predicted token according to the model\\u2019s vocabulary.  \\n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \\n\\nThe response is usually ordered by descending score, so the first entry is the model\\u2019s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.\", \"examples\": [{\"user_intent\": \"Predict the missing word in the French sentence about cheese.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Le camembert est <mask> :)\"}}}, {\"user_intent\": \"Complete this French sentence describing the weather.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Aujourd'hui, il fait tr\\u00e8s <mask> dehors.\"}}}, {\"user_intent\": \"Fill in the blank for a French sentence about happiness.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Je me sens vraiment <mask> aujourd'hui.\"}}}]}\n\n# User Query\nI need a text analysis tool that can automatically predict the most plausible missing text in a given sentence.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Predict the missing word or phrase in a given sentence.",
                    "tool_description": "Use a masked language model to analyze input text with missing tokens and generate the most probable replacements based on context.",
                    "expected_response": "A ranked list of candidate predictions for the masked token, including the completed sentence, the predicted token, a confidence score, and a human-readable token string."
                }
            ],
            "extracted_arguments": {
                "input_text": "The sentence containing one or more masked positions that need to be predicted.",
                "model": "The name or path of the pretrained masked language model to use for prediction.",
                "tokenizer": "The name or path of the tokenizer corresponding to the selected model to ensure correct tokenization."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_37",
        "query": "There's a collection of texts that we want to be able to analyze for their part-of-speech tags to better understand the structure of the sentences within the texts.",
        "instruction": "Given a `text analysis` task, retrieve tools that perform part-of-speech tagging to analyze the syntactic structure of text by processing sentence inputs and returning tagged data to facilitate further linguistic understanding.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Part-of-Speech Tagging",
                    "api_call": "SequenceTagger.load('flair/pos-english')",
                    "api_arguments": "sentence",
                    "python_environment_requirements": "flair (pip install flair)",
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger.load(flair/pos-english)\n\nsentence = Sentence(I love Berlin.)\n\ntagger.predict(sentence)\n\nprint(sentence)\n\nfor entity in sentence.get_spans('pos'):\n print(entity)",
                    "performance": {
                        "dataset": "Ontonotes",
                        "accuracy": "98.19"
                    },
                    "description": "This is the standard part-of-speech tagging model for English that ships with Flair. It predicts fine-grained POS tags based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_425",
                    "name": "flair/pos-english"
                },
                "id": "gorilla_huggingface_tool_421",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "Tool 1: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 2: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.",
            "Tool 3: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.",
            "Tool 4: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 5: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 6: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 7: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 8: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 9: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 10: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 11: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 12: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 13: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 14: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 15: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.",
            "Tool 16: A BERT-based model pre-trained on 152 million sentences from StackOverflow, designed for token-level classification tasks such as code and named entity recognition within StackOverflow posts.",
            "Tool 17: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 18: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.",
            "Tool 19: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts."
        ],
        "tool_view": {
            "description": "This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "parameters": {
                "sentence": {
                    "type": "string",
                    "description": "The input text sentence for which part-of-speech tags will be predicted. This should be a complete sentence in English.",
                    "required": true,
                    "example": "I love Berlin."
                }
            },
            "response": "The tool’s response is a structured annotation of a given text sentence with universal part-of-speech (POS) tags. The response typically has the following components:\n\n1. **Sentence-level object**  \n   - Represents the input text as a single sentence.  \n   - Contains metadata such as the raw text and potentially sentence boundaries if multiple sentences are supported.\n\n2. **Token-level annotations**  \n   - Each word or token in the sentence is represented as a separate unit.  \n   - For each token, the response includes:\n     - **Token text**: the string content of the word or punctuation.  \n     - **POS tag**: a universal POS label (e.g., ADJ, NOUN, VERB, PUNCT) predicted by the model.  \n     - **Confidence score**: a numeric value indicating the model’s confidence in the assigned POS tag.  \n\n3. **Span-level access (optional)**  \n   - The model allows retrieving POS tags as “spans” over tokens. Each span corresponds to a single token with its predicted POS tag and confidence.  \n   - This structure supports iteration over the sentence to extract all token-level predictions.\n\n4. **Integration with Flair objects**  \n   - The response is encapsulated in a `Sentence` object from Flair.  \n   - Methods such as `get_spans('pos')` return a list of POS-tagged spans for downstream processing or display.\n\nOverall, the response provides a token-by-token mapping of the input sentence to universal POS tags, including both the assigned label and associated confidence, all structured within Flair’s sentence and span objects.",
            "examples": [
                {
                    "user_intent": "I want to tag each word in the sentence 'Flair makes NLP tasks easier.' with its part of speech.",
                    "tool_call": {
                        "name": "flair/upos-english",
                        "arguments": {
                            "sentence": "Flair makes NLP tasks easier."
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentence 'Yesterday, I ran five kilometers in the park.' to identify nouns, verbs, and other parts of speech.",
                    "tool_call": {
                        "name": "flair/upos-english",
                        "arguments": {
                            "sentence": "Yesterday, I ran five kilometers in the park."
                        }
                    }
                },
                {
                    "user_intent": "Get POS tags for the complex sentence 'Although it was raining, the children played outside happily.'",
                    "tool_call": {
                        "name": "flair/upos-english",
                        "arguments": {
                            "sentence": "Although it was raining, the children played outside happily."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\nTool 1: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 2: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.\nTool 3: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.\nTool 4: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 5: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 6: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 7: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 8: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 9: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 10: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 11: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 12: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 13: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 14: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 15: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.\nTool 16: A BERT-based model pre-trained on 152 million sentences from StackOverflow, designed for token-level classification tasks such as code and named entity recognition within StackOverflow posts.\nTool 17: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 18: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.\nTool 19: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\", \"parameters\": {\"sentence\": {\"type\": \"string\", \"description\": \"The input text sentence for which part-of-speech tags will be predicted. This should be a complete sentence in English.\", \"required\": true, \"example\": \"I love Berlin.\"}}, \"response\": \"The tool\\u2019s response is a structured annotation of a given text sentence with universal part-of-speech (POS) tags. The response typically has the following components:\\n\\n1. **Sentence-level object**  \\n   - Represents the input text as a single sentence.  \\n   - Contains metadata such as the raw text and potentially sentence boundaries if multiple sentences are supported.\\n\\n2. **Token-level annotations**  \\n   - Each word or token in the sentence is represented as a separate unit.  \\n   - For each token, the response includes:\\n     - **Token text**: the string content of the word or punctuation.  \\n     - **POS tag**: a universal POS label (e.g., ADJ, NOUN, VERB, PUNCT) predicted by the model.  \\n     - **Confidence score**: a numeric value indicating the model\\u2019s confidence in the assigned POS tag.  \\n\\n3. **Span-level access (optional)**  \\n   - The model allows retrieving POS tags as \\u201cspans\\u201d over tokens. Each span corresponds to a single token with its predicted POS tag and confidence.  \\n   - This structure supports iteration over the sentence to extract all token-level predictions.\\n\\n4. **Integration with Flair objects**  \\n   - The response is encapsulated in a `Sentence` object from Flair.  \\n   - Methods such as `get_spans('pos')` return a list of POS-tagged spans for downstream processing or display.\\n\\nOverall, the response provides a token-by-token mapping of the input sentence to universal POS tags, including both the assigned label and associated confidence, all structured within Flair\\u2019s sentence and span objects.\", \"examples\": [{\"user_intent\": \"I want to tag each word in the sentence 'Flair makes NLP tasks easier.' with its part of speech.\", \"tool_call\": {\"name\": \"flair/upos-english\", \"arguments\": {\"sentence\": \"Flair makes NLP tasks easier.\"}}}, {\"user_intent\": \"Analyze the sentence 'Yesterday, I ran five kilometers in the park.' to identify nouns, verbs, and other parts of speech.\", \"tool_call\": {\"name\": \"flair/upos-english\", \"arguments\": {\"sentence\": \"Yesterday, I ran five kilometers in the park.\"}}}, {\"user_intent\": \"Get POS tags for the complex sentence 'Although it was raining, the children played outside happily.'\", \"tool_call\": {\"name\": \"flair/upos-english\", \"arguments\": {\"sentence\": \"Although it was raining, the children played outside happily.\"}}}]}\n\n# User Query\nThere's a collection of texts that we want to be able to analyze for their part-of-speech tags to better understand the structure of the sentences within the texts.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze each sentence in a collection of texts to identify their parts of speech.",
                    "tool_description": "Apply a high-accuracy English part-of-speech tagging model using Flair to label each word in the input sentences with universal POS tags such as NOUN, VERB, ADJ, and others, enabling syntactic analysis of the text.",
                    "expected_response": "A structured annotation for each sentence in the collection, providing token-level POS tags with confidence scores, including sentence-level objects and spans for each token."
                }
            ],
            "extracted_arguments": {
                "sentences": "A list of English text sentences to be analyzed for part-of-speech tags."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_407",
        "query": "I am a climate change agency, looking to have my research summaries translated into Chinese for international audiences.",
        "instruction": "Given a `translation task`, retrieve tools that specialize in Natural Language Processing, capable of translating text by processing input text and generating outputs in the desired language, aligning with the query's intent for international audience communication.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": [
                        "Translation",
                        "Summarization",
                        "Question Answering",
                        "Text Classification",
                        "Text Regression"
                    ],
                    "api_call": "T5Model.from_pretrained('t5-small')",
                    "api_arguments": {
                        "input_ids": "input tokenized text",
                        "decoder_input_ids": "input tokenized text for decoder"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import T5Tokenizer, T5Model\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\nmodel = T5Model.from_pretrained('t5-small')\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14 for full results"
                    },
                    "description": "T5-Small is a Text-To-Text Transfer Transformer (T5) model with 60 million parameters. It is designed to perform a variety of NLP tasks, including machine translation, document summarization, question answering, and classification tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be fine-tuned for specific tasks.",
                    "id": "huggingface_api_521",
                    "name": "t5-small"
                },
                "id": "gorilla_huggingface_tool_517",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 1: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 2: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 3: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 4: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 5: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 6: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 7: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.",
            "Tool 8: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.",
            "Tool 9: This tool provides zero-shot image classification using the Chinese CLIP model, which is trained on a large dataset of approximately 200 million Chinese image-text pairs. It leverages a Vision Transformer (ViT-B/16) as the image encoder and a RoBERTa-based text encoder to match images with textual descriptions without requiring task-specific fine-tuning. It is designed for applications like image retrieval and classification in Chinese contexts, offering strong performance across benchmarks such as CIFAR, COCO-CN, Flickr30K-CN, and MUGE.",
            "Tool 10: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 11: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.",
            "Tool 12: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "Tool 13: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 14: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 15: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 16: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 17: A T5-based text-to-text summarization model fine-tuned on the Samsum dataset, designed to generate concise summaries from input text without requiring explicit prompts. It can be used for natural language processing tasks that require transforming longer text into shorter, coherent summaries while maintaining key information.",
            "Tool 18: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 19: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`."
        ],
        "tool_view": {
            "description": "Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input Chinese text that you want the model to summarize. This should be a complete text string.",
                    "required": true,
                    "example": "今天天气很好，我们去公园散步吧。"
                },
                "max_length": {
                    "type": "integer",
                    "description": "The maximum number of tokens to generate in the summary. This limits the length of the output summary.",
                    "required": true,
                    "example": 150
                }
            },
            "response": "The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\n\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\n\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\"summary_text\"`) pointing to the summarized string. The content of the summary reflects the model’s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \n\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.",
            "examples": [
                {
                    "user_intent": "我有一篇关于人工智能发展的中文文章，想生成一段简短摘要。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "人工智能近年来取得了显著的发展，从自然语言处理到计算机视觉，各个领域都有突破性进展。技术的进步推动了产业升级，同时也带来了伦理和隐私等挑战。",
                            "max_length": 60
                        }
                    }
                },
                {
                    "user_intent": "需要对一条微博内容生成中文摘要，方便快速浏览。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "今天在上海举办的科技大会上，多家AI公司展示了最新的智能产品，包括自动驾驶、语音助手和智能家居设备，引发了广泛关注。",
                            "max_length": 40
                        }
                    }
                },
                {
                    "user_intent": "我有一篇教育领域的长篇文章，希望生成概括性摘要用于报告。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "近年来，中国教育领域进行了多项改革，尤其在中小学课程和教学方法上有所调整。在线教育和智能教育工具的应用提高了学习效率，同时也引发了教育公平性的讨论。",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 1: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 2: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 3: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 4: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 5: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 6: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 7: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.\nTool 8: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.\nTool 9: This tool provides zero-shot image classification using the Chinese CLIP model, which is trained on a large dataset of approximately 200 million Chinese image-text pairs. It leverages a Vision Transformer (ViT-B/16) as the image encoder and a RoBERTa-based text encoder to match images with textual descriptions without requiring task-specific fine-tuning. It is designed for applications like image retrieval and classification in Chinese contexts, offering strong performance across benchmarks such as CIFAR, COCO-CN, Flickr30K-CN, and MUGE.\nTool 10: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 11: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.\nTool 12: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\nTool 13: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 14: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 15: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 16: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 17: A T5-based text-to-text summarization model fine-tuned on the Samsum dataset, designed to generate concise summaries from input text without requiring explicit prompts. It can be used for natural language processing tasks that require transforming longer text into shorter, coherent summaries while maintaining key information.\nTool 18: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 19: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input Chinese text that you want the model to summarize. This should be a complete text string.\", \"required\": true, \"example\": \"\\u4eca\\u5929\\u5929\\u6c14\\u5f88\\u597d\\uff0c\\u6211\\u4eec\\u53bb\\u516c\\u56ed\\u6563\\u6b65\\u5427\\u3002\"}, \"max_length\": {\"type\": \"integer\", \"description\": \"The maximum number of tokens to generate in the summary. This limits the length of the output summary.\", \"required\": true, \"example\": 150}}, \"response\": \"The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\\n\\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\\n\\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\\\"summary_text\\\"`) pointing to the summarized string. The content of the summary reflects the model\\u2019s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \\n\\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.\", \"examples\": [{\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u5173\\u4e8e\\u4eba\\u5de5\\u667a\\u80fd\\u53d1\\u5c55\\u7684\\u4e2d\\u6587\\u6587\\u7ae0\\uff0c\\u60f3\\u751f\\u6210\\u4e00\\u6bb5\\u7b80\\u77ed\\u6458\\u8981\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eba\\u5de5\\u667a\\u80fd\\u8fd1\\u5e74\\u6765\\u53d6\\u5f97\\u4e86\\u663e\\u8457\\u7684\\u53d1\\u5c55\\uff0c\\u4ece\\u81ea\\u7136\\u8bed\\u8a00\\u5904\\u7406\\u5230\\u8ba1\\u7b97\\u673a\\u89c6\\u89c9\\uff0c\\u5404\\u4e2a\\u9886\\u57df\\u90fd\\u6709\\u7a81\\u7834\\u6027\\u8fdb\\u5c55\\u3002\\u6280\\u672f\\u7684\\u8fdb\\u6b65\\u63a8\\u52a8\\u4e86\\u4ea7\\u4e1a\\u5347\\u7ea7\\uff0c\\u540c\\u65f6\\u4e5f\\u5e26\\u6765\\u4e86\\u4f26\\u7406\\u548c\\u9690\\u79c1\\u7b49\\u6311\\u6218\\u3002\", \"max_length\": 60}}}, {\"user_intent\": \"\\u9700\\u8981\\u5bf9\\u4e00\\u6761\\u5fae\\u535a\\u5185\\u5bb9\\u751f\\u6210\\u4e2d\\u6587\\u6458\\u8981\\uff0c\\u65b9\\u4fbf\\u5feb\\u901f\\u6d4f\\u89c8\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eca\\u5929\\u5728\\u4e0a\\u6d77\\u4e3e\\u529e\\u7684\\u79d1\\u6280\\u5927\\u4f1a\\u4e0a\\uff0c\\u591a\\u5bb6AI\\u516c\\u53f8\\u5c55\\u793a\\u4e86\\u6700\\u65b0\\u7684\\u667a\\u80fd\\u4ea7\\u54c1\\uff0c\\u5305\\u62ec\\u81ea\\u52a8\\u9a7e\\u9a76\\u3001\\u8bed\\u97f3\\u52a9\\u624b\\u548c\\u667a\\u80fd\\u5bb6\\u5c45\\u8bbe\\u5907\\uff0c\\u5f15\\u53d1\\u4e86\\u5e7f\\u6cdb\\u5173\\u6ce8\\u3002\", \"max_length\": 40}}}, {\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u6559\\u80b2\\u9886\\u57df\\u7684\\u957f\\u7bc7\\u6587\\u7ae0\\uff0c\\u5e0c\\u671b\\u751f\\u6210\\u6982\\u62ec\\u6027\\u6458\\u8981\\u7528\\u4e8e\\u62a5\\u544a\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u8fd1\\u5e74\\u6765\\uff0c\\u4e2d\\u56fd\\u6559\\u80b2\\u9886\\u57df\\u8fdb\\u884c\\u4e86\\u591a\\u9879\\u6539\\u9769\\uff0c\\u5c24\\u5176\\u5728\\u4e2d\\u5c0f\\u5b66\\u8bfe\\u7a0b\\u548c\\u6559\\u5b66\\u65b9\\u6cd5\\u4e0a\\u6709\\u6240\\u8c03\\u6574\\u3002\\u5728\\u7ebf\\u6559\\u80b2\\u548c\\u667a\\u80fd\\u6559\\u80b2\\u5de5\\u5177\\u7684\\u5e94\\u7528\\u63d0\\u9ad8\\u4e86\\u5b66\\u4e60\\u6548\\u7387\\uff0c\\u540c\\u65f6\\u4e5f\\u5f15\\u53d1\\u4e86\\u6559\\u80b2\\u516c\\u5e73\\u6027\\u7684\\u8ba8\\u8bba\\u3002\", \"max_length\": 80}}}]}\n\n# User Query\nI am a climate change agency, looking to have my research summaries translated into Chinese for international audiences.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate my climate change research summaries from English to Chinese for international audiences.",
                    "tool_description": "Use a Chinese translation model to convert English text into accurate and fluent Chinese, preserving the meaning and context of the original research summaries.",
                    "expected_response": "A text output in Chinese representing the translated version of the original English summaries."
                }
            ],
            "extracted_arguments": {
                "source_text": "The English research summary text to be translated into Chinese."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_149",
        "query": "I am building an AI to create in work report, this model should be able to complete the sentence for the report.",
        "instruction": "Given a `sentence completion` task, retrieve tools that employ masked language modeling techniques to predict and fill in missing words or phrases in a text, thereby aiding in the creation of coherent and contextually appropriate report content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='xlm-roberta-base')",
                    "api_arguments": {
                        "model": "xlm-roberta-base"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='xlm-roberta-base')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": "CommonCrawl",
                        "accuracy": "N/A"
                    },
                    "description": "XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It can be used for masked language modeling and is intended to be fine-tuned on a downstream task.",
                    "id": "huggingface_api_657",
                    "name": "xlm-roberta-base"
                },
                "id": "gorilla_huggingface_tool_652",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 1: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 2: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 3: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 4: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 6: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 7: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 8: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 9: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 10: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 11: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 12: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 13: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 14: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 15: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 16: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 17: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 18: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.",
            "Tool 19: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations."
        ],
        "tool_view": {
            "description": "This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.",
                    "required": true,
                    "example": 0.25
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.",
                    "required": true,
                    "example": 0.45
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.",
                    "required": true,
                    "example": false
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections to return per image.",
                    "required": true,
                    "example": 1000
                },
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. Can be a local file path or a URL pointing to the image.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool’s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \n\n1. **`boxes`** – A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \n   - A confidence score indicating the model’s certainty that the object belongs to the target class.  \n   - Optional class label or ID identifying the object category (here, all are buildings).  \n\n2. **`masks`** – Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \n\n3. **`scores`** – Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \n\n4. **`class_ids` or `labels`** – Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \n\n5. **`metadata`** (optional) – May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \n\n6. **Visualization helpers** – Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \n\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.",
            "examples": [
                {
                    "user_intent": "I want to segment all buildings in a satellite image of a city block.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500,
                            "image": "https://example.com/satellite_city_block.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect and segment buildings in a rural area satellite image with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.6,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 200,
                            "image": "https://example.com/rural_area_satellite.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Process a satellite image to extract building masks for GIS analysis.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000,
                            "image": "https://example.com/gis_project_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 1: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 2: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 3: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 4: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 6: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 7: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 8: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 9: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 10: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 11: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 12: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 13: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 14: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 15: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 16: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 17: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 18: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.\nTool 19: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.\", \"required\": true, \"example\": 0.25}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.\", \"required\": true, \"example\": 0.45}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.\", \"required\": true, \"example\": false}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections to return per image.\", \"required\": true, \"example\": 1000}, \"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. Can be a local file path or a URL pointing to the image.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool\\u2019s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \\n\\n1. **`boxes`** \\u2013 A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \\n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \\n   - A confidence score indicating the model\\u2019s certainty that the object belongs to the target class.  \\n   - Optional class label or ID identifying the object category (here, all are buildings).  \\n\\n2. **`masks`** \\u2013 Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \\n\\n3. **`scores`** \\u2013 Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \\n\\n4. **`class_ids` or `labels`** \\u2013 Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \\n\\n5. **`metadata`** (optional) \\u2013 May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \\n\\n6. **Visualization helpers** \\u2013 Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \\n\\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.\", \"examples\": [{\"user_intent\": \"I want to segment all buildings in a satellite image of a city block.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500, \"image\": \"https://example.com/satellite_city_block.jpg\"}}}, {\"user_intent\": \"Detect and segment buildings in a rural area satellite image with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.6, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 200, \"image\": \"https://example.com/rural_area_satellite.jpg\"}}}, {\"user_intent\": \"Process a satellite image to extract building masks for GIS analysis.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://example.com/gis_project_image.jpg\"}}}]}\n\n# User Query\nI am building an AI to create in work report, this model should be able to complete the sentence for the report.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want an AI model to complete sentences in a work report.",
                    "tool_description": "Use a masked language modeling tool to predict and fill in missing words in text, enabling the AI to generate contextually appropriate sentence completions for reports.",
                    "expected_response": "The tool returns the input text with predicted words filled in at the masked positions, including confidence scores for each prediction."
                }
            ],
            "extracted_arguments": {
                "input_text": "The report text containing masked or incomplete sentences that require word prediction or completion."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_431",
        "query": "Our startup team is now building an app for diagnosing plant diseases based on images. We need to get the diagnosis for different types of plant issues.",
        "instruction": "Given an `image-based disease diagnosis` task, retrieve tools that employ zero-shot image classification to process images of plants and provide diagnostic insights into plant diseases without requiring additional training.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "clip.load('timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k')",
                    "api_arguments": "image, class_names",
                    "python_environment_requirements": "huggingface_hub, openai, transformers",
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training.",
                    "id": "huggingface_api_363",
                    "name": "timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k"
                },
                "id": "gorilla_huggingface_tool_359",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 1: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 2: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.",
            "Tool 3: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 4: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.",
            "Tool 5: A YOLOv8-based model for detecting and segmenting defects on printed circuit boards (PCBs), including issues like dry joints, incorrect installation, PCB damage, and short circuits, allowing precise localization and mask generation for quality control and inspection purposes.",
            "Tool 6: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 7: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 9: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 10: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.",
            "Tool 11: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 12: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 13: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 14: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 15: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 16: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 17: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.",
            "Tool 18: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 19: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. This can be a URL pointing to an online image or a local file path on your system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool is designed for image segmentation, specifically using the YOLOv8 model to segment buildings in satellite images. When invoked with an image, the model performs segmentation and returns results that likely include the following components:\n\n1. **Predicted Bounding Boxes**: The response will contain information about the detected bounding boxes around the segmented building areas in the input image. Each bounding box will likely be represented by its coordinates (e.g., top-left and bottom-right corners), confidence score, and possibly class information (though the focus is on building segmentation).\n\n2. **Masks**: A segmentation mask will likely be included for each detected building. This mask indicates the pixel-level segmentation for the building in the image. Each mask corresponds to a specific building and would include binary (or multi-class) values showing which pixels belong to the building.\n\n3. **Confidence Scores**: The model might return the confidence scores associated with each detected building or segmentation mask. These scores indicate the likelihood that the model correctly identified a building in the segmented area.\n\n4. **Additional Metadata**: The response might also include additional metadata like:\n   - **Model settings/overrides** used for the prediction, such as confidence thresholds (`conf`), intersection-over-union (IoU) thresholds (`iou`), and other configuration options (e.g., `agnostic_nms`).\n   - **Performance metrics** from the model evaluation on a specific dataset, such as the mAP (mean average precision) at IoU threshold 0.5 for both bounding boxes and masks.\n\nIn summary, the response would include segmentation results for the buildings in the input image, with bounding box coordinates, masks, confidence scores, and potentially some performance evaluation metrics from the underlying model.",
            "examples": [
                {
                    "user_intent": "Segment all buildings in this satellite image of a city center.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-building-segmentation')",
                        "arguments": {
                            "image": "https://example.com/satellite_images/city_center.jpg"
                        }
                    }
                },
                {
                    "user_intent": "I want to detect and outline buildings in a local aerial photo I took.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-building-segmentation')",
                        "arguments": {
                            "image": "/Users/alex/photos/aerial_building.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a satellite image from Google Earth to segment all visible buildings for urban planning.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-building-segmentation')",
                        "arguments": {
                            "image": "https://earth.google.com/satellite/buildings_area.png"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 1: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 2: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.\nTool 3: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 4: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.\nTool 5: A YOLOv8-based model for detecting and segmenting defects on printed circuit boards (PCBs), including issues like dry joints, incorrect installation, PCB damage, and short circuits, allowing precise localization and mask generation for quality control and inspection purposes.\nTool 6: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 7: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 9: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 10: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.\nTool 11: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 12: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 13: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 14: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 15: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 16: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 17: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.\nTool 18: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 19: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. This can be a URL pointing to an online image or a local file path on your system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool is designed for image segmentation, specifically using the YOLOv8 model to segment buildings in satellite images. When invoked with an image, the model performs segmentation and returns results that likely include the following components:\\n\\n1. **Predicted Bounding Boxes**: The response will contain information about the detected bounding boxes around the segmented building areas in the input image. Each bounding box will likely be represented by its coordinates (e.g., top-left and bottom-right corners), confidence score, and possibly class information (though the focus is on building segmentation).\\n\\n2. **Masks**: A segmentation mask will likely be included for each detected building. This mask indicates the pixel-level segmentation for the building in the image. Each mask corresponds to a specific building and would include binary (or multi-class) values showing which pixels belong to the building.\\n\\n3. **Confidence Scores**: The model might return the confidence scores associated with each detected building or segmentation mask. These scores indicate the likelihood that the model correctly identified a building in the segmented area.\\n\\n4. **Additional Metadata**: The response might also include additional metadata like:\\n   - **Model settings/overrides** used for the prediction, such as confidence thresholds (`conf`), intersection-over-union (IoU) thresholds (`iou`), and other configuration options (e.g., `agnostic_nms`).\\n   - **Performance metrics** from the model evaluation on a specific dataset, such as the mAP (mean average precision) at IoU threshold 0.5 for both bounding boxes and masks.\\n\\nIn summary, the response would include segmentation results for the buildings in the input image, with bounding box coordinates, masks, confidence scores, and potentially some performance evaluation metrics from the underlying model.\", \"examples\": [{\"user_intent\": \"Segment all buildings in this satellite image of a city center.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-building-segmentation')\", \"arguments\": {\"image\": \"https://example.com/satellite_images/city_center.jpg\"}}}, {\"user_intent\": \"I want to detect and outline buildings in a local aerial photo I took.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-building-segmentation')\", \"arguments\": {\"image\": \"/Users/alex/photos/aerial_building.jpg\"}}}, {\"user_intent\": \"Analyze a satellite image from Google Earth to segment all visible buildings for urban planning.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-building-segmentation')\", \"arguments\": {\"image\": \"https://earth.google.com/satellite/buildings_area.png\"}}}]}\n\n# User Query\nOur startup team is now building an app for diagnosing plant diseases based on images. We need to get the diagnosis for different types of plant issues.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze an image of a plant to detect and identify possible diseases or health issues.",
                    "tool_description": "Process an input plant image using a trained image recognition or segmentation model to detect visible disease symptoms and classify the type of plant health issue.",
                    "expected_response": "A structured result containing detected disease regions, classification labels for each identified issue, confidence scores, and possibly segmentation masks highlighting affected areas."
                }
            ],
            "extracted_arguments": {
                "plant_image": "The input image of the plant showing potential disease symptoms, provided as a URL or local file path."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_402",
        "query": "We want to enhance our search function by improving the ranking of search results.",
        "instruction": "Given a `search enhancement` task, retrieve tools that focus on improving the ranking of search results by utilizing models designed for information retrieval tasks, specifically those that leverage sequence classification to rerank query results effectively.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Information Retrieval",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')",
                    "api_arguments": {
                        "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "torch": "latest"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\nmodel.eval()\nwith torch.no_grad():\n scores = model(**features).logits\n print(scores)",
                    "performance": {
                        "dataset": "MS Marco Passage Reranking",
                        "accuracy": "MRR@10: 39.01%"
                    },
                    "description": "This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order.",
                    "id": "huggingface_api_386",
                    "name": "cross-encoder/ms-marco-MiniLM-L-6-v2"
                },
                "id": "gorilla_huggingface_tool_382",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.",
            "Tool 1: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 2: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 3: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "Tool 4: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.",
            "Tool 5: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 6: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 7: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 8: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.",
            "Tool 9: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 10: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 11: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 12: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.",
            "Tool 13: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.",
            "Tool 14: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.",
            "Tool 15: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 16: This tool uses the `sentence-transformers/paraphrase-distilroberta-base-v2` model to convert sentences or paragraphs into 768-dimensional dense vector embeddings, enabling applications such as semantic similarity comparison, clustering, and semantic search.",
            "Tool 17: The sentence-transformers/all-distilroberta-v1 model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering and semantic search. It uses the Hugging Face Transformers framework and is accessed through the SentenceTransformer API.",
            "Tool 18: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.",
            "Tool 19: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity."
        ],
        "tool_view": {
            "description": "This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pretrained model to load for sequence classification from Hugging Face Transformers.",
                    "required": true,
                    "example": "cross-encoder/ms-marco-TinyBERT-L-2-v2"
                },
                "queries": {
                    "type": "array",
                    "description": "A list of query strings for which the model will retrieve and rank relevant passages.",
                    "required": true,
                    "example": [
                        "How many people live in Berlin?",
                        "What is the population of New York City?"
                    ]
                },
                "passages": {
                    "type": "array",
                    "description": "A list of candidate passage strings corresponding to each query. Each query will be compared against all passages to compute relevance scores.",
                    "required": true,
                    "example": [
                        "Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.",
                        "New York City is famous for the Metropolitan Museum of Art."
                    ]
                },
                "padding": {
                    "type": "boolean",
                    "description": "Whether to pad sequences to the same length when tokenizing the queries and passages.",
                    "required": false,
                    "default": true,
                    "example": true
                },
                "truncation": {
                    "type": "boolean",
                    "description": "Whether to truncate sequences that are longer than the model's maximum input length during tokenization.",
                    "required": false,
                    "default": true,
                    "example": true
                },
                "return_tensors": {
                    "type": "string",
                    "description": "The format of the returned tokenized features. Typically 'pt' for PyTorch tensors.",
                    "required": false,
                    "default": "pt",
                    "example": "pt"
                }
            },
            "response": "The tool produces a numerical output representing the relevance or similarity scores between input query-passage pairs. The response is structured as a tensor (or array) of logits, where each element corresponds to a single query-passage pair from the input.  \n\n**Components of the response:**\n\n1. **Tensor/Array of logits (`scores`)**  \n   - Each value indicates the model’s predicted relevance of a specific passage to a given query.  \n   - Higher values represent higher estimated relevance.  \n   - The shape of the tensor corresponds to the number of input query-passage pairs.\n\n2. **Association to input pairs**  \n   - Each score directly maps to the query and passage pair at the same index in the input features.  \n   - This allows downstream processing, such as ranking passages by relevance for a given query.\n\n3. **Optional post-processing**  \n   - While the raw response is logits, these can be converted to probabilities or normalized scores if needed.  \n   - Sorting or ranking operations are typically applied to these scores to produce a ranked list of passages per query.  \n\nOverall, the response provides the relevance estimation necessary for Information Retrieval tasks, enabling ranking of passages according to their predicted suitability to the input query.",
            "examples": [
                {
                    "user_intent": "I want to rank a set of product descriptions based on how relevant they are to the query 'best wireless headphones'.",
                    "tool_call": {
                        "name": "cross-encoder/ms-marco-TinyBERT-L-2-v2",
                        "arguments": {
                            "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')",
                            "features": "features = tokenizer(['best wireless headphones'], ['These headphones have noise-canceling features.', 'This wired headset offers high fidelity sound.'], padding=True, truncation=True, return_tensors='pt')"
                        }
                    }
                },
                {
                    "user_intent": "I need to find the most relevant passages about 'climate change effects on polar bears' from a list of articles.",
                    "tool_call": {
                        "name": "cross-encoder/ms-marco-TinyBERT-L-2-v2",
                        "arguments": {
                            "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')",
                            "features": "features = tokenizer(['climate change effects on polar bears'], ['Polar bears are losing habitat due to melting ice.', 'Economic policies are changing worldwide.'], padding=True, truncation=True, return_tensors='pt')"
                        }
                    }
                },
                {
                    "user_intent": "Compare which city description best answers 'population of Tokyo' from multiple passages.",
                    "tool_call": {
                        "name": "cross-encoder/ms-marco-TinyBERT-L-2-v2",
                        "arguments": {
                            "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')",
                            "features": "features = tokenizer(['population of Tokyo'], ['Tokyo has over 13 million residents.', 'Paris is the capital of France.'], padding=True, truncation=True, return_tensors='pt')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.\nTool 1: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 2: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 3: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\nTool 4: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.\nTool 5: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 6: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 7: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 8: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.\nTool 9: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 10: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 11: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 12: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\nTool 13: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\nTool 14: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.\nTool 15: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 16: This tool uses the `sentence-transformers/paraphrase-distilroberta-base-v2` model to convert sentences or paragraphs into 768-dimensional dense vector embeddings, enabling applications such as semantic similarity comparison, clustering, and semantic search.\nTool 17: The sentence-transformers/all-distilroberta-v1 model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering and semantic search. It uses the Hugging Face Transformers framework and is accessed through the SentenceTransformer API.\nTool 18: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.\nTool 19: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pretrained model to load for sequence classification from Hugging Face Transformers.\", \"required\": true, \"example\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"}, \"queries\": {\"type\": \"array\", \"description\": \"A list of query strings for which the model will retrieve and rank relevant passages.\", \"required\": true, \"example\": [\"How many people live in Berlin?\", \"What is the population of New York City?\"]}, \"passages\": {\"type\": \"array\", \"description\": \"A list of candidate passage strings corresponding to each query. Each query will be compared against all passages to compute relevance scores.\", \"required\": true, \"example\": [\"Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\", \"New York City is famous for the Metropolitan Museum of Art.\"]}, \"padding\": {\"type\": \"boolean\", \"description\": \"Whether to pad sequences to the same length when tokenizing the queries and passages.\", \"required\": false, \"default\": true, \"example\": true}, \"truncation\": {\"type\": \"boolean\", \"description\": \"Whether to truncate sequences that are longer than the model's maximum input length during tokenization.\", \"required\": false, \"default\": true, \"example\": true}, \"return_tensors\": {\"type\": \"string\", \"description\": \"The format of the returned tokenized features. Typically 'pt' for PyTorch tensors.\", \"required\": false, \"default\": \"pt\", \"example\": \"pt\"}}, \"response\": \"The tool produces a numerical output representing the relevance or similarity scores between input query-passage pairs. The response is structured as a tensor (or array) of logits, where each element corresponds to a single query-passage pair from the input.  \\n\\n**Components of the response:**\\n\\n1. **Tensor/Array of logits (`scores`)**  \\n   - Each value indicates the model\\u2019s predicted relevance of a specific passage to a given query.  \\n   - Higher values represent higher estimated relevance.  \\n   - The shape of the tensor corresponds to the number of input query-passage pairs.\\n\\n2. **Association to input pairs**  \\n   - Each score directly maps to the query and passage pair at the same index in the input features.  \\n   - This allows downstream processing, such as ranking passages by relevance for a given query.\\n\\n3. **Optional post-processing**  \\n   - While the raw response is logits, these can be converted to probabilities or normalized scores if needed.  \\n   - Sorting or ranking operations are typically applied to these scores to produce a ranked list of passages per query.  \\n\\nOverall, the response provides the relevance estimation necessary for Information Retrieval tasks, enabling ranking of passages according to their predicted suitability to the input query.\", \"examples\": [{\"user_intent\": \"I want to rank a set of product descriptions based on how relevant they are to the query 'best wireless headphones'.\", \"tool_call\": {\"name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\", \"features\": \"features = tokenizer(['best wireless headphones'], ['These headphones have noise-canceling features.', 'This wired headset offers high fidelity sound.'], padding=True, truncation=True, return_tensors='pt')\"}}}, {\"user_intent\": \"I need to find the most relevant passages about 'climate change effects on polar bears' from a list of articles.\", \"tool_call\": {\"name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\", \"features\": \"features = tokenizer(['climate change effects on polar bears'], ['Polar bears are losing habitat due to melting ice.', 'Economic policies are changing worldwide.'], padding=True, truncation=True, return_tensors='pt')\"}}}, {\"user_intent\": \"Compare which city description best answers 'population of Tokyo' from multiple passages.\", \"tool_call\": {\"name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\", \"features\": \"features = tokenizer(['population of Tokyo'], ['Tokyo has over 13 million residents.', 'Paris is the capital of France.'], padding=True, truncation=True, return_tensors='pt')\"}}}]}\n\n# User Query\nWe want to enhance our search function by improving the ranking of search results.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to rank search results to improve their relevance for user queries.",
                    "tool_description": "Encode user queries and candidate passages using a cross-encoder model trained on MS Marco Passage Ranking, then compute relevance scores to rank passages.",
                    "expected_response": "A tensor or array of numerical relevance scores for each query-passage pair, allowing ranking of search results from most to least relevant."
                }
            ],
            "extracted_arguments": {
                "model_name": "The identifier of the pretrained cross-encoder model for ranking passages.",
                "queries": "A list of search query strings for which relevance rankings are needed.",
                "passages": "A list of candidate passages or documents to be ranked against each query.",
                "padding": "Optional boolean indicating whether to pad sequences during tokenization.",
                "truncation": "Optional boolean indicating whether to truncate sequences exceeding the model's maximum length.",
                "return_tensors": "Optional string specifying the output format of tokenized features, typically 'pt' for PyTorch tensors."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_217",
        "query": "I am running a chain of wine stores and I want to categorize wines and recommend them based on their quality.",
        "instruction": "Given a `wine categorization and recommendation` task, retrieve tools that utilize tabular data classification techniques to assess wine quality and generate recommendations based on quality analysis to meet the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Wine Quality classification",
                    "api_call": "joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))",
                    "api_arguments": "X",
                    "python_environment_requirements": [
                        "huggingface_hub",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = julien-c/wine-quality\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\ndata_file = cached_download(\n hf_hub_url(REPO_ID, winequality-red.csv)\n)\nwinedf = pd.read_csv(data_file, sep=;)\nX = winedf.drop([quality], axis=1)\nY = winedf[quality]\nprint(X[:3])\nlabels = model.predict(X[:3])",
                    "performance": {
                        "dataset": "winequality-red.csv",
                        "accuracy": 0.6616635397123202
                    },
                    "description": "A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.",
                    "id": "huggingface_api_847",
                    "name": "osanseviero/wine-quality"
                },
                "id": "gorilla_huggingface_tool_841",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "Tool 1: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.",
            "Tool 2: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 3: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 4: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 5: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 6: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 7: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.",
            "Tool 8: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 9: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 10: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 11: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 12: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.",
            "Tool 13: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 14: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 15: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.",
            "Tool 16: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 17: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.",
            "Tool 18: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 19: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian."
        ],
        "tool_view": {
            "description": "This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "parameters": {
                "X": {
                    "type": "DataFrame",
                    "description": "A pandas DataFrame containing the input features for predicting wine quality. Each row represents a wine sample, and each column represents a feature.",
                    "required": true,
                    "example": "pd.DataFrame({'fixed_acidity': [7.4, 7.8, 7.8], 'volatile_acidity': [0.70, 0.88, 0.76], 'citric_acid': [0.00, 0.00, 0.04]})"
                }
            },
            "response": "The tool returns a prediction output from a trained Scikit-learn classification model for wine quality. The response structure is as follows:\n\n- **Type**: Array or list of discrete labels.  \n- **Length**: Matches the number of input feature vectors provided in `X`. Each element corresponds to the predicted class for one input sample.  \n- **Element meaning**: Each label represents the predicted quality category of a wine sample, as determined by the model trained on the wine quality dataset. The labels are typically integer values corresponding to quality scores from the original dataset.  \n\nIn principle, the response provides a one-to-one mapping between input feature rows and predicted wine quality classes, enabling downstream evaluation or usage in further processing.",
            "examples": [
                {
                    "user_intent": "I want to classify wine quality using a pre-trained Scikit-learn model.",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "wine data to classify"
                        }
                    }
                },
                {
                    "user_intent": "Can I load a model and use it to predict the quality of wines from a CSV file?",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "data from winequality-red.csv"
                        }
                    }
                },
                {
                    "user_intent": "How do I load a pre-trained wine quality classifier and make predictions on a sample dataset?",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "sample wine data from winequality-red.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\nTool 1: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.\nTool 2: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 3: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 4: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 5: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 6: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 7: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\nTool 8: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 9: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 10: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 11: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 12: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.\nTool 13: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 14: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 15: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.\nTool 16: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 17: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.\nTool 18: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 19: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\", \"parameters\": {\"X\": {\"type\": \"DataFrame\", \"description\": \"A pandas DataFrame containing the input features for predicting wine quality. Each row represents a wine sample, and each column represents a feature.\", \"required\": true, \"example\": \"pd.DataFrame({'fixed_acidity': [7.4, 7.8, 7.8], 'volatile_acidity': [0.70, 0.88, 0.76], 'citric_acid': [0.00, 0.00, 0.04]})\"}}, \"response\": \"The tool returns a prediction output from a trained Scikit-learn classification model for wine quality. The response structure is as follows:\\n\\n- **Type**: Array or list of discrete labels.  \\n- **Length**: Matches the number of input feature vectors provided in `X`. Each element corresponds to the predicted class for one input sample.  \\n- **Element meaning**: Each label represents the predicted quality category of a wine sample, as determined by the model trained on the wine quality dataset. The labels are typically integer values corresponding to quality scores from the original dataset.  \\n\\nIn principle, the response provides a one-to-one mapping between input feature rows and predicted wine quality classes, enabling downstream evaluation or usage in further processing.\", \"examples\": [{\"user_intent\": \"I want to classify wine quality using a pre-trained Scikit-learn model.\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"wine data to classify\"}}}, {\"user_intent\": \"Can I load a model and use it to predict the quality of wines from a CSV file?\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"data from winequality-red.csv\"}}}, {\"user_intent\": \"How do I load a pre-trained wine quality classifier and make predictions on a sample dataset?\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"sample wine data from winequality-red.csv\"}}}]}\n\n# User Query\nI am running a chain of wine stores and I want to categorize wines and recommend them based on their quality.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify the quality of wines using a pre-trained model.",
                    "tool_description": "Use a Scikit-learn classification model to predict the quality category of wines based on their input features from tabular data.",
                    "expected_response": "A list of predicted quality scores for each wine sample, matching the number of input records."
                },
                {
                    "user_intent": "Recommend wines to customers based on their predicted quality scores.",
                    "tool_description": "Generate personalized wine recommendations by ranking or filtering wines according to their predicted quality scores.",
                    "expected_response": "A ranked list of wines or a set of wine recommendations suitable for customer selection."
                }
            ],
            "extracted_arguments": {
                "wine_features": "A structured dataset containing numerical and categorical attributes of wines used for quality prediction.",
                "predicted_quality": "The output quality score or category for each wine, used to guide recommendations."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_450",
        "query": "We want to develop a chatbot that can engage with multilingual users. Please help us create a model to encode sentences and understand user input in various languages.",
        "instruction": "Given a `multilingual chatbot development` task, retrieve tools that encode and understand user input in multiple languages by leveraging models capable of extracting language-agnostic sentence embeddings, enabling the chatbot to effectively handle diverse linguistic interactions.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BertModel.from_pretrained('setu4993/LaBSE')",
                    "api_arguments": [
                        "english_sentences",
                        "italian_sentences",
                        "japanese_sentences"
                    ],
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "import torch\nfrom transformers import BertModel, BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\nmodel = model.eval()\nenglish_sentences = [\n 'dog',\n 'Puppies are nice.',\n 'I enjoy taking long walks along the beach with my dog.',\n]\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\nwith torch.no_grad():\n english_outputs = model(**english_inputs)\nenglish_embeddings = english_outputs.pooler_output",
                    "performance": {
                        "dataset": "CommonCrawl and Wikipedia",
                        "accuracy": "Not Specified"
                    },
                    "description": "Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.",
                    "id": "huggingface_api_25",
                    "name": "setu4993/LaBSE"
                },
                "id": "gorilla_huggingface_tool_25",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 1: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 2: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.",
            "Tool 3: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 4: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 5: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.",
            "Tool 6: XLM-RoBERTa is a multilingual model designed for masked language modeling, pre-trained on 2.5TB of CommonCrawl data in 100 languages. It can be used to predict masked words in sentences and is intended for fine-tuning on specific downstream tasks. This model is accessed through the Hugging Face Transformers pipeline.",
            "Tool 7: mT5 is a multilingual text-to-text model based on the T5 architecture, pre-trained on a large Common Crawl dataset covering 101 languages. It uses a unified text-to-text framework to perform a wide range of multilingual NLP tasks, such as translation, summarization, and text generation, enabling consistent input-to-output processing across different languages.",
            "Tool 8: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 9: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.",
            "Tool 10: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 11: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 12: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 13: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 14: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.",
            "Tool 15: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 16: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 17: The \"sentence-transformers/all-MiniLM-L12-v1\" model maps sentences and paragraphs to a 384-dimensional dense vector space, enabling tasks like clustering and semantic search. It uses the SentenceTransformer framework from Hugging Face Transformers to encode sentences into embeddings for efficient similarity comparison.",
            "Tool 18: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 19: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity."
        ],
        "tool_view": {
            "description": "BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "parameters": {
                "message": {
                    "type": "string",
                    "description": "The input message or query that will initiate the conversation with the Blenderbot model.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a conversational response generated by the BlenderBot chatbot model. The response structure can be described as follows:  \n\n- **Text content**: The primary component is the textual reply from the chatbot, which is intended to be contextually relevant to the input message. This text reflects the model’s attempt to engage naturally, incorporating knowledge, empathy, personality, and appropriate conversational cues.  \n\n- **Optional metadata (typical in text-generation frameworks)**: While not explicitly defined in the JSON, responses from Hugging Face text-to-text models often include additional fields such as:  \n  - **generated tokens or IDs**: The underlying token sequence used to produce the text.  \n  - **generation info**: Parameters or scores related to the generation process, such as probabilities, beam search details, or sequence length.  \n\nIn practice, the tool accepts a single input, `\"message\"`, and outputs a single text string representing the chatbot’s reply. This reply is the main actionable content for downstream use, such as displaying to a user in a conversational interface.",
            "examples": [
                {
                    "user_intent": "I want to have a friendly chat about movies and get some recommendations.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "Hi! Can you suggest some good movies to watch this weekend?"
                        }
                    }
                },
                {
                    "user_intent": "I'm feeling a bit down and need someone to talk to for comfort.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "I'm feeling sad today. Can we talk for a bit?"
                        }
                    }
                },
                {
                    "user_intent": "I want to have a casual conversation about hobbies and interests.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "Hey! What are some fun hobbies you think I should try?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 1: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 2: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.\nTool 3: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 4: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 5: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\nTool 6: XLM-RoBERTa is a multilingual model designed for masked language modeling, pre-trained on 2.5TB of CommonCrawl data in 100 languages. It can be used to predict masked words in sentences and is intended for fine-tuning on specific downstream tasks. This model is accessed through the Hugging Face Transformers pipeline.\nTool 7: mT5 is a multilingual text-to-text model based on the T5 architecture, pre-trained on a large Common Crawl dataset covering 101 languages. It uses a unified text-to-text framework to perform a wide range of multilingual NLP tasks, such as translation, summarization, and text generation, enabling consistent input-to-output processing across different languages.\nTool 8: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 9: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.\nTool 10: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 11: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 12: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 13: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 14: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.\nTool 15: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 16: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 17: The \"sentence-transformers/all-MiniLM-L12-v1\" model maps sentences and paragraphs to a 384-dimensional dense vector space, enabling tasks like clustering and semantic search. It uses the SentenceTransformer framework from Hugging Face Transformers to encode sentences into embeddings for efficient similarity comparison.\nTool 18: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 19: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\", \"parameters\": {\"message\": {\"type\": \"string\", \"description\": \"The input message or query that will initiate the conversation with the Blenderbot model.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a conversational response generated by the BlenderBot chatbot model. The response structure can be described as follows:  \\n\\n- **Text content**: The primary component is the textual reply from the chatbot, which is intended to be contextually relevant to the input message. This text reflects the model\\u2019s attempt to engage naturally, incorporating knowledge, empathy, personality, and appropriate conversational cues.  \\n\\n- **Optional metadata (typical in text-generation frameworks)**: While not explicitly defined in the JSON, responses from Hugging Face text-to-text models often include additional fields such as:  \\n  - **generated tokens or IDs**: The underlying token sequence used to produce the text.  \\n  - **generation info**: Parameters or scores related to the generation process, such as probabilities, beam search details, or sequence length.  \\n\\nIn practice, the tool accepts a single input, `\\\"message\\\"`, and outputs a single text string representing the chatbot\\u2019s reply. This reply is the main actionable content for downstream use, such as displaying to a user in a conversational interface.\", \"examples\": [{\"user_intent\": \"I want to have a friendly chat about movies and get some recommendations.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"Hi! Can you suggest some good movies to watch this weekend?\"}}}, {\"user_intent\": \"I'm feeling a bit down and need someone to talk to for comfort.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"I'm feeling sad today. Can we talk for a bit?\"}}}, {\"user_intent\": \"I want to have a casual conversation about hobbies and interests.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"Hey! What are some fun hobbies you think I should try?\"}}}]}\n\n# User Query\nWe want to develop a chatbot that can engage with multilingual users. Please help us create a model to encode sentences and understand user input in various languages.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Create a chatbot that can engage in natural conversations with users.",
                    "tool_description": "Deploy a conversational chatbot model capable of generating contextually relevant replies, demonstrating knowledge, empathy, and personality in multi-turn dialogues.",
                    "expected_response": "A text response generated by the chatbot that is contextually appropriate and conversationally coherent."
                },
                {
                    "user_intent": "Encode user input sentences from multiple languages for understanding and semantic analysis.",
                    "tool_description": "Use a multilingual sentence embedding model to convert sentences or paragraphs from various languages into dense vector representations for semantic understanding and comparison.",
                    "expected_response": "A vector or embedding representation of the input sentences suitable for similarity comparison, clustering, or further processing in a multilingual NLP pipeline."
                }
            ],
            "extracted_arguments": {
                "message": "The input message or query from the user that the chatbot will respond to.",
                "sentence_input": "The user input text in any supported language to be encoded into a semantic vector representation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_19",
        "query": "Design a text-based interface where you can input text and get an image description from the given text.",
        "instruction": "Given a `text-based interface design` task, retrieve tools that transform text descriptions into visual outputs by generating images from the provided textual inputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "pipeline('text-to-image', model='prompthero/openjourney-v4')",
                    "api_arguments": {
                        "text": "string"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "generate_image('your text here')",
                    "performance": {
                        "dataset": "Midjourney v4 images",
                        "accuracy": "Not provided"
                    },
                    "description": "Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.",
                    "id": "huggingface_api_42",
                    "name": "prompthero/openjourney-v4"
                },
                "id": "gorilla_huggingface_tool_42",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 1: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.",
            "Tool 2: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 3: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 5: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 6: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 7: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.",
            "Tool 8: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 9: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 10: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 11: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.",
            "Tool 12: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.",
            "Tool 13: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 14: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.",
            "Tool 15: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 16: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.",
            "Tool 17: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.",
            "Tool 18: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.",
            "Tool 19: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "What is the capital of Germany?"
                },
                "context": {
                    "type": "string",
                    "description": "The text or passage that contains the information needed to answer the question.",
                    "required": true,
                    "example": "Berlin is the capital of Germany."
                }
            },
            "response": "The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \n\n1. **`answer`** – A string representing the model’s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \n\n2. **`score`** – A numerical confidence score indicating the model’s certainty about the predicted answer. Higher values correspond to higher confidence.  \n\n3. **`start`** – An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \n\n4. **`end`** – An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \n\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.",
            "examples": [
                {
                    "user_intent": "What's the main ingredient in a Caesar salad according to this recipe?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "What is the main ingredient in a Caesar salad?",
                            "context": "A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing."
                        }
                    }
                },
                {
                    "user_intent": "Who wrote the novel 'Pride and Prejudice'?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "Who is the author of 'Pride and Prejudice'?",
                            "context": "'Pride and Prejudice' is a classic novel written by Jane Austen in 1813."
                        }
                    }
                },
                {
                    "user_intent": "When did the Apollo 11 mission land on the moon?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "When did Apollo 11 land on the moon?",
                            "context": "Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 1: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.\nTool 2: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 3: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 5: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 6: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 7: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.\nTool 8: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 9: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 10: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 11: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.\nTool 12: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.\nTool 13: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 14: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.\nTool 15: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 16: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\nTool 17: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.\nTool 18: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\nTool 19: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"What is the capital of Germany?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text or passage that contains the information needed to answer the question.\", \"required\": true, \"example\": \"Berlin is the capital of Germany.\"}}, \"response\": \"The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \\n\\n2. **`score`** \\u2013 A numerical confidence score indicating the model\\u2019s certainty about the predicted answer. Higher values correspond to higher confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \\n\\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.\", \"examples\": [{\"user_intent\": \"What's the main ingredient in a Caesar salad according to this recipe?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"What is the main ingredient in a Caesar salad?\", \"context\": \"A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing.\"}}}, {\"user_intent\": \"Who wrote the novel 'Pride and Prejudice'?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"Who is the author of 'Pride and Prejudice'?\", \"context\": \"'Pride and Prejudice' is a classic novel written by Jane Austen in 1813.\"}}}, {\"user_intent\": \"When did the Apollo 11 mission land on the moon?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"When did Apollo 11 land on the moon?\", \"context\": \"Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969.\"}}}]}\n\n# User Query\nDesign a text-based interface where you can input text and get an image description from the given text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a description of an image based on a provided text prompt.",
                    "tool_description": "Use a multimodal model to generate text that describes the content of an image, based on a given textual input or image-based context.",
                    "expected_response": "A detailed textual description of an image generated from the provided prompt."
                }
            ],
            "extracted_arguments": {
                "text_prompt": "A natural language description that serves as the basis for generating an image description."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_28",
        "query": "We have a company that processes loan applications. Give us the answer for a question, based on a document.",
        "instruction": "Given a `document question answering` task, retrieve tools that specialize in extracting answers from documents by processing textual queries and leveraging advanced language models to provide accurate responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3",
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 8.5806
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.",
                    "id": "huggingface_api_140",
                    "name": "tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa"
                },
                "id": "gorilla_huggingface_tool_136",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 1: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 2: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 3: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 4: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 5: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 6: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 7: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 8: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 9: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 10: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 11: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 12: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 14: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 15: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 16: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.",
            "Tool 17: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 18: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 19: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset."
        ],
        "tool_view": {
            "description": "CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "parameters": {
                "url": {
                    "type": "string",
                    "description": "The URL of the document image or PDF from which information will be extracted to answer the question.",
                    "required": true,
                    "example": "https://templates.invoicehome.com/invoice-template-us-neat-750px.png"
                },
                "question": {
                    "type": "string",
                    "description": "The question to ask about the content of the document, which the model will attempt to answer.",
                    "required": true,
                    "example": "What is the invoice number?"
                }
            },
            "response": "The tool returns an answer to the question posed, based on a multimodal document (such as a PDF or image) provided in the `url` argument. The response consists of the following elements:\n\n1. **Answer**: The extracted information or the response to the user’s query based on the contents of the document provided via the `url`. This is typically a string of text.\n\n2. **Context**: The portion of the document that was used to derive the answer. This may include key excerpts or text passages from the document that directly relate to the answer. (Note: While not explicitly mentioned in the tool documentation, such context is often included in similar multimodal document question-answering systems to provide transparency on the answer derivation process.)\n\n3. **Accuracy (Optional)**: Although the performance section indicates accuracy metrics from evaluations on a dataset, it is not clear whether this metric is included in the direct response. However, it could be part of internal evaluation or debugging.\n\n4. **Error or No Answer (Optional)**: If the model is unable to find a relevant answer or cannot process the document properly, it may return an error message or indicate that no answer was found.\n\nThis response format is designed to provide the answer to a specific question based on the input document URL (which could be an image, PDF, etc.), leveraging the model's capability to interpret both the textual and visual content.",
            "examples": [
                {
                    "user_intent": "I want to extract the invoice number from an image of an invoice.",
                    "tool_call": {
                        "name": "CQI_Visual_Question_Awnser_PT_v0",
                        "arguments": {
                            "url": "https://templates.invoicehome.com/invoice-template-us-neat-750px.png",
                            "question": "What is the invoice number?"
                        }
                    }
                },
                {
                    "user_intent": "Can you tell me the purchase amount from this receipt image?",
                    "tool_call": {
                        "name": "CQI_Visual_Question_Awnser_PT_v0",
                        "arguments": {
                            "url": "https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg",
                            "question": "What is the purchase amount?"
                        }
                    }
                },
                {
                    "user_intent": "I need to know the net sales for the year 2020 from this income statement.",
                    "tool_call": {
                        "name": "CQI_Visual_Question_Awnser_PT_v0",
                        "arguments": {
                            "url": "https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png",
                            "question": "What are the 2020 net sales?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 1: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 2: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 3: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 4: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 5: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 6: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 7: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 8: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 9: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 10: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 11: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 12: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 14: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 15: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 16: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.\nTool 17: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 18: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 19: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\", \"parameters\": {\"url\": {\"type\": \"string\", \"description\": \"The URL of the document image or PDF from which information will be extracted to answer the question.\", \"required\": true, \"example\": \"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\"}, \"question\": {\"type\": \"string\", \"description\": \"The question to ask about the content of the document, which the model will attempt to answer.\", \"required\": true, \"example\": \"What is the invoice number?\"}}, \"response\": \"The tool returns an answer to the question posed, based on a multimodal document (such as a PDF or image) provided in the `url` argument. The response consists of the following elements:\\n\\n1. **Answer**: The extracted information or the response to the user\\u2019s query based on the contents of the document provided via the `url`. This is typically a string of text.\\n\\n2. **Context**: The portion of the document that was used to derive the answer. This may include key excerpts or text passages from the document that directly relate to the answer. (Note: While not explicitly mentioned in the tool documentation, such context is often included in similar multimodal document question-answering systems to provide transparency on the answer derivation process.)\\n\\n3. **Accuracy (Optional)**: Although the performance section indicates accuracy metrics from evaluations on a dataset, it is not clear whether this metric is included in the direct response. However, it could be part of internal evaluation or debugging.\\n\\n4. **Error or No Answer (Optional)**: If the model is unable to find a relevant answer or cannot process the document properly, it may return an error message or indicate that no answer was found.\\n\\nThis response format is designed to provide the answer to a specific question based on the input document URL (which could be an image, PDF, etc.), leveraging the model's capability to interpret both the textual and visual content.\", \"examples\": [{\"user_intent\": \"I want to extract the invoice number from an image of an invoice.\", \"tool_call\": {\"name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"arguments\": {\"url\": \"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\", \"question\": \"What is the invoice number?\"}}}, {\"user_intent\": \"Can you tell me the purchase amount from this receipt image?\", \"tool_call\": {\"name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"arguments\": {\"url\": \"https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\", \"question\": \"What is the purchase amount?\"}}}, {\"user_intent\": \"I need to know the net sales for the year 2020 from this income statement.\", \"tool_call\": {\"name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"arguments\": {\"url\": \"https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\", \"question\": \"What are the 2020 net sales?\"}}}]}\n\n# User Query\nWe have a company that processes loan applications. Give us the answer for a question, based on a document.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract information from a document related to a loan application by asking a specific question.",
                    "tool_description": "Use a multimodal document question-answering tool to process a document (image or PDF) and answer a user-provided question, interpreting both textual and visual content.",
                    "expected_response": "An answer string derived from the document content, potentially including context from the document and error or no-answer indications if applicable."
                }
            ],
            "extracted_arguments": {
                "document_url": "The URL of the document image or PDF to be analyzed.",
                "question": "The specific question regarding the document content that requires an answer."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_324",
        "query": "We want to communicate product information to online customers. Translate the information from English to French.",
        "instruction": "Given a `language translation` task, retrieve tools that process text inputs to translate content from one language to another, ensuring accurate and context-aware translations aligned with the query's language pair and communication requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation, Summarization, Question Answering, Sentiment Analysis",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('t5-3b')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": "transformers",
                    "example_code": "input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14"
                    },
                    "description": "T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.",
                    "id": "huggingface_api_535",
                    "name": "t5-3b"
                },
                "id": "gorilla_huggingface_tool_531",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 1: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 2: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 3: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 4: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 5: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 6: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 7: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 8: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 9: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 10: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 11: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 12: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 13: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 14: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 16: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 17: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 18: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 19: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents."
        ],
        "tool_view": {
            "description": "The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in French that needs to be translated into English.",
                    "required": true,
                    "example": "Bonjour, comment ça va?"
                }
            },
            "response": "The tool is a machine translation model that translates text from French to English. The response from this tool would typically contain the translated text corresponding to the input text provided.\n\nHere’s a description of the expected response:\n\n- **Translated Text**: This is the primary component of the response. It will be a string containing the English translation of the French input text provided to the model.\n  \nIn principle, the response will consist of the following:\n\n1. **Translated Text**: A string containing the English translation of the input French text.\n\nThe content will be directly derived from the model's output and is intended to provide a meaningful English equivalent for the French input text. The tool will output the translation in response to a given input phrase or sentence in French.\n\nNo other metadata (such as confidence scores or intermediate values) is expected to be included in the response unless further specified by the tool’s design or configuration.",
            "examples": [
                {
                    "user_intent": "Translate this French sentence to English: 'Je voudrais réserver une table pour deux ce soir.'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Je voudrais réserver une table pour deux ce soir."
                        }
                    }
                },
                {
                    "user_intent": "I need the English version of this French paragraph about climate change: 'Le réchauffement climatique est une menace sérieuse pour notre planète.'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Le réchauffement climatique est une menace sérieuse pour notre planète."
                        }
                    }
                },
                {
                    "user_intent": "Convert this casual French message into English: 'Salut! On se voit demain au café?'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Salut! On se voit demain au café?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 1: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 2: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 3: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 4: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 5: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 6: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 7: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 8: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 9: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 10: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 11: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 12: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 13: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 14: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 16: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 17: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 18: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 19: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in French that needs to be translated into English.\", \"required\": true, \"example\": \"Bonjour, comment \\u00e7a va?\"}}, \"response\": \"The tool is a machine translation model that translates text from French to English. The response from this tool would typically contain the translated text corresponding to the input text provided.\\n\\nHere\\u2019s a description of the expected response:\\n\\n- **Translated Text**: This is the primary component of the response. It will be a string containing the English translation of the French input text provided to the model.\\n  \\nIn principle, the response will consist of the following:\\n\\n1. **Translated Text**: A string containing the English translation of the input French text.\\n\\nThe content will be directly derived from the model's output and is intended to provide a meaningful English equivalent for the French input text. The tool will output the translation in response to a given input phrase or sentence in French.\\n\\nNo other metadata (such as confidence scores or intermediate values) is expected to be included in the response unless further specified by the tool\\u2019s design or configuration.\", \"examples\": [{\"user_intent\": \"Translate this French sentence to English: 'Je voudrais r\\u00e9server une table pour deux ce soir.'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Je voudrais r\\u00e9server une table pour deux ce soir.\"}}}, {\"user_intent\": \"I need the English version of this French paragraph about climate change: 'Le r\\u00e9chauffement climatique est une menace s\\u00e9rieuse pour notre plan\\u00e8te.'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Le r\\u00e9chauffement climatique est une menace s\\u00e9rieuse pour notre plan\\u00e8te.\"}}}, {\"user_intent\": \"Convert this casual French message into English: 'Salut! On se voit demain au caf\\u00e9?'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Salut! On se voit demain au caf\\u00e9?\"}}}]}\n\n# User Query\nWe want to communicate product information to online customers. Translate the information from English to French.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate product information from English to French for online customers.",
                    "tool_description": "Convert input text from English into French using a machine translation model, ensuring accurate and fluent translation suitable for customer-facing content.",
                    "expected_response": "A string containing the French translation of the provided English product information."
                }
            ],
            "extracted_arguments": {
                "text": "The English product information text that needs to be translated into French."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_143",
        "query": "I'm an author and want to create a short video based on a brief passage from my book. Can you generate a video based on this text?",
        "instruction": "Given a `text-to-video` task, retrieve tools that transform text inputs into video outputs, aligning the generated video content with the narrative provided in the text.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Video",
                    "api_call": "pipeline('text-to-video', model='camenduru/text2-video-zero')",
                    "api_arguments": [
                        "input_text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.",
                    "id": "huggingface_api_93",
                    "name": "camenduru/text2-video-zero"
                },
                "id": "gorilla_huggingface_tool_93",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 1: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 2: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 3: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 4: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 5: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 6: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 7: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 8: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 9: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 10: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 11: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 12: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.",
            "Tool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 14: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 15: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 16: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 17: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 18: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 19: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy."
        ],
        "tool_view": {
            "description": "A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A natural language text description that specifies the content and actions to appear in the generated video.",
                    "required": true,
                    "example": "A superhero flying over a city at sunset"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of diffusion steps to run during video generation. Higher values typically improve quality but increase computation time.",
                    "required": true,
                    "example": 25
                },
                "num_frames": {
                    "type": "number",
                    "description": "The total number of video frames to generate for the output video.",
                    "required": true,
                    "example": 16
                }
            },
            "response": "The tool performs text-to-video synthesis using a multimodal diffusion model, where the response is expected to be a video generated from the provided textual prompt.\n\n### Expected Response Structure:\n\n1. **Video Frames**:\n   - **Type**: Array/Sequence of Image Frames\n   - **Description**: The primary output consists of a sequence of video frames that correspond to the given text prompt. These frames represent the visual content generated based on the input description.\n   - **Meaning**: Each frame is a snapshot of the video at a specific moment in time, forming part of the overall video when played sequentially.\n\n2. **Video File (Optional)**:\n   - **Type**: Path or URL\n   - **Description**: A reference to the final video file generated from the sequence of frames. This could be a local path or an online URL where the video can be accessed.\n   - **Meaning**: This component encapsulates the entire video in a playable format.\n\n3. **Inference Details (Optional)**:\n   - **Type**: Metadata (optional)\n   - **Description**: Information regarding the number of inference steps or frames used to generate the video. This can include:\n     - `num_inference_steps`: Number of inference steps taken during the model's process.\n     - `num_frames`: Total number of frames in the generated video.\n   - **Meaning**: Provides insights into the model's processing parameters and can be useful for debugging or understanding the quality/complexity of the generated video.\n\nIn summary, the response contains primarily the frames of the video and may include additional metadata about the inference process and a reference to the final video file if applicable.",
            "examples": [
                {
                    "user_intent": "I want to create a video based on the text 'A dog playing in the park'. Can you generate that for me?",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A dog playing in the park",
                            "num_inference_steps": 30,
                            "num_frames": 60
                        }
                    }
                },
                {
                    "user_intent": "Can you make a video from the description 'A futuristic city at night with flying cars'?",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A futuristic city at night with flying cars",
                            "num_inference_steps": 40,
                            "num_frames": 90
                        }
                    }
                },
                {
                    "user_intent": "I need a short video based on the phrase 'A cat jumping over a fence' with minimal steps.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A cat jumping over a fence",
                            "num_inference_steps": 15,
                            "num_frames": 30
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 1: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 2: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 3: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 4: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 5: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 6: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 7: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 8: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 9: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 10: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 11: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 12: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.\nTool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 14: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 15: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 16: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 17: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 18: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 19: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A natural language text description that specifies the content and actions to appear in the generated video.\", \"required\": true, \"example\": \"A superhero flying over a city at sunset\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of diffusion steps to run during video generation. Higher values typically improve quality but increase computation time.\", \"required\": true, \"example\": 25}, \"num_frames\": {\"type\": \"number\", \"description\": \"The total number of video frames to generate for the output video.\", \"required\": true, \"example\": 16}}, \"response\": \"The tool performs text-to-video synthesis using a multimodal diffusion model, where the response is expected to be a video generated from the provided textual prompt.\\n\\n### Expected Response Structure:\\n\\n1. **Video Frames**:\\n   - **Type**: Array/Sequence of Image Frames\\n   - **Description**: The primary output consists of a sequence of video frames that correspond to the given text prompt. These frames represent the visual content generated based on the input description.\\n   - **Meaning**: Each frame is a snapshot of the video at a specific moment in time, forming part of the overall video when played sequentially.\\n\\n2. **Video File (Optional)**:\\n   - **Type**: Path or URL\\n   - **Description**: A reference to the final video file generated from the sequence of frames. This could be a local path or an online URL where the video can be accessed.\\n   - **Meaning**: This component encapsulates the entire video in a playable format.\\n\\n3. **Inference Details (Optional)**:\\n   - **Type**: Metadata (optional)\\n   - **Description**: Information regarding the number of inference steps or frames used to generate the video. This can include:\\n     - `num_inference_steps`: Number of inference steps taken during the model's process.\\n     - `num_frames`: Total number of frames in the generated video.\\n   - **Meaning**: Provides insights into the model's processing parameters and can be useful for debugging or understanding the quality/complexity of the generated video.\\n\\nIn summary, the response contains primarily the frames of the video and may include additional metadata about the inference process and a reference to the final video file if applicable.\", \"examples\": [{\"user_intent\": \"I want to create a video based on the text 'A dog playing in the park'. Can you generate that for me?\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A dog playing in the park\", \"num_inference_steps\": 30, \"num_frames\": 60}}}, {\"user_intent\": \"Can you make a video from the description 'A futuristic city at night with flying cars'?\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A futuristic city at night with flying cars\", \"num_inference_steps\": 40, \"num_frames\": 90}}}, {\"user_intent\": \"I need a short video based on the phrase 'A cat jumping over a fence' with minimal steps.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A cat jumping over a fence\", \"num_inference_steps\": 15, \"num_frames\": 30}}}]}\n\n# User Query\nI'm an author and want to create a short video based on a brief passage from my book. Can you generate a video based on this text?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a video based on a brief passage from my book.",
                    "tool_description": "Generate a video by converting a text description from a book into a visual format using a text-to-video synthesis model. The video is created through a sequence of frames generated progressively from the input text.",
                    "expected_response": "A sequence of video frames representing the content of the provided passage, with an optional reference to the final video file or metadata about the inference process."
                }
            ],
            "extracted_arguments": {
                "prompt": "The natural language text description from the user's book passage.",
                "num_inference_steps": "The number of diffusion steps to run for video generation, impacting the quality and processing time.",
                "num_frames": "The total number of frames to generate for the output video."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_336",
        "query": "Our company collects data on the salesperson performance in different regions for each month. We want to use the most suitable API from our subscribed_huggingface.co to analyze that in specific table format and then based on provided question, answer accordingly.",
        "instruction": "Given a `table analysis and question answering` task, retrieve tools that utilize natural language processing capabilities to analyze table data and answer questions based on the provided input using table comprehension techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')",
                    "api_arguments": {
                        "model_name": "google/tapas-base-finetuned-wtq"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.12.0"
                    },
                    "example_code": "from transformers import TapasTokenizer, TapasForQuestionAnswering\n\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": 0.4638
                    },
                    "description": "TAPAS base model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion, and then fine-tuned on SQA, WikiSQL, and finally WTQ. It can be used for answering questions related to a table.",
                    "id": "huggingface_api_430",
                    "name": "google/tapas-base-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_426",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 1: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 2: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 3: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 4: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 5: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 6: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.",
            "Tool 7: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 8: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 9: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 10: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 11: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 12: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 13: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 14: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.",
            "Tool 15: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 16: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 17: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.",
            "Tool 18: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 19: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks."
        ],
        "tool_view": {
            "description": "OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "parameters": {
                "table": {
                    "type": "pd.DataFrame",
                    "description": "A pandas DataFrame containing the table data on which the question answering will be performed. Each column should represent a feature and each row a record.",
                    "required": true,
                    "example": "pd.DataFrame({'year': [1896, 1900], 'city': ['athens', 'paris']})"
                },
                "query": {
                    "type": "string",
                    "description": "A natural language question string to be answered using the provided table data.",
                    "required": true,
                    "example": "In which year did Beijing host the Olympic Games?"
                }
            },
            "response": "The tool produces a textual answer to a natural language question about the contents of a table. The response is typically structured as a sequence of text tokens decoded into a human-readable string. Specifically:  \n\n- **Primary content:** A single or multi-sentence string that directly answers the input query based on the data in the provided table.  \n- **Format:** The output is returned as a decoded string (from token IDs generated by the model), with special tokens removed if requested (e.g., `skip_special_tokens=True`).  \n- **Optional batch structure:** If multiple queries or tables are provided, the output can be a list of strings, each corresponding to one query-table pair.  \n- **Semantic role:** Each output string conveys the model’s prediction of the answer to the user’s question, derived by reasoning over the table structure, headers, and cell values.  \n\nThere are no additional metadata fields returned by the model by default; the response is purely the generated answer text.",
            "examples": [
                {
                    "user_intent": "I have a table of countries and their populations. I want to know which country has the largest population.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Country': ['USA', 'China', 'India', 'Brazil'], 'Population': [331000000, 1441000000, 1393000000, 212000000]})",
                            "query": "Which country has the largest population?"
                        }
                    }
                },
                {
                    "user_intent": "Given a table of sales data per quarter, I want to find out in which quarter the company made the highest revenue.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Quarter': ['Q1', 'Q2', 'Q3', 'Q4'], 'Revenue': [120000, 150000, 130000, 170000]})",
                            "query": "Which quarter had the highest revenue?"
                        }
                    }
                },
                {
                    "user_intent": "I have Olympic host cities and years in a table. I want to know when London hosted the Olympics.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Year': [1896, 1900, 1904, 2004, 2008, 2012], 'City': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']})",
                            "query": "In which year did London host the Olympic Games?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 1: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 2: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 3: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 4: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 5: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 6: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.\nTool 7: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 8: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 9: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 10: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 11: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 12: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 13: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 14: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.\nTool 15: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 16: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 17: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.\nTool 18: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 19: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\", \"parameters\": {\"table\": {\"type\": \"pd.DataFrame\", \"description\": \"A pandas DataFrame containing the table data on which the question answering will be performed. Each column should represent a feature and each row a record.\", \"required\": true, \"example\": \"pd.DataFrame({'year': [1896, 1900], 'city': ['athens', 'paris']})\"}, \"query\": {\"type\": \"string\", \"description\": \"A natural language question string to be answered using the provided table data.\", \"required\": true, \"example\": \"In which year did Beijing host the Olympic Games?\"}}, \"response\": \"The tool produces a textual answer to a natural language question about the contents of a table. The response is typically structured as a sequence of text tokens decoded into a human-readable string. Specifically:  \\n\\n- **Primary content:** A single or multi-sentence string that directly answers the input query based on the data in the provided table.  \\n- **Format:** The output is returned as a decoded string (from token IDs generated by the model), with special tokens removed if requested (e.g., `skip_special_tokens=True`).  \\n- **Optional batch structure:** If multiple queries or tables are provided, the output can be a list of strings, each corresponding to one query-table pair.  \\n- **Semantic role:** Each output string conveys the model\\u2019s prediction of the answer to the user\\u2019s question, derived by reasoning over the table structure, headers, and cell values.  \\n\\nThere are no additional metadata fields returned by the model by default; the response is purely the generated answer text.\", \"examples\": [{\"user_intent\": \"I have a table of countries and their populations. I want to know which country has the largest population.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Country': ['USA', 'China', 'India', 'Brazil'], 'Population': [331000000, 1441000000, 1393000000, 212000000]})\", \"query\": \"Which country has the largest population?\"}}}, {\"user_intent\": \"Given a table of sales data per quarter, I want to find out in which quarter the company made the highest revenue.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Quarter': ['Q1', 'Q2', 'Q3', 'Q4'], 'Revenue': [120000, 150000, 130000, 170000]})\", \"query\": \"Which quarter had the highest revenue?\"}}}, {\"user_intent\": \"I have Olympic host cities and years in a table. I want to know when London hosted the Olympics.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Year': [1896, 1900, 1904, 2004, 2008, 2012], 'City': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']})\", \"query\": \"In which year did London host the Olympic Games?\"}}}]}\n\n# User Query\nOur company collects data on the salesperson performance in different regions for each month. We want to use the most suitable API from our subscribed_huggingface.co to analyze that in specific table format and then based on provided question, answer accordingly.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze salesperson performance in different regions for each month using tabular data.",
                    "tool_description": "Retrieve a list of data entries for salesperson performance by region and month, formatted as a table. This data will be processed to generate insights based on specific queries.",
                    "expected_response": "A structured table with columns for salesperson, region, month, and performance metrics."
                },
                {
                    "user_intent": "Answer specific questions based on the provided performance data table.",
                    "tool_description": "Use a table-based question-answering model to answer natural language questions related to the salesperson performance data. The query is answered based on the context provided by the table structure.",
                    "expected_response": "A textual answer to the natural language query derived from the table data."
                }
            ],
            "extracted_arguments": {
                "table": "A pandas DataFrame containing data on salesperson performance across different regions and months.",
                "query": "A natural language question to be answered based on the table data."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_286",
        "query": "The company needs a tool to analyze customers' reviews about their products. We need to find out which ones are positive, neutral, or negative.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that utilize customer review data to determine the sentiment polarity as positive, neutral, or negative by processing textual inputs and returning sentiment classifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "snli, multi_nli, ms_marco",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_707",
                    "name": "sentence-transformers/paraphrase-MiniLM-L3-v2"
                },
                "id": "gorilla_huggingface_tool_702",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 1: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 2: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 3: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 4: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 5: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 6: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 7: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 8: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 9: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 10: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 11: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 12: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 13: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 14: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 15: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 16: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 17: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 18: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 19: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task."
        ],
        "tool_view": {
            "description": "A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "parameters": {},
            "response": "The tool is designed for sentiment analysis using a pre-trained model, specifically the \"finiteautomata/beto-sentiment-analysis\" model, which is based on the BETO architecture for Spanish text. Given a piece of text as input, the tool will return the sentiment classification of that text.\n\n### Expected Response Description:\n\nThe tool's response will contain the sentiment label(s) for the provided text input. These labels are typically categorical values that represent the sentiment of the text. The possible labels are:\n\n1. **POS**: Positive sentiment\n2. **NEG**: Negative sentiment\n3. **NEU**: Neutral sentiment\n\n### Structure of the Response:\n\n1. **label** (string): The sentiment label assigned to the input text (one of \"POS\", \"NEG\", or \"NEU\").\n2. **score** (float): A numerical score indicating the model's confidence in the sentiment label. This value is typically between 0 and 1, with higher values indicating higher confidence in the assigned label.\n\nThe response could include one or more sentiment labels (depending on the model's output), with each label paired with a corresponding confidence score.\n\nIn principle, for each input text, the response will contain a list of dictionaries where each dictionary has the following keys:\n\n- **label**: The sentiment classification (e.g., \"POS\", \"NEG\", \"NEU\").\n- **score**: The confidence score of that sentiment classification (a float between 0 and 1).",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this Spanish tweet: 'Me encanta cómo ha quedado el nuevo álbum de mi artista favorito.'",
                    "tool_call": {
                        "name": "finiteautomata/beto-sentiment-analysis",
                        "arguments": {
                            "text": "Me encanta cómo ha quedado el nuevo álbum de mi artista favorito."
                        }
                    }
                },
                {
                    "user_intent": "I want to check if this Spanish review is positive, negative, or neutral: 'El servicio del restaurante fue muy lento y poco amable.'",
                    "tool_call": {
                        "name": "finiteautomata/beto-sentiment-analysis",
                        "arguments": {
                            "text": "El servicio del restaurante fue muy lento y poco amable."
                        }
                    }
                },
                {
                    "user_intent": "Determine the sentiment for this casual Spanish message: 'Hoy el clima está regular, ni frío ni caliente, nada especial.'",
                    "tool_call": {
                        "name": "finiteautomata/beto-sentiment-analysis",
                        "arguments": {
                            "text": "Hoy el clima está regular, ni frío ni caliente, nada especial."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 1: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 2: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 3: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 4: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 5: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 6: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 7: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 8: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 9: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 10: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 11: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 12: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 13: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 14: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 15: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 16: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 17: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 18: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 19: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\", \"parameters\": {}, \"response\": \"The tool is designed for sentiment analysis using a pre-trained model, specifically the \\\"finiteautomata/beto-sentiment-analysis\\\" model, which is based on the BETO architecture for Spanish text. Given a piece of text as input, the tool will return the sentiment classification of that text.\\n\\n### Expected Response Description:\\n\\nThe tool's response will contain the sentiment label(s) for the provided text input. These labels are typically categorical values that represent the sentiment of the text. The possible labels are:\\n\\n1. **POS**: Positive sentiment\\n2. **NEG**: Negative sentiment\\n3. **NEU**: Neutral sentiment\\n\\n### Structure of the Response:\\n\\n1. **label** (string): The sentiment label assigned to the input text (one of \\\"POS\\\", \\\"NEG\\\", or \\\"NEU\\\").\\n2. **score** (float): A numerical score indicating the model's confidence in the sentiment label. This value is typically between 0 and 1, with higher values indicating higher confidence in the assigned label.\\n\\nThe response could include one or more sentiment labels (depending on the model's output), with each label paired with a corresponding confidence score.\\n\\nIn principle, for each input text, the response will contain a list of dictionaries where each dictionary has the following keys:\\n\\n- **label**: The sentiment classification (e.g., \\\"POS\\\", \\\"NEG\\\", \\\"NEU\\\").\\n- **score**: The confidence score of that sentiment classification (a float between 0 and 1).\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this Spanish tweet: 'Me encanta c\\u00f3mo ha quedado el nuevo \\u00e1lbum de mi artista favorito.'\", \"tool_call\": {\"name\": \"finiteautomata/beto-sentiment-analysis\", \"arguments\": {\"text\": \"Me encanta c\\u00f3mo ha quedado el nuevo \\u00e1lbum de mi artista favorito.\"}}}, {\"user_intent\": \"I want to check if this Spanish review is positive, negative, or neutral: 'El servicio del restaurante fue muy lento y poco amable.'\", \"tool_call\": {\"name\": \"finiteautomata/beto-sentiment-analysis\", \"arguments\": {\"text\": \"El servicio del restaurante fue muy lento y poco amable.\"}}}, {\"user_intent\": \"Determine the sentiment for this casual Spanish message: 'Hoy el clima est\\u00e1 regular, ni fr\\u00edo ni caliente, nada especial.'\", \"tool_call\": {\"name\": \"finiteautomata/beto-sentiment-analysis\", \"arguments\": {\"text\": \"Hoy el clima est\\u00e1 regular, ni fr\\u00edo ni caliente, nada especial.\"}}}]}\n\n# User Query\nThe company needs a tool to analyze customers' reviews about their products. We need to find out which ones are positive, neutral, or negative.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the sentiment of customer reviews to determine if they are positive, negative, or neutral.",
                    "tool_description": "Perform sentiment analysis on Spanish text using the BETO BERT model trained on the TASS 2020 corpus, classifying each review as positive, negative, or neutral.",
                    "expected_response": "A list of sentiment labels for each review, where each label indicates POS (positive), NEG (negative), or NEU (neutral), along with a confidence score for each classification."
                }
            ],
            "extracted_arguments": {
                "text": "The textual content of a customer review in Spanish to be analyzed for sentiment."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_113",
        "query": "Tell me the day of the game when it was played given the following context: \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"",
        "instruction": "Given a `question answering` task, retrieve tools that utilize natural language processing to analyze and interpret textual context in order to extract specific information, such as dates, from the provided content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='csarron/bert-base-uncased-squad-v1', tokenizer='csarron/bert-base-uncased-squad-v1')",
                    "api_arguments": {
                        "model": "csarron/bert-base-uncased-squad-v1",
                        "tokenizer": "csarron/bert-base-uncased-squad-v1"
                    },
                    "python_environment_requirements": "Python 3.7.5",
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline(\n question-answering,\n model=csarron/bert-base-uncased-squad-v1,\n tokenizer=csarron/bert-base-uncased-squad-v1\n)\npredictions = qa_pipeline({\n 'context': The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.,\n 'question': What day was the game played on?\n})\nprint(predictions)",
                    "performance": {
                        "dataset": "SQuAD1.1",
                        "accuracy": {
                            "EM": 80.9,
                            "F1": 88.2
                        }
                    },
                    "description": "BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.",
                    "id": "huggingface_api_483",
                    "name": "csarron/bert-base-uncased-squad-v1"
                },
                "id": "gorilla_huggingface_tool_479",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 2: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 4: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "Tool 5: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 6: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 7: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 8: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 9: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 10: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 12: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 13: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 14: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.",
            "Tool 15: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 16: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 17: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 18: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 19: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The tool’s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\n\n1. **Boxes / Bounding Boxes**  \n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \n   - These coordinates define the rectangular region enclosing each detected object.\n\n2. **Class / Label**  \n   - Each detected object is assigned a class label indicating its category, such as \"dropped spike,\" \"enemy,\" \"planted spike,\" or \"teammate.\"  \n   - The label identifies the type of object detected by the model.\n\n3. **Confidence Score**  \n   - A numerical value representing the model’s confidence that the detected object belongs to the predicted class.  \n   - This value is constrained by the `conf` threshold specified in the API arguments.\n\n4. **Optional Additional Metadata**  \n   - Depending on the framework, the response may include additional information such as:\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\n     - **Tracking ID** if sequential detection or object tracking is enabled.\n     - **Mask or segmentation data** if the model supports instance segmentation.\n\n5. **Rendering / Visualization (Optional)**  \n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\n\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.",
            "examples": [
                {
                    "user_intent": "I want to detect all enemies and teammates in a Valorant gameplay screenshot.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000
                        }
                    }
                },
                {
                    "user_intent": "Scan a live stream frame from Valorant to check if the spike has been dropped or planted.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500
                        }
                    }
                },
                {
                    "user_intent": "Analyze a sequence of Valorant game images to locate all objects with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 1000
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 2: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 4: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\nTool 5: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 6: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 7: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 8: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 9: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 10: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 12: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 13: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 14: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.\nTool 15: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 16: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 17: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 18: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 19: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The tool\\u2019s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\\n\\n1. **Boxes / Bounding Boxes**  \\n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \\n   - These coordinates define the rectangular region enclosing each detected object.\\n\\n2. **Class / Label**  \\n   - Each detected object is assigned a class label indicating its category, such as \\\"dropped spike,\\\" \\\"enemy,\\\" \\\"planted spike,\\\" or \\\"teammate.\\\"  \\n   - The label identifies the type of object detected by the model.\\n\\n3. **Confidence Score**  \\n   - A numerical value representing the model\\u2019s confidence that the detected object belongs to the predicted class.  \\n   - This value is constrained by the `conf` threshold specified in the API arguments.\\n\\n4. **Optional Additional Metadata**  \\n   - Depending on the framework, the response may include additional information such as:\\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\\n     - **Tracking ID** if sequential detection or object tracking is enabled.\\n     - **Mask or segmentation data** if the model supports instance segmentation.\\n\\n5. **Rendering / Visualization (Optional)**  \\n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \\n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\\n\\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.\", \"examples\": [{\"user_intent\": \"I want to detect all enemies and teammates in a Valorant gameplay screenshot.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}}}, {\"user_intent\": \"Scan a live stream frame from Valorant to check if the spike has been dropped or planted.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500}}}, {\"user_intent\": \"Analyze a sequence of Valorant game images to locate all objects with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 1000}}}]}\n\n# User Query\nTell me the day of the game when it was played given the following context: \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract the specific date of the game from a provided text context.",
                    "tool_description": "Use a question-answering model to identify and extract the date of an event mentioned within a given text context.",
                    "expected_response": "A string representing the date of the game as mentioned in the text context."
                }
            ],
            "extracted_arguments": {
                "context_text": "The text passage containing details about the game, including date, location, and other relevant information.",
                "question": "The natural language query asking for the specific day or date of the game."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_380",
        "query": "I want to create a visual representation based on a short description: \"A magical forest with unicorns and a rainbow.\".",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that produce visual representations based on textual descriptions by processing input prompts to generate coherent and visually appealing images aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Diffusion-based text-to-image generation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')",
                    "api_arguments": {
                        "checkpoint": "lllyasviel/control_v11p_sd15_softedge",
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "controlnet_aux==0.3.0"
                    ],
                    "example_code": "import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom controlnet_aux import PidiNetDetector, HEDdetector\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\nimage = load_image(\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\n)\nprompt = royal chamber with fancy bed\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\ncontrol_image = processor(image, safe=True)\ncontrol_image.save(./images/control.png)\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\nimage.save('images/image_out.png')",
                    "performance": {
                        "dataset": "ControlNet",
                        "accuracy": "Not provided"
                    },
                    "description": "Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.",
                    "id": "huggingface_api_276",
                    "name": "lllyasviel/control_v11p_sd15_softedge"
                },
                "id": "gorilla_huggingface_tool_272",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 1: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 2: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 3: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 4: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 5: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "Tool 6: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 7: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 8: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 9: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 10: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 11: A YOLOv8-based model for detecting and segmenting defects on printed circuit boards (PCBs), including issues like dry joints, incorrect installation, PCB damage, and short circuits, allowing precise localization and mask generation for quality control and inspection purposes.",
            "Tool 12: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 13: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 14: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 15: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 16: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 17: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 18: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 19: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions."
        ],
        "tool_view": {
            "description": "A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A natural language text description that specifies the content and actions to appear in the generated video.",
                    "required": true,
                    "example": "A superhero flying over a city at sunset"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of diffusion steps to run during video generation. Higher values typically improve quality but increase computation time.",
                    "required": true,
                    "example": 25
                },
                "num_frames": {
                    "type": "number",
                    "description": "The total number of video frames to generate for the output video.",
                    "required": true,
                    "example": 16
                }
            },
            "response": "The tool performs text-to-video synthesis using a multimodal diffusion model, where the response is expected to be a video generated from the provided textual prompt.\n\n### Expected Response Structure:\n\n1. **Video Frames**:\n   - **Type**: Array/Sequence of Image Frames\n   - **Description**: The primary output consists of a sequence of video frames that correspond to the given text prompt. These frames represent the visual content generated based on the input description.\n   - **Meaning**: Each frame is a snapshot of the video at a specific moment in time, forming part of the overall video when played sequentially.\n\n2. **Video File (Optional)**:\n   - **Type**: Path or URL\n   - **Description**: A reference to the final video file generated from the sequence of frames. This could be a local path or an online URL where the video can be accessed.\n   - **Meaning**: This component encapsulates the entire video in a playable format.\n\n3. **Inference Details (Optional)**:\n   - **Type**: Metadata (optional)\n   - **Description**: Information regarding the number of inference steps or frames used to generate the video. This can include:\n     - `num_inference_steps`: Number of inference steps taken during the model's process.\n     - `num_frames`: Total number of frames in the generated video.\n   - **Meaning**: Provides insights into the model's processing parameters and can be useful for debugging or understanding the quality/complexity of the generated video.\n\nIn summary, the response contains primarily the frames of the video and may include additional metadata about the inference process and a reference to the final video file if applicable.",
            "examples": [
                {
                    "user_intent": "I want to create a video based on the text 'A dog playing in the park'. Can you generate that for me?",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A dog playing in the park",
                            "num_inference_steps": 30,
                            "num_frames": 60
                        }
                    }
                },
                {
                    "user_intent": "Can you make a video from the description 'A futuristic city at night with flying cars'?",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A futuristic city at night with flying cars",
                            "num_inference_steps": 40,
                            "num_frames": 90
                        }
                    }
                },
                {
                    "user_intent": "I need a short video based on the phrase 'A cat jumping over a fence' with minimal steps.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A cat jumping over a fence",
                            "num_inference_steps": 15,
                            "num_frames": 30
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 1: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 2: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 3: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 4: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 5: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\nTool 6: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 7: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 8: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 9: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 10: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 11: A YOLOv8-based model for detecting and segmenting defects on printed circuit boards (PCBs), including issues like dry joints, incorrect installation, PCB damage, and short circuits, allowing precise localization and mask generation for quality control and inspection purposes.\nTool 12: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 13: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 14: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 15: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 16: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 17: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 18: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 19: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A natural language text description that specifies the content and actions to appear in the generated video.\", \"required\": true, \"example\": \"A superhero flying over a city at sunset\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of diffusion steps to run during video generation. Higher values typically improve quality but increase computation time.\", \"required\": true, \"example\": 25}, \"num_frames\": {\"type\": \"number\", \"description\": \"The total number of video frames to generate for the output video.\", \"required\": true, \"example\": 16}}, \"response\": \"The tool performs text-to-video synthesis using a multimodal diffusion model, where the response is expected to be a video generated from the provided textual prompt.\\n\\n### Expected Response Structure:\\n\\n1. **Video Frames**:\\n   - **Type**: Array/Sequence of Image Frames\\n   - **Description**: The primary output consists of a sequence of video frames that correspond to the given text prompt. These frames represent the visual content generated based on the input description.\\n   - **Meaning**: Each frame is a snapshot of the video at a specific moment in time, forming part of the overall video when played sequentially.\\n\\n2. **Video File (Optional)**:\\n   - **Type**: Path or URL\\n   - **Description**: A reference to the final video file generated from the sequence of frames. This could be a local path or an online URL where the video can be accessed.\\n   - **Meaning**: This component encapsulates the entire video in a playable format.\\n\\n3. **Inference Details (Optional)**:\\n   - **Type**: Metadata (optional)\\n   - **Description**: Information regarding the number of inference steps or frames used to generate the video. This can include:\\n     - `num_inference_steps`: Number of inference steps taken during the model's process.\\n     - `num_frames`: Total number of frames in the generated video.\\n   - **Meaning**: Provides insights into the model's processing parameters and can be useful for debugging or understanding the quality/complexity of the generated video.\\n\\nIn summary, the response contains primarily the frames of the video and may include additional metadata about the inference process and a reference to the final video file if applicable.\", \"examples\": [{\"user_intent\": \"I want to create a video based on the text 'A dog playing in the park'. Can you generate that for me?\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A dog playing in the park\", \"num_inference_steps\": 30, \"num_frames\": 60}}}, {\"user_intent\": \"Can you make a video from the description 'A futuristic city at night with flying cars'?\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A futuristic city at night with flying cars\", \"num_inference_steps\": 40, \"num_frames\": 90}}}, {\"user_intent\": \"I need a short video based on the phrase 'A cat jumping over a fence' with minimal steps.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A cat jumping over a fence\", \"num_inference_steps\": 15, \"num_frames\": 30}}}]}\n\n# User Query\nI want to create a visual representation based on a short description: \"A magical forest with unicorns and a rainbow.\".\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a video from the description 'A magical forest with unicorns and a rainbow.'",
                    "tool_description": "Generate a video by converting an English text description into a sequence of video frames using a multi-stage diffusion-based text-to-video model.",
                    "expected_response": "A sequence of video frames representing the magical forest scene with unicorns and a rainbow, optionally including a compiled video file and metadata about inference steps and total frames."
                }
            ],
            "extracted_arguments": {
                "prompt": "A natural language text description specifying the scene and actions to appear in the video.",
                "num_inference_steps": "The number of diffusion steps to run during video generation, influencing video quality and computation time.",
                "num_frames": "The total number of frames to generate for the output video."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_188",
        "query": "Our company deals with insurance claims. We need a smart assistant who can retrieve information from documents, especially invoices, such as total amount, date of invoice, and name of the service provider.",
        "instruction": "Given a `document information retrieval` task, retrieve tools that can process documents, especially invoices, by extracting and returning key information such as total amount, date, and service provider name from the content.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "vision-encoder-decoder",
                    "api_call": "pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')",
                    "api_arguments": "image_path, question",
                    "python_environment_requirements": "transformers",
                    "example_code": "doc_vqa(image_path='path/to/image.jpg', question='What is the title?')",
                    "performance": {
                        "dataset": "DocVQA",
                        "accuracy": "Not provided"
                    },
                    "description": "Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.",
                    "id": "huggingface_api_124",
                    "name": "jinhybr/OCR-DocVQA-Donut"
                },
                "id": "gorilla_huggingface_tool_123",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 1: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 2: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.",
            "Tool 3: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 4: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 5: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 6: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 7: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 8: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 9: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 10: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 11: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 12: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 13: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 14: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 15: A fine-tuned version of Microsoft’s LayoutLMv3-large model designed for multimodal document question answering, capable of extracting and classifying information from documents with high accuracy, achieving over 98% accuracy on the SROIE dataset.",
            "Tool 16: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 17: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 18: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 19: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents."
        ],
        "tool_view": {
            "description": "A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The question you want to ask about the document or invoice. This is the input query that the model will try to answer based on the provided context.",
                    "required": true,
                    "example": "What is the total amount?"
                },
                "context": {
                    "type": "string",
                    "description": "The textual content of the document or invoice where the model will search for the answer to the question. This can include the full text extracted from the document.",
                    "required": true,
                    "example": "Invoice #12345 dated 2025-11-01 with total amount $1,250.00 for services rendered."
                }
            },
            "response": "The tool returns a structured answer to a user-provided question based on the content of a document. The response typically contains the following components:\n\n1. **Answer Text**: The main textual output of the model, representing the answer extracted or generated from the document context. This may include non-consecutive sequences of tokens, not limited to contiguous spans.\n\n2. **Score/Confidence (optional)**: A numerical value indicating the model’s confidence in the predicted answer. This may be included if the model provides a probability or confidence score for the extracted answer.\n\n3. **Answer Spans/Offsets (optional)**: Information about the positions of the relevant tokens in the input document, which may be provided when the model identifies non-contiguous spans contributing to the answer.\n\n4. **Additional Metadata (optional)**: Any auxiliary information such as document sections used, attention weights, or reasoning indicators, depending on the underlying implementation.\n\nOverall, the response conveys the extracted or predicted answer to the input question, potentially augmented with confidence measures and positional metadata, enabling users to interpret and verify the answer within the context of the document.",
            "examples": [
                {
                    "user_intent": "I have a PDF invoice and I want to know the total amount billed.",
                    "tool_call": {
                        "name": "layoutlm-invoices",
                        "arguments": {
                            "question": "What is the total amount?",
                            "context": "Invoice from ABC Corp dated 2025-12-01 with line items for office supplies and services totaling $1,245.50."
                        }
                    }
                },
                {
                    "user_intent": "Extract the invoice number from this scanned document.",
                    "tool_call": {
                        "name": "layoutlm-invoices",
                        "arguments": {
                            "question": "What is the invoice number?",
                            "context": "Scanned invoice showing invoice number INV-2025-045, issued to XYZ Ltd., dated 2025-11-20."
                        }
                    }
                },
                {
                    "user_intent": "Check who is the vendor on this receipt.",
                    "tool_call": {
                        "name": "layoutlm-invoices",
                        "arguments": {
                            "question": "Who is the vendor?",
                            "context": "Receipt from OfficeMart, 123 Main St., purchased office chairs and desks on 2025-12-10, total $678.90."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 1: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 2: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.\nTool 3: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 4: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 5: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 6: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 7: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 8: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 9: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 10: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 11: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 12: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 13: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 14: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 15: A fine-tuned version of Microsoft’s LayoutLMv3-large model designed for multimodal document question answering, capable of extracting and classifying information from documents with high accuracy, achieving over 98% accuracy on the SROIE dataset.\nTool 16: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 17: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 18: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 19: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The question you want to ask about the document or invoice. This is the input query that the model will try to answer based on the provided context.\", \"required\": true, \"example\": \"What is the total amount?\"}, \"context\": {\"type\": \"string\", \"description\": \"The textual content of the document or invoice where the model will search for the answer to the question. This can include the full text extracted from the document.\", \"required\": true, \"example\": \"Invoice #12345 dated 2025-11-01 with total amount $1,250.00 for services rendered.\"}}, \"response\": \"The tool returns a structured answer to a user-provided question based on the content of a document. The response typically contains the following components:\\n\\n1. **Answer Text**: The main textual output of the model, representing the answer extracted or generated from the document context. This may include non-consecutive sequences of tokens, not limited to contiguous spans.\\n\\n2. **Score/Confidence (optional)**: A numerical value indicating the model\\u2019s confidence in the predicted answer. This may be included if the model provides a probability or confidence score for the extracted answer.\\n\\n3. **Answer Spans/Offsets (optional)**: Information about the positions of the relevant tokens in the input document, which may be provided when the model identifies non-contiguous spans contributing to the answer.\\n\\n4. **Additional Metadata (optional)**: Any auxiliary information such as document sections used, attention weights, or reasoning indicators, depending on the underlying implementation.\\n\\nOverall, the response conveys the extracted or predicted answer to the input question, potentially augmented with confidence measures and positional metadata, enabling users to interpret and verify the answer within the context of the document.\", \"examples\": [{\"user_intent\": \"I have a PDF invoice and I want to know the total amount billed.\", \"tool_call\": {\"name\": \"layoutlm-invoices\", \"arguments\": {\"question\": \"What is the total amount?\", \"context\": \"Invoice from ABC Corp dated 2025-12-01 with line items for office supplies and services totaling $1,245.50.\"}}}, {\"user_intent\": \"Extract the invoice number from this scanned document.\", \"tool_call\": {\"name\": \"layoutlm-invoices\", \"arguments\": {\"question\": \"What is the invoice number?\", \"context\": \"Scanned invoice showing invoice number INV-2025-045, issued to XYZ Ltd., dated 2025-11-20.\"}}}, {\"user_intent\": \"Check who is the vendor on this receipt.\", \"tool_call\": {\"name\": \"layoutlm-invoices\", \"arguments\": {\"question\": \"Who is the vendor?\", \"context\": \"Receipt from OfficeMart, 123 Main St., purchased office chairs and desks on 2025-12-10, total $678.90.\"}}}]}\n\n# User Query\nOur company deals with insurance claims. We need a smart assistant who can retrieve information from documents, especially invoices, such as total amount, date of invoice, and name of the service provider.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to retrieve information such as the total amount from an invoice document.",
                    "tool_description": "Extract specific details, such as the total amount billed, from an invoice or other document content.",
                    "expected_response": "A structured answer containing the extracted total amount from the provided document context."
                },
                {
                    "user_intent": "I need to extract the date of the invoice from the document.",
                    "tool_description": "Extract the date of the invoice from a document by identifying the relevant text from the provided context.",
                    "expected_response": "A structured answer containing the extracted invoice date from the document."
                },
                {
                    "user_intent": "I want to identify the name of the service provider from the invoice.",
                    "tool_description": "Extract the name of the service provider from an invoice or document by identifying relevant entity information in the text.",
                    "expected_response": "A structured answer containing the extracted service provider name from the document."
                }
            ],
            "extracted_arguments": {
                "question": "The specific query or information request about the document content, such as 'What is the total amount?', 'What is the invoice date?', or 'Who is the service provider?'",
                "context": "The full textual content extracted from the invoice or document where the model will search for answers to the question."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_163",
        "query": "We are developing a virtual assistant. One of its major functionalities is to convert written text to speech.",
        "instruction": "Given a `text-to-speech` task, retrieve tools that convert written text inputs into spoken audio outputs by using functionalities such as model ensemble and text-to-speech synthesis, ensuring the virtual assistant can perform speech functions as required.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "fairseq",
                        "IPython"
                    ],
                    "example_code": "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\nimport IPython.display as ipd\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/fastspeech2-en-200_speaker-cv4,\n arg_overrides={vocoder: hifigan, fp16: False}\n)\nmodel = models[0]\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\ngenerator = task.build_generator(model, cfg)\ntext = Hello, this is a test run.\nsample = TTSHubInterface.get_model_input(task, text)\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\nipd.Audio(wav, rate=rate)",
                    "performance": {
                        "dataset": "common_voice",
                        "accuracy": null
                    },
                    "description": "FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.",
                    "id": "huggingface_api_735",
                    "name": "fastspeech2-en-male1"
                },
                "id": "gorilla_huggingface_tool_730",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 1: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 2: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 3: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 4: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 5: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 6: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 7: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.",
            "Tool 8: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 9: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.",
            "Tool 10: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 11: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.",
            "Tool 12: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 13: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 14: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.",
            "Tool 15: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.",
            "Tool 16: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 17: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 18: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 19: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text."
        ],
        "tool_view": {
            "description": "Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in Telugu that you want the model to convert into speech audio.",
                    "required": true,
                    "example": "హలో, మీరు ఎలా ఉన్నారు?"
                }
            },
            "response": "The tool produces a synthesized audio output corresponding to the input text in Telugu, using a male voice. The response is typically a structured object containing at least the following components:  \n\n1. **Audio Data**: This is the primary output representing the spoken version of the input text. It may be encoded as raw waveform data, a byte stream, or a file reference (e.g., WAV or MP3 format).  \n\n2. **Sampling Rate / Audio Metadata** (optional but common): Information about the audio such as sampling rate, number of channels, duration, or format, which is necessary for correctly playing or processing the generated audio.  \n\n3. **Input Text Echo** (optional): Some implementations may include the original input text for reference or verification purposes.  \n\nIn summary, the response encapsulates the conversion of the provided Telugu text into audible speech, typically providing both the audio data and, optionally, metadata about the generated audio.",
            "examples": [
                {
                    "user_intent": "I want to convert some text into Telugu speech using a male voice.",
                    "tool_call": {
                        "name": "SYSPIN/Telugu_Male_TTS",
                        "arguments": {
                            "text": "Hello, how are you today?"
                        }
                    }
                },
                {
                    "user_intent": "Convert the following sentence into speech with a Telugu male voice: 'Welcome to our service!'",
                    "tool_call": {
                        "name": "SYSPIN/Telugu_Male_TTS",
                        "arguments": {
                            "text": "Welcome to our service!"
                        }
                    }
                },
                {
                    "user_intent": "Can you convert this Telugu sentence into speech? 'మీరు ఎలా ఉన్నారు?'",
                    "tool_call": {
                        "name": "SYSPIN/Telugu_Male_TTS",
                        "arguments": {
                            "text": "మీరు ఎలా ఉన్నారు?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 1: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 2: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 3: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 4: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 5: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 6: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 7: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.\nTool 8: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 9: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.\nTool 10: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 11: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.\nTool 12: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 13: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 14: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.\nTool 15: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.\nTool 16: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 17: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 18: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 19: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in Telugu that you want the model to convert into speech audio.\", \"required\": true, \"example\": \"\\u0c39\\u0c32\\u0c4b, \\u0c2e\\u0c40\\u0c30\\u0c41 \\u0c0e\\u0c32\\u0c3e \\u0c09\\u0c28\\u0c4d\\u0c28\\u0c3e\\u0c30\\u0c41?\"}}, \"response\": \"The tool produces a synthesized audio output corresponding to the input text in Telugu, using a male voice. The response is typically a structured object containing at least the following components:  \\n\\n1. **Audio Data**: This is the primary output representing the spoken version of the input text. It may be encoded as raw waveform data, a byte stream, or a file reference (e.g., WAV or MP3 format).  \\n\\n2. **Sampling Rate / Audio Metadata** (optional but common): Information about the audio such as sampling rate, number of channels, duration, or format, which is necessary for correctly playing or processing the generated audio.  \\n\\n3. **Input Text Echo** (optional): Some implementations may include the original input text for reference or verification purposes.  \\n\\nIn summary, the response encapsulates the conversion of the provided Telugu text into audible speech, typically providing both the audio data and, optionally, metadata about the generated audio.\", \"examples\": [{\"user_intent\": \"I want to convert some text into Telugu speech using a male voice.\", \"tool_call\": {\"name\": \"SYSPIN/Telugu_Male_TTS\", \"arguments\": {\"text\": \"Hello, how are you today?\"}}}, {\"user_intent\": \"Convert the following sentence into speech with a Telugu male voice: 'Welcome to our service!'\", \"tool_call\": {\"name\": \"SYSPIN/Telugu_Male_TTS\", \"arguments\": {\"text\": \"Welcome to our service!\"}}}, {\"user_intent\": \"Can you convert this Telugu sentence into speech? '\\u0c2e\\u0c40\\u0c30\\u0c41 \\u0c0e\\u0c32\\u0c3e \\u0c09\\u0c28\\u0c4d\\u0c28\\u0c3e\\u0c30\\u0c41?'\", \"tool_call\": {\"name\": \"SYSPIN/Telugu_Male_TTS\", \"arguments\": {\"text\": \"\\u0c2e\\u0c40\\u0c30\\u0c41 \\u0c0e\\u0c32\\u0c3e \\u0c09\\u0c28\\u0c4d\\u0c28\\u0c3e\\u0c30\\u0c41?\"}}}]}\n\n# User Query\nWe are developing a virtual assistant. One of its major functionalities is to convert written text to speech.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert written text into natural-sounding speech for the virtual assistant.",
                    "tool_description": "Take input text and generate corresponding speech audio using a text-to-speech model, allowing the virtual assistant to audibly communicate.",
                    "expected_response": "A synthesized audio output representing the spoken version of the input text, including optional metadata like sampling rate, duration, or audio format."
                }
            ],
            "extracted_arguments": {
                "text": "The input text string that needs to be converted into speech audio."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_304",
        "query": "We are building a social media site which creates automatic captions for users when they post a picture",
        "instruction": "Given an `image captioning` task, retrieve tools that generate textual captions for images by processing visual inputs from users' photos and automatically producing descriptive text suitable for social media posts.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Captioning",
                    "api_call": "VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')",
                    "api_arguments": {
                        "model": "nlpconnect/vit-gpt2-image-captioning"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "PIL"
                    ],
                    "example_code": "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\nmodel.to(device)\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\ndef predict_step(image_paths):\n images = []\n for image_path in image_paths:\n i_image = Image.open(image_path)\n if i_image.mode != RGB:\n i_image = i_image.convert(mode=RGB)\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\n pixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']",
                    "performance": {
                        "dataset": "Not provided",
                        "accuracy": "Not provided"
                    },
                    "description": "An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.",
                    "id": "huggingface_api_58",
                    "name": "nlpconnect/vit-gpt2-image-captioning"
                },
                "id": "gorilla_huggingface_tool_58",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 1: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 4: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 5: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 6: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 7: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 8: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 9: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.",
            "Tool 10: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 11: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 12: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.",
            "Tool 13: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 14: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 15: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 16: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 17: Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.",
            "Tool 18: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 19: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input."
        ],
        "tool_view": {
            "description": "A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The path to the pre-trained model or model identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "cardiffnlp/twitter-roberta-base-sentiment-latest"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The path to the pre-trained tokenizer or tokenizer identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "cardiffnlp/twitter-roberta-base-sentiment-latest"
                }
            },
            "response": "The tool is designed to perform sentiment analysis on text inputs, specifically leveraging a RoBERTa-based model trained on a substantial dataset of tweets. The expected response from the tool would typically include the following components:\n\n1. **Label**: This component indicates the sentiment classification assigned to the input text. It may represent categories such as positive, negative, or neutral, reflecting the emotional tone of the analyzed text.\n\n2. **Score**: This component provides a numerical value representing the confidence level of the sentiment classification. It quantifies the model's certainty regarding the assigned label, often expressed as a probability or percentage.\n\n3. **Input Text**: This component may echo the original text that was analyzed, providing context for the sentiment classification and allowing users to verify the input against the output.\n\nOverall, the response encapsulates the sentiment analysis results, offering insights into the emotional content of the provided text along with the model's confidence in its assessment.",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this tweet: 'I love the new features in the latest smartphone update!'",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": "I love the new features in the latest smartphone update!"
                        }
                    }
                },
                {
                    "user_intent": "Check whether this tweet about the stock market is positive, negative, or neutral: 'Markets crashed today after unexpected news.'",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": "Markets crashed today after unexpected news."
                        }
                    }
                },
                {
                    "user_intent": "Determine the sentiment of multiple tweets regarding a recent movie release.",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": [
                                "Absolutely loved the new movie! Best I've seen this year.",
                                "The movie was okay, not really memorable.",
                                "Terrible film, wasted two hours of my life."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 1: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 4: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 5: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 6: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 7: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 8: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 9: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.\nTool 10: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 11: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 12: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.\nTool 13: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 14: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 15: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 16: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 17: Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.\nTool 18: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 19: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The path to the pre-trained model or model identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The path to the pre-trained tokenizer or tokenizer identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}}, \"response\": \"The tool is designed to perform sentiment analysis on text inputs, specifically leveraging a RoBERTa-based model trained on a substantial dataset of tweets. The expected response from the tool would typically include the following components:\\n\\n1. **Label**: This component indicates the sentiment classification assigned to the input text. It may represent categories such as positive, negative, or neutral, reflecting the emotional tone of the analyzed text.\\n\\n2. **Score**: This component provides a numerical value representing the confidence level of the sentiment classification. It quantifies the model's certainty regarding the assigned label, often expressed as a probability or percentage.\\n\\n3. **Input Text**: This component may echo the original text that was analyzed, providing context for the sentiment classification and allowing users to verify the input against the output.\\n\\nOverall, the response encapsulates the sentiment analysis results, offering insights into the emotional content of the provided text along with the model's confidence in its assessment.\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this tweet: 'I love the new features in the latest smartphone update!'\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": \"I love the new features in the latest smartphone update!\"}}}, {\"user_intent\": \"Check whether this tweet about the stock market is positive, negative, or neutral: 'Markets crashed today after unexpected news.'\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": \"Markets crashed today after unexpected news.\"}}}, {\"user_intent\": \"Determine the sentiment of multiple tweets regarding a recent movie release.\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": [\"Absolutely loved the new movie! Best I've seen this year.\", \"The movie was okay, not really memorable.\", \"Terrible film, wasted two hours of my life.\"]}}}]}\n\n# User Query\nWe are building a social media site which creates automatic captions for users when they post a picture\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a descriptive caption for an image posted by a user.",
                    "tool_description": "Use a text-to-image understanding or image captioning model to analyze the content of the uploaded image and generate a relevant, natural language caption.",
                    "expected_response": "A textual description of the image that accurately summarizes its content in a human-readable sentence or short paragraph."
                }
            ],
            "extracted_arguments": {
                "image_input": "The image file or image data uploaded by the user for which a caption should be generated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_296",
        "query": "I wish to automatically classify a given text's emotion. What Transformers model should I use?",
        "instruction": "Given a `emotion classification` task, retrieve tools that utilize Transformers models to analyze text inputs and determine the emotional content by performing text classification as specified by the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Text Classification",
                    "api_call": "pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')",
                    "api_arguments": "text",
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "tensorflow"
                    ],
                    "example_code": "from transformers import pipeline\nnlp = pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\nresult = nlp('I am so happy today!')",
                    "performance": {
                        "dataset": "go_emotions"
                    },
                    "description": "This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.",
                    "id": "huggingface_api_395",
                    "name": "joeddav/distilbert-base-uncased-go-emotions-student"
                },
                "id": "gorilla_huggingface_tool_391",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 1: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.",
            "Tool 2: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.",
            "Tool 3: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.",
            "Tool 4: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 5: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 6: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 7: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.",
            "Tool 8: This tool performs emotion recognition on audio data using the wav2vec2 model fine-tuned for the SUPERB Emotion Recognition task. It requires a 16kHz sampled speech input and provides classification results based on the IEMOCAP dataset. The model is implemented using PyTorch and Transformers, and it can be accessed via the `pipeline('audio-classification', model='superb/wav2vec2-base-superb-er')` API call.",
            "Tool 9: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.",
            "Tool 10: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 11: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 12: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 13: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.",
            "Tool 14: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.",
            "Tool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 16: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.",
            "Tool 17: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 18: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 19: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data."
        ],
        "tool_view": {
            "description": "This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "parameters": {
                "model_name_or_path": {
                    "type": "string",
                    "description": "The name or path to the pretrained model to be used for speech emotion recognition.",
                    "required": true,
                    "example": "harshit345/xlsr-wav2vec-speech-emotion-recognition"
                }
            },
            "response": "The tool’s response is a structured output representing the predicted emotional content of a given speech audio input. A typical response would include the following components:\n\n1. **Predicted Emotion Label**  \n   - A categorical value indicating the most likely emotion detected in the audio.  \n   - Possible categories correspond to the emotions the model is trained to recognize, such as anger, disgust, fear, happiness, and sadness.\n\n2. **Confidence Scores / Probabilities**  \n   - A numeric score or probability for each possible emotion class, representing the model’s confidence that the audio belongs to that class.  \n   - These scores are typically normalized to sum to 1, allowing for comparison across classes.\n\n3. **Optional Metadata**  \n   - Information about the input audio may sometimes be included, such as the sampling rate, duration, or file path, to contextualize the prediction.\n\nIn essence, the response provides both a single predicted emotion and a detailed confidence distribution over all supported emotion categories, enabling interpretation and potential thresholding for downstream applications.",
            "examples": [
                {
                    "user_intent": "I have a recorded customer support call and want to detect if the speaker was frustrated or angry.",
                    "tool_call": {
                        "name": "harshit345/xlsr-wav2vec-speech-emotion-recognition",
                        "arguments": {
                            "audio_path": "/home/user/recordings/support_call_03.wav",
                            "sampling_rate": 16000
                        }
                    }
                },
                {
                    "user_intent": "Analyze a short audio clip from my podcast to see if the speaker sounds happy or sad.",
                    "tool_call": {
                        "name": "harshit345/xlsr-wav2vec-speech-emotion-recognition",
                        "arguments": {
                            "audio_path": "/home/user/podcast/episode5_intro.wav",
                            "sampling_rate": 16000
                        }
                    }
                },
                {
                    "user_intent": "I want to process multiple voice messages to classify the emotions expressed in each one.",
                    "tool_call": {
                        "name": "harshit345/xlsr-wav2vec-speech-emotion-recognition",
                        "arguments": {
                            "audio_path": "/home/user/messages/message_batch_01.wav",
                            "sampling_rate": 16000
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 1: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\nTool 2: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.\nTool 3: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.\nTool 4: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 5: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 6: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 7: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\nTool 8: This tool performs emotion recognition on audio data using the wav2vec2 model fine-tuned for the SUPERB Emotion Recognition task. It requires a 16kHz sampled speech input and provides classification results based on the IEMOCAP dataset. The model is implemented using PyTorch and Transformers, and it can be accessed via the `pipeline('audio-classification', model='superb/wav2vec2-base-superb-er')` API call.\nTool 9: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.\nTool 10: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 11: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 12: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 13: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.\nTool 14: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.\nTool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 16: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.\nTool 17: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 18: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 19: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\", \"parameters\": {\"model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path to the pretrained model to be used for speech emotion recognition.\", \"required\": true, \"example\": \"harshit345/xlsr-wav2vec-speech-emotion-recognition\"}}, \"response\": \"The tool\\u2019s response is a structured output representing the predicted emotional content of a given speech audio input. A typical response would include the following components:\\n\\n1. **Predicted Emotion Label**  \\n   - A categorical value indicating the most likely emotion detected in the audio.  \\n   - Possible categories correspond to the emotions the model is trained to recognize, such as anger, disgust, fear, happiness, and sadness.\\n\\n2. **Confidence Scores / Probabilities**  \\n   - A numeric score or probability for each possible emotion class, representing the model\\u2019s confidence that the audio belongs to that class.  \\n   - These scores are typically normalized to sum to 1, allowing for comparison across classes.\\n\\n3. **Optional Metadata**  \\n   - Information about the input audio may sometimes be included, such as the sampling rate, duration, or file path, to contextualize the prediction.\\n\\nIn essence, the response provides both a single predicted emotion and a detailed confidence distribution over all supported emotion categories, enabling interpretation and potential thresholding for downstream applications.\", \"examples\": [{\"user_intent\": \"I have a recorded customer support call and want to detect if the speaker was frustrated or angry.\", \"tool_call\": {\"name\": \"harshit345/xlsr-wav2vec-speech-emotion-recognition\", \"arguments\": {\"audio_path\": \"/home/user/recordings/support_call_03.wav\", \"sampling_rate\": 16000}}}, {\"user_intent\": \"Analyze a short audio clip from my podcast to see if the speaker sounds happy or sad.\", \"tool_call\": {\"name\": \"harshit345/xlsr-wav2vec-speech-emotion-recognition\", \"arguments\": {\"audio_path\": \"/home/user/podcast/episode5_intro.wav\", \"sampling_rate\": 16000}}}, {\"user_intent\": \"I want to process multiple voice messages to classify the emotions expressed in each one.\", \"tool_call\": {\"name\": \"harshit345/xlsr-wav2vec-speech-emotion-recognition\", \"arguments\": {\"audio_path\": \"/home/user/messages/message_batch_01.wav\", \"sampling_rate\": 16000}}}]}\n\n# User Query\nI wish to automatically classify a given text's emotion. What Transformers model should I use?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify the emotion in a given text.",
                    "tool_description": "Classify the emotional tone of a provided text using a pretrained emotion detection model based on the Hugging Face Transformers framework.",
                    "expected_response": "A prediction object containing the classified emotion label (e.g., anger, happiness, sadness) and its associated confidence score."
                }
            ],
            "extracted_arguments": {
                "text_input": "The text input that needs to be analyzed for emotional content."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_406",
        "query": "A group of students are doing a project on European capitals. They need to know the capital city of Germany.",
        "instruction": "Given a `geographical information retrieval` task, retrieve tools capable of answering factual questions by processing queries related to geographical knowledge, such as identifying the capital city of a country.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='deepset/roberta-large-squad2')",
                    "api_arguments": [
                        "question",
                        "context"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": "Not provided"
                    },
                    "description": "A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.",
                    "id": "huggingface_api_471",
                    "name": "deepset/roberta-large-squad2"
                },
                "id": "gorilla_huggingface_tool_467",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 1: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 2: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 3: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 4: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 5: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 6: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 7: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 8: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 9: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 10: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.",
            "Tool 11: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 12: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 13: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.",
            "Tool 14: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 15: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 16: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.",
            "Tool 17: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.",
            "Tool 18: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 19: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library."
        ],
        "tool_view": {
            "description": "A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "parameters": {
                "src_text": {
                    "type": "array",
                    "description": "A list of text strings that need to be translated. Each string should include the appropriate language token (e.g., '>>por<<' for Portuguese) to indicate the source language.",
                    "required": true,
                    "example": [
                        ">>por<< Tom tried to stab me.",
                        ">>por<< He has been to Hawaii several times."
                    ]
                }
            },
            "response": "The tool returns the result of neural machine translation (NMT) from English (en) to Portuguese (pt) for a given list of text strings. \n\nThe structure of the response can be expected to contain:\n\n1. **Translated Texts**: \n   - A collection of translated strings, where each input string from the `src_text` is mapped to a translated string in Portuguese (pt). The number of translated strings will correspond to the number of input text strings provided.\n\n2. **Model Details**:\n   - Information about the specific model used for the translation, in this case, the `opus-mt-tc-big-en-pt` model, which is part of the OPUS-MT project. This can include details like the model's training data or target languages (though these might not be explicitly included in every response).\n\n3. **Translation Accuracy**:\n   - While the accuracy is typically part of the performance metrics for the model, this may not be part of each individual response. However, the model has shown an accuracy of 50.4% on the `flores101-devtest` dataset and 49.6% on the `tatoeba-test-v2021-08-07` dataset, as shown in the tool’s documentation. These figures represent the model's performance on standard benchmarks.\n\nThe exact format of the translated output strings (such as whether they are returned as a simple list or along with metadata) will depend on the implementation of the specific translation API. However, the response would generally be a collection of the translated texts that corresponds in order to the input text strings.",
            "examples": [
                {
                    "user_intent": "Translate a list of English sentences into Portuguese.",
                    "tool_call": {
                        "name": "opus-mt-tc-big-en-pt",
                        "arguments": {
                            "src_text": [
                                ">>por<< Tom tried to stab me.",
                                ">>por<< He has been to Hawaii several times."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I need to translate some English text into Portuguese using a neural machine translation model.",
                    "tool_call": {
                        "name": "opus-mt-tc-big-en-pt",
                        "arguments": {
                            "src_text": [
                                ">>por<< I love learning new languages.",
                                ">>por<< The weather is great today."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Can you help me translate this sentence into Portuguese?",
                    "tool_call": {
                        "name": "opus-mt-tc-big-en-pt",
                        "arguments": {
                            "src_text": [
                                ">>por<< This is an amazing translation model!"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 1: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 2: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 3: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 4: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 5: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 6: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 7: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 8: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 9: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 10: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.\nTool 11: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 12: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 13: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.\nTool 14: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 15: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 16: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.\nTool 17: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.\nTool 18: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 19: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\", \"parameters\": {\"src_text\": {\"type\": \"array\", \"description\": \"A list of text strings that need to be translated. Each string should include the appropriate language token (e.g., '>>por<<' for Portuguese) to indicate the source language.\", \"required\": true, \"example\": [\">>por<< Tom tried to stab me.\", \">>por<< He has been to Hawaii several times.\"]}}, \"response\": \"The tool returns the result of neural machine translation (NMT) from English (en) to Portuguese (pt) for a given list of text strings. \\n\\nThe structure of the response can be expected to contain:\\n\\n1. **Translated Texts**: \\n   - A collection of translated strings, where each input string from the `src_text` is mapped to a translated string in Portuguese (pt). The number of translated strings will correspond to the number of input text strings provided.\\n\\n2. **Model Details**:\\n   - Information about the specific model used for the translation, in this case, the `opus-mt-tc-big-en-pt` model, which is part of the OPUS-MT project. This can include details like the model's training data or target languages (though these might not be explicitly included in every response).\\n\\n3. **Translation Accuracy**:\\n   - While the accuracy is typically part of the performance metrics for the model, this may not be part of each individual response. However, the model has shown an accuracy of 50.4% on the `flores101-devtest` dataset and 49.6% on the `tatoeba-test-v2021-08-07` dataset, as shown in the tool\\u2019s documentation. These figures represent the model's performance on standard benchmarks.\\n\\nThe exact format of the translated output strings (such as whether they are returned as a simple list or along with metadata) will depend on the implementation of the specific translation API. However, the response would generally be a collection of the translated texts that corresponds in order to the input text strings.\", \"examples\": [{\"user_intent\": \"Translate a list of English sentences into Portuguese.\", \"tool_call\": {\"name\": \"opus-mt-tc-big-en-pt\", \"arguments\": {\"src_text\": [\">>por<< Tom tried to stab me.\", \">>por<< He has been to Hawaii several times.\"]}}}, {\"user_intent\": \"I need to translate some English text into Portuguese using a neural machine translation model.\", \"tool_call\": {\"name\": \"opus-mt-tc-big-en-pt\", \"arguments\": {\"src_text\": [\">>por<< I love learning new languages.\", \">>por<< The weather is great today.\"]}}}, {\"user_intent\": \"Can you help me translate this sentence into Portuguese?\", \"tool_call\": {\"name\": \"opus-mt-tc-big-en-pt\", \"arguments\": {\"src_text\": [\">>por<< This is an amazing translation model!\"]}}}]}\n\n# User Query\nA group of students are doing a project on European capitals. They need to know the capital city of Germany.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to know the capital city of Germany.",
                    "tool_description": "Retrieve the capital city of Germany for educational purposes.",
                    "expected_response": "A string containing the name of the capital city of Germany."
                }
            ],
            "extracted_arguments": {
                "country": "The name of the country for which the capital city is requested."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_11",
        "query": "A product is built that analyzes book reviews in order to determine how similar two examples from multiple books are to each other.",
        "instruction": "Given a `sentence similarity analysis` task, retrieve tools that can assess the similarity between text samples by utilizing models designed for natural language processing, specifically those capable of feature extraction and embedding generation.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')",
                    "api_arguments": null,
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": null,
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.",
                    "id": "huggingface_api_2",
                    "name": "princeton-nlp/unsup-simcse-roberta-base"
                },
                "id": "gorilla_huggingface_tool_2",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 2: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.",
            "Tool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 4: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 5: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 6: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 7: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 8: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 9: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 10: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 11: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 12: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 13: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.",
            "Tool 14: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 15: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 16: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 17: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 18: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 19: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 2: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.\nTool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 4: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 5: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 6: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 7: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 8: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 9: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 10: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 11: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 12: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 13: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.\nTool 14: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 15: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 16: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 17: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 18: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 19: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nA product is built that analyzes book reviews in order to determine how similar two examples from multiple books are to each other.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze the similarity between two book reviews.",
                    "tool_description": "Compute a semantic similarity score between two text inputs, determining how closely their content or meaning aligns.",
                    "expected_response": "A numerical similarity score or ranking that indicates the degree of semantic similarity between the two reviews."
                }
            ],
            "extracted_arguments": {
                "review_1": "The first text input representing a book review.",
                "review_2": "The second text input representing a book review."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_487",
        "query": "Our marketing team wants a video commercial for our product. They are looking for a video with a couple sitting in a cafe and laughing while using our product.",
        "instruction": "Given a `video synthesis` task, retrieve tools that utilize text descriptions to generate video content by processing the narrative details of the scene to create visual representations that align with the specified concepts.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-video-synthesis",
                    "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')",
                    "api_arguments": [
                        "prompt",
                        "num_inference_steps",
                        "num_frames"
                    ],
                    "python_environment_requirements": [
                        "pip install git+https://github.com/huggingface/diffusers transformers accelerate"
                    ],
                    "example_code": "pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)",
                    "performance": {
                        "dataset": "Webvid",
                        "accuracy": "Not specified"
                    },
                    "description": "A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.",
                    "id": "huggingface_api_95",
                    "name": "damo-vilab/text-to-video-ms-1.7b"
                },
                "id": "gorilla_huggingface_tool_90",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 2: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 3: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 4: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 5: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 7: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 8: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 9: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 10: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 11: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 12: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 13: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.",
            "Tool 14: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 15: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 16: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 17: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 18: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 19: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 2: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 3: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 4: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 5: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 7: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 8: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 9: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 10: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 11: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 12: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 13: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.\nTool 14: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 15: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 16: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 17: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 18: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 19: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nOur marketing team wants a video commercial for our product. They are looking for a video with a couple sitting in a cafe and laughing while using our product.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a video commercial showing a couple sitting in a cafe and laughing while using our product.",
                    "tool_description": "Create a video from a textual description using a text-to-video generation model, allowing customization of visual content based on the prompt.",
                    "expected_response": "A video file or video object visually depicting the scene described in the input text prompt."
                }
            ],
            "extracted_arguments": {
                "text_prompt": "A textual description of the scene and actions to be depicted in the video, including setting, characters, and activities."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_377",
        "query": "A journalist is looking for historical Olympic host cities and wants to find the year when Beijing hosted the games.",
        "instruction": "Given a `historical data query` task, retrieve tools that perform table-based reasoning by processing a query for specific details from historical data tables, such as identifying years associated with specific events or locations.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "BartForConditionalGeneration.from_pretrained('microsoft/tapex-base')",
                    "api_arguments": {
                        "table": "pd.DataFrame",
                        "query": "str"
                    },
                    "python_environment_requirements": {
                        "libraries": [
                            "transformers",
                            "pandas"
                        ]
                    },
                    "example_code": "from transformers import TapexTokenizer, BartForConditionalGeneration\nimport pandas as pd\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base)\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = select year where city = beijing\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "arxiv:2107.07653",
                        "accuracy": "Not provided"
                    },
                    "description": "TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.",
                    "id": "huggingface_api_443",
                    "name": "microsoft/tapex-base"
                },
                "id": "gorilla_huggingface_tool_439",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 1: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 2: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 3: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 4: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 5: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 6: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 7: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 8: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.",
            "Tool 9: Generate high-quality, unconditional images using a score-based generative model with stochastic differential equations (SDE). This model, accessible via Hugging Face’s DiffusionPipeline, produces high-fidelity 1024×1024 images and achieves state-of-the-art performance on benchmarks like CIFAR-10, making it suitable for tasks requiring realistic image synthesis.",
            "Tool 10: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.",
            "Tool 11: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 12: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 13: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 14: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.",
            "Tool 15: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 16: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 17: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 18: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.",
            "Tool 19: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis."
        ],
        "tool_view": {
            "description": "Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "parameters": {},
            "response": "The tool performs voice activity detection (VAD) on an audio input, identifying segments where speech is present. The typical response is a structured collection of speech segments. Each segment in the response generally includes:\n\n- **start_time**: The timestamp indicating when the speech segment begins within the audio.  \n- **end_time**: The timestamp indicating when the speech segment ends.  \n- **score** (optional): A confidence score or probability indicating the likelihood that the segment contains speech.  \n\nThe overall response is a list or array of these segment objects, ordered chronologically, covering all detected speech regions in the audio. This structure allows downstream processing to identify, extract, or analyze spoken portions of the audio efficiently.",
            "examples": [
                {
                    "user_intent": "I have an audio file in Hindi, and I want to detect when someone is speaking in it.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "audio/hindi_conversation.wav"
                        }
                    }
                },
                {
                    "user_intent": "Analyze my Tamil podcast recording to find all segments where there is active speech.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "podcasts/tamil_episode1.mp3"
                        }
                    }
                },
                {
                    "user_intent": "I need to process a batch of Telugu interview recordings to mark the speech intervals automatically.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "interviews/telugu_batch1.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 1: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 2: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 3: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 4: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 5: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 6: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 7: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 8: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.\nTool 9: Generate high-quality, unconditional images using a score-based generative model with stochastic differential equations (SDE). This model, accessible via Hugging Face’s DiffusionPipeline, produces high-fidelity 1024×1024 images and achieves state-of-the-art performance on benchmarks like CIFAR-10, making it suitable for tasks requiring realistic image synthesis.\nTool 10: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.\nTool 11: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 12: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 13: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 14: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.\nTool 15: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 16: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 17: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 18: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.\nTool 19: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\", \"parameters\": {}, \"response\": \"The tool performs voice activity detection (VAD) on an audio input, identifying segments where speech is present. The typical response is a structured collection of speech segments. Each segment in the response generally includes:\\n\\n- **start_time**: The timestamp indicating when the speech segment begins within the audio.  \\n- **end_time**: The timestamp indicating when the speech segment ends.  \\n- **score** (optional): A confidence score or probability indicating the likelihood that the segment contains speech.  \\n\\nThe overall response is a list or array of these segment objects, ordered chronologically, covering all detected speech regions in the audio. This structure allows downstream processing to identify, extract, or analyze spoken portions of the audio efficiently.\", \"examples\": [{\"user_intent\": \"I have an audio file in Hindi, and I want to detect when someone is speaking in it.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"audio/hindi_conversation.wav\"}}}, {\"user_intent\": \"Analyze my Tamil podcast recording to find all segments where there is active speech.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"podcasts/tamil_episode1.mp3\"}}}, {\"user_intent\": \"I need to process a batch of Telugu interview recordings to mark the speech intervals automatically.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"interviews/telugu_batch1.wav\"}}}]}\n\n# User Query\nA journalist is looking for historical Olympic host cities and wants to find the year when Beijing hosted the games.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find a list of all historical Olympic host cities along with their corresponding years.",
                    "tool_description": "Retrieve a dataset or record of past Olympic Games, including the host city and the year each edition was held.",
                    "expected_response": "A structured list or table where each entry includes the Olympic year and the corresponding host city."
                },
                {
                    "user_intent": "Identify the year when Beijing hosted the Olympic Games.",
                    "tool_description": "Query the historical Olympic data to extract the year(s) associated with Beijing as the host city.",
                    "expected_response": "The specific year or years when Beijing served as the host city for the Olympic Games."
                }
            ],
            "extracted_arguments": {
                "host_city": "The name of the city where the Olympic Games were held, specified as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_189",
        "query": "We are designing an application for professional athletes. They need a tool to categorize their exercises based on videos.",
        "instruction": "Given a `video classification` task, retrieve tools that categorize exercises by interpreting video data, leveraging models capable of understanding and classifying actions within video content to meet the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')",
                    "api_arguments": [
                        "images"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, TimesformerForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(8, 3, 224, 224))\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\ninputs = processor(images=video, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-600",
                        "accuracy": null
                    },
                    "description": "TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_318",
                    "name": "facebook/timesformer-base-finetuned-k600"
                },
                "id": "gorilla_huggingface_tool_314",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.",
            "Tool 1: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 2: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 3: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 4: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 5: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.",
            "Tool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 7: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 8: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 9: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 10: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 11: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 12: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.",
            "Tool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 14: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 15: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 16: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 17: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 18: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 19: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset."
        ],
        "tool_view": {
            "description": "A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.",
            "parameters": {},
            "response": "The tool is a video classification model implemented using the Hugging Face Transformers framework. Its response, when given a video input, would typically be structured as a prediction of class labels and associated confidence scores. The expected response can be described as follows:\n\n- **Predicted Classes / Labels**: The response includes one or more class names representing the categories the video is classified into. These classes correspond to the labels the model was fine-tuned on.  \n\n- **Scores / Probabilities**: Each predicted class is accompanied by a numerical score or probability indicating the model’s confidence in that classification. The scores are usually normalized to sum to 1 if multiple classes are returned, reflecting the likelihood of each class.  \n\n- **Optional Top-K Selection**: The response may optionally include only the top-k predictions, sorted by confidence score, rather than a score for every possible class.  \n\nIn summary, the response conveys which action, event, or category the model believes is most likely depicted in the input video, along with the associated confidence for each predicted category.",
            "examples": [
                {
                    "user_intent": "I have a short video clip of a person waving and I want to classify the action.",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb",
                            "video_path": "videos/person_waving.mp4"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a security camera footage to detect aggressive behaviors.",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb",
                            "video_path": "footage/security_cam_01.mp4"
                        }
                    }
                },
                {
                    "user_intent": "Classify actions in a sports video to detect different movements like jumping, running, or throwing.",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb",
                            "video_path": "videos/sports_match_highlights.mp4"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.\nTool 1: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 2: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 3: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 4: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 5: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\nTool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 7: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 8: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 9: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 10: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 11: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 12: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.\nTool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 14: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 15: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 16: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 17: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 18: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 19: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.\", \"parameters\": {}, \"response\": \"The tool is a video classification model implemented using the Hugging Face Transformers framework. Its response, when given a video input, would typically be structured as a prediction of class labels and associated confidence scores. The expected response can be described as follows:\\n\\n- **Predicted Classes / Labels**: The response includes one or more class names representing the categories the video is classified into. These classes correspond to the labels the model was fine-tuned on.  \\n\\n- **Scores / Probabilities**: Each predicted class is accompanied by a numerical score or probability indicating the model\\u2019s confidence in that classification. The scores are usually normalized to sum to 1 if multiple classes are returned, reflecting the likelihood of each class.  \\n\\n- **Optional Top-K Selection**: The response may optionally include only the top-k predictions, sorted by confidence score, rather than a score for every possible class.  \\n\\nIn summary, the response conveys which action, event, or category the model believes is most likely depicted in the input video, along with the associated confidence for each predicted category.\", \"examples\": [{\"user_intent\": \"I have a short video clip of a person waving and I want to classify the action.\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb\", \"video_path\": \"videos/person_waving.mp4\"}}}, {\"user_intent\": \"Analyze a security camera footage to detect aggressive behaviors.\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb\", \"video_path\": \"footage/security_cam_01.mp4\"}}}, {\"user_intent\": \"Classify actions in a sports video to detect different movements like jumping, running, or throwing.\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb\", \"video_path\": \"videos/sports_match_highlights.mp4\"}}}]}\n\n# User Query\nWe are designing an application for professional athletes. They need a tool to categorize their exercises based on videos.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify exercise videos to categorize different types of workouts or movements.",
                    "tool_description": "Use a video classification model fine-tuned on human action datasets to analyze input videos and assign them to predefined exercise or movement categories.",
                    "expected_response": "A set of predicted class labels representing exercise types, each with associated confidence scores, optionally limited to the top-k most probable classes."
                }
            ],
            "extracted_arguments": {
                "video_input": "The file path or video data representing the recorded exercise session to be classified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_23",
        "query": "I want to build an AI model that can analyze images and answer questions about the content of the image.",
        "instruction": "Given an `AI model development` task, retrieve tools that facilitate the creation and training of artificial intelligence models capable of multimodal analysis, specifically those that process images to answer questions regarding their content, by employing advanced frameworks and pre-trained architectures to enhance usability and performance in image-based question-answering scenarios.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')",
                    "api_arguments": "image, question",
                    "python_environment_requirements": "transformers",
                    "example_code": "For code examples, we refer to the documentation.",
                    "performance": {
                        "dataset": "TextVQA",
                        "accuracy": "See table 11 in the paper for more details."
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).",
                    "id": "huggingface_api_106",
                    "name": "git-large-textvqa"
                },
                "id": "gorilla_huggingface_tool_105",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 1: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 2: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 3: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 4: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 5: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 6: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 7: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 8: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 9: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 10: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 11: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 12: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 13: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 14: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 15: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 16: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 17: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 18: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.",
            "Tool 19: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path to the image that contains the document to be processed for question answering.",
                    "required": true,
                    "example": "path/to/image"
                },
                "question": {
                    "type": "string",
                    "description": "The question that you want to ask regarding the content of the image.",
                    "required": true,
                    "example": "What is the name of the company in the document?"
                }
            },
            "response": "The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\n\nThe response would typically include:\n\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\n\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\n\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\n\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.",
            "examples": [
                {
                    "user_intent": "I have a scanned contract and I want to know the effective date mentioned in it.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/contract_scan_01.png",
                            "question": "What is the effective date of this contract?"
                        }
                    }
                },
                {
                    "user_intent": "Check this invoice image and tell me the total amount due.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/invoice_march2025.png",
                            "question": "What is the total amount to be paid?"
                        }
                    }
                },
                {
                    "user_intent": "I need to extract the name of the company from this PDF screenshot.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/company_profile_screenshot.png",
                            "question": "What is the name of the company in this document?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 1: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 2: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 3: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 4: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 5: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 6: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 7: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 8: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 9: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 10: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 11: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 12: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 13: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 14: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 15: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 16: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 17: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 18: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.\nTool 19: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path to the image that contains the document to be processed for question answering.\", \"required\": true, \"example\": \"path/to/image\"}, \"question\": {\"type\": \"string\", \"description\": \"The question that you want to ask regarding the content of the image.\", \"required\": true, \"example\": \"What is the name of the company in the document?\"}}, \"response\": \"The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\\n\\nThe response would typically include:\\n\\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\\n\\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\\n\\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\\n\\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.\", \"examples\": [{\"user_intent\": \"I have a scanned contract and I want to know the effective date mentioned in it.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/contract_scan_01.png\", \"question\": \"What is the effective date of this contract?\"}}}, {\"user_intent\": \"Check this invoice image and tell me the total amount due.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/invoice_march2025.png\", \"question\": \"What is the total amount to be paid?\"}}}, {\"user_intent\": \"I need to extract the name of the company from this PDF screenshot.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/company_profile_screenshot.png\", \"question\": \"What is the name of the company in this document?\"}}}]}\n\n# User Query\nI want to build an AI model that can analyze images and answer questions about the content of the image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to develop a model capable of answering questions based on image content.",
                    "tool_description": "Develop an AI model for visual question answering (VQA) that analyzes images and provides answers to user queries regarding the content of the images.",
                    "expected_response": "A trained visual question answering model capable of processing images and responding to text-based queries related to the visual content."
                }
            ],
            "extracted_arguments": {
                "image_analysis_model": "The type of AI model designed to process images and answer questions about their content.",
                "question_input": "A natural language question regarding the image content that the model will answer."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_13",
        "query": "Assit me to process and segment an image for further analysis.",
        "instruction": "Given an `image segmentation` task, retrieve tools that process and analyze images to segment them into meaningful parts for further analysis, utilizing advanced computer vision techniques and model architectures.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')",
                    "api_arguments": [
                        "images",
                        "return_tensors"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits",
                    "performance": {
                        "dataset": "ADE20K",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.",
                    "id": "huggingface_api_234",
                    "name": "nvidia/segformer-b5-finetuned-ade-640-640"
                },
                "id": "gorilla_huggingface_tool_230",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 1: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.",
            "Tool 2: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.",
            "Tool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 4: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 5: DePlot is a model that converts images of plots or charts into structured, linearized tables by first translating the visual content into text and then reasoning over that text. This enables users to extract underlying data from visualizations, which can be directly used to prompt large language models for further analysis or reasoning, making it useful for tasks that combine visual understanding with text-based data interpretation.",
            "Tool 6: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 7: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.",
            "Tool 8: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 9: ConvNeXT is a convolutional neural network model inspired by Vision Transformer designs, optimized for image classification tasks. It can process images to predict their class labels and is useful for applications that require automated recognition or categorization of visual content.",
            "Tool 10: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.",
            "Tool 11: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 12: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.",
            "Tool 13: A Hugging Face Transformers image classification model fine-tuned for detecting diabetic retinopathy from medical images, enabling automated analysis of retinal scans with an accuracy of 77.44%.",
            "Tool 14: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.",
            "Tool 15: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 16: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 17: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.",
            "Tool 18: The `microsoft/trocr-large-handwritten` tool is a Transformer-based model for optical character recognition (OCR) that converts handwritten images into text. It uses an image Transformer encoder to process visual input and a text Transformer decoder to generate corresponding text, making it suitable for digitizing handwritten documents or extracting text from images.",
            "Tool 19: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials."
        ],
        "tool_view": {
            "description": "A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "parameters": {
                "image": {
                    "type": "object",
                    "description": "The input image to be segmented. This can be a PIL Image object, typically loaded from a file or URL.",
                    "required": true,
                    "example": "Image.open('path_to_image.jpg')"
                }
            },
            "response": "The tool produces a semantic segmentation output for a given input image. The response can be described as follows:\n\n- **logits**: A multi-dimensional tensor representing the raw, unnormalized class scores for each pixel in the input image. Its shape typically corresponds to `(batch_size, num_classes, height, width)` before any resizing or post-processing. Each channel represents the likelihood of a particular class at each pixel location.\n\n- **upsampled_logits**: A tensor derived from `logits` that has been resized (interpolated) to match the spatial dimensions of the original input image. This allows for pixel-wise alignment between the model predictions and the original image.\n\n- **pred_seg**: A 2D tensor (or array) representing the final predicted segmentation map. Each element corresponds to a pixel in the input image and contains an integer indicating the predicted class label for that pixel. This is obtained by taking the `argmax` over the class dimension of the `upsampled_logits`.\n\n- **Visual representation**: While not strictly part of the model output, the predicted segmentation map (`pred_seg`) is often visualized using a plotting library, where different class labels are rendered with distinct colors to indicate segmented regions.\n\nIn essence, the response encodes, for each pixel in the input image, which clothing or body-related class the model predicts it belongs to, optionally providing both raw logits and a finalized segmentation map suitable for visualization or further processing.",
            "examples": [
                {
                    "user_intent": "Segment the clothing items in this portrait image to identify shirts, pants, and jackets.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a group photo to separate and highlight different clothing types for fashion research.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e"
                        }
                    }
                },
                {
                    "user_intent": "Process a full-body street style image to extract the clothing regions for a style recommendation app.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1595152772835-219674b2a8a2"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 1: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.\nTool 2: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.\nTool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 4: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 5: DePlot is a model that converts images of plots or charts into structured, linearized tables by first translating the visual content into text and then reasoning over that text. This enables users to extract underlying data from visualizations, which can be directly used to prompt large language models for further analysis or reasoning, making it useful for tasks that combine visual understanding with text-based data interpretation.\nTool 6: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 7: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.\nTool 8: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 9: ConvNeXT is a convolutional neural network model inspired by Vision Transformer designs, optimized for image classification tasks. It can process images to predict their class labels and is useful for applications that require automated recognition or categorization of visual content.\nTool 10: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.\nTool 11: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 12: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.\nTool 13: A Hugging Face Transformers image classification model fine-tuned for detecting diabetic retinopathy from medical images, enabling automated analysis of retinal scans with an accuracy of 77.44%.\nTool 14: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.\nTool 15: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 16: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 17: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.\nTool 18: The `microsoft/trocr-large-handwritten` tool is a Transformer-based model for optical character recognition (OCR) that converts handwritten images into text. It uses an image Transformer encoder to process visual input and a text Transformer decoder to generate corresponding text, making it suitable for digitizing handwritten documents or extracting text from images.\nTool 19: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\", \"parameters\": {\"image\": {\"type\": \"object\", \"description\": \"The input image to be segmented. This can be a PIL Image object, typically loaded from a file or URL.\", \"required\": true, \"example\": \"Image.open('path_to_image.jpg')\"}}, \"response\": \"The tool produces a semantic segmentation output for a given input image. The response can be described as follows:\\n\\n- **logits**: A multi-dimensional tensor representing the raw, unnormalized class scores for each pixel in the input image. Its shape typically corresponds to `(batch_size, num_classes, height, width)` before any resizing or post-processing. Each channel represents the likelihood of a particular class at each pixel location.\\n\\n- **upsampled_logits**: A tensor derived from `logits` that has been resized (interpolated) to match the spatial dimensions of the original input image. This allows for pixel-wise alignment between the model predictions and the original image.\\n\\n- **pred_seg**: A 2D tensor (or array) representing the final predicted segmentation map. Each element corresponds to a pixel in the input image and contains an integer indicating the predicted class label for that pixel. This is obtained by taking the `argmax` over the class dimension of the `upsampled_logits`.\\n\\n- **Visual representation**: While not strictly part of the model output, the predicted segmentation map (`pred_seg`) is often visualized using a plotting library, where different class labels are rendered with distinct colors to indicate segmented regions.\\n\\nIn essence, the response encodes, for each pixel in the input image, which clothing or body-related class the model predicts it belongs to, optionally providing both raw logits and a finalized segmentation map suitable for visualization or further processing.\", \"examples\": [{\"user_intent\": \"Segment the clothing items in this portrait image to identify shirts, pants, and jackets.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d\"}}}, {\"user_intent\": \"Analyze a group photo to separate and highlight different clothing types for fashion research.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e\"}}}, {\"user_intent\": \"Process a full-body street style image to extract the clothing regions for a style recommendation app.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1595152772835-219674b2a8a2\"}}}]}\n\n# User Query\nAssit me to process and segment an image for further analysis.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to process an image and segment it for further analysis.",
                    "tool_description": "Perform image segmentation on a provided input image, generating pixel-wise classification of clothing regions or other segments of interest for subsequent analysis.",
                    "expected_response": "A segmentation map that highlights different regions of interest in the image, such as clothing types or other objects."
                }
            ],
            "extracted_arguments": {
                "image": "The input image to be processed, typically a file path or URL pointing to an image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_387",
        "query": "We need to extract useful features from Russian text for downstream tasks.",
        "instruction": "Given a `feature extraction from text` task, retrieve tools that are designed to process text inputs, specifically in Russian, to extract useful features for downstream tasks, by utilizing models capable of analyzing language-specific nuances and generating structured outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Russian part of Wikipedia and news data",
                        "accuracy": ""
                    },
                    "description": "RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].",
                    "id": "huggingface_api_15",
                    "name": "DeepPavlov/rubert-base-cased"
                },
                "id": "gorilla_huggingface_tool_15",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BART is a transformer-based encoder-decoder model that combines a bidirectional encoder with an autoregressive decoder, designed for generating and understanding text. It can extract features from input text, making it useful for tasks like text summarization, translation, classification, and question answering. This tool uses the Hugging Face Transformers framework to load a pre-trained BART model and obtain contextualized representations of text for downstream NLP applications.",
            "Tool 1: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.",
            "Tool 2: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 3: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 6: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.",
            "Tool 7: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.",
            "Tool 8: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 9: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 10: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 11: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.",
            "Tool 12: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.",
            "Tool 13: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 14: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.",
            "Tool 15: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.",
            "Tool 16: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.",
            "Tool 17: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 18: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 19: IndoBERT is a pretrained BERT-based language model for Indonesian that generates contextualized representations of text. It is trained using masked language modeling and next sentence prediction, making it useful for natural language understanding tasks, feature extraction, and embedding Indonesian text in downstream NLP applications."
        ],
        "tool_view": {
            "description": "BART is a transformer-based encoder-decoder model that combines a bidirectional encoder with an autoregressive decoder, designed for generating and understanding text. It can extract features from input text, making it useful for tasks like text summarization, translation, classification, and question answering. This tool uses the Hugging Face Transformers framework to load a pre-trained BART model and obtain contextualized representations of text for downstream NLP applications.",
            "parameters": {
                "inputs": {
                    "type": "string",
                    "description": "The input text or batch of texts to be processed by the BART model for feature extraction. This should be tokenized appropriately before passing to the model.",
                    "required": true,
                    "example": "Hello, my dog is cute"
                }
            },
            "response": "The tool facilitates feature extraction using the BART model, which is a transformer-based encoder-decoder model. The response to an API call (such as `BartModel.from_pretrained('facebook/bart-base')`) would provide the following elements:\n\n1. **Model Initialization**: \n   - The BART model (`facebook/bart-base`) is loaded and initialized. The response would typically acknowledge the successful loading of the model, indicating that the model is ready for further processing.\n\n2. **Input Processing**:\n   - The response may include information about the processing of input data. For example, if an input text is provided (such as \"Hello, my dog is cute\"), the model would tokenize the input using the tokenizer associated with BART (`BartTokenizer.from_pretrained('facebook/bart-base')`). \n   - The tokenized input would be returned in tensor form, ready for feeding into the model.\n\n3. **Feature Extraction**:\n   - The output of the model would contain extracted features based on the input. This could include:\n     - **Last Hidden State**: The last hidden states of the model would typically be returned. These represent the feature embeddings for the input text at each token position after processing by the model’s encoder. This feature is usually stored as a tensor of shape `[batch_size, sequence_length, hidden_size]`.\n     - If requested, additional information could be included (such as attention weights, decoder states, or logits), depending on how the model is configured.\n\n4. **Performance and Metrics**:\n   - While the documentation mentions that accuracy is \"not provided\" for the specific configuration, the response would not include performance metrics or accuracy scores unless explicitly requested through further fine-tuning or evaluation.\n\n5. **Description of Functionality**:\n   - The response may contain an optional descriptive summary of what the model does, especially in the context of feature extraction. In this case, it could include details such as:\n     - BART’s architecture as a transformer with a bidirectional encoder (like BERT) and an autoregressive decoder (like GPT).\n     - The pre-training method, where BART reconstructs original text from corrupted versions of it.\n     - The intended applications, such as text generation, summarization, translation, classification, and question answering.\n\nIn summary, the response would primarily contain the processed features (e.g., last hidden states), and depending on the context, it might also provide the model's initialization status or other feature extraction details like attention scores or intermediary embeddings.",
            "examples": [
                {
                    "user_intent": "Extract the contextual embeddings from a sentence for further analysis.",
                    "tool_call": {
                        "name": "facebook/bart-base",
                        "arguments": {
                            "inputs": "The quick brown fox jumps over the lazy dog."
                        }
                    }
                },
                {
                    "user_intent": "Generate feature representations for multiple sentences to use in a downstream classification model.",
                    "tool_call": {
                        "name": "facebook/bart-base",
                        "arguments": {
                            "inputs": [
                                "Artificial intelligence is transforming the tech industry.",
                                "Climate change is a pressing global issue."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Obtain hidden states from a paragraph to analyze semantic similarity between different sections.",
                    "tool_call": {
                        "name": "facebook/bart-base",
                        "arguments": {
                            "inputs": "BART is a transformer model combining bidirectional encoding and autoregressive decoding. It is trained to reconstruct corrupted text and performs well on summarization tasks."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BART is a transformer-based encoder-decoder model that combines a bidirectional encoder with an autoregressive decoder, designed for generating and understanding text. It can extract features from input text, making it useful for tasks like text summarization, translation, classification, and question answering. This tool uses the Hugging Face Transformers framework to load a pre-trained BART model and obtain contextualized representations of text for downstream NLP applications.\nTool 1: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.\nTool 2: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 3: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 6: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.\nTool 7: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.\nTool 8: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 9: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 10: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 11: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.\nTool 12: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.\nTool 13: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 14: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.\nTool 15: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.\nTool 16: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.\nTool 17: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 18: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 19: IndoBERT is a pretrained BERT-based language model for Indonesian that generates contextualized representations of text. It is trained using masked language modeling and next sentence prediction, making it useful for natural language understanding tasks, feature extraction, and embedding Indonesian text in downstream NLP applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BART is a transformer-based encoder-decoder model that combines a bidirectional encoder with an autoregressive decoder, designed for generating and understanding text. It can extract features from input text, making it useful for tasks like text summarization, translation, classification, and question answering. This tool uses the Hugging Face Transformers framework to load a pre-trained BART model and obtain contextualized representations of text for downstream NLP applications.\", \"parameters\": {\"inputs\": {\"type\": \"string\", \"description\": \"The input text or batch of texts to be processed by the BART model for feature extraction. This should be tokenized appropriately before passing to the model.\", \"required\": true, \"example\": \"Hello, my dog is cute\"}}, \"response\": \"The tool facilitates feature extraction using the BART model, which is a transformer-based encoder-decoder model. The response to an API call (such as `BartModel.from_pretrained('facebook/bart-base')`) would provide the following elements:\\n\\n1. **Model Initialization**: \\n   - The BART model (`facebook/bart-base`) is loaded and initialized. The response would typically acknowledge the successful loading of the model, indicating that the model is ready for further processing.\\n\\n2. **Input Processing**:\\n   - The response may include information about the processing of input data. For example, if an input text is provided (such as \\\"Hello, my dog is cute\\\"), the model would tokenize the input using the tokenizer associated with BART (`BartTokenizer.from_pretrained('facebook/bart-base')`). \\n   - The tokenized input would be returned in tensor form, ready for feeding into the model.\\n\\n3. **Feature Extraction**:\\n   - The output of the model would contain extracted features based on the input. This could include:\\n     - **Last Hidden State**: The last hidden states of the model would typically be returned. These represent the feature embeddings for the input text at each token position after processing by the model\\u2019s encoder. This feature is usually stored as a tensor of shape `[batch_size, sequence_length, hidden_size]`.\\n     - If requested, additional information could be included (such as attention weights, decoder states, or logits), depending on how the model is configured.\\n\\n4. **Performance and Metrics**:\\n   - While the documentation mentions that accuracy is \\\"not provided\\\" for the specific configuration, the response would not include performance metrics or accuracy scores unless explicitly requested through further fine-tuning or evaluation.\\n\\n5. **Description of Functionality**:\\n   - The response may contain an optional descriptive summary of what the model does, especially in the context of feature extraction. In this case, it could include details such as:\\n     - BART\\u2019s architecture as a transformer with a bidirectional encoder (like BERT) and an autoregressive decoder (like GPT).\\n     - The pre-training method, where BART reconstructs original text from corrupted versions of it.\\n     - The intended applications, such as text generation, summarization, translation, classification, and question answering.\\n\\nIn summary, the response would primarily contain the processed features (e.g., last hidden states), and depending on the context, it might also provide the model's initialization status or other feature extraction details like attention scores or intermediary embeddings.\", \"examples\": [{\"user_intent\": \"Extract the contextual embeddings from a sentence for further analysis.\", \"tool_call\": {\"name\": \"facebook/bart-base\", \"arguments\": {\"inputs\": \"The quick brown fox jumps over the lazy dog.\"}}}, {\"user_intent\": \"Generate feature representations for multiple sentences to use in a downstream classification model.\", \"tool_call\": {\"name\": \"facebook/bart-base\", \"arguments\": {\"inputs\": [\"Artificial intelligence is transforming the tech industry.\", \"Climate change is a pressing global issue.\"]}}}, {\"user_intent\": \"Obtain hidden states from a paragraph to analyze semantic similarity between different sections.\", \"tool_call\": {\"name\": \"facebook/bart-base\", \"arguments\": {\"inputs\": \"BART is a transformer model combining bidirectional encoding and autoregressive decoding. It is trained to reconstruct corrupted text and performs well on summarization tasks.\"}}}]}\n\n# User Query\nWe need to extract useful features from Russian text for downstream tasks.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract useful features from Russian text for downstream tasks.",
                    "tool_description": "Extract contextualized feature representations from Russian text using a pre-trained language model for downstream natural language processing tasks.",
                    "expected_response": "A set of feature embeddings representing the semantic content of the input Russian text, typically including token-level or sentence-level embeddings."
                }
            ],
            "extracted_arguments": {
                "inputs": "The Russian text or batch of texts to be processed for feature extraction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_125",
        "query": "We are researching multi-view 3D scanning, and we would like to improve our depth estimation with a pretrained model.",
        "instruction": "Given a `depth estimation enhancement` task, retrieve tools that utilize pretrained models for multi-view 3D scanning improvements by employing computer vision techniques to refine depth accuracy using relevant datasets and frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')",
                    "api_arguments": "pretrained_model_name",
                    "python_environment_requirements": "transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2",
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3421,
                            "Mae": 0.27,
                            "Rmse": 0.4042,
                            "Abs Rel": 0.3279,
                            "Log Mae": 0.1132,
                            "Log Rmse": 0.1688,
                            "Delta1": 0.5839,
                            "Delta2": 0.8408,
                            "Delta3": 0.9309
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.",
                    "id": "huggingface_api_168",
                    "name": "glpn-nyu-finetuned-diode-221122-082237"
                },
                "id": "gorilla_huggingface_tool_164",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 1: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 2: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 3: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.",
            "Tool 4: This tool provides a depth estimation model fine-tuned on the diode-subset dataset using the Hugging Face Transformers framework. It allows for depth prediction from images by utilizing a pre-trained model, offering performance metrics like loss, MAE, RMSE, and relative accuracy.",
            "Tool 5: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 6: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 7: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 8: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 9: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "Tool 10: A small, random DPT model for depth estimation built using the Hugging Face Transformers library.",
            "Tool 11: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 12: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.",
            "Tool 13: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 14: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.",
            "Tool 15: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.",
            "Tool 16: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.",
            "Tool 17: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.",
            "Tool 18: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.",
            "Tool 19: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API."
        ],
        "tool_view": {
            "description": "A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "parameters": {},
            "response": "The tool returns a depth estimation model based on the DPT (Dense Prediction Transformer) architecture. The response would contain the following key components:\n\n1. **Model Object**: The core of the response is the model itself, an instance of a depth estimation model, which has been pre-trained using the specified architecture ('nielsr/dpt-large-redesign'). This model is designed for estimating depth from input images.\n\n2. **Model Configuration**: It would include various parameters associated with the model’s configuration, such as layer details, input/output format, and any specific hyperparameters used in its design. This configuration ensures that the model is ready to process inputs correctly.\n\n3. **Model Weights**: The model weights are pre-trained, meaning that they are the learned parameters from training the model on relevant datasets. These weights are used to perform depth estimation on new input data.\n\n4. **Dependencies**: The response would indicate that the model requires the use of the `torch` and `transformers` libraries in the Python environment to function properly. These libraries are necessary for running the model and leveraging the DPT architecture.\n\n5. **Performance Details**: Although the performance metrics such as dataset and accuracy are not provided, in a typical response, the model might include details on its performance metrics, including accuracy on relevant depth estimation datasets or benchmark tests.\n\n6. **Description**: A brief description stating that the model is based on the DPT architecture and is intended for depth estimation tasks, helping users understand its purpose and functionality.\n\nThe overall response is a model object, ready for inference tasks, with a description of its architecture and dependencies.",
            "examples": [
                {
                    "user_intent": "I want to estimate the depth map of an image to use for 3D reconstruction.",
                    "tool_call": {
                        "name": "dpt-large-redesign",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Generate depth predictions for a set of photos from my drone survey.",
                    "tool_call": {
                        "name": "dpt-large-redesign",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Use a pre-trained DPT model to extract scene depth from a single RGB image.",
                    "tool_call": {
                        "name": "dpt-large-redesign",
                        "arguments": {}
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 1: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 2: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 3: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.\nTool 4: This tool provides a depth estimation model fine-tuned on the diode-subset dataset using the Hugging Face Transformers framework. It allows for depth prediction from images by utilizing a pre-trained model, offering performance metrics like loss, MAE, RMSE, and relative accuracy.\nTool 5: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 6: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 7: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 8: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 9: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\nTool 10: A small, random DPT model for depth estimation built using the Hugging Face Transformers library.\nTool 11: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 12: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.\nTool 13: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 14: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.\nTool 15: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\nTool 16: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.\nTool 17: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.\nTool 18: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\nTool 19: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\", \"parameters\": {}, \"response\": \"The tool returns a depth estimation model based on the DPT (Dense Prediction Transformer) architecture. The response would contain the following key components:\\n\\n1. **Model Object**: The core of the response is the model itself, an instance of a depth estimation model, which has been pre-trained using the specified architecture ('nielsr/dpt-large-redesign'). This model is designed for estimating depth from input images.\\n\\n2. **Model Configuration**: It would include various parameters associated with the model\\u2019s configuration, such as layer details, input/output format, and any specific hyperparameters used in its design. This configuration ensures that the model is ready to process inputs correctly.\\n\\n3. **Model Weights**: The model weights are pre-trained, meaning that they are the learned parameters from training the model on relevant datasets. These weights are used to perform depth estimation on new input data.\\n\\n4. **Dependencies**: The response would indicate that the model requires the use of the `torch` and `transformers` libraries in the Python environment to function properly. These libraries are necessary for running the model and leveraging the DPT architecture.\\n\\n5. **Performance Details**: Although the performance metrics such as dataset and accuracy are not provided, in a typical response, the model might include details on its performance metrics, including accuracy on relevant depth estimation datasets or benchmark tests.\\n\\n6. **Description**: A brief description stating that the model is based on the DPT architecture and is intended for depth estimation tasks, helping users understand its purpose and functionality.\\n\\nThe overall response is a model object, ready for inference tasks, with a description of its architecture and dependencies.\", \"examples\": [{\"user_intent\": \"I want to estimate the depth map of an image to use for 3D reconstruction.\", \"tool_call\": {\"name\": \"dpt-large-redesign\", \"arguments\": {}}}, {\"user_intent\": \"Generate depth predictions for a set of photos from my drone survey.\", \"tool_call\": {\"name\": \"dpt-large-redesign\", \"arguments\": {}}}, {\"user_intent\": \"Use a pre-trained DPT model to extract scene depth from a single RGB image.\", \"tool_call\": {\"name\": \"dpt-large-redesign\", \"arguments\": {}}}]}\n\n# User Query\nWe are researching multi-view 3D scanning, and we would like to improve our depth estimation with a pretrained model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to use a pre-trained model to estimate depth from images for 3D scanning research.",
                    "tool_description": "Load and utilize a pre-trained depth estimation model based on the DPT architecture to predict per-pixel depth from input images, facilitating multi-view 3D reconstruction.",
                    "expected_response": "A model object capable of performing depth estimation, including its configuration, pre-trained weights, and dependencies required for inference."
                }
            ],
            "extracted_arguments": {
                "input_images": "A collection of RGB images or frames from multiple views for which depth maps are to be predicted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_228",
        "query": "I have a text question about an image, and I would like to receive an appropriate answer.",
        "instruction": "Given an `image-based question answering` task, retrieve tools that handle multimodal inputs by processing both text and image data to deliver accurate answers based on the query's visual content and associated question.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face",
                    "functionality": "Visual Question Answering",
                    "api_call": "pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')",
                    "api_arguments": {
                        "model": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "tokenizer": "JosephusCheung/GuanacoVQAOnConsumerHardware"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "torch": "latest"
                    },
                    "example_code": "vqa(image_path, question)",
                    "performance": {
                        "dataset": "JosephusCheung/GuanacoVQADataset",
                        "accuracy": "unknown"
                    },
                    "description": "A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.",
                    "id": "huggingface_api_113",
                    "name": "JosephusCheung/GuanacoVQAOnConsumerHardware"
                },
                "id": "gorilla_huggingface_tool_112",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 1: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 2: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.",
            "Tool 3: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 5: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 6: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 7: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 8: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 9: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 10: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 11: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 12: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 13: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 14: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.",
            "Tool 15: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 16: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 17: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 18: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 19: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language."
        ],
        "tool_view": {
            "description": "A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path or URL of the image on which the visual question will be asked.",
                    "required": true,
                    "example": "images/dog.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The question to be answered about the given image.",
                    "required": true,
                    "example": "What breed is the dog in the image?"
                }
            },
            "response": "The tool produces a response that provides an answer to a user-specified question about a given image. The response is typically structured as a dictionary or object containing the following components:\n\n1. **`answer`** – A string representing the model’s textual answer to the question posed about the image. This is the primary output of the tool.  \n2. **`score`** (optional) – A numeric confidence score indicating the model’s certainty in the provided answer. This may be a probability or other relevance measure.  \n3. **`question`** (optional) – The text of the input question, often echoed in the response for clarity.  \n4. **`image`** (optional) – A reference or identifier for the input image, included for traceability when multiple queries are processed.  \n\nIn essence, the response conveys the model’s interpretation of the visual content in relation to the user’s question, potentially accompanied by metadata that quantifies confidence or links back to the original inputs.",
            "examples": [
                {
                    "user_intent": "I have a photo of my living room. Can you tell me how many chairs are visible in it?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/living_room.jpg",
                            "question": "How many chairs are in this room?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this image of a street scene. What color is the car in the foreground?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/street_scene.png",
                            "question": "What is the color of the car in the foreground?"
                        }
                    }
                },
                {
                    "user_intent": "Here's a picture of a group of people at a party. Can you tell me how many people are wearing hats?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/party_group.jpeg",
                            "question": "How many people are wearing hats in this image?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 1: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 2: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.\nTool 3: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 5: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 6: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 7: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 8: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 9: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 10: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 11: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 12: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 13: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 14: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.\nTool 15: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 16: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 17: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 18: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 19: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab\\u2019s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path or URL of the image on which the visual question will be asked.\", \"required\": true, \"example\": \"images/dog.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The question to be answered about the given image.\", \"required\": true, \"example\": \"What breed is the dog in the image?\"}}, \"response\": \"The tool produces a response that provides an answer to a user-specified question about a given image. The response is typically structured as a dictionary or object containing the following components:\\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s textual answer to the question posed about the image. This is the primary output of the tool.  \\n2. **`score`** (optional) \\u2013 A numeric confidence score indicating the model\\u2019s certainty in the provided answer. This may be a probability or other relevance measure.  \\n3. **`question`** (optional) \\u2013 The text of the input question, often echoed in the response for clarity.  \\n4. **`image`** (optional) \\u2013 A reference or identifier for the input image, included for traceability when multiple queries are processed.  \\n\\nIn essence, the response conveys the model\\u2019s interpretation of the visual content in relation to the user\\u2019s question, potentially accompanied by metadata that quantifies confidence or links back to the original inputs.\", \"examples\": [{\"user_intent\": \"I have a photo of my living room. Can you tell me how many chairs are visible in it?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/living_room.jpg\", \"question\": \"How many chairs are in this room?\"}}}, {\"user_intent\": \"Look at this image of a street scene. What color is the car in the foreground?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/street_scene.png\", \"question\": \"What is the color of the car in the foreground?\"}}}, {\"user_intent\": \"Here's a picture of a group of people at a party. Can you tell me how many people are wearing hats?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/party_group.jpeg\", \"question\": \"How many people are wearing hats in this image?\"}}}]}\n\n# User Query\nI have a text question about an image, and I would like to receive an appropriate answer.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to ask a question about an image and receive a relevant answer.",
                    "tool_description": "Retrieve an answer to a natural-language question posed about a given image by using a Visual Question Answering model.",
                    "expected_response": "A dictionary containing an answer to the question, potentially with a confidence score, the original question, and the input image reference."
                }
            ],
            "extracted_arguments": {
                "image_path": "The file path or URL of the image on which the visual question will be asked.",
                "question": "The question to be answered about the given image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_203",
        "query": "I have jobs descriptions in French for sales manager, please highlight names of organizations or cities within the text.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that specialize in identifying and highlighting entities such as organizations and cities within text. These tools should effectively process textual content in different languages and provide precise results based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/camembert-ner')",
                    "api_arguments": {
                        "model": "model",
                        "tokenizer": "tokenizer",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForTokenClassification, pipeline"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\n\nfrom transformers import pipeline\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot « computer  est retir le 9 janvier 2015.)",
                    "performance": {
                        "dataset": "wikiner-fr",
                        "accuracy": {
                            "overall_f1": 0.8914,
                            "PER_f1": 0.9483,
                            "ORG_f1": 0.8181,
                            "LOC_f1": 0.8955,
                            "MISC_f1": 0.8146
                        }
                    },
                    "description": "camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.",
                    "id": "huggingface_api_405",
                    "name": "Jean-Baptiste/camembert-ner"
                },
                "id": "gorilla_huggingface_tool_401",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 1: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 2: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 3: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 4: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 6: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 7: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 8: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 9: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 10: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 11: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 12: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 13: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.",
            "Tool 14: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 15: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 16: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 17: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 18: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 19: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts."
        ],
        "tool_view": {
            "description": "This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "parameters": {
                "Sentence": {
                    "type": "object",
                    "description": "A Sentence object from the Flair library, which contains the text for Named Entity Recognition (NER) tagging.",
                    "required": true,
                    "example": "Sentence('George Washington ging nach Washington')"
                }
            },
            "response": "The tool produces a response representing the results of Named Entity Recognition (NER) on a given input sentence. The response is associated with the `Sentence` object provided to the model and consists of one or more **entity spans** detected within the text. Each span contains structured information describing a recognized named entity. The typical structure and meaning of each component are as follows:\n\n1. **Sentence Object**  \n   - Represents the input text that was analyzed.  \n   - Contains the original tokens of the sentence.  \n   - After prediction, it holds a collection of entity spans that the model detected.\n\n2. **Entity Spans**  \n   Each entity span represents a contiguous sequence of tokens in the sentence that the model has labeled as a named entity. Each span typically includes:\n   - **Text**: The substring of the sentence corresponding to the entity.  \n   - **Start and End Positions**: Character indices or token positions marking where the entity occurs in the sentence.  \n   - **NER Label**: The predicted entity type, which for this model can be one of `PER` (person), `LOC` (location), `ORG` (organization), or `MISC` (miscellaneous).  \n   - **Confidence Score**: A numeric value indicating the model’s confidence in the prediction for that entity (optional depending on the implementation).  \n\n3. **Aggregated Information**  \n   - The sentence object can provide methods to retrieve all detected entities (`get_spans('ner')`) as a list of spans.  \n   - Each span can be iterated over for downstream processing, printing, or structured output.\n\nIn summary, the response is a structured representation of all named entities found in the input sentence, where each entity includes its text, location within the sentence, and predicted label. This enables downstream applications to extract, categorize, or highlight entities in the text.",
            "examples": [
                {
                    "user_intent": "I want to extract named entities from a German sentence using Flair's NER model.",
                    "tool_call": {
                        "name": "flair/ner-german",
                        "arguments": {
                            "Sentence": "George Washington ging nach Washington"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this sentence to identify any people, locations, or organizations in German text.",
                    "tool_call": {
                        "name": "flair/ner-german",
                        "arguments": {
                            "Sentence": "Angela Merkel besuchte Berlin im Jahr 2019."
                        }
                    }
                },
                {
                    "user_intent": "Can I extract named entities from the sentence 'Apple hat neue Produkte in Cupertino vorgestellt'?",
                    "tool_call": {
                        "name": "flair/ner-german",
                        "arguments": {
                            "Sentence": "Apple hat neue Produkte in Cupertino vorgestellt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 1: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 2: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 3: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 4: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 6: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 7: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 8: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 9: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 10: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 11: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 12: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 13: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.\nTool 14: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 15: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 16: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 17: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 18: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 19: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities\\u2014person names, locations, organizations, and miscellaneous names\\u2014within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\", \"parameters\": {\"Sentence\": {\"type\": \"object\", \"description\": \"A Sentence object from the Flair library, which contains the text for Named Entity Recognition (NER) tagging.\", \"required\": true, \"example\": \"Sentence('George Washington ging nach Washington')\"}}, \"response\": \"The tool produces a response representing the results of Named Entity Recognition (NER) on a given input sentence. The response is associated with the `Sentence` object provided to the model and consists of one or more **entity spans** detected within the text. Each span contains structured information describing a recognized named entity. The typical structure and meaning of each component are as follows:\\n\\n1. **Sentence Object**  \\n   - Represents the input text that was analyzed.  \\n   - Contains the original tokens of the sentence.  \\n   - After prediction, it holds a collection of entity spans that the model detected.\\n\\n2. **Entity Spans**  \\n   Each entity span represents a contiguous sequence of tokens in the sentence that the model has labeled as a named entity. Each span typically includes:\\n   - **Text**: The substring of the sentence corresponding to the entity.  \\n   - **Start and End Positions**: Character indices or token positions marking where the entity occurs in the sentence.  \\n   - **NER Label**: The predicted entity type, which for this model can be one of `PER` (person), `LOC` (location), `ORG` (organization), or `MISC` (miscellaneous).  \\n   - **Confidence Score**: A numeric value indicating the model\\u2019s confidence in the prediction for that entity (optional depending on the implementation).  \\n\\n3. **Aggregated Information**  \\n   - The sentence object can provide methods to retrieve all detected entities (`get_spans('ner')`) as a list of spans.  \\n   - Each span can be iterated over for downstream processing, printing, or structured output.\\n\\nIn summary, the response is a structured representation of all named entities found in the input sentence, where each entity includes its text, location within the sentence, and predicted label. This enables downstream applications to extract, categorize, or highlight entities in the text.\", \"examples\": [{\"user_intent\": \"I want to extract named entities from a German sentence using Flair's NER model.\", \"tool_call\": {\"name\": \"flair/ner-german\", \"arguments\": {\"Sentence\": \"George Washington ging nach Washington\"}}}, {\"user_intent\": \"Analyze this sentence to identify any people, locations, or organizations in German text.\", \"tool_call\": {\"name\": \"flair/ner-german\", \"arguments\": {\"Sentence\": \"Angela Merkel besuchte Berlin im Jahr 2019.\"}}}, {\"user_intent\": \"Can I extract named entities from the sentence 'Apple hat neue Produkte in Cupertino vorgestellt'?\", \"tool_call\": {\"name\": \"flair/ner-german\", \"arguments\": {\"Sentence\": \"Apple hat neue Produkte in Cupertino vorgestellt\"}}}]}\n\n# User Query\nI have jobs descriptions in French for sales manager, please highlight names of organizations or cities within the text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract names of organizations and cities from a French job description.",
                    "tool_description": "Use a French Named Entity Recognition (NER) model to identify and extract entities such as organizations and cities from a French text.",
                    "expected_response": "A structured response containing all recognized organizations (ORG) and locations (LOC) from the provided French text."
                }
            ],
            "extracted_arguments": {
                "text": "The French job description text from which to extract named entities, including organizations and locations."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_162",
        "query": "Help me to build a visual-question-answering model that takes a question and an image as input and returns an answer based on the image.",
        "instruction": "Given a `visual-question-answering model building` task, retrieve tools that facilitate creating models which process image and text inputs to provide answers based on the visual content. These tools should support integration with libraries such as Hugging Face Transformers to enable effective multimodal processing.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Visual Question Answering",
                    "api_call": "pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')",
                    "api_arguments": {
                        "model": "Bingsu/temp_vilt_vqa",
                        "tokenizer": "Bingsu/temp_vilt_vqa"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A visual question answering model for answering questions related to images using the Hugging Face Transformers library.",
                    "id": "huggingface_api_112",
                    "name": "temp_vilt_vqa"
                },
                "id": "gorilla_huggingface_tool_111",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.",
            "Tool 1: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.",
            "Tool 2: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 3: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 4: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 5: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.",
            "Tool 6: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 7: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.",
            "Tool 8: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 9: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.",
            "Tool 10: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 11: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 12: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.",
            "Tool 13: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 14: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.",
            "Tool 15: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.",
            "Tool 16: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 17: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 18: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 19: The `microsoft/git-base-coco` model is a generative image-to-text Transformer that converts images into descriptive text. Fine-tuned on the COCO dataset, it uses a Transformer decoder conditioned on both image and text tokens, enabling tasks such as image and video captioning, visual question answering, and text-based image classification. It is designed for applications that require interpreting visual content and generating natural-language descriptions."
        ],
        "tool_view": {
            "description": "A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path to the input image that will be used for visual question answering.",
                    "required": true,
                    "example": "path/to/image.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The natural language question related to the input image that the model should answer.",
                    "required": true,
                    "example": "What is the color of the car in the image?"
                }
            },
            "response": "The tool is designed for Visual Question Answering (VQA) using the VILT framework, which combines vision and language tasks. Given an input image and a question, the model processes the visual content of the image along with the textual query to provide an answer.\n\nSince the specific response format is not defined in the provided documentation, a reasonable and typical response from this tool would include:\n\n1. **Answer**: A textual string representing the answer to the question based on the analysis of the image and the provided query.\n\n2. **Confidence/Probability (optional)**: An optional field indicating the model's confidence in the generated answer. This could be a score between 0 and 1, reflecting how likely the answer is correct according to the model.\n\n3. **Question**: The original question posed to the model, for context and validation of the response.\n\n4. **Image ID (optional)**: An identifier or reference to the input image, especially if the model processes multiple images in one batch or if additional context is needed for verification.\n\nThe primary role of this tool is to provide an answer based on both visual and textual information, and the response would typically include the most relevant answer to the query, possibly along with confidence or contextual data.",
            "examples": [
                {
                    "user_intent": "I have a photo of a cat sitting on a sofa. Can you tell me what animal is in the picture?",
                    "tool_call": {
                        "name": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering",
                        "arguments": {
                            "image": "images/cat_sofa.jpg",
                            "question": "What animal is in this picture?"
                        }
                    }
                },
                {
                    "user_intent": "I took a picture of my breakfast. I want to know if it contains any fruits.",
                    "tool_call": {
                        "name": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering",
                        "arguments": {
                            "image": "images/breakfast_plate.png",
                            "question": "Does this meal contain any fruits?"
                        }
                    }
                },
                {
                    "user_intent": "I have a photo of a busy street. Can you tell me how many vehicles are visible?",
                    "tool_call": {
                        "name": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering",
                        "arguments": {
                            "image": "images/busy_street.jpeg",
                            "question": "How many vehicles are in this image?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.\nTool 1: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\nTool 2: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 3: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 4: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 5: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.\nTool 6: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 7: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.\nTool 8: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 9: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.\nTool 10: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 11: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 12: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.\nTool 13: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 14: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.\nTool 15: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.\nTool 16: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 17: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 18: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 19: The `microsoft/git-base-coco` model is a generative image-to-text Transformer that converts images into descriptive text. Fine-tuned on the COCO dataset, it uses a Transformer decoder conditioned on both image and text tokens, enabling tasks such as image and video captioning, visual question answering, and text-based image classification. It is designed for applications that require interpreting visual content and generating natural-language descriptions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path to the input image that will be used for visual question answering.\", \"required\": true, \"example\": \"path/to/image.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The natural language question related to the input image that the model should answer.\", \"required\": true, \"example\": \"What is the color of the car in the image?\"}}, \"response\": \"The tool is designed for Visual Question Answering (VQA) using the VILT framework, which combines vision and language tasks. Given an input image and a question, the model processes the visual content of the image along with the textual query to provide an answer.\\n\\nSince the specific response format is not defined in the provided documentation, a reasonable and typical response from this tool would include:\\n\\n1. **Answer**: A textual string representing the answer to the question based on the analysis of the image and the provided query.\\n\\n2. **Confidence/Probability (optional)**: An optional field indicating the model's confidence in the generated answer. This could be a score between 0 and 1, reflecting how likely the answer is correct according to the model.\\n\\n3. **Question**: The original question posed to the model, for context and validation of the response.\\n\\n4. **Image ID (optional)**: An identifier or reference to the input image, especially if the model processes multiple images in one batch or if additional context is needed for verification.\\n\\nThe primary role of this tool is to provide an answer based on both visual and textual information, and the response would typically include the most relevant answer to the query, possibly along with confidence or contextual data.\", \"examples\": [{\"user_intent\": \"I have a photo of a cat sitting on a sofa. Can you tell me what animal is in the picture?\", \"tool_call\": {\"name\": \"hf-tiny-model-private/tiny-random-ViltForQuestionAnswering\", \"arguments\": {\"image\": \"images/cat_sofa.jpg\", \"question\": \"What animal is in this picture?\"}}}, {\"user_intent\": \"I took a picture of my breakfast. I want to know if it contains any fruits.\", \"tool_call\": {\"name\": \"hf-tiny-model-private/tiny-random-ViltForQuestionAnswering\", \"arguments\": {\"image\": \"images/breakfast_plate.png\", \"question\": \"Does this meal contain any fruits?\"}}}, {\"user_intent\": \"I have a photo of a busy street. Can you tell me how many vehicles are visible?\", \"tool_call\": {\"name\": \"hf-tiny-model-private/tiny-random-ViltForQuestionAnswering\", \"arguments\": {\"image\": \"images/busy_street.jpeg\", \"question\": \"How many vehicles are in this image?\"}}}]}\n\n# User Query\nHelp me to build a visual-question-answering model that takes a question and an image as input and returns an answer based on the image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to input an image and a question to get an answer about the visual content.",
                    "tool_description": "A Visual Question Answering (VQA) model that processes an input image along with a natural language question to generate an answer based on the image content.",
                    "expected_response": "A textual answer string representing the model's response to the question about the image, optionally including confidence scores and context."
                }
            ],
            "extracted_arguments": {
                "image": "The file path or data of the input image to be analyzed.",
                "question": "The natural language question related to the visual content of the image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_175",
        "query": "As a journalist, I am curious about speech sentiment analysis in a group of people in a crowd. I want to extract features from the audio to run sentiment analysis.",
        "instruction": "Given a `speech sentiment analysis` task, retrieve tools that perform audio feature extraction to analyze and interpret the sentiment present in a group of people within an audio input, utilizing advanced speech representation learning techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "HubertModel.from_pretrained('facebook/hubert-large-ll60k')",
                    "api_arguments": "pretrained model name",
                    "python_environment_requirements": "transformers",
                    "example_code": "hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')",
                    "performance": {
                        "dataset": "Libri-Light",
                        "accuracy": "matches or improves upon the state-of-the-art wav2vec 2.0 performance"
                    },
                    "description": "Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.",
                    "id": "huggingface_api_12",
                    "name": "hubert-large-ll60k"
                },
                "id": "gorilla_huggingface_tool_12",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 1: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 2: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 3: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 4: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 5: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.",
            "Tool 6: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.",
            "Tool 7: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 8: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 9: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 10: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 11: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 12: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 13: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 14: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 15: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 16: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "Tool 17: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.",
            "Tool 18: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 19: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis."
        ],
        "tool_view": {
            "description": "A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "parameters": {
                "data": {
                    "type": "string",
                    "description": "The path to a CSV file containing the input data for the model. The data should include the features required for prediction, which will be renamed internally with a 'feat_' prefix for each column.",
                    "required": true,
                    "example": "data.csv"
                }
            },
            "response": "The tool produces a response that consists of predicted class labels for each input data instance. Specifically:  \n\n- **Predictions Array**: The core component of the response is a sequence (e.g., a list or array) where each element corresponds to a single row in the input tabular data. Each element represents the predicted class for that data row. Since this is a multi-class classification model, each predicted class is one of several possible discrete categories.  \n\n- **Order Alignment**: The predictions maintain the same order as the input data rows, ensuring that each predicted label directly corresponds to the respective input record.  \n\n- **Data Type**: Each predicted class is typically represented as a string or numeric label, depending on how the model was trained and encoded.  \n\n- **Optional Metadata** (not explicitly defined but typical): Some implementations may also provide confidence scores or probability distributions over all classes for each prediction, although this is not specified in the raw JSON.  \n\nIn essence, the response maps input tabular rows to their predicted sentiment class, enabling downstream tasks like evaluation, reporting, or decision-making based on these predictions.",
            "examples": [
                {
                    "user_intent": "I want to predict the sentiment of new movie reviews stored in a CSV file.",
                    "tool_call": {
                        "name": "Alexei1/imdb",
                        "arguments": {
                            "data": "new_movie_reviews.csv"
                        }
                    }
                },
                {
                    "user_intent": "Classify the emotions in my dataset of film critiques.",
                    "tool_call": {
                        "name": "Alexei1/imdb",
                        "arguments": {
                            "data": "film_critiques.csv"
                        }
                    }
                },
                {
                    "user_intent": "Run the IMDB sentiment analysis model on the latest batch of user comments.",
                    "tool_call": {
                        "name": "Alexei1/imdb",
                        "arguments": {
                            "data": "user_comments_batch.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 1: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 2: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 3: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 4: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 5: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.\nTool 6: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.\nTool 7: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 8: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 9: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 10: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 11: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 12: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 13: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 14: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 15: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 16: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\nTool 17: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.\nTool 18: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 19: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\", \"parameters\": {\"data\": {\"type\": \"string\", \"description\": \"The path to a CSV file containing the input data for the model. The data should include the features required for prediction, which will be renamed internally with a 'feat_' prefix for each column.\", \"required\": true, \"example\": \"data.csv\"}}, \"response\": \"The tool produces a response that consists of predicted class labels for each input data instance. Specifically:  \\n\\n- **Predictions Array**: The core component of the response is a sequence (e.g., a list or array) where each element corresponds to a single row in the input tabular data. Each element represents the predicted class for that data row. Since this is a multi-class classification model, each predicted class is one of several possible discrete categories.  \\n\\n- **Order Alignment**: The predictions maintain the same order as the input data rows, ensuring that each predicted label directly corresponds to the respective input record.  \\n\\n- **Data Type**: Each predicted class is typically represented as a string or numeric label, depending on how the model was trained and encoded.  \\n\\n- **Optional Metadata** (not explicitly defined but typical): Some implementations may also provide confidence scores or probability distributions over all classes for each prediction, although this is not specified in the raw JSON.  \\n\\nIn essence, the response maps input tabular rows to their predicted sentiment class, enabling downstream tasks like evaluation, reporting, or decision-making based on these predictions.\", \"examples\": [{\"user_intent\": \"I want to predict the sentiment of new movie reviews stored in a CSV file.\", \"tool_call\": {\"name\": \"Alexei1/imdb\", \"arguments\": {\"data\": \"new_movie_reviews.csv\"}}}, {\"user_intent\": \"Classify the emotions in my dataset of film critiques.\", \"tool_call\": {\"name\": \"Alexei1/imdb\", \"arguments\": {\"data\": \"film_critiques.csv\"}}}, {\"user_intent\": \"Run the IMDB sentiment analysis model on the latest batch of user comments.\", \"tool_call\": {\"name\": \"Alexei1/imdb\", \"arguments\": {\"data\": \"user_comments_batch.csv\"}}}]}\n\n# User Query\nAs a journalist, I am curious about speech sentiment analysis in a group of people in a crowd. I want to extract features from the audio to run sentiment analysis.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract audio features from a recording of crowd speech to prepare for sentiment analysis.",
                    "tool_description": "Process raw audio data to extract relevant features using an audio spectrogram transformer or similar feature extraction model, suitable for downstream sentiment or emotion analysis.",
                    "expected_response": "A structured representation of audio features, such as spectrogram matrices or embeddings, that can be used as input for sentiment classification models."
                },
                {
                    "user_intent": "Analyze the extracted audio features to determine the sentiment of speech in the crowd.",
                    "tool_description": "Apply a speech sentiment analysis model to the extracted audio features to classify the overall sentiment or emotional tone expressed in the speech segments.",
                    "expected_response": "Predicted sentiment labels or emotion categories for each analyzed audio segment, optionally with confidence scores."
                }
            ],
            "extracted_arguments": {
                "audio_data": "The raw audio recordings capturing speech from the crowd, in a format suitable for feature extraction.",
                "features_model": "The model used to extract meaningful features from the audio data for sentiment analysis.",
                "sentiment_model": "The speech sentiment analysis model that classifies sentiment based on the extracted audio features."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_466",
        "query": "I work at GreenTech, a startup that provides eco-friendly solutions, and need to determine if a given set of input data will result in high carbon emissions or not.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools designed for binary classification using tabular data inputs to predict whether the provided data will result in high carbon emissions, incorporating machine learning model functionalities.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "Validation Metrics",
                        "accuracy": 0.983
                    },
                    "description": "Binary Classification model for Carbon Emissions prediction",
                    "id": "huggingface_api_864",
                    "name": "jwan2021/autotrain-jwan-autotrain1-1768961489"
                },
                "id": "gorilla_huggingface_tool_857",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 1: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 2: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 4: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 5: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 6: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 7: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 8: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 9: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 10: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 11: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 12: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 13: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 14: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 15: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 16: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 17: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "Tool 18: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 19: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text."
        ],
        "tool_view": {
            "description": "This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "parameters": {
                "data": {
                    "type": "object",
                    "description": "The dataset used for making predictions with the model. This should be a DataFrame containing the features expected by the model.",
                    "required": true,
                    "example": {
                        "feat_1": 0.5,
                        "feat_2": 1.2,
                        "feat_3": 3.4
                    }
                }
            },
            "response": "The tool described is a machine learning model focused on tabular classification, specifically predicting carbon emissions. The response from this tool would likely be a prediction based on the input data provided, along with relevant metadata about the model’s performance and configuration. Here’s an outline of the expected response structure:\n\n### 1. **Predictions**:\n   - This part of the response would contain the carbon emission predictions made by the model, which are calculated from the input data.\n   - The predictions could be in the form of a list, array, or structured data corresponding to the input rows.\n   - Each prediction would represent the model’s estimate of carbon emissions in grams based on the tabular data input.\n\n### 2. **Performance Metrics**:\n   - The response may include performance metrics from the validation dataset, such as:\n     - **Accuracy**: This is typically a percentage or a fraction representing how often the model's predictions were correct during validation (here, it’s given as 99.1% accuracy).\n\n### 3. **Model Information**:\n   - The response may also include high-level metadata about the model, such as:\n     - **Name**: \"omarques/autotrain-in-class-test-demo-1659958764\" (model identifier).\n     - **Description**: A brief explanation of the model, which in this case is described as a tabular classification model for predicting carbon emissions trained using AutoTrain.\n     - **Framework**: \"Joblib\" (the framework used to load and run the model).\n\n### 4. **Input Data Information**:\n   - While not explicitly required in the response, the response may also include references to the input data used for the prediction, particularly the features selected for prediction. This could include:\n     - **Features**: The specific features selected for the prediction, as outlined in the `config.json` file, though this information would usually be part of the input to the model rather than the response itself.\n\n### 5. **Technical Details**:\n   - Information regarding the Python environment required for the model to work, such as dependencies (`joblib`, `pandas`).\n   - The API call used to invoke the model (`joblib.load('model.joblib')`) might be referenced in logs or in a debugging context, but is unlikely to appear in the direct response from the tool.\n\nIn summary, the tool would return **carbon emission predictions** based on tabular input data, **performance metrics** such as accuracy, and **metadata** about the model's name, framework, and description.",
            "examples": [
                {
                    "user_intent": "Predict the carbon emissions for a CSV file containing vehicle data.",
                    "tool_call": {
                        "name": "omarques/autotrain-in-class-test-demo-1659958764",
                        "arguments": {
                            "data": "vehicles_2025.csv"
                        }
                    }
                },
                {
                    "user_intent": "Estimate carbon emissions for a new batch of factory production records.",
                    "tool_call": {
                        "name": "omarques/autotrain-in-class-test-demo-1659958764",
                        "arguments": {
                            "data": "factory_output_q4.csv"
                        }
                    }
                },
                {
                    "user_intent": "Run the carbon emissions prediction model on historical shipment data to compare trends.",
                    "tool_call": {
                        "name": "omarques/autotrain-in-class-test-demo-1659958764",
                        "arguments": {
                            "data": "shipment_history.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 1: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 2: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 4: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 5: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 6: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 7: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 8: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 9: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 10: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 11: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 12: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 13: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 14: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 15: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 16: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 17: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\nTool 18: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 19: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\", \"parameters\": {\"data\": {\"type\": \"object\", \"description\": \"The dataset used for making predictions with the model. This should be a DataFrame containing the features expected by the model.\", \"required\": true, \"example\": {\"feat_1\": 0.5, \"feat_2\": 1.2, \"feat_3\": 3.4}}}, \"response\": \"The tool described is a machine learning model focused on tabular classification, specifically predicting carbon emissions. The response from this tool would likely be a prediction based on the input data provided, along with relevant metadata about the model\\u2019s performance and configuration. Here\\u2019s an outline of the expected response structure:\\n\\n### 1. **Predictions**:\\n   - This part of the response would contain the carbon emission predictions made by the model, which are calculated from the input data.\\n   - The predictions could be in the form of a list, array, or structured data corresponding to the input rows.\\n   - Each prediction would represent the model\\u2019s estimate of carbon emissions in grams based on the tabular data input.\\n\\n### 2. **Performance Metrics**:\\n   - The response may include performance metrics from the validation dataset, such as:\\n     - **Accuracy**: This is typically a percentage or a fraction representing how often the model's predictions were correct during validation (here, it\\u2019s given as 99.1% accuracy).\\n\\n### 3. **Model Information**:\\n   - The response may also include high-level metadata about the model, such as:\\n     - **Name**: \\\"omarques/autotrain-in-class-test-demo-1659958764\\\" (model identifier).\\n     - **Description**: A brief explanation of the model, which in this case is described as a tabular classification model for predicting carbon emissions trained using AutoTrain.\\n     - **Framework**: \\\"Joblib\\\" (the framework used to load and run the model).\\n\\n### 4. **Input Data Information**:\\n   - While not explicitly required in the response, the response may also include references to the input data used for the prediction, particularly the features selected for prediction. This could include:\\n     - **Features**: The specific features selected for the prediction, as outlined in the `config.json` file, though this information would usually be part of the input to the model rather than the response itself.\\n\\n### 5. **Technical Details**:\\n   - Information regarding the Python environment required for the model to work, such as dependencies (`joblib`, `pandas`).\\n   - The API call used to invoke the model (`joblib.load('model.joblib')`) might be referenced in logs or in a debugging context, but is unlikely to appear in the direct response from the tool.\\n\\nIn summary, the tool would return **carbon emission predictions** based on tabular input data, **performance metrics** such as accuracy, and **metadata** about the model's name, framework, and description.\", \"examples\": [{\"user_intent\": \"Predict the carbon emissions for a CSV file containing vehicle data.\", \"tool_call\": {\"name\": \"omarques/autotrain-in-class-test-demo-1659958764\", \"arguments\": {\"data\": \"vehicles_2025.csv\"}}}, {\"user_intent\": \"Estimate carbon emissions for a new batch of factory production records.\", \"tool_call\": {\"name\": \"omarques/autotrain-in-class-test-demo-1659958764\", \"arguments\": {\"data\": \"factory_output_q4.csv\"}}}, {\"user_intent\": \"Run the carbon emissions prediction model on historical shipment data to compare trends.\", \"tool_call\": {\"name\": \"omarques/autotrain-in-class-test-demo-1659958764\", \"arguments\": {\"data\": \"shipment_history.csv\"}}}]}\n\n# User Query\nI work at GreenTech, a startup that provides eco-friendly solutions, and need to determine if a given set of input data will result in high carbon emissions or not.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Determine whether the input data will result in high carbon emissions.",
                    "tool_description": "Use a tabular classification model trained with AutoTrain to predict carbon emission levels based on structured input data. The model is loaded with Joblib and processes data using pandas to output emission predictions.",
                    "expected_response": "Predictions indicating the estimated carbon emission level for each input row, along with model metadata and performance metrics such as accuracy."
                }
            ],
            "extracted_arguments": {
                "data": "A structured dataset containing the features required by the carbon emission prediction model, formatted as a DataFrame or CSV."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_87",
        "query": "Our company manages a restaurant. We need to analyze customers' tips and predict how much tip a new customer would give based on their total bill, sex, smoker, day, time, and party size. We should use a pre-trained model.",
        "instruction": "Given a `customer tipping prediction` task, retrieve tools that utilize pre-trained regression models to analyze input parameters such as total bill, customer demographics, and visit details to predict the tip amount for new customers.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Scikit-learn",
                    "functionality": "baseline-trainer",
                    "api_call": "joblib.load(hf_hub_download('merve/tips5wx_sbh5-tip-regression', 'sklearn_model.joblib'))",
                    "api_arguments": {
                        "alpha": 10
                    },
                    "python_environment_requirements": [
                        "dabl"
                    ],
                    "example_code": "Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\ntotal_bill True False False ... False False False\nsex False False False ... False False False\nsmoker False False False ... False False False\nday False False False ... False False False\ntime False False False ... False False False\nsize False False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])",
                    "performance": {
                        "dataset": "tips5wx_sbh5",
                        "r2": 0.389363,
                        "neg_mean_squared_error": -1.092356
                    },
                    "description": "Baseline Model trained on tips5wx_sbh5 to apply regression on tip",
                    "id": "huggingface_api_885",
                    "name": "merve/tips5wx_sbh5-tip-regression"
                },
                "id": "gorilla_huggingface_tool_878",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 1: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 2: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 4: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 6: Provides a pre-trained baseline regression model for predicting tips using the `tips5wx_sbh5` dataset, implemented in Scikit-learn, useful for quickly establishing a reference performance for tabular regression tasks.",
            "Tool 7: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 8: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 9: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 10: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 11: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 12: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 13: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 14: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.",
            "Tool 15: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 16: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 17: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 18: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets."
        ],
        "tool_view": {
            "description": "A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "parameters": {
                "images": {
                    "type": "array",
                    "description": "A list of video frames represented as arrays or tensors. Each frame should have shape (channels, height, width), typically (3, 224, 224). The list represents the temporal sequence of frames for the video.",
                    "required": true,
                    "example": "[np.random.randn(3, 224, 224) for _ in range(8)]"
                }
            },
            "response": "The tool produces a response containing the model’s predictions for a given video input. The response structure typically includes the following components:\n\n1. **logits**: A tensor containing raw, unnormalized prediction scores for each class in the model’s label space. Each element corresponds to the model’s confidence for a particular class.\n\n2. **predicted class index**: A derived integer representing the index of the class with the highest logit score. This is often computed using an `argmax` operation over the logits.\n\n3. **predicted class label**: A human-readable string corresponding to the predicted class index. The mapping from index to label is provided by the model configuration (`model.config.id2label`).\n\nThe response may also include additional outputs depending on the model, such as intermediate hidden states or attention maps, but the primary information relevant for video classification is the logits and the resulting predicted class.",
            "examples": [
                {
                    "user_intent": "Classify the action happening in this short sports video clip I recorded.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-k600",
                        "arguments": {
                            "images": "sports_clip_frames.npy"
                        }
                    }
                },
                {
                    "user_intent": "I want to analyze a set of surveillance video frames to detect unusual activities.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-k600",
                        "arguments": {
                            "images": "surveillance_video_frames.pt"
                        }
                    }
                },
                {
                    "user_intent": "Predict the category of this movie scene clip using the TimeSformer model.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-k600",
                        "arguments": {
                            "images": "movie_scene_frames.npy"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 1: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 2: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 4: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 6: Provides a pre-trained baseline regression model for predicting tips using the `tips5wx_sbh5` dataset, implemented in Scikit-learn, useful for quickly establishing a reference performance for tabular regression tasks.\nTool 7: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 8: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 9: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 10: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 11: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 12: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 13: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 14: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.\nTool 15: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 16: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 17: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 18: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\", \"parameters\": {\"images\": {\"type\": \"array\", \"description\": \"A list of video frames represented as arrays or tensors. Each frame should have shape (channels, height, width), typically (3, 224, 224). The list represents the temporal sequence of frames for the video.\", \"required\": true, \"example\": \"[np.random.randn(3, 224, 224) for _ in range(8)]\"}}, \"response\": \"The tool produces a response containing the model\\u2019s predictions for a given video input. The response structure typically includes the following components:\\n\\n1. **logits**: A tensor containing raw, unnormalized prediction scores for each class in the model\\u2019s label space. Each element corresponds to the model\\u2019s confidence for a particular class.\\n\\n2. **predicted class index**: A derived integer representing the index of the class with the highest logit score. This is often computed using an `argmax` operation over the logits.\\n\\n3. **predicted class label**: A human-readable string corresponding to the predicted class index. The mapping from index to label is provided by the model configuration (`model.config.id2label`).\\n\\nThe response may also include additional outputs depending on the model, such as intermediate hidden states or attention maps, but the primary information relevant for video classification is the logits and the resulting predicted class.\", \"examples\": [{\"user_intent\": \"Classify the action happening in this short sports video clip I recorded.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-k600\", \"arguments\": {\"images\": \"sports_clip_frames.npy\"}}}, {\"user_intent\": \"I want to analyze a set of surveillance video frames to detect unusual activities.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-k600\", \"arguments\": {\"images\": \"surveillance_video_frames.pt\"}}}, {\"user_intent\": \"Predict the category of this movie scene clip using the TimeSformer model.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-k600\", \"arguments\": {\"images\": \"movie_scene_frames.npy\"}}}]}\n\n# User Query\nOur company manages a restaurant. We need to analyze customers' tips and predict how much tip a new customer would give based on their total bill, sex, smoker, day, time, and party size. We should use a pre-trained model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Predict the tip amount a new restaurant customer would give based on their details.",
                    "tool_description": "Use a pre-trained tabular regression model to estimate tip amounts from customer and visit features such as total bill, sex, smoker status, day, time, and party size.",
                    "expected_response": "A numeric prediction representing the expected tip amount for the given customer and visit features."
                }
            ],
            "extracted_arguments": {
                "total_bill": "The total bill amount for the customer's order, as a numeric value.",
                "sex": "The gender of the customer, represented as a categorical variable.",
                "smoker": "Whether the customer is a smoker, represented as a categorical variable (yes/no).",
                "day": "The day of the week of the visit, represented as a categorical variable.",
                "time": "The time of the meal (e.g., lunch or dinner), represented as a categorical variable.",
                "size": "The number of people in the customer's party, as a numeric value."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_403",
        "query": "We are building a memory game where a description is displayed for a few seconds and later on, questions on what is shown comes up for the user to remember.",
        "instruction": "Given a `memory game design` task, retrieve tools that facilitate the creation of interactive memory games by processing game elements like descriptions and questions to enhance user engagement and learning.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')",
                    "api_arguments": [
                        "question",
                        "context"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nquestion_answerer = pipeline(question-answering, model='distilbert-base-uncased-distilled-squad')\ncontext = r\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\nprint(\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\n...)",
                    "performance": {
                        "dataset": "SQuAD v1.1",
                        "accuracy": "86.9 F1 score"
                    },
                    "description": "DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.",
                    "id": "huggingface_api_464",
                    "name": "distilbert-base-uncased-distilled-squad"
                },
                "id": "gorilla_huggingface_tool_460",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 1: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 3: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 6: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 7: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 8: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 9: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 10: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 11: The tool uses a fine-tuned version of DistilBERT (distilbert-base-uncased-distilled-squad) for extractive question answering, where it identifies answers to questions based on a provided context. It is designed to be faster and more efficient than the original BERT model, with 40% fewer parameters and 60% faster execution, while maintaining over 95% of BERT's performance on language understanding tasks. This model is particularly suited for tasks like answering questions from datasets such as SQuAD.",
            "Tool 12: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 13: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 14: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.",
            "Tool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 16: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.",
            "Tool 17: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 18: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 19: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The tool’s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\n\n1. **Boxes / Bounding Boxes**  \n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \n   - These coordinates define the rectangular region enclosing each detected object.\n\n2. **Class / Label**  \n   - Each detected object is assigned a class label indicating its category, such as \"dropped spike,\" \"enemy,\" \"planted spike,\" or \"teammate.\"  \n   - The label identifies the type of object detected by the model.\n\n3. **Confidence Score**  \n   - A numerical value representing the model’s confidence that the detected object belongs to the predicted class.  \n   - This value is constrained by the `conf` threshold specified in the API arguments.\n\n4. **Optional Additional Metadata**  \n   - Depending on the framework, the response may include additional information such as:\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\n     - **Tracking ID** if sequential detection or object tracking is enabled.\n     - **Mask or segmentation data** if the model supports instance segmentation.\n\n5. **Rendering / Visualization (Optional)**  \n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\n\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.",
            "examples": [
                {
                    "user_intent": "I want to detect all enemies and teammates in a Valorant gameplay screenshot.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000
                        }
                    }
                },
                {
                    "user_intent": "Scan a live stream frame from Valorant to check if the spike has been dropped or planted.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500
                        }
                    }
                },
                {
                    "user_intent": "Analyze a sequence of Valorant game images to locate all objects with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 1000
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 1: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 3: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 6: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 7: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 8: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 9: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 10: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 11: The tool uses a fine-tuned version of DistilBERT (distilbert-base-uncased-distilled-squad) for extractive question answering, where it identifies answers to questions based on a provided context. It is designed to be faster and more efficient than the original BERT model, with 40% fewer parameters and 60% faster execution, while maintaining over 95% of BERT's performance on language understanding tasks. This model is particularly suited for tasks like answering questions from datasets such as SQuAD.\nTool 12: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 13: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 14: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.\nTool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 16: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.\nTool 17: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 18: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 19: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The tool\\u2019s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\\n\\n1. **Boxes / Bounding Boxes**  \\n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \\n   - These coordinates define the rectangular region enclosing each detected object.\\n\\n2. **Class / Label**  \\n   - Each detected object is assigned a class label indicating its category, such as \\\"dropped spike,\\\" \\\"enemy,\\\" \\\"planted spike,\\\" or \\\"teammate.\\\"  \\n   - The label identifies the type of object detected by the model.\\n\\n3. **Confidence Score**  \\n   - A numerical value representing the model\\u2019s confidence that the detected object belongs to the predicted class.  \\n   - This value is constrained by the `conf` threshold specified in the API arguments.\\n\\n4. **Optional Additional Metadata**  \\n   - Depending on the framework, the response may include additional information such as:\\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\\n     - **Tracking ID** if sequential detection or object tracking is enabled.\\n     - **Mask or segmentation data** if the model supports instance segmentation.\\n\\n5. **Rendering / Visualization (Optional)**  \\n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \\n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\\n\\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.\", \"examples\": [{\"user_intent\": \"I want to detect all enemies and teammates in a Valorant gameplay screenshot.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}}}, {\"user_intent\": \"Scan a live stream frame from Valorant to check if the spike has been dropped or planted.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500}}}, {\"user_intent\": \"Analyze a sequence of Valorant game images to locate all objects with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 1000}}}]}\n\n# User Query\nWe are building a memory game where a description is displayed for a few seconds and later on, questions on what is shown comes up for the user to remember.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate descriptive captions for images to display to users in the memory game.",
                    "tool_description": "Use an image-to-text model to automatically generate textual descriptions from input images, capturing key objects and scene details.",
                    "expected_response": "A textual description or caption representing the contents of the input image."
                },
                {
                    "user_intent": "Create questions based on the previously generated descriptions for the memory test.",
                    "tool_description": "Use a question generation or extractive QA model to formulate questions from a given text description, suitable for testing memory recall.",
                    "expected_response": "A set of questions and possibly answer options derived from the input description."
                }
            ],
            "extracted_arguments": {
                "image_input": "The image file or frame to be described for the memory game, used as input for caption generation.",
                "display_duration": "The amount of time in seconds each description should be shown to the user.",
                "description_text": "The textual output generated from the image, used for subsequent question generation.",
                "question_type": "The format or style of memory test questions to generate from the description (e.g., multiple choice, true/false, fill-in-the-blank)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_53",
        "query": "Your company is developing a chatbot and requires accurate summaries of lengthy dialogues without losing context.",
        "instruction": "Given a `dialogue summarization` task, retrieve tools that specialize in processing long dialogue inputs to generate concise and coherent summaries without losing context. These tools should leverage advanced natural language processing architectures to handle extensive dialogue data effectively.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text2Text Generation",
                    "api_call": "LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "arxiv",
                        "accuracy": "2109.02492"
                    },
                    "description": "DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.",
                    "id": "huggingface_api_642",
                    "name": "DialogLED-base-16384"
                },
                "id": "gorilla_huggingface_tool_637",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.",
            "Tool 1: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 2: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 3: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.",
            "Tool 4: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 5: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 6: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.",
            "Tool 7: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 8: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 9: A T5-based text-to-text summarization model fine-tuned on the Samsum dataset, designed to generate concise summaries from input text without requiring explicit prompts. It can be used for natural language processing tasks that require transforming longer text into shorter, coherent summaries while maintaining key information.",
            "Tool 10: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 12: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 13: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.",
            "Tool 14: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 15: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.",
            "Tool 16: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 17: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 18: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 19: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations."
        ],
        "tool_view": {
            "description": "DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.",
            "parameters": {},
            "response": "The tool produces a generated text output based on a given input text. The response is typically a single textual string that represents the model’s prediction, completion, or transformation of the input according to the specified task (e.g., dialogue understanding, summarization, or other text-to-text generation).  \n\nKey components of the response include:  \n\n1. **Generated Text**: The primary content of the response is the text output produced by the model. This is the model’s interpretation or continuation of the input text, reflecting summarization, dialogue response, or other conditional text generation tasks.  \n\n2. **Optional Metadata (implementation-dependent)**: Depending on the framework and generation settings, the response may also include auxiliary information such as:  \n   - **Sequence scores or likelihoods** indicating the model’s confidence in the generated output.  \n   - **Token-level details** showing the tokens generated at each step (less common unless explicitly requested).  \n   - **Attention or hidden state outputs** for more advanced analysis or debugging (if the API call is configured to return these).  \n\nIn practice, the typical usage focuses on the main generated text, which is a coherent, contextually appropriate transformation of the input text, constrained by the model’s maximum input length (16,384 tokens for this pre-trained model).",
            "examples": [
                {
                    "user_intent": "Summarize this long customer support chat log into key points.",
                    "tool_call": {
                        "name": "DialogLED-base-16384",
                        "arguments": {
                            "input_text": "Customer: I'm having trouble with my account login... [very long chat log continues]"
                        }
                    }
                },
                {
                    "user_intent": "Generate a concise summary of a 10-page meeting transcript for internal notes.",
                    "tool_call": {
                        "name": "DialogLED-base-16384",
                        "arguments": {
                            "input_text": "Meeting Transcript: Today we discussed the Q4 targets, marketing strategies, and the product launch schedule... [full transcript]"
                        }
                    }
                },
                {
                    "user_intent": "Condense a long online forum discussion into the main arguments and conclusions.",
                    "tool_call": {
                        "name": "DialogLED-base-16384",
                        "arguments": {
                            "input_text": "Forum Thread: User1: I think the new feature is great because... User2: I disagree, the issue is... [entire thread]"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.\nTool 1: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 2: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 3: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.\nTool 4: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 5: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 6: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.\nTool 7: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 8: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 9: A T5-based text-to-text summarization model fine-tuned on the Samsum dataset, designed to generate concise summaries from input text without requiring explicit prompts. It can be used for natural language processing tasks that require transforming longer text into shorter, coherent summaries while maintaining key information.\nTool 10: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 12: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 13: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.\nTool 14: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 15: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.\nTool 16: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 17: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 18: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 19: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.\", \"parameters\": {}, \"response\": \"The tool produces a generated text output based on a given input text. The response is typically a single textual string that represents the model\\u2019s prediction, completion, or transformation of the input according to the specified task (e.g., dialogue understanding, summarization, or other text-to-text generation).  \\n\\nKey components of the response include:  \\n\\n1. **Generated Text**: The primary content of the response is the text output produced by the model. This is the model\\u2019s interpretation or continuation of the input text, reflecting summarization, dialogue response, or other conditional text generation tasks.  \\n\\n2. **Optional Metadata (implementation-dependent)**: Depending on the framework and generation settings, the response may also include auxiliary information such as:  \\n   - **Sequence scores or likelihoods** indicating the model\\u2019s confidence in the generated output.  \\n   - **Token-level details** showing the tokens generated at each step (less common unless explicitly requested).  \\n   - **Attention or hidden state outputs** for more advanced analysis or debugging (if the API call is configured to return these).  \\n\\nIn practice, the typical usage focuses on the main generated text, which is a coherent, contextually appropriate transformation of the input text, constrained by the model\\u2019s maximum input length (16,384 tokens for this pre-trained model).\", \"examples\": [{\"user_intent\": \"Summarize this long customer support chat log into key points.\", \"tool_call\": {\"name\": \"DialogLED-base-16384\", \"arguments\": {\"input_text\": \"Customer: I'm having trouble with my account login... [very long chat log continues]\"}}}, {\"user_intent\": \"Generate a concise summary of a 10-page meeting transcript for internal notes.\", \"tool_call\": {\"name\": \"DialogLED-base-16384\", \"arguments\": {\"input_text\": \"Meeting Transcript: Today we discussed the Q4 targets, marketing strategies, and the product launch schedule... [full transcript]\"}}}, {\"user_intent\": \"Condense a long online forum discussion into the main arguments and conclusions.\", \"tool_call\": {\"name\": \"DialogLED-base-16384\", \"arguments\": {\"input_text\": \"Forum Thread: User1: I think the new feature is great because... User2: I disagree, the issue is... [entire thread]\"}}}]}\n\n# User Query\nYour company is developing a chatbot and requires accurate summaries of lengthy dialogues without losing context.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate accurate summaries of lengthy dialogues without losing context.",
                    "tool_description": "Generate concise, context-aware summaries of long dialogues while preserving key details and the overall meaning.",
                    "expected_response": "A summary of the dialogue that retains the core information and context of the original conversation."
                }
            ],
            "extracted_arguments": {
                "input_text": "The entire dialogue or conversation to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_154",
        "query": "Our company is exploring the Chinese market and we need to communicate effectively with our clients. Help me create a generic Chinese response.",
        "instruction": "Given a `language generation` task, retrieve tools that facilitate effective communication by generating coherent responses in a specified language, focusing on natural language processing models tailored for the target linguistic needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "AutoModelForMaskedLM.from_pretrained('bert-base-chinese')",
                    "api_arguments": {
                        "pretrained_model_name": "bert-base-chinese"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModelForMaskedLM"
                    },
                    "example_code": "tokenizer = AutoTokenizer.from_pretrained(bert-base-chinese)\nmodel = AutoModelForMaskedLM.from_pretrained(bert-base-chinese)",
                    "performance": {
                        "dataset": "[More Information Needed]",
                        "accuracy": "[More Information Needed]"
                    },
                    "description": "This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling.",
                    "id": "huggingface_api_667",
                    "name": "bert-base-chinese"
                },
                "id": "gorilla_huggingface_tool_662",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 1: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 2: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 3: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 4: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.",
            "Tool 5: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 6: This tool provides zero-shot image classification using the Chinese CLIP model, which is trained on a large dataset of approximately 200 million Chinese image-text pairs. It leverages a Vision Transformer (ViT-B/16) as the image encoder and a RoBERTa-based text encoder to match images with textual descriptions without requiring task-specific fine-tuning. It is designed for applications like image retrieval and classification in Chinese contexts, offering strong performance across benchmarks such as CIFAR, COCO-CN, Flickr30K-CN, and MUGE.",
            "Tool 7: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 8: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.",
            "Tool 9: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "Tool 10: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.",
            "Tool 11: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.",
            "Tool 12: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.",
            "Tool 13: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 14: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 15: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 16: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 17: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 18: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently."
        ],
        "tool_view": {
            "description": "Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input Chinese text that you want the model to summarize. This should be a complete text string.",
                    "required": true,
                    "example": "今天天气很好，我们去公园散步吧。"
                },
                "max_length": {
                    "type": "integer",
                    "description": "The maximum number of tokens to generate in the summary. This limits the length of the output summary.",
                    "required": true,
                    "example": 150
                }
            },
            "response": "The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\n\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\n\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\"summary_text\"`) pointing to the summarized string. The content of the summary reflects the model’s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \n\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.",
            "examples": [
                {
                    "user_intent": "我有一篇关于人工智能发展的中文文章，想生成一段简短摘要。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "人工智能近年来取得了显著的发展，从自然语言处理到计算机视觉，各个领域都有突破性进展。技术的进步推动了产业升级，同时也带来了伦理和隐私等挑战。",
                            "max_length": 60
                        }
                    }
                },
                {
                    "user_intent": "需要对一条微博内容生成中文摘要，方便快速浏览。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "今天在上海举办的科技大会上，多家AI公司展示了最新的智能产品，包括自动驾驶、语音助手和智能家居设备，引发了广泛关注。",
                            "max_length": 40
                        }
                    }
                },
                {
                    "user_intent": "我有一篇教育领域的长篇文章，希望生成概括性摘要用于报告。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "近年来，中国教育领域进行了多项改革，尤其在中小学课程和教学方法上有所调整。在线教育和智能教育工具的应用提高了学习效率，同时也引发了教育公平性的讨论。",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 1: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 2: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 3: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 4: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.\nTool 5: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 6: This tool provides zero-shot image classification using the Chinese CLIP model, which is trained on a large dataset of approximately 200 million Chinese image-text pairs. It leverages a Vision Transformer (ViT-B/16) as the image encoder and a RoBERTa-based text encoder to match images with textual descriptions without requiring task-specific fine-tuning. It is designed for applications like image retrieval and classification in Chinese contexts, offering strong performance across benchmarks such as CIFAR, COCO-CN, Flickr30K-CN, and MUGE.\nTool 7: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 8: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.\nTool 9: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\nTool 10: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.\nTool 11: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.\nTool 12: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.\nTool 13: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 14: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 15: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 16: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 17: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 18: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input Chinese text that you want the model to summarize. This should be a complete text string.\", \"required\": true, \"example\": \"\\u4eca\\u5929\\u5929\\u6c14\\u5f88\\u597d\\uff0c\\u6211\\u4eec\\u53bb\\u516c\\u56ed\\u6563\\u6b65\\u5427\\u3002\"}, \"max_length\": {\"type\": \"integer\", \"description\": \"The maximum number of tokens to generate in the summary. This limits the length of the output summary.\", \"required\": true, \"example\": 150}}, \"response\": \"The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\\n\\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\\n\\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\\\"summary_text\\\"`) pointing to the summarized string. The content of the summary reflects the model\\u2019s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \\n\\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.\", \"examples\": [{\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u5173\\u4e8e\\u4eba\\u5de5\\u667a\\u80fd\\u53d1\\u5c55\\u7684\\u4e2d\\u6587\\u6587\\u7ae0\\uff0c\\u60f3\\u751f\\u6210\\u4e00\\u6bb5\\u7b80\\u77ed\\u6458\\u8981\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eba\\u5de5\\u667a\\u80fd\\u8fd1\\u5e74\\u6765\\u53d6\\u5f97\\u4e86\\u663e\\u8457\\u7684\\u53d1\\u5c55\\uff0c\\u4ece\\u81ea\\u7136\\u8bed\\u8a00\\u5904\\u7406\\u5230\\u8ba1\\u7b97\\u673a\\u89c6\\u89c9\\uff0c\\u5404\\u4e2a\\u9886\\u57df\\u90fd\\u6709\\u7a81\\u7834\\u6027\\u8fdb\\u5c55\\u3002\\u6280\\u672f\\u7684\\u8fdb\\u6b65\\u63a8\\u52a8\\u4e86\\u4ea7\\u4e1a\\u5347\\u7ea7\\uff0c\\u540c\\u65f6\\u4e5f\\u5e26\\u6765\\u4e86\\u4f26\\u7406\\u548c\\u9690\\u79c1\\u7b49\\u6311\\u6218\\u3002\", \"max_length\": 60}}}, {\"user_intent\": \"\\u9700\\u8981\\u5bf9\\u4e00\\u6761\\u5fae\\u535a\\u5185\\u5bb9\\u751f\\u6210\\u4e2d\\u6587\\u6458\\u8981\\uff0c\\u65b9\\u4fbf\\u5feb\\u901f\\u6d4f\\u89c8\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eca\\u5929\\u5728\\u4e0a\\u6d77\\u4e3e\\u529e\\u7684\\u79d1\\u6280\\u5927\\u4f1a\\u4e0a\\uff0c\\u591a\\u5bb6AI\\u516c\\u53f8\\u5c55\\u793a\\u4e86\\u6700\\u65b0\\u7684\\u667a\\u80fd\\u4ea7\\u54c1\\uff0c\\u5305\\u62ec\\u81ea\\u52a8\\u9a7e\\u9a76\\u3001\\u8bed\\u97f3\\u52a9\\u624b\\u548c\\u667a\\u80fd\\u5bb6\\u5c45\\u8bbe\\u5907\\uff0c\\u5f15\\u53d1\\u4e86\\u5e7f\\u6cdb\\u5173\\u6ce8\\u3002\", \"max_length\": 40}}}, {\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u6559\\u80b2\\u9886\\u57df\\u7684\\u957f\\u7bc7\\u6587\\u7ae0\\uff0c\\u5e0c\\u671b\\u751f\\u6210\\u6982\\u62ec\\u6027\\u6458\\u8981\\u7528\\u4e8e\\u62a5\\u544a\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u8fd1\\u5e74\\u6765\\uff0c\\u4e2d\\u56fd\\u6559\\u80b2\\u9886\\u57df\\u8fdb\\u884c\\u4e86\\u591a\\u9879\\u6539\\u9769\\uff0c\\u5c24\\u5176\\u5728\\u4e2d\\u5c0f\\u5b66\\u8bfe\\u7a0b\\u548c\\u6559\\u5b66\\u65b9\\u6cd5\\u4e0a\\u6709\\u6240\\u8c03\\u6574\\u3002\\u5728\\u7ebf\\u6559\\u80b2\\u548c\\u667a\\u80fd\\u6559\\u80b2\\u5de5\\u5177\\u7684\\u5e94\\u7528\\u63d0\\u9ad8\\u4e86\\u5b66\\u4e60\\u6548\\u7387\\uff0c\\u540c\\u65f6\\u4e5f\\u5f15\\u53d1\\u4e86\\u6559\\u80b2\\u516c\\u5e73\\u6027\\u7684\\u8ba8\\u8bba\\u3002\", \"max_length\": 80}}}]}\n\n# User Query\nOur company is exploring the Chinese market and we need to communicate effectively with our clients. Help me create a generic Chinese response.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a Chinese text response for business communication.",
                    "tool_description": "Generate a Chinese text response tailored for business communication using a text generation model.",
                    "expected_response": "A coherent and contextually appropriate Chinese response suitable for business communication."
                }
            ],
            "extracted_arguments": {
                "text_type": "The desired purpose or context for the generated response, such as business communication.",
                "language": "The target language for the response, in this case, Chinese."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_221",
        "query": "I need a solution to detect whether a piece of news is talking about technology, sports, or politics.",
        "instruction": "Given a `news topic classification` task, retrieve tools capable of analyzing text inputs to determine the relevant category, such as technology, sports, or politics, using natural language processing techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "CrossEncoder('cross-encoder/nli-roberta-base')",
                    "api_arguments": [
                        "sentence1",
                        "sentence2"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\nsent = Apple just announced the newest iPhone X\ncandidate_labels = [technology, sports, politics]\nres = classifier(sent, candidate_labels)\nprint(res)",
                    "performance": {
                        "dataset": [
                            "SNLI",
                            "MultiNLI"
                        ],
                        "accuracy": "See SBERT.net - Pretrained Cross-Encoder"
                    },
                    "description": "Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.",
                    "id": "huggingface_api_503",
                    "name": "cross-encoder/nli-roberta-base"
                },
                "id": "gorilla_huggingface_tool_499",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 1: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 2: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 3: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 4: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 6: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 7: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 8: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 9: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 11: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 12: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 13: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 14: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 15: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 16: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 17: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 18: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 19: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis."
        ],
        "tool_view": {
            "description": "The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "parameters": {
                "pretrained_model_name": {
                    "type": "string",
                    "description": "The name or path of the pre-trained model to load for translation. This is the identifier for the specific translation model from the Hugging Face Model Hub.",
                    "required": true,
                    "example": "Helsinki-NLP/opus-mt-zh-en"
                }
            },
            "response": "The tool provides a machine translation model for converting Chinese text to English using the **Helsinki-NLP/opus-mt-zh-en** model. The response from the tool would typically contain the following components:\n\n1. **Translation Output**:  \n   This would include the translated text from Chinese to English based on the input provided. It is the primary result of the tool’s functionality.\n\n2. **Model Information**:  \n   This would describe the specific model used for the translation, in this case, the \"Helsinki-NLP/opus-mt-zh-en\" model, which is based on the Marian NMT framework. This section would confirm the model version and architecture being applied.\n\n3. **Performance Metrics (Optional)**:  \n   If relevant performance metrics are included in the response, this would show the evaluation results for the model on standard translation tasks. These metrics might include:\n   - **BLEU Score**: A metric for translation quality based on n-gram precision, in this case, 36.1.\n   - **chr-F**: A character-level metric for translation quality, in this case, 0.548.\n   \n4. **API Call Confirmation**:  \n   The response might also confirm that the model was loaded correctly from the `Helsinki-NLP/opus-mt-zh-en` model available in the Hugging Face library, as indicated by the API call structure (`AutoModelForSeq2SeqLM.from_pretrained`).\n\n5. **Text Tokenization Information (Optional)**:  \n   Depending on the implementation, there might be a reference to the tokenization process, noting that the input text will be tokenized using the appropriate tokenizer, which aligns with the model's expected input format.\n\nThe response would typically focus on providing the translated text, with optional additional details about the model's performance and configuration.",
            "examples": [
                {
                    "user_intent": "Translate this Chinese sentence into English: '今天天气很好，我想去散步。'",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-zh-en",
                        "arguments": {
                            "pretrained_model_name": "Helsinki-NLP/opus-mt-zh-en",
                            "text": "今天天气很好，我想去散步。"
                        }
                    }
                },
                {
                    "user_intent": "I have a paragraph in Chinese and need an English translation for my research paper.",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-zh-en",
                        "arguments": {
                            "pretrained_model_name": "Helsinki-NLP/opus-mt-zh-en",
                            "text": "人工智能正在迅速发展，它在医疗、金融和教育等多个领域都有广泛应用。"
                        }
                    }
                },
                {
                    "user_intent": "Convert this list of Chinese phrases to English for a multilingual app.",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-zh-en",
                        "arguments": {
                            "pretrained_model_name": "Helsinki-NLP/opus-mt-zh-en",
                            "text": "['你好', '谢谢', '再见']"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 1: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 2: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 3: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 4: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 6: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 7: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 8: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 9: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 11: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 12: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 13: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 14: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 15: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 16: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 17: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 18: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 19: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\", \"parameters\": {\"pretrained_model_name\": {\"type\": \"string\", \"description\": \"The name or path of the pre-trained model to load for translation. This is the identifier for the specific translation model from the Hugging Face Model Hub.\", \"required\": true, \"example\": \"Helsinki-NLP/opus-mt-zh-en\"}}, \"response\": \"The tool provides a machine translation model for converting Chinese text to English using the **Helsinki-NLP/opus-mt-zh-en** model. The response from the tool would typically contain the following components:\\n\\n1. **Translation Output**:  \\n   This would include the translated text from Chinese to English based on the input provided. It is the primary result of the tool\\u2019s functionality.\\n\\n2. **Model Information**:  \\n   This would describe the specific model used for the translation, in this case, the \\\"Helsinki-NLP/opus-mt-zh-en\\\" model, which is based on the Marian NMT framework. This section would confirm the model version and architecture being applied.\\n\\n3. **Performance Metrics (Optional)**:  \\n   If relevant performance metrics are included in the response, this would show the evaluation results for the model on standard translation tasks. These metrics might include:\\n   - **BLEU Score**: A metric for translation quality based on n-gram precision, in this case, 36.1.\\n   - **chr-F**: A character-level metric for translation quality, in this case, 0.548.\\n   \\n4. **API Call Confirmation**:  \\n   The response might also confirm that the model was loaded correctly from the `Helsinki-NLP/opus-mt-zh-en` model available in the Hugging Face library, as indicated by the API call structure (`AutoModelForSeq2SeqLM.from_pretrained`).\\n\\n5. **Text Tokenization Information (Optional)**:  \\n   Depending on the implementation, there might be a reference to the tokenization process, noting that the input text will be tokenized using the appropriate tokenizer, which aligns with the model's expected input format.\\n\\nThe response would typically focus on providing the translated text, with optional additional details about the model's performance and configuration.\", \"examples\": [{\"user_intent\": \"Translate this Chinese sentence into English: '\\u4eca\\u5929\\u5929\\u6c14\\u5f88\\u597d\\uff0c\\u6211\\u60f3\\u53bb\\u6563\\u6b65\\u3002'\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-zh-en\", \"arguments\": {\"pretrained_model_name\": \"Helsinki-NLP/opus-mt-zh-en\", \"text\": \"\\u4eca\\u5929\\u5929\\u6c14\\u5f88\\u597d\\uff0c\\u6211\\u60f3\\u53bb\\u6563\\u6b65\\u3002\"}}}, {\"user_intent\": \"I have a paragraph in Chinese and need an English translation for my research paper.\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-zh-en\", \"arguments\": {\"pretrained_model_name\": \"Helsinki-NLP/opus-mt-zh-en\", \"text\": \"\\u4eba\\u5de5\\u667a\\u80fd\\u6b63\\u5728\\u8fc5\\u901f\\u53d1\\u5c55\\uff0c\\u5b83\\u5728\\u533b\\u7597\\u3001\\u91d1\\u878d\\u548c\\u6559\\u80b2\\u7b49\\u591a\\u4e2a\\u9886\\u57df\\u90fd\\u6709\\u5e7f\\u6cdb\\u5e94\\u7528\\u3002\"}}}, {\"user_intent\": \"Convert this list of Chinese phrases to English for a multilingual app.\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-zh-en\", \"arguments\": {\"pretrained_model_name\": \"Helsinki-NLP/opus-mt-zh-en\", \"text\": \"['\\u4f60\\u597d', '\\u8c22\\u8c22', '\\u518d\\u89c1']\"}}}]}\n\n# User Query\nI need a solution to detect whether a piece of news is talking about technology, sports, or politics.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify a news article into categories such as technology, sports, or politics.",
                    "tool_description": "Perform zero-shot text classification to categorize text into predefined labels without needing labeled training data, leveraging a fine-tuned model capable of handling Spanish or general language inputs.",
                    "expected_response": "A classification output indicating the most likely category for the input text, along with confidence scores for each possible label."
                }
            ],
            "extracted_arguments": {
                "text": "The full content of the news article or text passage to classify.",
                "labels": "A list of target categories for classification, e.g., technology, sports, politics."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_69",
        "query": "We need a quick summary of a news article we found online. Can you help us with that?",
        "instruction": "Given a `text summarization` task, retrieve tools that process lengthy text inputs to generate concise and coherent summaries, effectively capturing the main points and essential information of the original content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.11.0.dev0"
                    ],
                    "example_code": "import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nWHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ninput_ids = tokenizer(\n [WHITESPACE_HANDLER(article_text)],\n return_tensors=pt,\n padding=max_length,\n truncation=True,\n max_length=512\n)[input_ids]\noutput_ids = model.generate(\n input_ids=input_ids,\n max_length=84,\n no_repeat_ngram_size=2,\n num_beams=4\n)[0]\nsummary = tokenizer.decode(\n output_ids,\n skip_special_tokens=True,\n clean_up_tokenization_spaces=False\n)\nprint(summary)",
                    "performance": {
                        "dataset": "xsum",
                        "accuracy": {
                            "ROUGE-1": 36.5,
                            "ROUGE-2": 13.934,
                            "ROUGE-L": 28.988,
                            "ROUGE-LSUM": 28.996,
                            "loss": 2.067,
                            "gen_len": 26.973
                        }
                    },
                    "description": "This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.",
                    "id": "huggingface_api_562",
                    "name": "csebuetnlp/mT5_multilingual_XLSum"
                },
                "id": "gorilla_huggingface_tool_558",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 1: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 2: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 3: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 4: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 5: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 6: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 7: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 8: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 9: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.",
            "Tool 10: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 11: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 12: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 13: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.",
            "Tool 14: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 15: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 16: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 17: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 18: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 19: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement."
        ],
        "tool_view": {
            "description": "Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input Chinese text that you want the model to summarize. This should be a complete text string.",
                    "required": true,
                    "example": "今天天气很好，我们去公园散步吧。"
                },
                "max_length": {
                    "type": "integer",
                    "description": "The maximum number of tokens to generate in the summary. This limits the length of the output summary.",
                    "required": true,
                    "example": 150
                }
            },
            "response": "The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\n\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\n\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\"summary_text\"`) pointing to the summarized string. The content of the summary reflects the model’s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \n\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.",
            "examples": [
                {
                    "user_intent": "我有一篇关于人工智能发展的中文文章，想生成一段简短摘要。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "人工智能近年来取得了显著的发展，从自然语言处理到计算机视觉，各个领域都有突破性进展。技术的进步推动了产业升级，同时也带来了伦理和隐私等挑战。",
                            "max_length": 60
                        }
                    }
                },
                {
                    "user_intent": "需要对一条微博内容生成中文摘要，方便快速浏览。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "今天在上海举办的科技大会上，多家AI公司展示了最新的智能产品，包括自动驾驶、语音助手和智能家居设备，引发了广泛关注。",
                            "max_length": 40
                        }
                    }
                },
                {
                    "user_intent": "我有一篇教育领域的长篇文章，希望生成概括性摘要用于报告。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "近年来，中国教育领域进行了多项改革，尤其在中小学课程和教学方法上有所调整。在线教育和智能教育工具的应用提高了学习效率，同时也引发了教育公平性的讨论。",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 1: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 2: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 3: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 4: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 5: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 6: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 7: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 8: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 9: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.\nTool 10: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 11: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 12: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 13: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.\nTool 14: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 15: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 16: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 17: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 18: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 19: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input Chinese text that you want the model to summarize. This should be a complete text string.\", \"required\": true, \"example\": \"\\u4eca\\u5929\\u5929\\u6c14\\u5f88\\u597d\\uff0c\\u6211\\u4eec\\u53bb\\u516c\\u56ed\\u6563\\u6b65\\u5427\\u3002\"}, \"max_length\": {\"type\": \"integer\", \"description\": \"The maximum number of tokens to generate in the summary. This limits the length of the output summary.\", \"required\": true, \"example\": 150}}, \"response\": \"The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\\n\\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\\n\\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\\\"summary_text\\\"`) pointing to the summarized string. The content of the summary reflects the model\\u2019s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \\n\\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.\", \"examples\": [{\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u5173\\u4e8e\\u4eba\\u5de5\\u667a\\u80fd\\u53d1\\u5c55\\u7684\\u4e2d\\u6587\\u6587\\u7ae0\\uff0c\\u60f3\\u751f\\u6210\\u4e00\\u6bb5\\u7b80\\u77ed\\u6458\\u8981\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eba\\u5de5\\u667a\\u80fd\\u8fd1\\u5e74\\u6765\\u53d6\\u5f97\\u4e86\\u663e\\u8457\\u7684\\u53d1\\u5c55\\uff0c\\u4ece\\u81ea\\u7136\\u8bed\\u8a00\\u5904\\u7406\\u5230\\u8ba1\\u7b97\\u673a\\u89c6\\u89c9\\uff0c\\u5404\\u4e2a\\u9886\\u57df\\u90fd\\u6709\\u7a81\\u7834\\u6027\\u8fdb\\u5c55\\u3002\\u6280\\u672f\\u7684\\u8fdb\\u6b65\\u63a8\\u52a8\\u4e86\\u4ea7\\u4e1a\\u5347\\u7ea7\\uff0c\\u540c\\u65f6\\u4e5f\\u5e26\\u6765\\u4e86\\u4f26\\u7406\\u548c\\u9690\\u79c1\\u7b49\\u6311\\u6218\\u3002\", \"max_length\": 60}}}, {\"user_intent\": \"\\u9700\\u8981\\u5bf9\\u4e00\\u6761\\u5fae\\u535a\\u5185\\u5bb9\\u751f\\u6210\\u4e2d\\u6587\\u6458\\u8981\\uff0c\\u65b9\\u4fbf\\u5feb\\u901f\\u6d4f\\u89c8\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eca\\u5929\\u5728\\u4e0a\\u6d77\\u4e3e\\u529e\\u7684\\u79d1\\u6280\\u5927\\u4f1a\\u4e0a\\uff0c\\u591a\\u5bb6AI\\u516c\\u53f8\\u5c55\\u793a\\u4e86\\u6700\\u65b0\\u7684\\u667a\\u80fd\\u4ea7\\u54c1\\uff0c\\u5305\\u62ec\\u81ea\\u52a8\\u9a7e\\u9a76\\u3001\\u8bed\\u97f3\\u52a9\\u624b\\u548c\\u667a\\u80fd\\u5bb6\\u5c45\\u8bbe\\u5907\\uff0c\\u5f15\\u53d1\\u4e86\\u5e7f\\u6cdb\\u5173\\u6ce8\\u3002\", \"max_length\": 40}}}, {\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u6559\\u80b2\\u9886\\u57df\\u7684\\u957f\\u7bc7\\u6587\\u7ae0\\uff0c\\u5e0c\\u671b\\u751f\\u6210\\u6982\\u62ec\\u6027\\u6458\\u8981\\u7528\\u4e8e\\u62a5\\u544a\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u8fd1\\u5e74\\u6765\\uff0c\\u4e2d\\u56fd\\u6559\\u80b2\\u9886\\u57df\\u8fdb\\u884c\\u4e86\\u591a\\u9879\\u6539\\u9769\\uff0c\\u5c24\\u5176\\u5728\\u4e2d\\u5c0f\\u5b66\\u8bfe\\u7a0b\\u548c\\u6559\\u5b66\\u65b9\\u6cd5\\u4e0a\\u6709\\u6240\\u8c03\\u6574\\u3002\\u5728\\u7ebf\\u6559\\u80b2\\u548c\\u667a\\u80fd\\u6559\\u80b2\\u5de5\\u5177\\u7684\\u5e94\\u7528\\u63d0\\u9ad8\\u4e86\\u5b66\\u4e60\\u6548\\u7387\\uff0c\\u540c\\u65f6\\u4e5f\\u5f15\\u53d1\\u4e86\\u6559\\u80b2\\u516c\\u5e73\\u6027\\u7684\\u8ba8\\u8bba\\u3002\", \"max_length\": 80}}}]}\n\n# User Query\nWe need a quick summary of a news article we found online. Can you help us with that?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a concise summary of a news article I found online.",
                    "tool_description": "Generate an abstractive summary from the input news text using a pre-trained Pegasus-based summarization model, condensing the content while preserving the main points.",
                    "expected_response": "A single string containing the summarized version of the news article, coherent and contextually faithful to the original text."
                }
            ],
            "extracted_arguments": {
                "text": "The full text of the news article to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_39",
        "query": "Create a poem about nature for a school assignment.",
        "instruction": "Given a `poem creation` task, retrieve tools that specialize in generating creative text, specifically aimed at crafting poems, by processing input prompts related to the query's topic of nature to produce coherent and expressive outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')",
                    "api_arguments": {
                        "model": "sshleifer/tiny-gpt2"
                    },
                    "python_environment_requirements": {
                        "huggingface_transformers": ">=4.0.0"
                    },
                    "example_code": "from transformers import pipeline\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\nresult = nlp('Once upon a time')",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.",
                    "id": "huggingface_api_622",
                    "name": "sshleifer/tiny-gpt2"
                },
                "id": "gorilla_huggingface_tool_617",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 1: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.",
            "Tool 2: This tool provides a large DeBERTa model fine-tuned on the MNLI task for zero-shot natural language classification, enabling the assignment of candidate labels to input text without task-specific training. It leverages disentangled attention and an enhanced mask decoder to improve upon BERT and RoBERTa, delivering strong performance across various natural language understanding benchmarks.",
            "Tool 3: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 4: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 5: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 6: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 7: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 8: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 9: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 11: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 12: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 13: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 14: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 15: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 16: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 17: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 18: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 19: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries."
        ],
        "tool_view": {
            "description": "A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "parameters": {
                "words": {
                    "type": "string",
                    "description": "A string of words or concepts that will be used as input for generating a commonsense sentence. These words serve as the core elements around which the sentence is generated.",
                    "required": true,
                    "example": "tree plant ground hole dig"
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum length (in terms of tokens) of the generated sentence. This parameter determines how long the output sentence can be.",
                    "required": false,
                    "default": 32,
                    "example": 50
                }
            },
            "response": "The tool generates a single coherent natural language sentence that incorporates a given set of input concepts or words. The response is a plain text string representing the model’s output sentence.  \n\nKey components of the response:\n\n- **Generated Sentence (string):**  \n  This is the main content of the response. It is a grammatically coherent sentence that attempts to use all or most of the provided input words in a commonsense, contextually plausible manner. The sentence reflects everyday scenarios and relationships between the concepts.  \n\n- **Constraints and Characteristics:**  \n  - The sentence is typically limited in length by the `max_length` parameter specified in the model call.  \n  - It is designed to satisfy commonsense reasoning, meaning the arrangement of words should make logical sense and reflect realistic scenarios.  \n  - Special tokens used internally by the model (e.g., padding or end-of-sequence tokens) are removed from the final text.  \n\nThe response does **not** include structured metadata by default; it is a single text string representing the generative output.",
            "examples": [
                {
                    "user_intent": "Generate a sentence that includes the words 'dog', 'park', and 'ball'.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-common_gen",
                        "arguments": {
                            "words": "dog park ball",
                            "max_length": 32
                        }
                    }
                },
                {
                    "user_intent": "Create a sentence using the concepts 'coffee', 'morning', and 'cup'.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-common_gen",
                        "arguments": {
                            "words": "coffee morning cup",
                            "max_length": 32
                        }
                    }
                },
                {
                    "user_intent": "Generate a sentence with the words 'school', 'teacher', and 'homework'.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-common_gen",
                        "arguments": {
                            "words": "school teacher homework",
                            "max_length": 32
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 1: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.\nTool 2: This tool provides a large DeBERTa model fine-tuned on the MNLI task for zero-shot natural language classification, enabling the assignment of candidate labels to input text without task-specific training. It leverages disentangled attention and an enhanced mask decoder to improve upon BERT and RoBERTa, delivering strong performance across various natural language understanding benchmarks.\nTool 3: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 4: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 5: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 6: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 7: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 8: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 9: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 11: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 12: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 13: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 14: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 15: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 16: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 17: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 18: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 19: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine\\u2019s ability to reason about common situations using specified words.\", \"parameters\": {\"words\": {\"type\": \"string\", \"description\": \"A string of words or concepts that will be used as input for generating a commonsense sentence. These words serve as the core elements around which the sentence is generated.\", \"required\": true, \"example\": \"tree plant ground hole dig\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum length (in terms of tokens) of the generated sentence. This parameter determines how long the output sentence can be.\", \"required\": false, \"default\": 32, \"example\": 50}}, \"response\": \"The tool generates a single coherent natural language sentence that incorporates a given set of input concepts or words. The response is a plain text string representing the model\\u2019s output sentence.  \\n\\nKey components of the response:\\n\\n- **Generated Sentence (string):**  \\n  This is the main content of the response. It is a grammatically coherent sentence that attempts to use all or most of the provided input words in a commonsense, contextually plausible manner. The sentence reflects everyday scenarios and relationships between the concepts.  \\n\\n- **Constraints and Characteristics:**  \\n  - The sentence is typically limited in length by the `max_length` parameter specified in the model call.  \\n  - It is designed to satisfy commonsense reasoning, meaning the arrangement of words should make logical sense and reflect realistic scenarios.  \\n  - Special tokens used internally by the model (e.g., padding or end-of-sequence tokens) are removed from the final text.  \\n\\nThe response does **not** include structured metadata by default; it is a single text string representing the generative output.\", \"examples\": [{\"user_intent\": \"Generate a sentence that includes the words 'dog', 'park', and 'ball'.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-common_gen\", \"arguments\": {\"words\": \"dog park ball\", \"max_length\": 32}}}, {\"user_intent\": \"Create a sentence using the concepts 'coffee', 'morning', and 'cup'.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-common_gen\", \"arguments\": {\"words\": \"coffee morning cup\", \"max_length\": 32}}}, {\"user_intent\": \"Generate a sentence with the words 'school', 'teacher', and 'homework'.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-common_gen\", \"arguments\": {\"words\": \"school teacher homework\", \"max_length\": 32}}}]}\n\n# User Query\nCreate a poem about nature for a school assignment.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a creative text about nature suitable for a school assignment.",
                    "tool_description": "Use a generative commonsense reasoning model to create coherent sentences that incorporate given concepts related to nature, forming a poetic structure.",
                    "expected_response": "A text string containing sentences about nature that are contextually plausible, coherent, and stylistically appropriate for a school assignment."
                }
            ],
            "extracted_arguments": {
                "concepts": "A set of words or ideas related to nature that will guide the generative text.",
                "max_length": "Optional numeric value defining the maximum length of the generated poem in tokens."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_491",
        "query": "Tell me a text summary and answer a question from an image.",
        "instruction": "Given a `multimodal image-to-text analysis` task, retrieve tools that combine image processing with text generation to produce a text summary and answer questions based on the visual content, accommodating the diverse requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                    "api_arguments": {
                        "img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg",
                        "question": "how many dogs are in the picture?"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": {
                        "import_requests": "import requests",
                        "import_PIL": "from PIL import Image",
                        "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration",
                        "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')",
                        "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')",
                        "generate_output": "out = model.generate(**inputs)",
                        "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"
                    },
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_64",
                    "name": "blip2-opt-2.7b"
                },
                "id": "gorilla_huggingface_tool_64",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 1: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 2: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 3: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 4: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 5: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 6: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.",
            "Tool 7: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 8: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 9: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 10: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "Tool 11: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 12: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 13: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 14: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 15: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.",
            "Tool 16: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.",
            "Tool 17: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 18: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 19: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions."
        ],
        "tool_view": {
            "description": "A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path or URL of the document image on which the question will be asked. This image will be processed by the vision encoder to extract features for answering the question.",
                    "required": true,
                    "example": "path/to/document.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The question you want to ask about the content of the provided document image. The model will generate an answer based on the image content.",
                    "required": true,
                    "example": "What is the total amount on the invoice?"
                }
            },
            "response": "The response from this tool would provide an answer to the question posed, based on the content of the provided image. \n\nThe response structure would typically contain the following components:\n\n1. **Answer**: The textual answer to the question based on the content extracted from the image.\n   - **Type**: String\n   - **Description**: This is the direct answer to the user's question, which the model generates based on analyzing the image.\n\n2. **Confidence (Optional)**: A measure of how confident the model is in its answer, typically a numerical value or a qualitative assessment.\n   - **Type**: Float or String (e.g., 'high', 'medium', 'low')\n   - **Description**: Indicates the reliability of the generated answer. It may be provided by some implementations of the pipeline, but is not always guaranteed.\n\n3. **Context or Additional Information (Optional)**: Some models may also include relevant context or sources used to generate the answer, such as the text extracted from the image.\n   - **Type**: String or List of Strings\n   - **Description**: Provides additional details or references from the document that informed the answer, though this is not always part of the response.\n\nThis structure provides the user with a concise, direct answer, potentially supplemented by information on how confident the model is in its answer and context that might explain how the answer was derived from the image.",
            "examples": [
                {
                    "user_intent": "I have a scanned invoice and I want to know the total amount due.",
                    "tool_call": {
                        "name": "naver-clova-ix/donut-base-finetuned-docvqa",
                        "arguments": {
                            "image": "/home/user/documents/invoice_july.png",
                            "question": "What is the total amount due?"
                        }
                    }
                },
                {
                    "user_intent": "Check this receipt and tell me the date of purchase.",
                    "tool_call": {
                        "name": "naver-clova-ix/donut-base-finetuned-docvqa",
                        "arguments": {
                            "image": "/home/user/receipts/receipt_2025-12-01.jpg",
                            "question": "What is the date of purchase?"
                        }
                    }
                },
                {
                    "user_intent": "I scanned a contract and need to extract the name of the signing party.",
                    "tool_call": {
                        "name": "naver-clova-ix/donut-base-finetuned-docvqa",
                        "arguments": {
                            "image": "/home/user/contracts/contract_alpha.png",
                            "question": "Who is the signing party on this contract?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 1: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 2: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 3: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 4: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 5: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 6: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.\nTool 7: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 8: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 9: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 10: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\nTool 11: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 12: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 13: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 14: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 15: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.\nTool 16: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.\nTool 17: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 18: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 19: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path or URL of the document image on which the question will be asked. This image will be processed by the vision encoder to extract features for answering the question.\", \"required\": true, \"example\": \"path/to/document.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The question you want to ask about the content of the provided document image. The model will generate an answer based on the image content.\", \"required\": true, \"example\": \"What is the total amount on the invoice?\"}}, \"response\": \"The response from this tool would provide an answer to the question posed, based on the content of the provided image. \\n\\nThe response structure would typically contain the following components:\\n\\n1. **Answer**: The textual answer to the question based on the content extracted from the image.\\n   - **Type**: String\\n   - **Description**: This is the direct answer to the user's question, which the model generates based on analyzing the image.\\n\\n2. **Confidence (Optional)**: A measure of how confident the model is in its answer, typically a numerical value or a qualitative assessment.\\n   - **Type**: Float or String (e.g., 'high', 'medium', 'low')\\n   - **Description**: Indicates the reliability of the generated answer. It may be provided by some implementations of the pipeline, but is not always guaranteed.\\n\\n3. **Context or Additional Information (Optional)**: Some models may also include relevant context or sources used to generate the answer, such as the text extracted from the image.\\n   - **Type**: String or List of Strings\\n   - **Description**: Provides additional details or references from the document that informed the answer, though this is not always part of the response.\\n\\nThis structure provides the user with a concise, direct answer, potentially supplemented by information on how confident the model is in its answer and context that might explain how the answer was derived from the image.\", \"examples\": [{\"user_intent\": \"I have a scanned invoice and I want to know the total amount due.\", \"tool_call\": {\"name\": \"naver-clova-ix/donut-base-finetuned-docvqa\", \"arguments\": {\"image\": \"/home/user/documents/invoice_july.png\", \"question\": \"What is the total amount due?\"}}}, {\"user_intent\": \"Check this receipt and tell me the date of purchase.\", \"tool_call\": {\"name\": \"naver-clova-ix/donut-base-finetuned-docvqa\", \"arguments\": {\"image\": \"/home/user/receipts/receipt_2025-12-01.jpg\", \"question\": \"What is the date of purchase?\"}}}, {\"user_intent\": \"I scanned a contract and need to extract the name of the signing party.\", \"tool_call\": {\"name\": \"naver-clova-ix/donut-base-finetuned-docvqa\", \"arguments\": {\"image\": \"/home/user/contracts/contract_alpha.png\", \"question\": \"Who is the signing party on this contract?\"}}}]}\n\n# User Query\nTell me a text summary and answer a question from an image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a concise text summary from the content of a document or image.",
                    "tool_description": "Process a text or image input to produce a brief summary, capturing the key points and main ideas from the content.",
                    "expected_response": "A string containing the summarized version of the input content."
                },
                {
                    "user_intent": "Answer a specific question based on the content of a document image.",
                    "tool_description": "Use a document question-answering model to extract an answer directly from a document image by interpreting its visual content.",
                    "expected_response": "A textual answer to the question, potentially with confidence and additional context extracted from the image."
                }
            ],
            "extracted_arguments": {
                "image": "The file path or URL of the document or image to be analyzed for question answering and/or summarization.",
                "question": "The natural language question to be answered based on the content of the provided image or document."
            }
        }
    }
]