[
    {
        "id": "gorilla_huggingface_query_366",
        "query": "The company develops a digital assistant that can answer questions about software products. Implement a feature to provide answers to user questions.",
        "instruction": "Given a `question answering` task, retrieve tools that are capable of processing text inputs to provide relevant and accurate answers to user questions regarding software products, leveraging models fine-tuned on datasets like SQuAD2.0 for optimized answering capability.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')",
                    "api_arguments": {
                        "model_name_or_path": "deepset/deberta-v3-large-squad2",
                        "tokenizer": "deepset/deberta-v3-large-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "a": {
                            "code": "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)"
                        },
                        "b": {
                            "code": "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
                        }
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 87.6105449338836,
                            "f1": 90.75307008866517
                        }
                    },
                    "description": "This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.",
                    "id": "huggingface_api_488",
                    "name": "deepset/deberta-v3-large-squad2"
                },
                "id": "gorilla_huggingface_tool_484",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 1: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 2: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 4: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 5: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 6: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 7: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 8: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 9: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 10: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 11: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 12: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 13: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 14: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 15: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 16: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 17: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 18: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 19: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data."
        ],
        "tool_view": {
            "description": "A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path or URL of the image on which the visual question will be asked.",
                    "required": true,
                    "example": "images/dog.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The question to be answered about the given image.",
                    "required": true,
                    "example": "What breed is the dog in the image?"
                }
            },
            "response": "The tool produces a response that provides an answer to a user-specified question about a given image. The response is typically structured as a dictionary or object containing the following components:\n\n1. **`answer`** – A string representing the model’s textual answer to the question posed about the image. This is the primary output of the tool.  \n2. **`score`** (optional) – A numeric confidence score indicating the model’s certainty in the provided answer. This may be a probability or other relevance measure.  \n3. **`question`** (optional) – The text of the input question, often echoed in the response for clarity.  \n4. **`image`** (optional) – A reference or identifier for the input image, included for traceability when multiple queries are processed.  \n\nIn essence, the response conveys the model’s interpretation of the visual content in relation to the user’s question, potentially accompanied by metadata that quantifies confidence or links back to the original inputs.",
            "examples": [
                {
                    "user_intent": "I have a photo of my living room. Can you tell me how many chairs are visible in it?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/living_room.jpg",
                            "question": "How many chairs are in this room?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this image of a street scene. What color is the car in the foreground?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/street_scene.png",
                            "question": "What is the color of the car in the foreground?"
                        }
                    }
                },
                {
                    "user_intent": "Here's a picture of a group of people at a party. Can you tell me how many people are wearing hats?",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "arguments": {
                            "image_path": "images/party_group.jpeg",
                            "question": "How many people are wearing hats in this image?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 1: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 2: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 4: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 5: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 6: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 7: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 8: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 9: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 10: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 11: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 12: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 13: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 14: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 15: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 16: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 17: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 18: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 19: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab\\u2019s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path or URL of the image on which the visual question will be asked.\", \"required\": true, \"example\": \"images/dog.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The question to be answered about the given image.\", \"required\": true, \"example\": \"What breed is the dog in the image?\"}}, \"response\": \"The tool produces a response that provides an answer to a user-specified question about a given image. The response is typically structured as a dictionary or object containing the following components:\\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s textual answer to the question posed about the image. This is the primary output of the tool.  \\n2. **`score`** (optional) \\u2013 A numeric confidence score indicating the model\\u2019s certainty in the provided answer. This may be a probability or other relevance measure.  \\n3. **`question`** (optional) \\u2013 The text of the input question, often echoed in the response for clarity.  \\n4. **`image`** (optional) \\u2013 A reference or identifier for the input image, included for traceability when multiple queries are processed.  \\n\\nIn essence, the response conveys the model\\u2019s interpretation of the visual content in relation to the user\\u2019s question, potentially accompanied by metadata that quantifies confidence or links back to the original inputs.\", \"examples\": [{\"user_intent\": \"I have a photo of my living room. Can you tell me how many chairs are visible in it?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/living_room.jpg\", \"question\": \"How many chairs are in this room?\"}}}, {\"user_intent\": \"Look at this image of a street scene. What color is the car in the foreground?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/street_scene.png\", \"question\": \"What is the color of the car in the foreground?\"}}}, {\"user_intent\": \"Here's a picture of a group of people at a party. Can you tell me how many people are wearing hats?\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"arguments\": {\"image_path\": \"images/party_group.jpeg\", \"question\": \"How many people are wearing hats in this image?\"}}}]}\n\n# User Query\nThe company develops a digital assistant that can answer questions about software products. Implement a feature to provide answers to user questions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Provide answers to user questions about software products using a digital assistant.",
                    "tool_description": "Implement a question-answering system that takes a user query about software products and generates an informative response based on the product information database or knowledge base.",
                    "expected_response": "A textual answer or explanation relevant to the user's question about software products, possibly including metadata such as confidence score or source reference."
                }
            ],
            "extracted_arguments": {
                "user_question": "The natural language query or question submitted by the user about a software product."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_223",
        "query": "I am a drone maker that is building a navigation module for my drones. I need to sharpen the images captured from the drone in real-time.",
        "instruction": "Given an `image enhancement` task, retrieve tools that process real-time images captured from drones, enhancing clarity by applying image sharpening or super-resolution techniques to meet the task's visual improvement requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').",
                    "api_arguments": "feature_extractor, model",
                    "python_environment_requirements": "transformers, torch",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.",
                    "id": "huggingface_api_277",
                    "name": "swin2SR-lightweight-x2-64"
                },
                "id": "gorilla_huggingface_tool_273",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 1: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 2: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 3: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 4: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 5: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 6: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.",
            "Tool 7: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "Tool 8: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.",
            "Tool 9: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 10: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 11: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks like object manipulation and indoor navigation. It can be loaded using Hugging Face Transformers and applied to images to generate embeddings, making it useful for robotics applications that require visual understanding and interaction with the environment.",
            "Tool 12: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 13: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 14: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks such as object manipulation and indoor navigation. It can be loaded via Hugging Face Transformers to generate image embeddings, enabling applications that require understanding and interacting with visual environments from a first-person perspective.",
            "Tool 15: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.",
            "Tool 16: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 17: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 18: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 19: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization."
        ],
        "tool_view": {
            "description": "A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "parameters": {
                "audio file or recording": {
                    "type": "string",
                    "description": "The path to an audio file or a recording input that will be used as input for the speech-to-speech translation model.",
                    "required": true,
                    "example": "path/to/input_audio.wav"
                }
            },
            "response": "The tool’s response is expected to be an audio representation of the input speech translated from Romanian to English. Specifically, the response would typically include:  \n\n1. **Translated audio content**: A waveform or audio file containing the spoken translation in English, preserving natural prosody and intonation.  \n2. **Metadata (optional, depending on implementation)**: This may include information about the audio format (e.g., sample rate, encoding), duration of the generated audio, and possibly confidence scores or timing alignment if the system provides them.  \n\nIn essence, the primary output is a playable audio signal representing the speech-to-speech translation, produced from the input audio in Romanian.",
            "examples": [
                {
                    "user_intent": "Translate this Romanian audio recording of a short story into English speech.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "story_ro.wav"
                        }
                    }
                },
                {
                    "user_intent": "I have a 2-minute interview in Romanian. Convert it to English audio.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "interview_ro.mp3"
                        }
                    }
                },
                {
                    "user_intent": "Translate this Romanian podcast episode into English for my listeners.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "podcast_episode_ro.flac"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 1: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 2: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 3: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 4: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 5: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 6: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.\nTool 7: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\nTool 8: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.\nTool 9: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 10: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 11: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks like object manipulation and indoor navigation. It can be loaded using Hugging Face Transformers and applied to images to generate embeddings, making it useful for robotics applications that require visual understanding and interaction with the environment.\nTool 12: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 13: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 14: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks such as object manipulation and indoor navigation. It can be loaded via Hugging Face Transformers to generate image embeddings, enabling applications that require understanding and interacting with visual environments from a first-person perspective.\nTool 15: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.\nTool 16: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 17: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 18: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 19: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\", \"parameters\": {\"audio file or recording\": {\"type\": \"string\", \"description\": \"The path to an audio file or a recording input that will be used as input for the speech-to-speech translation model.\", \"required\": true, \"example\": \"path/to/input_audio.wav\"}}, \"response\": \"The tool\\u2019s response is expected to be an audio representation of the input speech translated from Romanian to English. Specifically, the response would typically include:  \\n\\n1. **Translated audio content**: A waveform or audio file containing the spoken translation in English, preserving natural prosody and intonation.  \\n2. **Metadata (optional, depending on implementation)**: This may include information about the audio format (e.g., sample rate, encoding), duration of the generated audio, and possibly confidence scores or timing alignment if the system provides them.  \\n\\nIn essence, the primary output is a playable audio signal representing the speech-to-speech translation, produced from the input audio in Romanian.\", \"examples\": [{\"user_intent\": \"Translate this Romanian audio recording of a short story into English speech.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"story_ro.wav\"}}}, {\"user_intent\": \"I have a 2-minute interview in Romanian. Convert it to English audio.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"interview_ro.mp3\"}}}, {\"user_intent\": \"Translate this Romanian podcast episode into English for my listeners.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"podcast_episode_ro.flac\"}}}]}\n\n# User Query\nI am a drone maker that is building a navigation module for my drones. I need to sharpen the images captured from the drone in real-time.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I need to enhance the quality of images captured by my drone in real-time.",
                    "tool_description": "Apply real-time image processing to sharpen or enhance the clarity of images captured by drones.",
                    "expected_response": "A processed image with enhanced sharpness or clarity, suitable for navigation tasks."
                }
            ],
            "extracted_arguments": {
                "image_input": "The image captured by the drone that needs enhancement.",
                "real_time_processing": "A flag or condition indicating that the image sharpening should occur in real-time."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_408",
        "query": "Our designer hired me to create an application to automatically translate colors from a design. Usually, our designer likes to work with English, to get the color in Italian.",
        "instruction": "Given a `color translation` task, retrieve tools that specialize in multilingual processing to translate textual color descriptions from one language to another, specifically converting English color names into Italian while ensuring accuracy and contextual relevance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text-to-Text Transfer Transformer",
                    "api_call": "MT5ForConditionalGeneration.from_pretrained('google/mt5-base')",
                    "api_arguments": [
                        "model_name",
                        "input_text",
                        "generated_text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\ntokenizer = MT5Tokenizer.from_pretrained('google/mt5-base')\ninputs = tokenizer.encode('translate English to German: The house is wonderful.', return_tensors='pt')\noutputs = model.generate(inputs, max_length=40, num_return_sequences=1)\ndecoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)",
                    "performance": {
                        "dataset": "mc4",
                        "accuracy": "Not provided"
                    },
                    "description": "mT5 is a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. It leverages a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of multilingual NLP tasks.",
                    "id": "huggingface_api_653",
                    "name": "google/mt5-base"
                },
                "id": "gorilla_huggingface_tool_648",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 1: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.",
            "Tool 2: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 3: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 4: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 5: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 6: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 7: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 8: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 9: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 10: This tool provides a Dutch-to-English translation model using the Helsinki-NLP/opus-mt-nl-en model, based on the OPUS dataset. It leverages a transformer-align architecture with normalization and SentencePiece pre-processing for accurate translations, and is designed to work within the Hugging Face Transformers framework. It is useful for translating Dutch text into English in NLP applications.",
            "Tool 11: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 12: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 13: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 14: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 15: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 16: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 17: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 18: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 19: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts."
        ],
        "tool_view": {
            "description": "A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "parameters": {},
            "response": "The tool is designed for text-to-text generation, specifically English-to-German translation, using a model from the Hugging Face Transformers library.\n\n**Response Description:**\n\nThe response from this tool would consist of a translated German text corresponding to the input English text. The model accepts an English sentence as input and outputs its translation in German.\n\nKey components of the response:\n\n- **Input Text**: The original English text passed to the translation model.\n- **Translated Text**: The corresponding German translation generated by the model.\n\nWhile the exact structure of the response is not specified in the raw documentation, it would typically follow this general form:\n\n- **Output Format**: The translation response is typically a string containing the translated German text.\n- **Possible Additional Metadata**: Depending on the specific configuration or implementation, the response may include metadata such as:\n  - **Model Information**: Information about the model used (e.g., 'sshleifer/tiny-marian-en-de').\n  - **Translation Quality Metrics**: If applicable, some tools may return quality scores or confidence levels for the translation, though this is not explicitly stated in the documentation.\n\nThus, the expected output will primarily consist of the translated German text in response to the provided English input.",
            "examples": [
                {
                    "user_intent": "Translate the sentence 'The weather is nice today' from English to German.",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "The weather is nice today"
                        }
                    }
                },
                {
                    "user_intent": "I want to translate this paragraph into German: 'Artificial intelligence is transforming the world rapidly.'",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "Artificial intelligence is transforming the world rapidly."
                        }
                    }
                },
                {
                    "user_intent": "Convert this short English dialogue to German: 'Hello! How are you doing?'",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "Hello! How are you doing?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 1: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.\nTool 2: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 3: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 4: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 5: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 6: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 7: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 8: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 9: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 10: This tool provides a Dutch-to-English translation model using the Helsinki-NLP/opus-mt-nl-en model, based on the OPUS dataset. It leverages a transformer-align architecture with normalization and SentencePiece pre-processing for accurate translations, and is designed to work within the Hugging Face Transformers framework. It is useful for translating Dutch text into English in NLP applications.\nTool 11: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 12: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 13: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 14: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 15: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 16: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 17: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 18: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 19: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\", \"parameters\": {}, \"response\": \"The tool is designed for text-to-text generation, specifically English-to-German translation, using a model from the Hugging Face Transformers library.\\n\\n**Response Description:**\\n\\nThe response from this tool would consist of a translated German text corresponding to the input English text. The model accepts an English sentence as input and outputs its translation in German.\\n\\nKey components of the response:\\n\\n- **Input Text**: The original English text passed to the translation model.\\n- **Translated Text**: The corresponding German translation generated by the model.\\n\\nWhile the exact structure of the response is not specified in the raw documentation, it would typically follow this general form:\\n\\n- **Output Format**: The translation response is typically a string containing the translated German text.\\n- **Possible Additional Metadata**: Depending on the specific configuration or implementation, the response may include metadata such as:\\n  - **Model Information**: Information about the model used (e.g., 'sshleifer/tiny-marian-en-de').\\n  - **Translation Quality Metrics**: If applicable, some tools may return quality scores or confidence levels for the translation, though this is not explicitly stated in the documentation.\\n\\nThus, the expected output will primarily consist of the translated German text in response to the provided English input.\", \"examples\": [{\"user_intent\": \"Translate the sentence 'The weather is nice today' from English to German.\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"The weather is nice today\"}}}, {\"user_intent\": \"I want to translate this paragraph into German: 'Artificial intelligence is transforming the world rapidly.'\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"Artificial intelligence is transforming the world rapidly.\"}}}, {\"user_intent\": \"Convert this short English dialogue to German: 'Hello! How are you doing?'\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"Hello! How are you doing?\"}}}]}\n\n# User Query\nOur designer hired me to create an application to automatically translate colors from a design. Usually, our designer likes to work with English, to get the color in Italian.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to translate a color description from English to Italian.",
                    "tool_description": "Translate the given English text into Italian using a transformer-based model trained for English-to-Italian translation.",
                    "expected_response": "The translated color description in Italian."
                }
            ],
            "extracted_arguments": {
                "source_language": "The language of the input text (English).",
                "target_language": "The language into which the text will be translated (Italian).",
                "text": "The color description in English to be translated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_99",
        "query": "I want to build an AI that identifies the best marketing strategies for my website by trying different combinations of headlines and images.",
        "instruction": "Given an `AI strategy optimization` task, retrieve tools that leverage AI and machine learning, particularly models and frameworks capable of exploring various combinations of digital marketing elements like headlines and images to identify effective strategies for enhancing website performance.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "MountainCar-v0",
                    "api_call": "load_from_hub(repo_id='sb3/dqn-MountainCar-v0')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "RL Zoo",
                        "SB3",
                        "SB3 Contrib"
                    ],
                    "example_code": [
                        "python -m rl_zoo3.load_from_hub --algo dqn --env MountainCar-v0 -orga sb3 -f logs/",
                        "python train.py --algo dqn --env MountainCar-v0 -f logs/",
                        "python -m rl_zoo3.push_to_hub --algo dqn --env MountainCar-v0 -f logs/ -orga sb3"
                    ],
                    "performance": {
                        "dataset": "MountainCar-v0",
                        "accuracy": "-103.40 +/- 7.49"
                    },
                    "description": "This is a trained model of a DQN agent playing MountainCar-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_895",
                    "name": "sb3/dqn-MountainCar-v0"
                },
                "id": "gorilla_huggingface_tool_888",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 1: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 2: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 3: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 4: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 5: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 6: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 7: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 8: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 9: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 10: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 11: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 12: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 13: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 14: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.",
            "Tool 15: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 16: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 17: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.",
            "Tool 18: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 19: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library."
        ],
        "tool_view": {
            "description": "This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "parameters": {
                "model_id": {
                    "type": "string",
                    "description": "The identifier of the pretrained DDPM model to load from Hugging Face Transformers. This specifies which model weights and configuration to use for image generation.",
                    "required": true,
                    "example": "google/ddpm-ema-cat-256"
                }
            },
            "response": "The tool generates high-quality images using a Denoising Diffusion Probabilistic Model (DDPM). Its response is structured as an object containing the generated image data and possibly additional metadata about the generation process. The typical components of the response include:\n\n1. **images**:  \n   - A collection (usually a list) of generated images.  \n   - Each entry represents one image produced by the model.  \n   - The images are typically returned in an in-memory image format compatible with libraries like PIL, allowing further processing or saving to disk.\n\n2. **metadata** (optional, depending on implementation):  \n   - Information about the generation process, such as the number of inference steps, scheduler type used, or random seed for reproducibility.  \n   - May also include performance-related metrics or warnings.\n\nIn practice, calling the pipeline returns an object where the primary access is through the `.images` attribute, which provides the generated images ready for display or storage. The response does not include textual descriptions or labels, as this is an unconditional image generation model.",
            "examples": [
                {
                    "user_intent": "Generate an image of a cat using the DDPM model.",
                    "tool_call": {
                        "name": "google/ddpm-ema-cat-256",
                        "arguments": [
                            "google/ddpm-ema-cat-256"
                        ]
                    }
                },
                {
                    "user_intent": "I want to use the DDPM model to generate a cat image and save it to a file.",
                    "tool_call": {
                        "name": "google/ddpm-ema-cat-256",
                        "arguments": [
                            "google/ddpm-ema-cat-256"
                        ]
                    }
                },
                {
                    "user_intent": "Can I generate a high-quality cat image using DDPM from Hugging Face?",
                    "tool_call": {
                        "name": "google/ddpm-ema-cat-256",
                        "arguments": [
                            "google/ddpm-ema-cat-256"
                        ]
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 1: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 2: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 3: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 4: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 5: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 6: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 7: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 8: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 9: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 10: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 11: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 12: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 13: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 14: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.\nTool 15: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 16: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 17: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.\nTool 18: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 19: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\", \"parameters\": {\"model_id\": {\"type\": \"string\", \"description\": \"The identifier of the pretrained DDPM model to load from Hugging Face Transformers. This specifies which model weights and configuration to use for image generation.\", \"required\": true, \"example\": \"google/ddpm-ema-cat-256\"}}, \"response\": \"The tool generates high-quality images using a Denoising Diffusion Probabilistic Model (DDPM). Its response is structured as an object containing the generated image data and possibly additional metadata about the generation process. The typical components of the response include:\\n\\n1. **images**:  \\n   - A collection (usually a list) of generated images.  \\n   - Each entry represents one image produced by the model.  \\n   - The images are typically returned in an in-memory image format compatible with libraries like PIL, allowing further processing or saving to disk.\\n\\n2. **metadata** (optional, depending on implementation):  \\n   - Information about the generation process, such as the number of inference steps, scheduler type used, or random seed for reproducibility.  \\n   - May also include performance-related metrics or warnings.\\n\\nIn practice, calling the pipeline returns an object where the primary access is through the `.images` attribute, which provides the generated images ready for display or storage. The response does not include textual descriptions or labels, as this is an unconditional image generation model.\", \"examples\": [{\"user_intent\": \"Generate an image of a cat using the DDPM model.\", \"tool_call\": {\"name\": \"google/ddpm-ema-cat-256\", \"arguments\": [\"google/ddpm-ema-cat-256\"]}}, {\"user_intent\": \"I want to use the DDPM model to generate a cat image and save it to a file.\", \"tool_call\": {\"name\": \"google/ddpm-ema-cat-256\", \"arguments\": [\"google/ddpm-ema-cat-256\"]}}, {\"user_intent\": \"Can I generate a high-quality cat image using DDPM from Hugging Face?\", \"tool_call\": {\"name\": \"google/ddpm-ema-cat-256\", \"arguments\": [\"google/ddpm-ema-cat-256\"]}}]}\n\n# User Query\nI want to build an AI that identifies the best marketing strategies for my website by trying different combinations of headlines and images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a variety of images to use in marketing experiments.",
                    "tool_description": "Use a pretrained Denoising Diffusion Probabilistic Model (DDPM) to create high-quality, unconditional images suitable for marketing or A/B testing purposes.",
                    "expected_response": "A collection of generated images ready to be used in website marketing experiments."
                },
                {
                    "user_intent": "Test different headline and image combinations to determine the most effective marketing strategy.",
                    "tool_description": "Conduct an A/B testing process or simulation where various headline and image combinations are evaluated for engagement or conversion performance.",
                    "expected_response": "Analytics data showing performance metrics for each headline-image combination, highlighting the most effective strategies."
                }
            ],
            "extracted_arguments": {
                "website_url": "The URL of the website where marketing strategies will be tested.",
                "headlines": "A list of potential headline text options for marketing experiments.",
                "image_model_id": "The identifier of the DDPM model to generate marketing images.",
                "testing_criteria": "Metrics or goals for evaluating marketing performance, such as clicks, conversions, or engagement."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_441",
        "query": "Create a program to separate music and vocals from an audio file using a pretrained model.",
        "instruction": "Given an `audio source separation` task, retrieve tools that utilize pretrained models to process audio inputs, enabling the separation of music and vocals, and delivering distinct audio outputs aligned with the task's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')",
                    "api_arguments": "audio_file",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "WHAM!",
                        "si_sdr": 19.316743490695334,
                        "si_sdr_imp": 19.317895273889842,
                        "sdr": 19.68085347190952,
                        "sdr_imp": 19.5298092932871,
                        "sir": 30.362213998701232,
                        "sir_imp": 30.21116982007881,
                        "sar": 20.15553251343315,
                        "sar_imp": -129.02091762351188,
                        "stoi": 0.97772664309074,
                        "stoi_imp": 0.23968091518217424
                    },
                    "description": "This model was trained by Manuel Pariente using the wham/DPRNN recipe in Asteroid. It was trained on the sep_clean task of the WHAM! dataset.",
                    "id": "huggingface_api_780",
                    "name": "mpariente/DPRNNTasNet-ks2_WHAM_sepclean"
                },
                "id": "gorilla_huggingface_tool_775",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 2: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.",
            "Tool 3: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "Tool 4: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 5: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 6: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 7: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.",
            "Tool 8: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.",
            "Tool 9: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 10: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.",
            "Tool 11: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 12: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 13: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.",
            "Tool 14: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 15: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.",
            "Tool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 17: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.",
            "Tool 18: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 19: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently."
        ],
        "tool_view": {
            "description": "This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "parameters": {
                "source": {
                    "type": "string",
                    "description": "The identifier or path of the pretrained SepFormer model to load from SpeechBrain.",
                    "required": true,
                    "example": "speechbrain/sepformer-whamr"
                },
                "savedir": {
                    "type": "string",
                    "description": "The local directory path where the pretrained model will be downloaded and saved.",
                    "required": true,
                    "example": "pretrained_models/sepformer-whamr"
                }
            },
            "response": "The tool is designed for performing audio source separation using the SepFormer model from the SpeechBrain framework. The response would contain the following elements:\n\n1. **Source Separation Results**:  \n   The response will include separated audio sources, which are the results of applying the SepFormer model to an input audio file. These separated sources would typically be presented as a multi-dimensional array or tensor, where each component corresponds to a distinct separated audio source (e.g., speech, noise, etc.).\n\n2. **File Output (Separated Audio)**:  \n   The response would indicate the output of the separation process, typically providing paths or references to saved audio files for each of the separated sources. These files are stored in formats such as WAV, and each separated source corresponds to a different output file. For example, two separate files could be generated for the two speech sources in a mixture.\n\n3. **Status or Error Information**:  \n   In case of successful execution, the response would likely include a success status or completion message, confirming that the separation process was completed and indicating the locations where the separated audio files were saved. If an error occurs, appropriate error messages or status codes would be included to help diagnose issues (e.g., invalid input file, model loading issues, etc.).\n\n4. **Performance Metrics**:  \n   Optionally, the response might include performance-related metrics such as the Signal-to-Noise Ratio improvement (SI-SNRi), which provides insight into the effectiveness of the source separation. In this case, it would likely reference the accuracy of 13.7 dB SI-SNRi based on the pretrained model's performance on the WHAMR! dataset.\n\nIn summary, the expected response would provide the separated audio sources (likely in the form of saved audio files), a status or success message, and potentially performance metrics related to the source separation task.",
            "examples": [
                {
                    "user_intent": "I have a noisy recording of two people talking at the same time and want to separate each speaker into its own audio file.",
                    "tool_call": {
                        "name": "separator.from_hparams",
                        "arguments": {
                            "source": "speechbrain/sepformer-whamr",
                            "savedir": "pretrained_models/sepformer-whamr",
                            "path": "recordings/meeting_mixture.wav"
                        }
                    }
                },
                {
                    "user_intent": "Can you separate the background noise from a music track so I can isolate the vocals and instruments?",
                    "tool_call": {
                        "name": "separator.from_hparams",
                        "arguments": {
                            "source": "speechbrain/sepformer-whamr",
                            "savedir": "pretrained_models/sepformer-whamr",
                            "path": "music_tracks/noisy_song.wav"
                        }
                    }
                },
                {
                    "user_intent": "I want to process a batch of audio clips from my podcast to split overlapping voices for transcription.",
                    "tool_call": {
                        "name": "separator.from_hparams",
                        "arguments": {
                            "source": "speechbrain/sepformer-whamr",
                            "savedir": "pretrained_models/sepformer-whamr",
                            "path": "podcast_episodes/episode1_mixture.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 2: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.\nTool 3: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\nTool 4: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 5: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 6: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 7: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.\nTool 8: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.\nTool 9: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 10: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.\nTool 11: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 12: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 13: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\nTool 14: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 15: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.\nTool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 17: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.\nTool 18: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 19: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\", \"parameters\": {\"source\": {\"type\": \"string\", \"description\": \"The identifier or path of the pretrained SepFormer model to load from SpeechBrain.\", \"required\": true, \"example\": \"speechbrain/sepformer-whamr\"}, \"savedir\": {\"type\": \"string\", \"description\": \"The local directory path where the pretrained model will be downloaded and saved.\", \"required\": true, \"example\": \"pretrained_models/sepformer-whamr\"}}, \"response\": \"The tool is designed for performing audio source separation using the SepFormer model from the SpeechBrain framework. The response would contain the following elements:\\n\\n1. **Source Separation Results**:  \\n   The response will include separated audio sources, which are the results of applying the SepFormer model to an input audio file. These separated sources would typically be presented as a multi-dimensional array or tensor, where each component corresponds to a distinct separated audio source (e.g., speech, noise, etc.).\\n\\n2. **File Output (Separated Audio)**:  \\n   The response would indicate the output of the separation process, typically providing paths or references to saved audio files for each of the separated sources. These files are stored in formats such as WAV, and each separated source corresponds to a different output file. For example, two separate files could be generated for the two speech sources in a mixture.\\n\\n3. **Status or Error Information**:  \\n   In case of successful execution, the response would likely include a success status or completion message, confirming that the separation process was completed and indicating the locations where the separated audio files were saved. If an error occurs, appropriate error messages or status codes would be included to help diagnose issues (e.g., invalid input file, model loading issues, etc.).\\n\\n4. **Performance Metrics**:  \\n   Optionally, the response might include performance-related metrics such as the Signal-to-Noise Ratio improvement (SI-SNRi), which provides insight into the effectiveness of the source separation. In this case, it would likely reference the accuracy of 13.7 dB SI-SNRi based on the pretrained model's performance on the WHAMR! dataset.\\n\\nIn summary, the expected response would provide the separated audio sources (likely in the form of saved audio files), a status or success message, and potentially performance metrics related to the source separation task.\", \"examples\": [{\"user_intent\": \"I have a noisy recording of two people talking at the same time and want to separate each speaker into its own audio file.\", \"tool_call\": {\"name\": \"separator.from_hparams\", \"arguments\": {\"source\": \"speechbrain/sepformer-whamr\", \"savedir\": \"pretrained_models/sepformer-whamr\", \"path\": \"recordings/meeting_mixture.wav\"}}}, {\"user_intent\": \"Can you separate the background noise from a music track so I can isolate the vocals and instruments?\", \"tool_call\": {\"name\": \"separator.from_hparams\", \"arguments\": {\"source\": \"speechbrain/sepformer-whamr\", \"savedir\": \"pretrained_models/sepformer-whamr\", \"path\": \"music_tracks/noisy_song.wav\"}}}, {\"user_intent\": \"I want to process a batch of audio clips from my podcast to split overlapping voices for transcription.\", \"tool_call\": {\"name\": \"separator.from_hparams\", \"arguments\": {\"source\": \"speechbrain/sepformer-whamr\", \"savedir\": \"pretrained_models/sepformer-whamr\", \"path\": \"podcast_episodes/episode1_mixture.wav\"}}}]}\n\n# User Query\nCreate a program to separate music and vocals from an audio file using a pretrained model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to separate music and vocals from an audio file using a pretrained model.",
                    "tool_description": "Perform audio source separation using a pretrained model, isolating music and vocal components from a mixed audio file.",
                    "expected_response": "The separated music and vocal components, likely in the form of saved audio files, and performance metrics if available."
                }
            ],
            "extracted_arguments": {
                "source": "The identifier or path of the pretrained model to load for source separation.",
                "savedir": "The local directory path where the pretrained model will be saved or loaded from.",
                "path": "The path to the mixed audio file that needs to be processed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_370",
        "query": "Design an efficient search engine that retrieves most relevant articles based on a pool of long text documents.",
        "instruction": "Given a `search engine design` task, retrieve tools that enable the development of search engines capable of processing long text documents to retrieve the most relevant articles based on search queries, leveraging models trained for generating or matching queries to appropriate content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text2Text Generation",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')",
                    "api_arguments": "text, max_length",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "MS MARCO",
                        "accuracy": "Not specified"
                    },
                    "description": "A T5 model trained on the MS MARCO dataset for generating queries from documents.",
                    "id": "huggingface_api_648",
                    "name": "castorini/doc2query-t5-base-msmarco"
                },
                "id": "gorilla_huggingface_tool_643",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 2: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 3: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "Tool 4: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 5: `sshleifer/distilbart-cnn-12-6` is a distilled BART model designed for text summarization, providing a fast and efficient way to generate concise summaries of input text. Trained on the CNN/DailyMail dataset, it can be easily loaded and used via the Hugging Face Transformers library for natural language processing tasks that require summarizing long documents.",
            "Tool 6: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 7: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 8: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.",
            "Tool 9: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.",
            "Tool 10: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 11: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.",
            "Tool 12: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 13: The \"google/bigbird-pegasus-large-arxiv\" tool is a text summarization model based on the BigBird architecture, which uses sparse attention to handle longer sequences more efficiently than traditional transformers like BERT. It is designed for summarizing long documents, such as scientific papers, with a lower compute cost while maintaining high performance. This model is ideal for tasks involving very long text sequences, achieving state-of-the-art results in summarization and question-answering tasks.",
            "Tool 14: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 15: The `facebook/dpr-question_encoder-single-nq-base` is a Dense Passage Retrieval (DPR) question encoder that converts natural language questions into dense vector embeddings for open-domain question answering. Trained on the Natural Questions (NQ) dataset, it is designed to capture semantic meaning of queries, enabling efficient retrieval of relevant passages from large text corpora. This model is used in NLP systems to improve the accuracy and speed of information retrieval in applications like search engines and QA pipelines.",
            "Tool 16: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 17: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 18: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 19: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text."
        ],
        "tool_view": {
            "description": "A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "parameters": {
                "input_text": {
                    "type": "string",
                    "description": "The text content that you want the PEGASUS model to summarize. This should be a complete passage or article in string format.",
                    "required": true,
                    "example": "India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj during the third Test against England."
                }
            },
            "response": "The tool produces a **textual summary** of the input content provided via the `input_text` argument. The response is a single string that represents a condensed version of the original text, retaining the most important information, key events, or main ideas while reducing length and removing less relevant details.  \n\nSpecifically:  \n\n- **Type:** String  \n- **Content:** A coherent, fluent summary of the input text. It is generated by the PEGASUS model fine-tuned for summarization (`tuner007/pegasus_summarizer`) and is intended to capture the essence of the input without including extraneous information.  \n- **Length:** Typically shorter than the input text, with the exact length depending on the model’s internal generation constraints and the input complexity.  \n- **Purpose:** To provide a concise representation of the input text suitable for quick reading or downstream NLP tasks.  \n\nThere are no additional structured fields; the output is purely the generated summary string.",
            "examples": "[\n    {\n        \"user_intent\": \"Summarize this news article about Rishabh Pant and crowd incidents during the India vs England Test match.\",\n        \"tool_call\": {\n            \"name\": \"tuner007/pegasus_summarizer\",\n            \"arguments\": {\n                \"input_text\": \"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day's play. You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket, I guess, he added. In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd. Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul. Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated. Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day"
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 2: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 3: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\nTool 4: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 5: `sshleifer/distilbart-cnn-12-6` is a distilled BART model designed for text summarization, providing a fast and efficient way to generate concise summaries of input text. Trained on the CNN/DailyMail dataset, it can be easily loaded and used via the Hugging Face Transformers library for natural language processing tasks that require summarizing long documents.\nTool 6: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 7: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 8: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.\nTool 9: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.\nTool 10: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 11: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.\nTool 12: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 13: The \"google/bigbird-pegasus-large-arxiv\" tool is a text summarization model based on the BigBird architecture, which uses sparse attention to handle longer sequences more efficiently than traditional transformers like BERT. It is designed for summarizing long documents, such as scientific papers, with a lower compute cost while maintaining high performance. This model is ideal for tasks involving very long text sequences, achieving state-of-the-art results in summarization and question-answering tasks.\nTool 14: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 15: The `facebook/dpr-question_encoder-single-nq-base` is a Dense Passage Retrieval (DPR) question encoder that converts natural language questions into dense vector embeddings for open-domain question answering. Trained on the Natural Questions (NQ) dataset, it is designed to capture semantic meaning of queries, enabling efficient retrieval of relevant passages from large text corpora. This model is used in NLP systems to improve the accuracy and speed of information retrieval in applications like search engines and QA pipelines.\nTool 16: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 17: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 18: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 19: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\", \"parameters\": {\"input_text\": {\"type\": \"string\", \"description\": \"The text content that you want the PEGASUS model to summarize. This should be a complete passage or article in string format.\", \"required\": true, \"example\": \"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj during the third Test against England.\"}}, \"response\": \"The tool produces a **textual summary** of the input content provided via the `input_text` argument. The response is a single string that represents a condensed version of the original text, retaining the most important information, key events, or main ideas while reducing length and removing less relevant details.  \\n\\nSpecifically:  \\n\\n- **Type:** String  \\n- **Content:** A coherent, fluent summary of the input text. It is generated by the PEGASUS model fine-tuned for summarization (`tuner007/pegasus_summarizer`) and is intended to capture the essence of the input without including extraneous information.  \\n- **Length:** Typically shorter than the input text, with the exact length depending on the model\\u2019s internal generation constraints and the input complexity.  \\n- **Purpose:** To provide a concise representation of the input text suitable for quick reading or downstream NLP tasks.  \\n\\nThere are no additional structured fields; the output is purely the generated summary string.\", \"examples\": \"[\\n    {\\n        \\\"user_intent\\\": \\\"Summarize this news article about Rishabh Pant and crowd incidents during the India vs England Test match.\\\",\\n        \\\"tool_call\\\": {\\n            \\\"name\\\": \\\"tuner007/pegasus_summarizer\\\",\\n            \\\"arguments\\\": {\\n                \\\"input_text\\\": \\\"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day's play. You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket, I guess, he added. In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd. Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul. Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated. Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day\"}\n\n# User Query\nDesign an efficient search engine that retrieves most relevant articles based on a pool of long text documents.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Encode a collection of long text documents into dense vector embeddings for semantic search.",
                    "tool_description": "Convert each document in a corpus into high-dimensional vector representations using a sentence-transformers model optimized for semantic similarity, enabling efficient comparison between queries and documents.",
                    "expected_response": "A list of dense vector embeddings corresponding to each document in the input corpus."
                },
                {
                    "user_intent": "Encode the user's search query into a vector to compare against document embeddings.",
                    "tool_description": "Transform the input search query into a dense vector embedding using the same sentence-transformers model as used for the documents, preserving semantic meaning for accurate similarity computation.",
                    "expected_response": "A single dense vector representing the semantic content of the query."
                },
                {
                    "user_intent": "Retrieve the most relevant articles by comparing the query embedding with document embeddings.",
                    "tool_description": "Compute similarity scores between the query embedding and each document embedding, ranking documents by relevance, and return the top results as the most semantically related articles.",
                    "expected_response": "A ranked list of documents or article identifiers sorted by their similarity to the query."
                }
            ],
            "extracted_arguments": {
                "document_corpus": "A collection of long text documents to be indexed for search, each represented as a string.",
                "search_query": "The natural language query input by the user, in string format.",
                "top_k": "Optional numeric value specifying how many top relevant articles to return based on similarity scores."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_233",
        "query": "Our client is involved in a sports organization, and they require a solution to classify sports videos efficiently.",
        "instruction": "Given a `video classification` task, retrieve tools that efficiently classify sports videos by processing video inputs to determine their categorical labels using advanced models such as TimeSformer.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')",
                    "api_arguments": {
                        "images": "video",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "numpy",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, TimesformerForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 448, 448))\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k600)\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k600)\ninputs = processor(images=video, return_tensors=pt)\nwith torch.no_grad():\n  outputs = model(**inputs)\n  logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-600",
                        "accuracy": "Not provided"
                    },
                    "description": "TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels.",
                    "id": "huggingface_api_320",
                    "name": "facebook/timesformer-hr-finetuned-k600"
                },
                "id": "gorilla_huggingface_tool_316",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 1: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 2: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 3: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 4: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 5: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 6: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 7: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 8: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 9: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 10: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 11: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 12: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 13: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification that uses residual learning to enable training of deep models. It is designed to classify images at 224×224 resolution and is trained on the ImageNet-1k dataset. This model improves on the original ResNet-50 by modifying the downsampling in bottleneck blocks, slightly increasing accuracy. It can be used with the Hugging Face Transformers library to process images and predict their class labels efficiently.",
            "Tool 14: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.",
            "Tool 15: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 16: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 17: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 18: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.",
            "Tool 19: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples."
        ],
        "tool_view": {
            "description": "This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "parameters": {
                "sequence": {
                    "type": "string",
                    "description": "The input text sequence that you want to classify using the zero-shot model.",
                    "required": true,
                    "example": "El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo"
                },
                "candidate_labels": {
                    "type": "array",
                    "description": "A list of labels or categories that the model will consider when classifying the input sequence.",
                    "required": true,
                    "example": [
                        "cultura",
                        "sociedad",
                        "economia",
                        "salud",
                        "deportes"
                    ]
                },
                "hypothesis_template": {
                    "type": "string",
                    "description": "A template string used to frame the classification hypothesis, where '{}' will be replaced with each candidate label.",
                    "required": true,
                    "example": "Este ejemplo es {}."
                }
            },
            "response": "The tool performs zero-shot classification using a pre-trained Spanish BERT model fine-tuned on the XNLI dataset. The response from this tool typically includes the following components:\n\n1. **Labels**: A list of candidate labels that the model has evaluated as the most relevant or likely classifications for the input text.\n   \n2. **Scores**: A corresponding list of numeric scores indicating the model's confidence or probability for each label. These scores typically represent the model's prediction confidence in the range [0, 1] for each label.\n\n3. **Sequence**: The input sequence of text that was classified. This may be included to confirm the processed input.\n\n4. **Chosen Label(s)**: Depending on the configuration, the model might also return the label with the highest confidence as the primary classification.\n\nThe response will reflect the likelihood of each candidate label in relation to the given text and hypothesis template, allowing users to interpret the predicted classification for the sequence. The exact structure of the output depends on the specific implementation but will generally include these key components.",
            "examples": [
                {
                    "user_intent": "I want to classify this Spanish sentence into categories: 'El gobierno anunció nuevas medidas económicas para combatir la inflación.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "El gobierno anunció nuevas medidas económicas para combatir la inflación.",
                            "candidate_labels": [
                                "economia",
                                "sociedad",
                                "salud",
                                "cultura",
                                "deportes"
                            ],
                            "hypothesis_template": "Este texto trata sobre {}."
                        }
                    }
                },
                {
                    "user_intent": "Determine whether this Spanish sentence is about health, culture, or sports: 'El Maratón de la Ciudad atrajo a miles de corredores de todo el mundo.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "El Maratón de la Ciudad atrajo a miles de corredores de todo el mundo.",
                            "candidate_labels": [
                                "deportes",
                                "salud",
                                "cultura",
                                "economia",
                                "sociedad"
                            ],
                            "hypothesis_template": "Este texto se refiere a {}."
                        }
                    }
                },
                {
                    "user_intent": "Classify this Spanish review about a museum visit: 'La exposición de arte contemporáneo fue fascinante y educativa.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "La exposición de arte contemporáneo fue fascinante y educativa.",
                            "candidate_labels": [
                                "cultura",
                                "sociedad",
                                "economia",
                                "deportes",
                                "salud"
                            ],
                            "hypothesis_template": "Este texto es sobre {}."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 1: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 2: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 3: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 4: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 5: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 6: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 7: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 8: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 9: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 10: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 11: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 12: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 13: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification that uses residual learning to enable training of deep models. It is designed to classify images at 224×224 resolution and is trained on the ImageNet-1k dataset. This model improves on the original ResNet-50 by modifying the downsampling in bottleneck blocks, slightly increasing accuracy. It can be used with the Hugging Face Transformers library to process images and predict their class labels efficiently.\nTool 14: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.\nTool 15: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 16: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 17: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 18: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.\nTool 19: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\", \"parameters\": {\"sequence\": {\"type\": \"string\", \"description\": \"The input text sequence that you want to classify using the zero-shot model.\", \"required\": true, \"example\": \"El autor se perfila, a los 50 a\\u00f1os de su muerte, como uno de los grandes de su siglo\"}, \"candidate_labels\": {\"type\": \"array\", \"description\": \"A list of labels or categories that the model will consider when classifying the input sequence.\", \"required\": true, \"example\": [\"cultura\", \"sociedad\", \"economia\", \"salud\", \"deportes\"]}, \"hypothesis_template\": {\"type\": \"string\", \"description\": \"A template string used to frame the classification hypothesis, where '{}' will be replaced with each candidate label.\", \"required\": true, \"example\": \"Este ejemplo es {}.\"}}, \"response\": \"The tool performs zero-shot classification using a pre-trained Spanish BERT model fine-tuned on the XNLI dataset. The response from this tool typically includes the following components:\\n\\n1. **Labels**: A list of candidate labels that the model has evaluated as the most relevant or likely classifications for the input text.\\n   \\n2. **Scores**: A corresponding list of numeric scores indicating the model's confidence or probability for each label. These scores typically represent the model's prediction confidence in the range [0, 1] for each label.\\n\\n3. **Sequence**: The input sequence of text that was classified. This may be included to confirm the processed input.\\n\\n4. **Chosen Label(s)**: Depending on the configuration, the model might also return the label with the highest confidence as the primary classification.\\n\\nThe response will reflect the likelihood of each candidate label in relation to the given text and hypothesis template, allowing users to interpret the predicted classification for the sequence. The exact structure of the output depends on the specific implementation but will generally include these key components.\", \"examples\": [{\"user_intent\": \"I want to classify this Spanish sentence into categories: 'El gobierno anunci\\u00f3 nuevas medidas econ\\u00f3micas para combatir la inflaci\\u00f3n.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"El gobierno anunci\\u00f3 nuevas medidas econ\\u00f3micas para combatir la inflaci\\u00f3n.\", \"candidate_labels\": [\"economia\", \"sociedad\", \"salud\", \"cultura\", \"deportes\"], \"hypothesis_template\": \"Este texto trata sobre {}.\"}}}, {\"user_intent\": \"Determine whether this Spanish sentence is about health, culture, or sports: 'El Marat\\u00f3n de la Ciudad atrajo a miles de corredores de todo el mundo.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"El Marat\\u00f3n de la Ciudad atrajo a miles de corredores de todo el mundo.\", \"candidate_labels\": [\"deportes\", \"salud\", \"cultura\", \"economia\", \"sociedad\"], \"hypothesis_template\": \"Este texto se refiere a {}.\"}}}, {\"user_intent\": \"Classify this Spanish review about a museum visit: 'La exposici\\u00f3n de arte contempor\\u00e1neo fue fascinante y educativa.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"La exposici\\u00f3n de arte contempor\\u00e1neo fue fascinante y educativa.\", \"candidate_labels\": [\"cultura\", \"sociedad\", \"economia\", \"deportes\", \"salud\"], \"hypothesis_template\": \"Este texto es sobre {}.\"}}}]}\n\n# User Query\nOur client is involved in a sports organization, and they require a solution to classify sports videos efficiently.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify videos to identify sports-related actions and activities.",
                    "tool_description": "Use a pre-trained or fine-tuned video classification model to process video files and predict action labels relevant to sports using space-time attention or VideoMAE frameworks.",
                    "expected_response": "A list of predicted action labels for each input video, including confidence scores for each label."
                }
            ],
            "extracted_arguments": {
                "video_input": "The video file or video stream to be analyzed for action classification."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_187",
        "query": "We were given an audio file of a company presentation, and we need it transcribed verbatim.",
        "instruction": "Given an `audio transcription` task, retrieve tools that specialize in processing audio inputs for verbatim transcriptions, ensuring accurate and detailed conversion of spoken content to text, aligned with the query's specifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transcription and Translation",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')",
                    "api_arguments": [
                        "sample",
                        "sampling_rate",
                        "language",
                        "task",
                        "skip_special_tokens"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "datasets"
                    ],
                    "example_code": "from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\n\nmodel.config.forced_decoder_ids = None\n\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\nsample = ds[0][audio]\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "LibriSpeech (clean)",
                                "accuracy": 2.9
                            },
                            {
                                "name": "LibriSpeech (other)",
                                "accuracy": 5.9
                            },
                            {
                                "name": "Common Voice 11.0",
                                "accuracy": 53.87
                            }
                        ]
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.",
                    "id": "huggingface_api_767",
                    "name": "openai/whisper-medium"
                },
                "id": "gorilla_huggingface_tool_762",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 2: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 3: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 4: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 5: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 6: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 7: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 8: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 9: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "Tool 10: A fine-tuned Audio Spectrogram Transformer (AST) model that performs audio classification by converting audio into spectrogram images and applying a Vision Transformer. It is trained on AudioSet and achieves state-of-the-art results on multiple audio classification benchmarks, making it useful for identifying and categorizing sounds in audio data.",
            "Tool 11: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.",
            "Tool 12: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 13: A fine-tuned audio classification model based on Facebook’s data2vec-audio-base-960h, designed to accurately classify audio inputs using the Hugging Face Transformers pipeline.",
            "Tool 14: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 15: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 16: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 17: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.",
            "Tool 18: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 19: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI."
        ],
        "tool_view": {
            "description": "A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "What is the capital of Germany?"
                },
                "context": {
                    "type": "string",
                    "description": "The text or passage that contains the information needed to answer the question.",
                    "required": true,
                    "example": "Berlin is the capital of Germany."
                }
            },
            "response": "The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \n\n1. **`answer`** – A string representing the model’s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \n\n2. **`score`** – A numerical confidence score indicating the model’s certainty about the predicted answer. Higher values correspond to higher confidence.  \n\n3. **`start`** – An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \n\n4. **`end`** – An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \n\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.",
            "examples": [
                {
                    "user_intent": "What's the main ingredient in a Caesar salad according to this recipe?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "What is the main ingredient in a Caesar salad?",
                            "context": "A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing."
                        }
                    }
                },
                {
                    "user_intent": "Who wrote the novel 'Pride and Prejudice'?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "Who is the author of 'Pride and Prejudice'?",
                            "context": "'Pride and Prejudice' is a classic novel written by Jane Austen in 1813."
                        }
                    }
                },
                {
                    "user_intent": "When did the Apollo 11 mission land on the moon?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "When did Apollo 11 land on the moon?",
                            "context": "Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 2: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 3: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 4: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 5: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 6: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 7: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 8: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 9: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\nTool 10: A fine-tuned Audio Spectrogram Transformer (AST) model that performs audio classification by converting audio into spectrogram images and applying a Vision Transformer. It is trained on AudioSet and achieves state-of-the-art results on multiple audio classification benchmarks, making it useful for identifying and categorizing sounds in audio data.\nTool 11: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.\nTool 12: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 13: A fine-tuned audio classification model based on Facebook’s data2vec-audio-base-960h, designed to accurately classify audio inputs using the Hugging Face Transformers pipeline.\nTool 14: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 15: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 16: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 17: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.\nTool 18: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 19: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"What is the capital of Germany?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text or passage that contains the information needed to answer the question.\", \"required\": true, \"example\": \"Berlin is the capital of Germany.\"}}, \"response\": \"The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \\n\\n2. **`score`** \\u2013 A numerical confidence score indicating the model\\u2019s certainty about the predicted answer. Higher values correspond to higher confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \\n\\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.\", \"examples\": [{\"user_intent\": \"What's the main ingredient in a Caesar salad according to this recipe?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"What is the main ingredient in a Caesar salad?\", \"context\": \"A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing.\"}}}, {\"user_intent\": \"Who wrote the novel 'Pride and Prejudice'?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"Who is the author of 'Pride and Prejudice'?\", \"context\": \"'Pride and Prejudice' is a classic novel written by Jane Austen in 1813.\"}}}, {\"user_intent\": \"When did the Apollo 11 mission land on the moon?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"When did Apollo 11 land on the moon?\", \"context\": \"Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969.\"}}}]}\n\n# User Query\nWe were given an audio file of a company presentation, and we need it transcribed verbatim.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect all speech segments in an audio recording.",
                    "tool_description": "Use a pretrained voice activity detection model to identify time ranges in an audio file where speech is present.",
                    "expected_response": "A list of time-stamped segments indicating where speech occurs in the audio."
                },
                {
                    "user_intent": "Transcribe the detected speech segments into text verbatim.",
                    "tool_description": "Apply an automatic speech recognition (ASR) model to convert spoken words in the audio segments into a written transcript, preserving exact wording.",
                    "expected_response": "A text transcript of the audio, corresponding to the spoken content in the identified segments."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The input audio file containing the company presentation to be transcribed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_132",
        "query": "We are a security company and we need a video classification model to analyze CCTV footage for suspicious activities.",
        "instruction": "Given a `video classification` task, retrieve tools that analyze video inputs to categorize and identify suspicious activities in CCTV footage, aligning with the specific security requirements outlined in the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2')",
                    "api_arguments": "video_path",
                    "python_environment_requirements": "transformers==4.27.4, torch==2.0.0+cu117, datasets==2.11.0, tokenizers==0.13.2",
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": 0.7212
                    },
                    "description": "This model is a fine-tuned version of MCG-NJU/videomae-large-finetuned-kinetics on an unknown dataset.",
                    "id": "huggingface_api_339",
                    "name": "lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2"
                },
                "id": "gorilla_huggingface_tool_335",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 1: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 2: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 3: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 4: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 5: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 6: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 8: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 9: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 10: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 11: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 12: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.",
            "Tool 14: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 15: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 16: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 17: The MCG-NJU/videomae-base-finetuned-kinetics is a pre-trained video classification model, fine-tuned on the Kinetics-400 dataset. It uses the VideoMAE (Masked Autoencoders for Video Pre-training) architecture, which was trained for 1600 epochs in a self-supervised manner and further fine-tuned for supervised learning. The model achieves top-1 accuracy of 80.9% and top-5 accuracy of 94.7% on the Kinetics-400 dataset. It can be used for video classification tasks by processing video input with the VideoMAEImageProcessor and making predictions with the VideoMAEForVideoClassification model in a Hugging Face Transformers environment.",
            "Tool 18: This tool provides a pretrained VideoMAE model fine-tuned for video classification on the Kinetics-400 dataset, enabling accurate identification of actions or events in video input. It leverages self-supervised pretraining combined with supervised fine-tuning to deliver high performance, making it suitable for applications requiring efficient and reliable video understanding.",
            "Tool 19: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation."
        ],
        "tool_view": {
            "description": "The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "parameters": {
                "images": {
                    "type": "array",
                    "description": "A list or sequence of video frames represented as arrays, where each frame is typically a 3D array (channels, height, width). This input is used by the model for video classification.",
                    "required": true,
                    "example": [
                        [
                            [
                                [
                                    0.1,
                                    0.2
                                ],
                                [
                                    0.3,
                                    0.4
                                ]
                            ],
                            [
                                [
                                    0.5,
                                    0.6
                                ],
                                [
                                    0.7,
                                    0.8
                                ]
                            ],
                            [
                                [
                                    0.9,
                                    1.0
                                ],
                                [
                                    1.1,
                                    1.2
                                ]
                            ]
                        ]
                    ]
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors the processor should return. Common values include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool’s response is an object representing the output of a video classification model. Its structure and components are as follows:\n\n1. **`logits`**  \n   - This is a tensor containing the raw, unnormalized scores produced by the model for each class in the classification task.  \n   - The shape typically corresponds to `[batch_size, num_classes]`, where `batch_size` is the number of input video samples and `num_classes` is the total number of target classes the model can predict.  \n   - Higher values indicate stronger model confidence for the corresponding class, but they are not probabilities.\n\n2. **`predicted_class_idx`** (derived from `logits`)  \n   - This is the integer index of the class with the highest score in `logits` for a given input video.  \n   - It represents the model’s most likely prediction for that video.\n\n3. **`predicted_class_label`** (derived using `model.config.id2label`)  \n   - This is a human-readable string corresponding to `predicted_class_idx`.  \n   - It maps the integer class index to the semantic label defined in the model’s configuration (e.g., “picking something up” or “throwing an object”).\n\n4. **Optional additional fields**  \n   - Depending on the framework version and configuration, the response object may also include intermediate outputs such as attention weights or hidden states, but these are typically optional and not returned by default.  \n\nIn summary, the primary semantic content of the response is the model’s **raw class scores (`logits`)** and the **predicted class** for the input video, with a mapping to a human-readable label for interpretation.",
            "examples": [
                {
                    "user_intent": "Classify a video using the TimeSformer model pre-trained on the Something Something v2 dataset.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I want to run video classification on a sample video input with the TimeSformer model and get the predicted class.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Process a set of random images as a video input and classify it using TimeSformer.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 1: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 2: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 3: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 4: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 5: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 6: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 8: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 9: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 10: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 11: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 12: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.\nTool 14: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 15: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 16: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 17: The MCG-NJU/videomae-base-finetuned-kinetics is a pre-trained video classification model, fine-tuned on the Kinetics-400 dataset. It uses the VideoMAE (Masked Autoencoders for Video Pre-training) architecture, which was trained for 1600 epochs in a self-supervised manner and further fine-tuned for supervised learning. The model achieves top-1 accuracy of 80.9% and top-5 accuracy of 94.7% on the Kinetics-400 dataset. It can be used for video classification tasks by processing video input with the VideoMAEImageProcessor and making predictions with the VideoMAEForVideoClassification model in a Hugging Face Transformers environment.\nTool 18: This tool provides a pretrained VideoMAE model fine-tuned for video classification on the Kinetics-400 dataset, enabling accurate identification of actions or events in video input. It leverages self-supervised pretraining combined with supervised fine-tuning to deliver high performance, making it suitable for applications requiring efficient and reliable video understanding.\nTool 19: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \\\"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\\\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\", \"parameters\": {\"images\": {\"type\": \"array\", \"description\": \"A list or sequence of video frames represented as arrays, where each frame is typically a 3D array (channels, height, width). This input is used by the model for video classification.\", \"required\": true, \"example\": [[[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[0.9, 1.0], [1.1, 1.2]]]]}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors the processor should return. Common values include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool\\u2019s response is an object representing the output of a video classification model. Its structure and components are as follows:\\n\\n1. **`logits`**  \\n   - This is a tensor containing the raw, unnormalized scores produced by the model for each class in the classification task.  \\n   - The shape typically corresponds to `[batch_size, num_classes]`, where `batch_size` is the number of input video samples and `num_classes` is the total number of target classes the model can predict.  \\n   - Higher values indicate stronger model confidence for the corresponding class, but they are not probabilities.\\n\\n2. **`predicted_class_idx`** (derived from `logits`)  \\n   - This is the integer index of the class with the highest score in `logits` for a given input video.  \\n   - It represents the model\\u2019s most likely prediction for that video.\\n\\n3. **`predicted_class_label`** (derived using `model.config.id2label`)  \\n   - This is a human-readable string corresponding to `predicted_class_idx`.  \\n   - It maps the integer class index to the semantic label defined in the model\\u2019s configuration (e.g., \\u201cpicking something up\\u201d or \\u201cthrowing an object\\u201d).\\n\\n4. **Optional additional fields**  \\n   - Depending on the framework version and configuration, the response object may also include intermediate outputs such as attention weights or hidden states, but these are typically optional and not returned by default.  \\n\\nIn summary, the primary semantic content of the response is the model\\u2019s **raw class scores (`logits`)** and the **predicted class** for the input video, with a mapping to a human-readable label for interpretation.\", \"examples\": [{\"user_intent\": \"Classify a video using the TimeSformer model pre-trained on the Something Something v2 dataset.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I want to run video classification on a sample video input with the TimeSformer model and get the predicted class.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Process a set of random images as a video input and classify it using TimeSformer.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nWe are a security company and we need a video classification model to analyze CCTV footage for suspicious activities.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify CCTV video footage to detect suspicious activities.",
                    "tool_description": "Use a pre-trained video classification model that leverages space-time attention mechanisms to analyze video frames and categorize actions or events, suitable for security monitoring and anomaly detection.",
                    "expected_response": "An object containing raw class scores (`logits`) for each action class, the predicted class index, and the corresponding human-readable predicted class label indicating the type of activity detected in the video."
                }
            ],
            "extracted_arguments": {
                "video_frames": "A sequence of video frames extracted from CCTV footage, represented as arrays (channels, height, width).",
                "return_tensors": "The type of tensor format to return, e.g., 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_333",
        "query": "Create software that extracts answers from input documents when given a set of questions.",
        "instruction": "Given a `document question answering` task, retrieve tools that can extract answers from input documents by processing both the textual content of the documents and a set of questions to deliver accurate responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.12.2",
                        "torch==1.8.0+cu101",
                        "datasets==1.14.0",
                        "tokenizers==0.10.3"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 1.194
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.",
                    "id": "huggingface_api_115",
                    "name": "layoutlmv2-base-uncased-finetuned-docvqa"
                },
                "id": "gorilla_huggingface_tool_114",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 1: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 2: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 3: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 4: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 5: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 6: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 7: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 8: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 9: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.",
            "Tool 10: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.",
            "Tool 11: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 12: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 13: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 14: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 15: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 17: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 18: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 19: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "What is the capital of Germany?"
                },
                "context": {
                    "type": "string",
                    "description": "The text or passage that contains the information needed to answer the question.",
                    "required": true,
                    "example": "Berlin is the capital of Germany."
                }
            },
            "response": "The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \n\n1. **`answer`** – A string representing the model’s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \n\n2. **`score`** – A numerical confidence score indicating the model’s certainty about the predicted answer. Higher values correspond to higher confidence.  \n\n3. **`start`** – An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \n\n4. **`end`** – An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \n\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.",
            "examples": [
                {
                    "user_intent": "What's the main ingredient in a Caesar salad according to this recipe?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "What is the main ingredient in a Caesar salad?",
                            "context": "A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing."
                        }
                    }
                },
                {
                    "user_intent": "Who wrote the novel 'Pride and Prejudice'?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "Who is the author of 'Pride and Prejudice'?",
                            "context": "'Pride and Prejudice' is a classic novel written by Jane Austen in 1813."
                        }
                    }
                },
                {
                    "user_intent": "When did the Apollo 11 mission land on the moon?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "When did Apollo 11 land on the moon?",
                            "context": "Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 1: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 2: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 3: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 4: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 5: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 6: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 7: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 8: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 9: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.\nTool 10: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.\nTool 11: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 12: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 13: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 14: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 15: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 17: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 18: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 19: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"What is the capital of Germany?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text or passage that contains the information needed to answer the question.\", \"required\": true, \"example\": \"Berlin is the capital of Germany.\"}}, \"response\": \"The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \\n\\n2. **`score`** \\u2013 A numerical confidence score indicating the model\\u2019s certainty about the predicted answer. Higher values correspond to higher confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \\n\\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.\", \"examples\": [{\"user_intent\": \"What's the main ingredient in a Caesar salad according to this recipe?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"What is the main ingredient in a Caesar salad?\", \"context\": \"A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing.\"}}}, {\"user_intent\": \"Who wrote the novel 'Pride and Prejudice'?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"Who is the author of 'Pride and Prejudice'?\", \"context\": \"'Pride and Prejudice' is a classic novel written by Jane Austen in 1813.\"}}}, {\"user_intent\": \"When did the Apollo 11 mission land on the moon?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"When did Apollo 11 land on the moon?\", \"context\": \"Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969.\"}}}]}\n\n# User Query\nCreate software that extracts answers from input documents when given a set of questions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a system that extracts answers from documents based on specific questions.",
                    "tool_description": "Develop a system that can retrieve precise answers from a given document or passage in response to natural language questions.",
                    "expected_response": "A collection of extracted answers, each including the text snippet from the document that is the most likely answer, along with its location and confidence score."
                }
            ],
            "extracted_arguments": {
                "questions": "A list of natural language questions that the user wants to be answered from the given document or context.",
                "documents": "The text or documents from which the answers are to be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_193",
        "query": "A user wants to improve the quality of a small sized picture from her vacation. We need to upscale the picture to 2x its size without losing quality.",
        "instruction": "Given an `image upscaling` task, retrieve tools that enhance images by increasing their resolution while maintaining quality by processing small-sized image inputs to produce higher resolution outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Super-Resolution",
                    "api_call": "Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')",
                    "api_arguments": "image, model, feature_extractor",
                    "python_environment_requirements": "transformers",
                    "example_code": "Refer to the documentation.",
                    "performance": {
                        "dataset": "arxiv: 2209.11345",
                        "accuracy": "Not provided"
                    },
                    "description": "Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.",
                    "id": "huggingface_api_273",
                    "name": "caidas/swin2SR-classical-sr-x2-64"
                },
                "id": "gorilla_huggingface_tool_269",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 1: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 2: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 3: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 4: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 5: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.",
            "Tool 6: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "Tool 7: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.",
            "Tool 8: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 9: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.",
            "Tool 10: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 11: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "Tool 12: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.",
            "Tool 13: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 14: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 15: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.",
            "Tool 16: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 17: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.",
            "Tool 18: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.",
            "Tool 19: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content."
        ],
        "tool_view": {
            "description": "This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The model identifier to be used for loading the pre-trained Stable Diffusion model. This points to the specific version of the model stored on Hugging Face's repository.",
                    "required": true,
                    "example": "CompVis/stable-diffusion-v1-4"
                },
                "vae": {
                    "type": "string",
                    "description": "The pre-trained VAE (Variational Autoencoder) to be used for decoding. This parameter specifies the VAE model, which helps improve the quality of generated images.",
                    "required": true,
                    "example": "AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema')"
                }
            },
            "response": "The tool described in the raw JSON is a multimodal text-to-image generation system, leveraging the Stable Diffusion model pipeline. Based on the given information, the response would likely provide an image generated from a textual input. Here's a description of the response structure and its components:\n\n1. **Generated Image**: \n   - The core of the response is the image produced by the model, based on the provided textual input. The image is the result of a text-to-image transformation where the model interprets the prompt and generates visual content accordingly.\n\n2. **Image Metadata** (optional):\n   - **Dimensions**: The resolution of the generated image, likely in the form of width and height (e.g., 256x256, 512x512).\n   - **Model Used**: Information about the specific model that generated the image (e.g., StableDiffusionPipeline from CompVis/stable-diffusion-v1-4).\n   - **VAE Information**: The decoder used (e.g., AutoencoderKL from stabilityai/sd-vae-ft-ema) might be noted, especially if it influences the style or quality of the generated image.\n\n3. **Performance Metrics** (optional, not always included in the output but possible for more technical feedback):\n   - Metrics such as accuracy (rFID, PSNR, SSIM, PSIM) might be included, indicating the quality of the generated image, especially in comparison with datasets like COCO 2017 or LAION-Aesthetics 5+. These metrics provide insights into the image’s fidelity, structural quality, and aesthetic value.\n\n4. **Processing Information** (optional):\n   - Details on the processing environment or any notable configurations used during generation could be included. This might involve the model's configuration, the hardware used for the generation, or any additional modifications to the default settings.\n\n5. **Error Handling** (if applicable):\n   - In case of an error during the generation process, an error message might be returned, describing what went wrong (e.g., invalid input format, failure to load the model, etc.).\n\nIn summary, the response would consist of the generated image and possibly accompanying metadata, such as model and VAE details, image dimensions, and optional performance metrics.",
            "examples": [
                {
                    "user_intent": "Generate a high-resolution fantasy landscape with mountains, waterfalls, and a sunset sky.",
                    "tool_call": {
                        "name": "stabilityai/sd-vae-ft-ema",
                        "arguments": {
                            "prompt": "A breathtaking fantasy landscape with towering mountains, cascading waterfalls, and a vivid sunset sky, ultra-detailed, cinematic lighting"
                        }
                    }
                },
                {
                    "user_intent": "Create a futuristic cityscape at night with neon lights and flying cars.",
                    "tool_call": {
                        "name": "stabilityai/sd-vae-ft-ema",
                        "arguments": {
                            "prompt": "A futuristic cyberpunk city at night, glowing neon signs, flying cars, rainy streets, highly detailed, cinematic perspective"
                        }
                    }
                },
                {
                    "user_intent": "Generate a realistic portrait of an elderly woman smiling in natural sunlight.",
                    "tool_call": {
                        "name": "stabilityai/sd-vae-ft-ema",
                        "arguments": {
                            "prompt": "A photorealistic portrait of an elderly woman smiling, soft natural sunlight, warm tones, ultra-realistic skin texture"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 1: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 2: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 3: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 4: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 5: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.\nTool 6: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\nTool 7: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.\nTool 8: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 9: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.\nTool 10: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 11: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\nTool 12: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.\nTool 13: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 14: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 15: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.\nTool 16: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 17: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.\nTool 18: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.\nTool 19: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The model identifier to be used for loading the pre-trained Stable Diffusion model. This points to the specific version of the model stored on Hugging Face's repository.\", \"required\": true, \"example\": \"CompVis/stable-diffusion-v1-4\"}, \"vae\": {\"type\": \"string\", \"description\": \"The pre-trained VAE (Variational Autoencoder) to be used for decoding. This parameter specifies the VAE model, which helps improve the quality of generated images.\", \"required\": true, \"example\": \"AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema')\"}}, \"response\": \"The tool described in the raw JSON is a multimodal text-to-image generation system, leveraging the Stable Diffusion model pipeline. Based on the given information, the response would likely provide an image generated from a textual input. Here's a description of the response structure and its components:\\n\\n1. **Generated Image**: \\n   - The core of the response is the image produced by the model, based on the provided textual input. The image is the result of a text-to-image transformation where the model interprets the prompt and generates visual content accordingly.\\n\\n2. **Image Metadata** (optional):\\n   - **Dimensions**: The resolution of the generated image, likely in the form of width and height (e.g., 256x256, 512x512).\\n   - **Model Used**: Information about the specific model that generated the image (e.g., StableDiffusionPipeline from CompVis/stable-diffusion-v1-4).\\n   - **VAE Information**: The decoder used (e.g., AutoencoderKL from stabilityai/sd-vae-ft-ema) might be noted, especially if it influences the style or quality of the generated image.\\n\\n3. **Performance Metrics** (optional, not always included in the output but possible for more technical feedback):\\n   - Metrics such as accuracy (rFID, PSNR, SSIM, PSIM) might be included, indicating the quality of the generated image, especially in comparison with datasets like COCO 2017 or LAION-Aesthetics 5+. These metrics provide insights into the image\\u2019s fidelity, structural quality, and aesthetic value.\\n\\n4. **Processing Information** (optional):\\n   - Details on the processing environment or any notable configurations used during generation could be included. This might involve the model's configuration, the hardware used for the generation, or any additional modifications to the default settings.\\n\\n5. **Error Handling** (if applicable):\\n   - In case of an error during the generation process, an error message might be returned, describing what went wrong (e.g., invalid input format, failure to load the model, etc.).\\n\\nIn summary, the response would consist of the generated image and possibly accompanying metadata, such as model and VAE details, image dimensions, and optional performance metrics.\", \"examples\": [{\"user_intent\": \"Generate a high-resolution fantasy landscape with mountains, waterfalls, and a sunset sky.\", \"tool_call\": {\"name\": \"stabilityai/sd-vae-ft-ema\", \"arguments\": {\"prompt\": \"A breathtaking fantasy landscape with towering mountains, cascading waterfalls, and a vivid sunset sky, ultra-detailed, cinematic lighting\"}}}, {\"user_intent\": \"Create a futuristic cityscape at night with neon lights and flying cars.\", \"tool_call\": {\"name\": \"stabilityai/sd-vae-ft-ema\", \"arguments\": {\"prompt\": \"A futuristic cyberpunk city at night, glowing neon signs, flying cars, rainy streets, highly detailed, cinematic perspective\"}}}, {\"user_intent\": \"Generate a realistic portrait of an elderly woman smiling in natural sunlight.\", \"tool_call\": {\"name\": \"stabilityai/sd-vae-ft-ema\", \"arguments\": {\"prompt\": \"A photorealistic portrait of an elderly woman smiling, soft natural sunlight, warm tones, ultra-realistic skin texture\"}}}]}\n\n# User Query\nA user wants to improve the quality of a small sized picture from her vacation. We need to upscale the picture to 2x its size without losing quality.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to upscale a small-sized vacation photo to twice its original size while preserving quality.",
                    "tool_description": "Enhance the resolution of an input image using a Stable Diffusion-based latent upscaler, applying optional noise control to refine details and produce a higher-quality output.",
                    "expected_response": "An image file with doubled dimensions compared to the original, maintaining visual clarity and detail."
                }
            ],
            "extracted_arguments": {
                "input_image": "The original image file that needs to be upscaled.",
                "scale_factor": "The multiplier for enlarging the image dimensions, specified as a numeric value."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_375",
        "query": "Our client has a medical report and we are trying to assist him in finding relevant information.",
        "instruction": "Given a `medical report analysis` task, retrieve tools that assist in extracting relevant information from clinical documents by utilizing natural language processing techniques to enhance decision-making based on medical data.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')",
                    "api_arguments": [
                        "AutoTokenizer",
                        "AutoModel",
                        "from_pretrained"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\nmodel = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')",
                    "performance": {
                        "dataset": "MIMIC III",
                        "accuracy": "Not provided"
                    },
                    "description": "Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI).",
                    "id": "huggingface_api_674",
                    "name": "emilyalsentzer/Bio_ClinicalBERT"
                },
                "id": "gorilla_huggingface_tool_669",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 1: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.",
            "Tool 2: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 3: This tool uses the \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" model to convert sentences and paragraphs into 384-dimensional vector embeddings, which can be applied to tasks such as clustering and semantic search. It is based on the Hugging Face Transformers framework and is useful for comparing sentence similarity or finding related content in large datasets.",
            "Tool 4: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 5: A Hugging Face Transformers image classification model fine-tuned for detecting diabetic retinopathy from medical images, enabling automated analysis of retinal scans with an accuracy of 77.44%.",
            "Tool 6: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 7: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 8: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 9: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 10: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 11: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "Tool 12: The `facebook/dpr-question_encoder-single-nq-base` is a Dense Passage Retrieval (DPR) question encoder that converts natural language questions into dense vector embeddings for open-domain question answering. Trained on the Natural Questions (NQ) dataset, it is designed to capture semantic meaning of queries, enabling efficient retrieval of relevant passages from large text corpora. This model is used in NLP systems to improve the accuracy and speed of information retrieval in applications like search engines and QA pipelines.",
            "Tool 13: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 14: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 15: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.",
            "Tool 16: A text-generation model built with Hugging Face Transformers that combines GPT-J-6B-Janeway, PPO_HH_GPT-J, and Pygmalion-6b-DEV to generate natural language, suitable for tasks requiring coherent and contextually relevant text output.",
            "Tool 17: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 18: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 19: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications."
        ],
        "tool_view": {
            "description": "A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pretrained token classification model to load for named entity recognition, specified by its model identifier.",
                    "required": true,
                    "example": "d4data/biomedical-ner-all"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.",
                    "required": true,
                    "example": "d4data/biomedical-ner-all"
                },
                "aggregation_strategy": {
                    "type": "string",
                    "description": "The strategy to aggregate token-level predictions into entity-level results. Common strategies include 'simple', 'first', 'average', etc.",
                    "required": true,
                    "example": "simple"
                }
            },
            "response": "The tool produces a **Named Entity Recognition (NER) response** for a given input text. The response is typically structured as a sequence of entities detected in the text, with each entity containing several key pieces of information:\n\n1. **Entity text or span** – The exact substring from the input text that has been recognized as an entity.  \n2. **Entity type or label** – The category or class of the entity, corresponding to one of the 107 biomedical entity types the model was trained to recognize. Examples include medical conditions, medications, procedures, or anatomical terms.  \n3. **Start and end positions** – Integer indices indicating the position of the entity text within the input string. The `start` index marks the first character of the entity, and the `end` index marks the position immediately after the last character of the entity.  \n4. **Score or confidence** – A numeric value representing the model’s confidence in the predicted entity label. This is usually a probability between 0 and 1.  \n\nBecause the tool is used with an **aggregation strategy of \"simple\"**, overlapping or subword tokens may be merged so that each entity in the response corresponds to a coherent, human-readable span rather than fragmented subword tokens.\n\nOverall, the response is a list of such entity dictionaries, providing a structured identification of biomedical entities present in the input text.",
            "examples": [
                {
                    "user_intent": "I want to extract all biomedical entities like diseases, medications, and procedures from a patient's clinical note.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "The patient was prescribed 5 mg of amlodipine daily and showed improvement in hypertension symptoms within 2 weeks."
                        }
                    }
                },
                {
                    "user_intent": "Analyze a set of clinical trial abstracts to automatically identify genes, proteins, and other biomedical entities.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "BRCA1 mutations are linked to a higher risk of breast cancer, and patients often undergo genetic counseling before starting therapy."
                        }
                    }
                },
                {
                    "user_intent": "Detect and highlight any medical procedures and conditions mentioned in a cardiology case report.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "After undergoing catheter ablation, the patient reported no recurrence of palpitations at the 6-month follow-up."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 1: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.\nTool 2: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 3: This tool uses the \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" model to convert sentences and paragraphs into 384-dimensional vector embeddings, which can be applied to tasks such as clustering and semantic search. It is based on the Hugging Face Transformers framework and is useful for comparing sentence similarity or finding related content in large datasets.\nTool 4: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 5: A Hugging Face Transformers image classification model fine-tuned for detecting diabetic retinopathy from medical images, enabling automated analysis of retinal scans with an accuracy of 77.44%.\nTool 6: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 7: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 8: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 9: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 10: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 11: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\nTool 12: The `facebook/dpr-question_encoder-single-nq-base` is a Dense Passage Retrieval (DPR) question encoder that converts natural language questions into dense vector embeddings for open-domain question answering. Trained on the Natural Questions (NQ) dataset, it is designed to capture semantic meaning of queries, enabling efficient retrieval of relevant passages from large text corpora. This model is used in NLP systems to improve the accuracy and speed of information retrieval in applications like search engines and QA pipelines.\nTool 13: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 14: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 15: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.\nTool 16: A text-generation model built with Hugging Face Transformers that combines GPT-J-6B-Janeway, PPO_HH_GPT-J, and Pygmalion-6b-DEV to generate natural language, suitable for tasks requiring coherent and contextually relevant text output.\nTool 17: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 18: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 19: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pretrained token classification model to load for named entity recognition, specified by its model identifier.\", \"required\": true, \"example\": \"d4data/biomedical-ner-all\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.\", \"required\": true, \"example\": \"d4data/biomedical-ner-all\"}, \"aggregation_strategy\": {\"type\": \"string\", \"description\": \"The strategy to aggregate token-level predictions into entity-level results. Common strategies include 'simple', 'first', 'average', etc.\", \"required\": true, \"example\": \"simple\"}}, \"response\": \"The tool produces a **Named Entity Recognition (NER) response** for a given input text. The response is typically structured as a sequence of entities detected in the text, with each entity containing several key pieces of information:\\n\\n1. **Entity text or span** \\u2013 The exact substring from the input text that has been recognized as an entity.  \\n2. **Entity type or label** \\u2013 The category or class of the entity, corresponding to one of the 107 biomedical entity types the model was trained to recognize. Examples include medical conditions, medications, procedures, or anatomical terms.  \\n3. **Start and end positions** \\u2013 Integer indices indicating the position of the entity text within the input string. The `start` index marks the first character of the entity, and the `end` index marks the position immediately after the last character of the entity.  \\n4. **Score or confidence** \\u2013 A numeric value representing the model\\u2019s confidence in the predicted entity label. This is usually a probability between 0 and 1.  \\n\\nBecause the tool is used with an **aggregation strategy of \\\"simple\\\"**, overlapping or subword tokens may be merged so that each entity in the response corresponds to a coherent, human-readable span rather than fragmented subword tokens.\\n\\nOverall, the response is a list of such entity dictionaries, providing a structured identification of biomedical entities present in the input text.\", \"examples\": [{\"user_intent\": \"I want to extract all biomedical entities like diseases, medications, and procedures from a patient's clinical note.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"The patient was prescribed 5 mg of amlodipine daily and showed improvement in hypertension symptoms within 2 weeks.\"}}}, {\"user_intent\": \"Analyze a set of clinical trial abstracts to automatically identify genes, proteins, and other biomedical entities.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"BRCA1 mutations are linked to a higher risk of breast cancer, and patients often undergo genetic counseling before starting therapy.\"}}}, {\"user_intent\": \"Detect and highlight any medical procedures and conditions mentioned in a cardiology case report.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"After undergoing catheter ablation, the patient reported no recurrence of palpitations at the 6-month follow-up.\"}}}]}\n\n# User Query\nOur client has a medical report and we are trying to assist him in finding relevant information.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract all relevant biomedical entities from the client's medical report.",
                    "tool_description": "Use a Named Entity Recognition (NER) model to identify and classify biomedical entities such as diseases, medications, procedures, and anatomical terms within unstructured medical text.",
                    "expected_response": "A structured list of entities detected in the text, including entity text, entity type, start and end positions, and confidence scores."
                }
            ],
            "extracted_arguments": {
                "text": "The full content of the client's medical report to be analyzed for biomedical entities.",
                "model": "The pretrained NER model identifier for extracting biomedical entities.",
                "tokenizer": "The pretrained tokenizer corresponding to the NER model.",
                "aggregation_strategy": "The strategy to combine token-level predictions into entity-level results."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_434",
        "query": "We need to summarize a scientific article. The input must include all the important points discussed in the article, and the result should be a concise abstraction of the content.",
        "instruction": "Given a `summarization` task, retrieve tools that process extensive textual inputs to generate concise and coherent abstract summaries, ensuring the inclusion of critical points from the original content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Summarization",
                    "api_call": "pipeline('summarization', model='google/pegasus-large')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline('summarization', model='google/pegasus-large')\nsummary = summarizer('your_text_here')",
                    "performance": {
                        "dataset": [
                            {
                                "name": "xsum",
                                "accuracy": "47.60/24.83/39.64"
                            },
                            {
                                "name": "cnn_dailymail",
                                "accuracy": "44.16/21.56/41.30"
                            },
                            {
                                "name": "newsroom",
                                "accuracy": "45.98/34.20/42.18"
                            },
                            {
                                "name": "multi_news",
                                "accuracy": "47.65/18.75/24.95"
                            },
                            {
                                "name": "gigaword",
                                "accuracy": "39.65/20.47/36.76"
                            },
                            {
                                "name": "wikihow",
                                "accuracy": "46.39/22.12/38.41"
                            },
                            {
                                "name": "reddit_tifu",
                                "accuracy": "27.99/9.81/22.94"
                            },
                            {
                                "name": "big_patent",
                                "accuracy": "52.29/33.08/41.66"
                            },
                            {
                                "name": "arxiv",
                                "accuracy": "44.21/16.95/25.67"
                            },
                            {
                                "name": "pubmed",
                                "accuracy": "45.97/20.15/28.25"
                            },
                            {
                                "name": "aeslc",
                                "accuracy": "37.68/21.25/36.51"
                            },
                            {
                                "name": "billsum",
                                "accuracy": "59.67/41.58/47.59"
                            }
                        ]
                    },
                    "description": "google/pegasus-large is a pre-trained model for abstractive text summarization based on the PEGASUS architecture. It is trained on a mixture of C4 and HugeNews datasets and uses a sentencepiece tokenizer that can encode newline characters. The model has been fine-tuned for various summarization tasks and achieves state-of-the-art performance on multiple benchmarks.",
                    "id": "huggingface_api_558",
                    "name": "google/pegasus-large"
                },
                "id": "gorilla_huggingface_tool_554",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 1: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 4: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 5: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 6: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "Tool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 8: csebuetnlp/mT5_multilingual_XLSum is a multilingual abstractive summarization model fine-tuned on the XL-Sum dataset, capable of generating concise summaries from input text in 43 languages. It uses a text-to-text generation approach to condense long documents into shorter, coherent summaries, making it useful for quickly extracting key information from multilingual content.",
            "Tool 9: A pretrained PEGASUS model for abstractive text summarization, designed to generate concise summaries of input text using the CNN/DailyMail dataset.",
            "Tool 10: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.",
            "Tool 11: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 12: A German text summarization tool based on a fine-tuned T5 model, designed to generate concise summaries of input text using the Hugging Face Transformers framework.",
            "Tool 13: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 14: A Spanish text summarization tool based on BERT2BERT (BETO) fine-tuned on the MLSUM dataset, which generates concise summaries of input text using a Hugging Face Transformers sequence-to-sequence model.",
            "Tool 15: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 16: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 17: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 18: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.",
            "Tool 19: A T5-based text-to-text summarization model fine-tuned on the Samsum dataset, designed to generate concise summaries from input text without requiring explicit prompts. It can be used for natural language processing tasks that require transforming longer text into shorter, coherent summaries while maintaining key information."
        ],
        "tool_view": {
            "description": "The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "parameters": {},
            "response": "The tool’s response is not explicitly defined in the JSON, but based on its functionality as an abstractive summarization model using the Hugging Face Transformers framework, a typical response would have the following structure:\n\n- **summary_text**: A string containing the generated abstractive summary of the input text. This is the main output produced by the PEGASUS model. The summary condenses the input content while attempting to preserve the most important information and meaning.\n\nOptional or framework-specific components (depending on how the model is called) may include:\n\n- **input_tokens / tokenized_input**: A representation of the input text after tokenization, often as a list or array of token IDs used internally by the model.\n- **output_tokens / tokenized_output**: A list or array of token IDs representing the generated summary before decoding back into text.\n- **metadata / model_info**: Information about the generation process, such as attention scores, generation length, or decoding parameters (e.g., beam size, sampling temperature) if explicitly returned.\n- **confidence_scores / log_probs**: Optional numerical scores indicating the model’s confidence for each generated token or overall sequence likelihood.\n\nSemantically, the core of the response is the **summary_text**, while the additional fields provide internal or diagnostic information useful for analysis or further processing. The response is intended to allow downstream applications to obtain a concise, coherent summary of the original input text.",
            "examples": [
                {
                    "user_intent": "Summarize a long PubMed research article into a concise abstract.",
                    "tool_call": {
                        "name": "google/pegasus-pubmed",
                        "arguments": {
                            "input_text": "The study investigates the effects of a new anti-cancer drug on tumor growth in mice over a 12-week period. Various biomarkers were monitored, and significant reductions in tumor size were observed...",
                            "max_length": 150,
                            "min_length": 50,
                            "do_sample": true
                        }
                    }
                },
                {
                    "user_intent": "Generate a brief summary of a collection of biomedical case reports for quick reading.",
                    "tool_call": {
                        "name": "google/pegasus-pubmed",
                        "arguments": {
                            "input_text": "Case report 1: Patient presented with acute liver failure following drug exposure. Case report 2: A rare genetic mutation caused sudden cardiac arrest in a teenager...",
                            "max_length": 120,
                            "min_length": 40,
                            "do_sample": true
                        }
                    }
                },
                {
                    "user_intent": "Condense a PubMed review paper into key sentences highlighting main findings.",
                    "tool_call": {
                        "name": "google/pegasus-pubmed",
                        "arguments": {
                            "input_text": "This review summarizes recent advances in CRISPR-Cas9 gene editing technologies, including applications in disease modeling, therapeutic interventions, and ethical considerations in clinical trials...",
                            "max_length": 200,
                            "min_length": 60,
                            "do_sample": false
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 1: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 4: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 5: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 6: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\nTool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 8: csebuetnlp/mT5_multilingual_XLSum is a multilingual abstractive summarization model fine-tuned on the XL-Sum dataset, capable of generating concise summaries from input text in 43 languages. It uses a text-to-text generation approach to condense long documents into shorter, coherent summaries, making it useful for quickly extracting key information from multilingual content.\nTool 9: A pretrained PEGASUS model for abstractive text summarization, designed to generate concise summaries of input text using the CNN/DailyMail dataset.\nTool 10: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.\nTool 11: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 12: A German text summarization tool based on a fine-tuned T5 model, designed to generate concise summaries of input text using the Hugging Face Transformers framework.\nTool 13: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 14: A Spanish text summarization tool based on BERT2BERT (BETO) fine-tuned on the MLSUM dataset, which generates concise summaries of input text using a Hugging Face Transformers sequence-to-sequence model.\nTool 15: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 16: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 17: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 18: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.\nTool 19: A T5-based text-to-text summarization model fine-tuned on the Samsum dataset, designed to generate concise summaries from input text without requiring explicit prompts. It can be used for natural language processing tasks that require transforming longer text into shorter, coherent summaries while maintaining key information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"google/pegasus-pubmed\\\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\", \"parameters\": {}, \"response\": \"The tool\\u2019s response is not explicitly defined in the JSON, but based on its functionality as an abstractive summarization model using the Hugging Face Transformers framework, a typical response would have the following structure:\\n\\n- **summary_text**: A string containing the generated abstractive summary of the input text. This is the main output produced by the PEGASUS model. The summary condenses the input content while attempting to preserve the most important information and meaning.\\n\\nOptional or framework-specific components (depending on how the model is called) may include:\\n\\n- **input_tokens / tokenized_input**: A representation of the input text after tokenization, often as a list or array of token IDs used internally by the model.\\n- **output_tokens / tokenized_output**: A list or array of token IDs representing the generated summary before decoding back into text.\\n- **metadata / model_info**: Information about the generation process, such as attention scores, generation length, or decoding parameters (e.g., beam size, sampling temperature) if explicitly returned.\\n- **confidence_scores / log_probs**: Optional numerical scores indicating the model\\u2019s confidence for each generated token or overall sequence likelihood.\\n\\nSemantically, the core of the response is the **summary_text**, while the additional fields provide internal or diagnostic information useful for analysis or further processing. The response is intended to allow downstream applications to obtain a concise, coherent summary of the original input text.\", \"examples\": [{\"user_intent\": \"Summarize a long PubMed research article into a concise abstract.\", \"tool_call\": {\"name\": \"google/pegasus-pubmed\", \"arguments\": {\"input_text\": \"The study investigates the effects of a new anti-cancer drug on tumor growth in mice over a 12-week period. Various biomarkers were monitored, and significant reductions in tumor size were observed...\", \"max_length\": 150, \"min_length\": 50, \"do_sample\": true}}}, {\"user_intent\": \"Generate a brief summary of a collection of biomedical case reports for quick reading.\", \"tool_call\": {\"name\": \"google/pegasus-pubmed\", \"arguments\": {\"input_text\": \"Case report 1: Patient presented with acute liver failure following drug exposure. Case report 2: A rare genetic mutation caused sudden cardiac arrest in a teenager...\", \"max_length\": 120, \"min_length\": 40, \"do_sample\": true}}}, {\"user_intent\": \"Condense a PubMed review paper into key sentences highlighting main findings.\", \"tool_call\": {\"name\": \"google/pegasus-pubmed\", \"arguments\": {\"input_text\": \"This review summarizes recent advances in CRISPR-Cas9 gene editing technologies, including applications in disease modeling, therapeutic interventions, and ethical considerations in clinical trials...\", \"max_length\": 200, \"min_length\": 60, \"do_sample\": false}}}]}\n\n# User Query\nWe need to summarize a scientific article. The input must include all the important points discussed in the article, and the result should be a concise abstraction of the content.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Summarize a scientific article into a concise abstract highlighting all important points.",
                    "tool_description": "Use a pre-trained PEGASUS model fine-tuned on medical and scientific datasets to perform abstractive summarization, generating a concise summary that captures the key information from the full text.",
                    "expected_response": "An object containing a string field 'summary_text' with the generated concise abstract of the scientific article."
                }
            ],
            "extracted_arguments": {
                "input_text": "The full text content of the scientific article to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_88",
        "query": "Can you initiate a conversation with an AI model that plays the role of a friend who just got back from a conference?",
        "instruction": "Given a `conversational AI initiation` task, retrieve tools designed for conversational interactions that emulate a specified role. These tools should generate dialogue based on contextual narratives, role instructions, and conversation history to engage realistically with users.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Conversational",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "allenai/cosmo-xl"
                    },
                    "python_environment_requirements": {
                        "torch": "latest",
                        "transformers": "latest"
                    },
                    "example_code": {
                        "import": [
                            "import torch",
                            "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
                        ],
                        "initialize": [
                            "device = torch.device(cuda if torch.cuda.is_available() else cpu)",
                            "tokenizer = AutoTokenizer.from_pretrained(allenai/cosmo-xl)",
                            "model = AutoModelForSeq2SeqLM.from_pretrained(allenai/cosmo-xl).to(device)"
                        ],
                        "example": [
                            "def set_input(situation_narrative, role_instruction, conversation_history):",
                            " input_text =  <turn> .join(conversation_history)",
                            "if role_instruction != :",
                            " input_text = {} &lt;sep&gt; {}'.format(role_instruction, input_text)",
                            "if situation_narrative != :",
                            " input_text = {} &lt;sep&gt; {}'.format(situation_narrative, input_text)",
                            "return input_text",
                            "def generate(situation_narrative, role_instruction, conversation_history):",
                            " input_text = set_input(situation_narrative, role_instruction, conversation_history)",
                            " inputs = tokenizer([input_text], return_tensors=pt).to(device)",
                            " outputs = model.generate(inputs[input_ids], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)",
                            " response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)",
                            " return response",
                            "situation = Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.",
                            "instruction = You are Cosmo and you are talking to a friend.",
                            "conversation = [",
                            " Hey, how was your trip to Abu Dhabi?",
                            "]",
                            "response = generate(situation, instruction, conversation)",
                            "print(response)"
                        ]
                    },
                    "performance": {
                        "dataset": {
                            "allenai/soda": "",
                            "allenai/prosocial-dialog": ""
                        },
                        "accuracy": ""
                    },
                    "description": "COSMO is a conversation agent with greater generalizability on both in- and out-of-domain chitchat datasets (e.g., DailyDialog, BlendedSkillTalk). It is trained on two datasets: SODA and ProsocialDialog. COSMO is especially aiming to model natural human conversations. It can accept situation descriptions as well as instructions on what role it should play in the situation.",
                    "id": "huggingface_api_604",
                    "name": "allenai/cosmo-xl"
                },
                "id": "gorilla_huggingface_tool_599",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 2: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 3: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 4: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 5: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 6: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 7: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.",
            "Tool 8: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 9: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 10: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 11: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 12: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 13: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 14: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 15: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 16: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 17: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 18: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 19: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions."
        ],
        "tool_view": {
            "description": "COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pretrained model to load. This specifies which model weights to use for the conversation agent.",
                    "required": true,
                    "example": "allenai/cosmo-xl"
                }
            },
            "response": "The tool produces a generated conversational text response. The structure and meaning of its response can be described as follows:\n\n- **Response type:** A single string of text representing the model’s generated utterance.  \n- **Semantic content:** The text corresponds to the model’s continuation of a conversation given three inputs:\n  1. **Situation narrative** – a description of the scenario or context in which the conversation is taking place.\n  2. **Role instruction** – guidance about the persona, character, or role the model should adopt in the conversation.\n  3. **Conversation history** – a sequence of prior conversational turns, which the model uses to maintain context and coherence.\n\n- **Purpose of the response:** The generated text simulates a natural human conversational reply, reflecting the role specified and responding appropriately to the context and prior conversation. It is designed to handle both in-domain and out-of-domain conversational scenarios.  \n\n- **Optional characteristics of the response:** Depending on generation parameters (e.g., `temperature`, `top_p`, `max_new_tokens`), the response may vary in creativity, length, and specificity, but it will always be a coherent conversational utterance intended to continue the dialogue naturally.  \n\nIn summary, the tool returns a single string containing the model-generated conversational reply, conditioned on the provided situation, role, and conversation history.",
            "examples": [
                {
                    "user_intent": "Simulate a conversation where Cosmo is a helpful customer service agent assisting a user with a billing issue.",
                    "tool_call": {
                        "name": "allenai/cosmo-xl",
                        "arguments": {
                            "situation_narrative": "Cosmo is acting as a customer service representative helping a user resolve a billing problem.",
                            "role_instruction": "You are Cosmo, a friendly and patient customer service agent.",
                            "conversation_history": [
                                "Hello, I noticed an unexpected charge on my account. Can you help me with this?"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Have Cosmo chat informally with a friend about a recent vacation to Paris.",
                    "tool_call": {
                        "name": "allenai/cosmo-xl",
                        "arguments": {
                            "situation_narrative": "Cosmo recently returned from a vacation to Paris and is sharing the experience with a close friend.",
                            "role_instruction": "You are Cosmo, casually chatting with your friend about your trip.",
                            "conversation_history": [
                                "Hey! How was your trip to Paris?"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Generate a dialogue where Cosmo plays the role of a motivational coach encouraging someone to pursue their goals.",
                    "tool_call": {
                        "name": "allenai/cosmo-xl",
                        "arguments": {
                            "situation_narrative": "Cosmo is a motivational coach giving supportive advice to a client feeling uncertain about their future.",
                            "role_instruction": "You are Cosmo, a positive and encouraging life coach.",
                            "conversation_history": [
                                "I'm not sure if I have what it takes to start my own business."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 2: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 3: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 4: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 5: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 6: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 7: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.\nTool 8: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 9: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 10: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 11: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 12: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 13: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 14: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 15: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 16: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 17: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 18: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 19: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained model to load. This specifies which model weights to use for the conversation agent.\", \"required\": true, \"example\": \"allenai/cosmo-xl\"}}, \"response\": \"The tool produces a generated conversational text response. The structure and meaning of its response can be described as follows:\\n\\n- **Response type:** A single string of text representing the model\\u2019s generated utterance.  \\n- **Semantic content:** The text corresponds to the model\\u2019s continuation of a conversation given three inputs:\\n  1. **Situation narrative** \\u2013 a description of the scenario or context in which the conversation is taking place.\\n  2. **Role instruction** \\u2013 guidance about the persona, character, or role the model should adopt in the conversation.\\n  3. **Conversation history** \\u2013 a sequence of prior conversational turns, which the model uses to maintain context and coherence.\\n\\n- **Purpose of the response:** The generated text simulates a natural human conversational reply, reflecting the role specified and responding appropriately to the context and prior conversation. It is designed to handle both in-domain and out-of-domain conversational scenarios.  \\n\\n- **Optional characteristics of the response:** Depending on generation parameters (e.g., `temperature`, `top_p`, `max_new_tokens`), the response may vary in creativity, length, and specificity, but it will always be a coherent conversational utterance intended to continue the dialogue naturally.  \\n\\nIn summary, the tool returns a single string containing the model-generated conversational reply, conditioned on the provided situation, role, and conversation history.\", \"examples\": [{\"user_intent\": \"Simulate a conversation where Cosmo is a helpful customer service agent assisting a user with a billing issue.\", \"tool_call\": {\"name\": \"allenai/cosmo-xl\", \"arguments\": {\"situation_narrative\": \"Cosmo is acting as a customer service representative helping a user resolve a billing problem.\", \"role_instruction\": \"You are Cosmo, a friendly and patient customer service agent.\", \"conversation_history\": [\"Hello, I noticed an unexpected charge on my account. Can you help me with this?\"]}}}, {\"user_intent\": \"Have Cosmo chat informally with a friend about a recent vacation to Paris.\", \"tool_call\": {\"name\": \"allenai/cosmo-xl\", \"arguments\": {\"situation_narrative\": \"Cosmo recently returned from a vacation to Paris and is sharing the experience with a close friend.\", \"role_instruction\": \"You are Cosmo, casually chatting with your friend about your trip.\", \"conversation_history\": [\"Hey! How was your trip to Paris?\"]}}}, {\"user_intent\": \"Generate a dialogue where Cosmo plays the role of a motivational coach encouraging someone to pursue their goals.\", \"tool_call\": {\"name\": \"allenai/cosmo-xl\", \"arguments\": {\"situation_narrative\": \"Cosmo is a motivational coach giving supportive advice to a client feeling uncertain about their future.\", \"role_instruction\": \"You are Cosmo, a positive and encouraging life coach.\", \"conversation_history\": [\"I'm not sure if I have what it takes to start my own business.\"]}}}]}\n\n# User Query\nCan you initiate a conversation with an AI model that plays the role of a friend who just got back from a conference?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Start a conversation with an AI model acting as a friend returning from a conference.",
                    "tool_description": "Generate a conversational text response using a model that simulates a human-like friend character, taking into account the context of just returning from a conference, role instructions, and conversation history.",
                    "expected_response": "A single string of text representing the AI friend's response, reflecting the role of a friend who just returned from a conference and continuing the dialogue naturally."
                }
            ],
            "extracted_arguments": {
                "situation_narrative": "A description of the scenario in which the AI friend has just returned from a conference.",
                "role_instruction": "Instructions specifying that the AI should adopt the persona of a friendly, conversational friend.",
                "conversation_history": "A list of previous dialogue turns to maintain context, starting with the user's initial greeting or inquiry."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_437",
        "query": "Design an explanation on how to use Pix2Struct to analyze and generate text based on visuals such as graphs and charts.",
        "instruction": "Given a `model explanation` task, retrieve tools that provide detailed insights and methods for utilizing Pix2Struct in tasks involving visual data analysis, such as parsing charts and graphs, by outlining its functionalities, application procedures, and potential use cases.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')",
                    "api_arguments": [
                        "t5x_checkpoint_path",
                        "pytorch_dump_path",
                        "use-large"
                    ],
                    "python_environment_requirements": "transformers",
                    "example_code": "python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE",
                    "performance": {
                        "dataset": "ChartQA",
                        "accuracy": "Not provided"
                    },
                    "description": "Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.",
                    "id": "huggingface_api_78",
                    "name": "google/pix2struct-chartqa-base"
                },
                "id": "gorilla_huggingface_tool_78",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 1: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.",
            "Tool 2: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 3: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 4: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 5: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 6: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.",
            "Tool 7: DePlot is a model that converts images of plots or charts into structured, linearized tables by first translating the visual content into text and then reasoning over that text. This enables users to extract underlying data from visualizations, which can be directly used to prompt large language models for further analysis or reasoning, making it useful for tasks that combine visual understanding with text-based data interpretation.",
            "Tool 8: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 9: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 10: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 11: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.",
            "Tool 12: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 13: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.",
            "Tool 14: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 15: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 16: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 17: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.",
            "Tool 18: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 19: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text."
        ],
        "tool_view": {
            "description": "Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description or phrase that guides the image generation. The model will create an image that matches the content and style of this prompt.",
                    "required": true,
                    "example": "a futuristic cityscape at sunset"
                }
            },
            "response": "The tool described here is a text-to-image generation model, specifically the Stable Diffusion v1.4 model. This tool accepts a textual description (prompt) as input and generates an image based on that description. The expected response from the tool, when the text input (prompt) is provided, would contain an image generated by the model. \n\nThe structure and components of the response would likely include:\n\n1. **Generated Image**:\n   - The response will include the image generated by the model, which is based on the text prompt provided. The image will be in a format appropriate for display or further processing (such as a PIL image object in Python).\n   - This image will represent the best possible interpretation of the prompt in the form of a photo-realistic image (as per the capabilities of Stable Diffusion).\n\n2. **Metadata (potentially)**:\n   - If specified, additional metadata about the generation process might be included. For instance, this could include information about the model's settings, version, and processing time, but this is not explicitly mentioned in the documentation and is therefore optional.\n\n### Breakdown of the Image Generation Process:\n- **Prompt**: The input text (e.g., \"a photo of an astronaut riding a horse on mars\") that directs the model on what to generate.\n- **Model Output**: The tool generates an image that represents the described scenario, which is returned to the user.\n\nThe key element of the response is the image, with the potential for some metadata describing the output process.",
            "examples": [
                {
                    "user_intent": "Create a realistic image of a futuristic city skyline at sunset.",
                    "tool_call": {
                        "name": "CompVis/stable-diffusion-v1-4",
                        "arguments": {
                            "prompt": "A futuristic city skyline at sunset, highly detailed, photorealistic, with flying cars and neon lights"
                        }
                    }
                },
                {
                    "user_intent": "Generate an artistic illustration of a dragon perched on a mountain peak.",
                    "tool_call": {
                        "name": "CompVis/stable-diffusion-v1-4",
                        "arguments": {
                            "prompt": "An epic fantasy illustration of a dragon perched on a mountain peak, dramatic lighting, detailed scales, concept art style"
                        }
                    }
                },
                {
                    "user_intent": "Produce a cute cartoon-style image of a cat playing with yarn.",
                    "tool_call": {
                        "name": "CompVis/stable-diffusion-v1-4",
                        "arguments": {
                            "prompt": "A cute cartoon cat playing with a ball of yarn, colorful, playful, in a children's book illustration style"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 1: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.\nTool 2: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 3: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 4: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 5: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 6: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.\nTool 7: DePlot is a model that converts images of plots or charts into structured, linearized tables by first translating the visual content into text and then reasoning over that text. This enables users to extract underlying data from visualizations, which can be directly used to prompt large language models for further analysis or reasoning, making it useful for tasks that combine visual understanding with text-based data interpretation.\nTool 8: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 9: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 10: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 11: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.\nTool 12: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 13: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.\nTool 14: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 15: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 16: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 17: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.\nTool 18: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 19: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description or phrase that guides the image generation. The model will create an image that matches the content and style of this prompt.\", \"required\": true, \"example\": \"a futuristic cityscape at sunset\"}}, \"response\": \"The tool described here is a text-to-image generation model, specifically the Stable Diffusion v1.4 model. This tool accepts a textual description (prompt) as input and generates an image based on that description. The expected response from the tool, when the text input (prompt) is provided, would contain an image generated by the model. \\n\\nThe structure and components of the response would likely include:\\n\\n1. **Generated Image**:\\n   - The response will include the image generated by the model, which is based on the text prompt provided. The image will be in a format appropriate for display or further processing (such as a PIL image object in Python).\\n   - This image will represent the best possible interpretation of the prompt in the form of a photo-realistic image (as per the capabilities of Stable Diffusion).\\n\\n2. **Metadata (potentially)**:\\n   - If specified, additional metadata about the generation process might be included. For instance, this could include information about the model's settings, version, and processing time, but this is not explicitly mentioned in the documentation and is therefore optional.\\n\\n### Breakdown of the Image Generation Process:\\n- **Prompt**: The input text (e.g., \\\"a photo of an astronaut riding a horse on mars\\\") that directs the model on what to generate.\\n- **Model Output**: The tool generates an image that represents the described scenario, which is returned to the user.\\n\\nThe key element of the response is the image, with the potential for some metadata describing the output process.\", \"examples\": [{\"user_intent\": \"Create a realistic image of a futuristic city skyline at sunset.\", \"tool_call\": {\"name\": \"CompVis/stable-diffusion-v1-4\", \"arguments\": {\"prompt\": \"A futuristic city skyline at sunset, highly detailed, photorealistic, with flying cars and neon lights\"}}}, {\"user_intent\": \"Generate an artistic illustration of a dragon perched on a mountain peak.\", \"tool_call\": {\"name\": \"CompVis/stable-diffusion-v1-4\", \"arguments\": {\"prompt\": \"An epic fantasy illustration of a dragon perched on a mountain peak, dramatic lighting, detailed scales, concept art style\"}}}, {\"user_intent\": \"Produce a cute cartoon-style image of a cat playing with yarn.\", \"tool_call\": {\"name\": \"CompVis/stable-diffusion-v1-4\", \"arguments\": {\"prompt\": \"A cute cartoon cat playing with a ball of yarn, colorful, playful, in a children's book illustration style\"}}}]}\n\n# User Query\nDesign an explanation on how to use Pix2Struct to analyze and generate text based on visuals such as graphs and charts.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Explain how to use Pix2Struct to analyze visual content like graphs and charts.",
                    "tool_description": "Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It processes visual input and generates structured textual representations or summaries.",
                    "expected_response": "A detailed explanation or guide on using Pix2Struct to interpret and generate text from images, including potential use cases with charts and graphs."
                },
                {
                    "user_intent": "Demonstrate generating textual descriptions from a graph or chart image using Pix2Struct.",
                    "tool_description": "Apply the Pix2Struct model to an image of a graph or chart to extract textual information, captions, or structured data from the visual input.",
                    "expected_response": "Textual output representing the content of the visual input, such as captions, summaries, or structured representations of chart data."
                }
            ],
            "extracted_arguments": {
                "visual_input": "The image containing visual content such as graphs, charts, or diagrams that Pix2Struct will analyze.",
                "task_type": "The type of text generation task to perform on the image, e.g., captioning, summarization, or structured data extraction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_92",
        "query": "We want to automatically generate hashtags for the provided image URL to improve social media post performance.",
        "instruction": "Given an `image hashtag generation` task, retrieve tools that analyze images and extract features to generate relevant hashtags, enhancing the visibility and engagement of social media posts by leveraging computer vision techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "google/vit-base-patch16-224-in21k"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "ImageNet-21k",
                        "accuracy": "Refer to tables 2 and 5 of the original paper"
                    },
                    "description": "The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.",
                    "id": "huggingface_api_17",
                    "name": "google/vit-base-patch16-224-in21k"
                },
                "id": "gorilla_huggingface_tool_17",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 1: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 4: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 5: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 6: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 7: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 8: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 9: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 10: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 11: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.",
            "Tool 12: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 13: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 14: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.",
            "Tool 15: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.",
            "Tool 16: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "Tool 17: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 18: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 19: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models."
        ],
        "tool_view": {
            "description": "A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The path to the pre-trained model or model identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "cardiffnlp/twitter-roberta-base-sentiment-latest"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The path to the pre-trained tokenizer or tokenizer identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "cardiffnlp/twitter-roberta-base-sentiment-latest"
                }
            },
            "response": "The tool is designed to perform sentiment analysis on text inputs, specifically leveraging a RoBERTa-based model trained on a substantial dataset of tweets. The expected response from the tool would typically include the following components:\n\n1. **Label**: This component indicates the sentiment classification assigned to the input text. It may represent categories such as positive, negative, or neutral, reflecting the emotional tone of the analyzed text.\n\n2. **Score**: This component provides a numerical value representing the confidence level of the sentiment classification. It quantifies the model's certainty regarding the assigned label, often expressed as a probability or percentage.\n\n3. **Input Text**: This component may echo the original text that was analyzed, providing context for the sentiment classification and allowing users to verify the input against the output.\n\nOverall, the response encapsulates the sentiment analysis results, offering insights into the emotional content of the provided text along with the model's confidence in its assessment.",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this tweet: 'I love the new features in the latest smartphone update!'",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": "I love the new features in the latest smartphone update!"
                        }
                    }
                },
                {
                    "user_intent": "Check whether this tweet about the stock market is positive, negative, or neutral: 'Markets crashed today after unexpected news.'",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": "Markets crashed today after unexpected news."
                        }
                    }
                },
                {
                    "user_intent": "Determine the sentiment of multiple tweets regarding a recent movie release.",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": [
                                "Absolutely loved the new movie! Best I've seen this year.",
                                "The movie was okay, not really memorable.",
                                "Terrible film, wasted two hours of my life."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 1: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 4: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 5: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 6: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 7: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 8: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 9: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 10: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 11: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.\nTool 12: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 13: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 14: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.\nTool 15: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.\nTool 16: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\nTool 17: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 18: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 19: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The path to the pre-trained model or model identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The path to the pre-trained tokenizer or tokenizer identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}}, \"response\": \"The tool is designed to perform sentiment analysis on text inputs, specifically leveraging a RoBERTa-based model trained on a substantial dataset of tweets. The expected response from the tool would typically include the following components:\\n\\n1. **Label**: This component indicates the sentiment classification assigned to the input text. It may represent categories such as positive, negative, or neutral, reflecting the emotional tone of the analyzed text.\\n\\n2. **Score**: This component provides a numerical value representing the confidence level of the sentiment classification. It quantifies the model's certainty regarding the assigned label, often expressed as a probability or percentage.\\n\\n3. **Input Text**: This component may echo the original text that was analyzed, providing context for the sentiment classification and allowing users to verify the input against the output.\\n\\nOverall, the response encapsulates the sentiment analysis results, offering insights into the emotional content of the provided text along with the model's confidence in its assessment.\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this tweet: 'I love the new features in the latest smartphone update!'\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": \"I love the new features in the latest smartphone update!\"}}}, {\"user_intent\": \"Check whether this tweet about the stock market is positive, negative, or neutral: 'Markets crashed today after unexpected news.'\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": \"Markets crashed today after unexpected news.\"}}}, {\"user_intent\": \"Determine the sentiment of multiple tweets regarding a recent movie release.\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": [\"Absolutely loved the new movie! Best I've seen this year.\", \"The movie was okay, not really memorable.\", \"Terrible film, wasted two hours of my life.\"]}}}]}\n\n# User Query\nWe want to automatically generate hashtags for the provided image URL to improve social media post performance.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a descriptive caption for the given image to understand its content.",
                    "tool_description": "Use a multimodal Vision-Encoder-Decoder model to generate concise textual descriptions from an image input.",
                    "expected_response": "A text string summarizing the visual content of the image, suitable for further processing."
                },
                {
                    "user_intent": "Create relevant hashtags based on the image caption to optimize social media engagement.",
                    "tool_description": "Analyze the generated image caption and produce a set of contextually appropriate hashtags that reflect the content and themes of the image.",
                    "expected_response": "A list of hashtag strings derived from the image caption."
                }
            ],
            "extracted_arguments": {
                "image_url": "The URL of the image for which hashtags need to be generated, used as input for image captioning."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_238",
        "query": "We are a company managing hotel bookings. We need to answer our customer's questions regarding rental rates from our pricing document.",
        "instruction": "Given a `document question answering` task, retrieve tools that utilize document analysis models to answer questions by processing textual context and structured data from documents.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('question-answering', model='pardeepSF/layoutlm-vqa')",
                    "api_arguments": {
                        "question": "string",
                        "context": "string"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A model for document question answering using the LayoutLM architecture.",
                    "id": "huggingface_api_121",
                    "name": "layoutlm-vqa"
                },
                "id": "gorilla_huggingface_tool_120",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 1: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 2: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 3: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 4: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 5: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 6: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 7: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 9: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 11: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 12: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 13: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 14: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 16: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 17: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 18: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information."
        ],
        "tool_view": {
            "description": "This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or path to the pre-trained model to be used for document question answering. This model should be fine-tuned for the task of answering questions based on document input.",
                    "required": true,
                    "example": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or path to the tokenizer associated with the pre-trained model. It is used to convert text into tokenized input for the model.",
                    "required": true,
                    "example": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa"
                }
            },
            "response": "The tool is designed for document question answering using a fine-tuned version of the LayoutLMv2 model on the DocVQA dataset. The expected response from the tool would typically contain:\n\n1. **Answer**: A string representing the answer to the question provided, extracted from the input document. This would be the model’s best interpretation of the query based on the context of the document it has been applied to.\n\n2. **Context**: Optionally, the response might include the specific part of the document or text that was most relevant in helping the model derive the answer. This could be a portion or span of the original document that helped to identify the correct response.\n\n3. **Score (Confidence)**: An optional numeric value indicating the model's confidence in the accuracy of its answer. This score typically ranges between 0 and 1, with higher values indicating greater confidence.\n\n4. **Question**: The original question provided to the model, which can be echoed in the response for clarity or context.\n\nGiven that the tool is based on a Hugging Face pipeline, the response would likely be in a structured format that can easily be interpreted by downstream applications. The model's output may vary depending on the specific implementation and any additional configurations set by the user or the underlying API.",
            "examples": [
                {
                    "user_intent": "I have a scanned invoice and I want to know the total amount due.",
                    "tool_call": {
                        "name": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "question": "What is the total amount due?",
                            "context": "Image or PDF of the invoice document"
                        }
                    }
                },
                {
                    "user_intent": "Check the date of issue on this official letter.",
                    "tool_call": {
                        "name": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "question": "What is the date of issue?",
                            "context": "Scanned letter document"
                        }
                    }
                },
                {
                    "user_intent": "I want to extract the recipient's name from this filled-out form.",
                    "tool_call": {
                        "name": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "question": "Who is the recipient?",
                            "context": "Image or PDF of the completed form"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 1: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 2: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 3: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 4: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 5: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 6: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 7: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 9: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 11: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 12: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 13: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 14: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 16: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 17: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 18: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or path to the pre-trained model to be used for document question answering. This model should be fine-tuned for the task of answering questions based on document input.\", \"required\": true, \"example\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or path to the tokenizer associated with the pre-trained model. It is used to convert text into tokenized input for the model.\", \"required\": true, \"example\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\"}}, \"response\": \"The tool is designed for document question answering using a fine-tuned version of the LayoutLMv2 model on the DocVQA dataset. The expected response from the tool would typically contain:\\n\\n1. **Answer**: A string representing the answer to the question provided, extracted from the input document. This would be the model\\u2019s best interpretation of the query based on the context of the document it has been applied to.\\n\\n2. **Context**: Optionally, the response might include the specific part of the document or text that was most relevant in helping the model derive the answer. This could be a portion or span of the original document that helped to identify the correct response.\\n\\n3. **Score (Confidence)**: An optional numeric value indicating the model's confidence in the accuracy of its answer. This score typically ranges between 0 and 1, with higher values indicating greater confidence.\\n\\n4. **Question**: The original question provided to the model, which can be echoed in the response for clarity or context.\\n\\nGiven that the tool is based on a Hugging Face pipeline, the response would likely be in a structured format that can easily be interpreted by downstream applications. The model's output may vary depending on the specific implementation and any additional configurations set by the user or the underlying API.\", \"examples\": [{\"user_intent\": \"I have a scanned invoice and I want to know the total amount due.\", \"tool_call\": {\"name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"question\": \"What is the total amount due?\", \"context\": \"Image or PDF of the invoice document\"}}}, {\"user_intent\": \"Check the date of issue on this official letter.\", \"tool_call\": {\"name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"question\": \"What is the date of issue?\", \"context\": \"Scanned letter document\"}}}, {\"user_intent\": \"I want to extract the recipient's name from this filled-out form.\", \"tool_call\": {\"name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"question\": \"Who is the recipient?\", \"context\": \"Image or PDF of the completed form\"}}}]}\n\n# User Query\nWe are a company managing hotel bookings. We need to answer our customer's questions regarding rental rates from our pricing document.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Answer customer questions about hotel rental rates using our pricing document.",
                    "tool_description": "Use a document question-answering model fine-tuned on DocVQA with LayoutLMv2 to extract answers from a hotel pricing document based on specific customer queries.",
                    "expected_response": "A string representing the rental rate or relevant pricing information extracted from the document, optionally including the context span from the document and a confidence score."
                }
            ],
            "extracted_arguments": {
                "question": "The customer's query about hotel rental rates in natural language.",
                "context": "The hotel pricing document provided as an image or PDF containing the rates and related information."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_312",
        "query": "We are a voice assistant service, and we need to verify the speaker identity for enhanced security when users access the system.",
        "instruction": "Given a `speaker verification` task, retrieve tools that facilitate speaker identity verification by processing audio inputs to ensure enhanced security through accurate speaker classification.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('audio-classification', model='superb/wav2vec2-base-superb-sid')",
                    "api_arguments": {
                        "model": "superb/wav2vec2-base-superb-sid"
                    },
                    "python_environment_requirements": {
                        "datasets": "load_dataset",
                        "transformers": "pipeline"
                    },
                    "example_code": "from datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(anton-l/superb_demo, si, split=test)\nclassifier = pipeline(audio-classification, model=superb/wav2vec2-base-superb-sid)\nlabels = classifier(dataset[0][file], top_k=5)",
                    "performance": {
                        "dataset": "VoxCeleb1",
                        "accuracy": 0.7518
                    },
                    "description": "This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Identification task. The base model is wav2vec2-base, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.",
                    "id": "huggingface_api_829",
                    "name": "superb/wav2vec2-base-superb-sid"
                },
                "id": "gorilla_huggingface_tool_824",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.",
            "Tool 1: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 2: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 3: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 4: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 6: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 7: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.",
            "Tool 8: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 9: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 10: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 11: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 12: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 13: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 14: This tool is a MetricGAN-trained speech enhancement model that improves the quality and intelligibility of noisy audio. It is designed to process audio inputs and produce clearer, enhanced speech, making it useful for applications such as voice communication, transcription, and audio restoration. The model is trained on the Voicebank dataset and provides measurable improvements in perceptual quality (PESQ) and intelligibility (STOI).",
            "Tool 15: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 16: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.",
            "Tool 17: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 18: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 19: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts."
        ],
        "tool_view": {
            "description": "This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.",
            "parameters": {
                "source": {
                    "type": "string",
                    "description": "The source identifier for the pretrained model to load. Typically a model name or path in Hugging Face or SpeechBrain repositories.",
                    "required": true,
                    "example": "speechbrain/spkrec-xvect-voxceleb"
                },
                "savedir": {
                    "type": "string",
                    "description": "The directory path where the pretrained model and its weights will be saved locally after download.",
                    "required": true,
                    "example": "pretrained_models/spkrec-xvect-voxceleb"
                }
            },
            "response": "The tool performs **speaker verification** by utilizing a pretrained model from SpeechBrain. The response typically contains the following components:\n\n1. **Speaker Embeddings**: The primary output is a set of embeddings, which are numeric representations of the speaker's voice extracted from the input audio signal. These embeddings are used for verifying the identity of the speaker by comparing them to a database of known speaker embeddings.\n\n2. **Confidence or Similarity Score**: In some cases, the response may include a score indicating the similarity between the input audio and a reference speaker's embedding. This score reflects how confidently the system can verify the speaker’s identity.\n\n3. **Audio File Processing Information**: The tool processes an input audio file and generates embeddings for the signal in the form of a tensor or array. The response may include metadata about the processing, such as the input signal’s sample rate (fs) or other related parameters.\n\n4. **Performance Metrics**: If relevant, the response could contain performance metrics of the pretrained model on the dataset, such as the Equal Error Rate (EER) or accuracy (as seen in the performance field), which indicates how well the model performs in speaker verification tasks.\n\n5. **Model Information**: The response could provide some metadata about the model used, such as the model name (`speechbrain/spkrec-xvect-voxceleb`), the source of the pretrained model, or the specific training data used (Voxceleb1 and Voxceleb2 datasets).\n\nIn summary, the response provides the embeddings generated for the input speech signal, optionally accompanied by performance metrics or similarity scores relevant to the speaker verification task.",
            "examples": [
                {
                    "user_intent": "I want to verify if two audio recordings are from the same speaker.",
                    "tool_call": {
                        "name": "EncoderClassifier.from_hparams",
                        "arguments": {
                            "source": "speechbrain/spkrec-xvect-voxceleb",
                            "savedir": "pretrained_models/spkrec-xvect-voxceleb"
                        }
                    }
                },
                {
                    "user_intent": "Extract embeddings from a batch of podcast recordings for speaker clustering.",
                    "tool_call": {
                        "name": "EncoderClassifier.from_hparams",
                        "arguments": {
                            "source": "speechbrain/spkrec-xvect-voxceleb",
                            "savedir": "models/speaker_embeddings"
                        }
                    }
                },
                {
                    "user_intent": "I need a pretrained model to analyze speakers in customer service call recordings.",
                    "tool_call": {
                        "name": "EncoderClassifier.from_hparams",
                        "arguments": {
                            "source": "speechbrain/spkrec-xvect-voxceleb",
                            "savedir": "saved_models/call_center_spkrec"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.\nTool 1: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 2: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 3: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 4: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 6: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 7: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.\nTool 8: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 9: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 10: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 11: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 12: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 13: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 14: This tool is a MetricGAN-trained speech enhancement model that improves the quality and intelligibility of noisy audio. It is designed to process audio inputs and produce clearer, enhanced speech, making it useful for applications such as voice communication, transcription, and audio restoration. The model is trained on the Voicebank dataset and provides measurable improvements in perceptual quality (PESQ) and intelligibility (STOI).\nTool 15: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 16: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.\nTool 17: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 18: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 19: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.\", \"parameters\": {\"source\": {\"type\": \"string\", \"description\": \"The source identifier for the pretrained model to load. Typically a model name or path in Hugging Face or SpeechBrain repositories.\", \"required\": true, \"example\": \"speechbrain/spkrec-xvect-voxceleb\"}, \"savedir\": {\"type\": \"string\", \"description\": \"The directory path where the pretrained model and its weights will be saved locally after download.\", \"required\": true, \"example\": \"pretrained_models/spkrec-xvect-voxceleb\"}}, \"response\": \"The tool performs **speaker verification** by utilizing a pretrained model from SpeechBrain. The response typically contains the following components:\\n\\n1. **Speaker Embeddings**: The primary output is a set of embeddings, which are numeric representations of the speaker's voice extracted from the input audio signal. These embeddings are used for verifying the identity of the speaker by comparing them to a database of known speaker embeddings.\\n\\n2. **Confidence or Similarity Score**: In some cases, the response may include a score indicating the similarity between the input audio and a reference speaker's embedding. This score reflects how confidently the system can verify the speaker\\u2019s identity.\\n\\n3. **Audio File Processing Information**: The tool processes an input audio file and generates embeddings for the signal in the form of a tensor or array. The response may include metadata about the processing, such as the input signal\\u2019s sample rate (fs) or other related parameters.\\n\\n4. **Performance Metrics**: If relevant, the response could contain performance metrics of the pretrained model on the dataset, such as the Equal Error Rate (EER) or accuracy (as seen in the performance field), which indicates how well the model performs in speaker verification tasks.\\n\\n5. **Model Information**: The response could provide some metadata about the model used, such as the model name (`speechbrain/spkrec-xvect-voxceleb`), the source of the pretrained model, or the specific training data used (Voxceleb1 and Voxceleb2 datasets).\\n\\nIn summary, the response provides the embeddings generated for the input speech signal, optionally accompanied by performance metrics or similarity scores relevant to the speaker verification task.\", \"examples\": [{\"user_intent\": \"I want to verify if two audio recordings are from the same speaker.\", \"tool_call\": {\"name\": \"EncoderClassifier.from_hparams\", \"arguments\": {\"source\": \"speechbrain/spkrec-xvect-voxceleb\", \"savedir\": \"pretrained_models/spkrec-xvect-voxceleb\"}}}, {\"user_intent\": \"Extract embeddings from a batch of podcast recordings for speaker clustering.\", \"tool_call\": {\"name\": \"EncoderClassifier.from_hparams\", \"arguments\": {\"source\": \"speechbrain/spkrec-xvect-voxceleb\", \"savedir\": \"models/speaker_embeddings\"}}}, {\"user_intent\": \"I need a pretrained model to analyze speakers in customer service call recordings.\", \"tool_call\": {\"name\": \"EncoderClassifier.from_hparams\", \"arguments\": {\"source\": \"speechbrain/spkrec-xvect-voxceleb\", \"savedir\": \"saved_models/call_center_spkrec\"}}}]}\n\n# User Query\nWe are a voice assistant service, and we need to verify the speaker identity for enhanced security when users access the system.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Verify the identity of a speaker in audio recordings for security purposes.",
                    "tool_description": "Utilize a pretrained SpeechBrain model to extract speaker embeddings from input audio and compare them against known speaker embeddings to confirm identity.",
                    "expected_response": "Speaker embeddings representing the input audio, along with similarity scores or verification confidence indicating whether the speaker matches the expected identity."
                }
            ],
            "extracted_arguments": {
                "source": "The identifier or path of the pretrained SpeechBrain speaker verification model to load.",
                "savedir": "The directory path where the pretrained model and its weights will be stored locally."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_415",
        "query": "An editor wants to summarize his article in French.",
        "instruction": "Given a `text summarization` task, retrieve tools capable of performing abstractive summarization by processing text inputs to produce concise output, specifically in the French language, while maintaining the essence of the original content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Abstractive Text Summarization",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')",
                    "api_arguments": {
                        "input_text": "summarize: ARTICLE"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import T5Tokenizer, T5ForConditionalGeneration"
                    },
                    "example_code": "tokenizer = T5Tokenizer.from_pretrained(plguillou/t5-base-fr-sum-cnndm)\nmodel = T5ForConditionalGeneration.from_pretrained(plguillou/t5-base-fr-sum-cnndm)",
                    "performance": {
                        "dataset": "cnn_dailymail",
                        "ROUGE-1": 44.5252,
                        "ROUGE-2": 22.652,
                        "ROUGE-L": 29.8866
                    },
                    "description": "This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization.",
                    "id": "huggingface_api_563",
                    "name": "plguillou/t5-base-fr-sum-cnndm"
                },
                "id": "gorilla_huggingface_tool_559",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "Tool 1: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.",
            "Tool 2: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 3: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 4: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 5: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 6: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.",
            "Tool 7: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 8: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 9: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 10: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 11: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 12: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 13: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 14: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 15: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 16: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 17: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 18: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 19: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows."
        ],
        "tool_view": {
            "description": "A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "parameters": {},
            "response": "The tool produces a text-generation response in a conversational format. Its response structure is typically a string representing the model’s generated continuation or reply to the provided input message.  \n\nKey components of the response:  \n\n- **Generated Text**: The main content of the response is a natural language string that represents the model’s reply in a conversational style, intended to mimic Elon Musk’s tweeting or speaking style. This text may include informal language, emojis, or humorous/non-factual content.  \n- **Optional Metadata (framework-dependent)**: Depending on the Hugging Face Transformers pipeline configuration, the response may be returned as a list of dictionaries, where each dictionary contains a key like `\"generated_text\"` mapping to the generated string. Additional metadata, such as token usage or generation parameters, might also be included if explicitly enabled, though it is not required by default.  \n\nSemantically, the response conveys the AI’s attempt to continue the conversation coherently based on the input prompt, reflecting the stylistic characteristics learned from Elon Musk’s tweets. It may vary in relevance and factual accuracy, with occasional nonsensical output.",
            "examples": [
                {
                    "user_intent": "I want to chat with a version of Elon Musk. Can you start a conversation?",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "Hello, Elon!"
                        }
                    }
                },
                {
                    "user_intent": "Let's see if this AI can make a joke like Elon Musk would.",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "Tell me a joke, Elon!"
                        }
                    }
                },
                {
                    "user_intent": "I need to test how Elon Musk would respond to a random question. Ask him about space.",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "What do you think about Mars colonization?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\nTool 1: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.\nTool 2: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 3: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 4: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 5: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 6: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.\nTool 7: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 8: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 9: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 10: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 11: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 12: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 13: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 14: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 15: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 16: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 17: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 18: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 19: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A conversational AI model that mimics Elon Musk\\u2019s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\", \"parameters\": {}, \"response\": \"The tool produces a text-generation response in a conversational format. Its response structure is typically a string representing the model\\u2019s generated continuation or reply to the provided input message.  \\n\\nKey components of the response:  \\n\\n- **Generated Text**: The main content of the response is a natural language string that represents the model\\u2019s reply in a conversational style, intended to mimic Elon Musk\\u2019s tweeting or speaking style. This text may include informal language, emojis, or humorous/non-factual content.  \\n- **Optional Metadata (framework-dependent)**: Depending on the Hugging Face Transformers pipeline configuration, the response may be returned as a list of dictionaries, where each dictionary contains a key like `\\\"generated_text\\\"` mapping to the generated string. Additional metadata, such as token usage or generation parameters, might also be included if explicitly enabled, though it is not required by default.  \\n\\nSemantically, the response conveys the AI\\u2019s attempt to continue the conversation coherently based on the input prompt, reflecting the stylistic characteristics learned from Elon Musk\\u2019s tweets. It may vary in relevance and factual accuracy, with occasional nonsensical output.\", \"examples\": [{\"user_intent\": \"I want to chat with a version of Elon Musk. Can you start a conversation?\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"Hello, Elon!\"}}}, {\"user_intent\": \"Let's see if this AI can make a joke like Elon Musk would.\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"Tell me a joke, Elon!\"}}}, {\"user_intent\": \"I need to test how Elon Musk would respond to a random question. Ask him about space.\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"What do you think about Mars colonization?\"}}}]}\n\n# User Query\nAn editor wants to summarize his article in French.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a concise summary of a French article.",
                    "tool_description": "Use a French-language summarization model to create an abstractive summary of the provided text.",
                    "expected_response": "A shorter French text capturing the main points and key information of the original article."
                }
            ],
            "extracted_arguments": {
                "article_text": "The full content of the French article to be summarized, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_75",
        "query": "Our business is growing rapidly, and we've received an increasing number of questions related to product images. We need the model to provide answers based on images.",
        "instruction": "Given a `visual question answering` task, retrieve tools that process visual inputs to provide answers derived from product images, ensuring that the tools are capable of interpreting images in conjunction with textual queries to address the increasing number of image-related questions.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face",
                    "functionality": "Visual Question Answering",
                    "api_call": "pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')",
                    "api_arguments": {
                        "model": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "tokenizer": "JosephusCheung/GuanacoVQAOnConsumerHardware"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "torch": "latest"
                    },
                    "example_code": "vqa(image_path, question)",
                    "performance": {
                        "dataset": "JosephusCheung/GuanacoVQADataset",
                        "accuracy": "unknown"
                    },
                    "description": "A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.",
                    "id": "huggingface_api_113",
                    "name": "JosephusCheung/GuanacoVQAOnConsumerHardware"
                },
                "id": "gorilla_huggingface_tool_112",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 1: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 2: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 3: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 4: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 5: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 6: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 7: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 8: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 9: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 10: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 11: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 12: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 13: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification that uses residual learning to enable training of deep models. It is designed to classify images at 224×224 resolution and is trained on the ImageNet-1k dataset. This model improves on the original ResNet-50 by modifying the downsampling in bottleneck blocks, slightly increasing accuracy. It can be used with the Hugging Face Transformers library to process images and predict their class labels efficiently.",
            "Tool 14: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 15: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 17: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 18: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 19: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images."
        ],
        "tool_view": {
            "description": "The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path or URL of the image that is to be analyzed for the visual question answering task.",
                    "required": true,
                    "example": "path/to/image.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The question related to the image that needs to be answered by the model.",
                    "required": true,
                    "example": "What is in the image?"
                }
            },
            "response": "The tool produces a response that contains the answer to a natural-language question about a given image. The typical structure and semantic components of the response are as follows:\n\n- **`answer`**: A string representing the model’s textual answer to the provided question. This is the primary output and directly addresses the content of the image as queried.  \n- **Optional metadata** (depending on the implementation of the Hugging Face `pipeline`): Some versions of the visual-question-answering pipeline may include additional fields such as:  \n  - **`score`**: A numerical confidence score indicating the model’s certainty about the answer.  \n  - **`logits` or `probabilities`**: Raw or normalized prediction scores for possible answer candidates (more common in classification-based VQA models).  \n\nIn principle, the response is structured as a simple dictionary (JSON-like object) where the `answer` key is the essential component. Any additional fields provide supplementary information about the confidence or internal reasoning of the model.",
            "examples": [
                {
                    "user_intent": "Identify the main object in this photo of a busy street.",
                    "tool_call": {
                        "name": "microsoft/git-base-vqav2",
                        "arguments": {
                            "image": "images/busy_street.jpg",
                            "question": "What is the main object in this image?"
                        }
                    }
                },
                {
                    "user_intent": "Describe what the cat is doing in this picture.",
                    "tool_call": {
                        "name": "microsoft/git-base-vqav2",
                        "arguments": {
                            "image": "images/playing_cat.png",
                            "question": "What is the cat doing in this image?"
                        }
                    }
                },
                {
                    "user_intent": "Check if there are any vehicles in this aerial photo of a parking lot.",
                    "tool_call": {
                        "name": "microsoft/git-base-vqav2",
                        "arguments": {
                            "image": "images/parking_lot_aerial.jpg",
                            "question": "Are there any vehicles in this image?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 1: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 2: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 3: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 4: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 5: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 6: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 7: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 8: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 9: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 10: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 11: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 12: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 13: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification that uses residual learning to enable training of deep models. It is designed to classify images at 224×224 resolution and is trained on the ImageNet-1k dataset. This model improves on the original ResNet-50 by modifying the downsampling in bottleneck blocks, slightly increasing accuracy. It can be used with the Hugging Face Transformers library to process images and predict their class labels efficiently.\nTool 14: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 15: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 17: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 18: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 19: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"microsoft/git-base-vqav2\\\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path or URL of the image that is to be analyzed for the visual question answering task.\", \"required\": true, \"example\": \"path/to/image.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The question related to the image that needs to be answered by the model.\", \"required\": true, \"example\": \"What is in the image?\"}}, \"response\": \"The tool produces a response that contains the answer to a natural-language question about a given image. The typical structure and semantic components of the response are as follows:\\n\\n- **`answer`**: A string representing the model\\u2019s textual answer to the provided question. This is the primary output and directly addresses the content of the image as queried.  \\n- **Optional metadata** (depending on the implementation of the Hugging Face `pipeline`): Some versions of the visual-question-answering pipeline may include additional fields such as:  \\n  - **`score`**: A numerical confidence score indicating the model\\u2019s certainty about the answer.  \\n  - **`logits` or `probabilities`**: Raw or normalized prediction scores for possible answer candidates (more common in classification-based VQA models).  \\n\\nIn principle, the response is structured as a simple dictionary (JSON-like object) where the `answer` key is the essential component. Any additional fields provide supplementary information about the confidence or internal reasoning of the model.\", \"examples\": [{\"user_intent\": \"Identify the main object in this photo of a busy street.\", \"tool_call\": {\"name\": \"microsoft/git-base-vqav2\", \"arguments\": {\"image\": \"images/busy_street.jpg\", \"question\": \"What is the main object in this image?\"}}}, {\"user_intent\": \"Describe what the cat is doing in this picture.\", \"tool_call\": {\"name\": \"microsoft/git-base-vqav2\", \"arguments\": {\"image\": \"images/playing_cat.png\", \"question\": \"What is the cat doing in this image?\"}}}, {\"user_intent\": \"Check if there are any vehicles in this aerial photo of a parking lot.\", \"tool_call\": {\"name\": \"microsoft/git-base-vqav2\", \"arguments\": {\"image\": \"images/parking_lot_aerial.jpg\", \"question\": \"Are there any vehicles in this image?\"}}}]}\n\n# User Query\nOur business is growing rapidly, and we've received an increasing number of questions related to product images. We need the model to provide answers based on images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Answer questions about our product images automatically.",
                    "tool_description": "Use a visual question answering (VQA) model to generate textual answers based on provided images and related questions about products.",
                    "expected_response": "A structured response containing the textual answer to the question about the image, optionally including a confidence score or other metadata."
                }
            ],
            "extracted_arguments": {
                "image": "The file path or URL of the product image to be analyzed.",
                "question": "The natural language question related to the product image that requires an answer."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_479",
        "query": "As a researcher, I am trying to find an answer to my question in a table containing information about animals and their characteristics.",
        "instruction": "Given a `table-based question answering` task, retrieve tools that utilize natural language processing to interpret inquiries about tables, processing both tabular data and textual questions to provide accurate responses aligned with the researcher's query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-finetuned-wtq')",
                    "api_arguments": {
                        "table": "pd.DataFrame",
                        "query": "str"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForSeq2SeqLM",
                        "pandas": "pd"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-finetuned-wtq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-finetuned-wtq)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = In which year did beijing host the Olympic Games?\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": null
                    },
                    "description": "OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab.",
                    "id": "huggingface_api_442",
                    "name": "neulab/omnitab-large-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_438",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 1: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 2: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 3: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 4: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 5: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 6: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 7: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 8: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 9: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 10: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 11: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 12: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.",
            "Tool 13: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 14: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 15: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 16: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 17: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.",
            "Tool 18: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 19: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs."
        ],
        "tool_view": {
            "description": "A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "parameters": {},
            "response": "The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \n\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \n\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.",
            "examples": [
                {
                    "user_intent": "I have an image of a street scene and I want to know how many cars are visible in it.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/street_scene.jpg",
                            "question": "How many cars are there in this image?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this photo of a kitchen and tell me what color the refrigerator is.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/kitchen.jpg",
                            "question": "What color is the refrigerator?"
                        }
                    }
                },
                {
                    "user_intent": "I want to analyze a picture of a park and find out if there are any people sitting on benches.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/park.jpg",
                            "question": "Are there people sitting on benches?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 1: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 2: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 3: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 4: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 5: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 6: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 7: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 8: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 9: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 10: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 11: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 12: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.\nTool 13: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 14: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 15: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 16: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 17: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.\nTool 18: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 19: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\", \"parameters\": {}, \"response\": \"The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \\n\\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \\n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \\n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \\n\\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.\", \"examples\": [{\"user_intent\": \"I have an image of a street scene and I want to know how many cars are visible in it.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/street_scene.jpg\", \"question\": \"How many cars are there in this image?\"}}}, {\"user_intent\": \"Look at this photo of a kitchen and tell me what color the refrigerator is.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/kitchen.jpg\", \"question\": \"What color is the refrigerator?\"}}}, {\"user_intent\": \"I want to analyze a picture of a park and find out if there are any people sitting on benches.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/park.jpg\", \"question\": \"Are there people sitting on benches?\"}}}]}\n\n# User Query\nAs a researcher, I am trying to find an answer to my question in a table containing information about animals and their characteristics.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to search for information in a table about animals and their characteristics.",
                    "tool_description": "Use a table-based question-answering model to extract information from a table about animals and their characteristics based on a natural language query.",
                    "expected_response": "A structured answer derived from the table, which could include specific values or information related to the animals and their characteristics."
                }
            ],
            "extracted_arguments": {
                "table": "A structured dataset containing rows and columns with information about animals and their characteristics.",
                "query": "A natural language question asking about specific details within the animal characteristics table."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_302",
        "query": "Our company is working on an app that allows music producers to detect beats in a sample. We want to use the Hubert-large-ll60k model for this.",
        "instruction": "Given a `music feature extraction` task, retrieve tools that are designed for detecting musical elements like beats in audio samples, utilizing advanced models such as Hubert-large-ll60k for accurate audio analysis.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "HubertModel.from_pretrained('facebook/hubert-large-ll60k')",
                    "api_arguments": "pretrained model name",
                    "python_environment_requirements": "transformers",
                    "example_code": "hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')",
                    "performance": {
                        "dataset": "Libri-Light",
                        "accuracy": "matches or improves upon the state-of-the-art wav2vec 2.0 performance"
                    },
                    "description": "Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.",
                    "id": "huggingface_api_12",
                    "name": "hubert-large-ll60k"
                },
                "id": "gorilla_huggingface_tool_12",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 1: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 2: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 3: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.",
            "Tool 4: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.",
            "Tool 5: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 6: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.",
            "Tool 7: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 8: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 9: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 10: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 11: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 12: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 13: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 14: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 15: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 16: This tool uses the ConvTasNet model, trained on the sep_clean task of the Libri3Mix dataset, to perform audio source separation. It processes mixed audio signals into individual sources, leveraging Asteroid and Hugging Face Transformers frameworks. The model is designed to work with 3-source mixtures, operating at a sample rate of 8kHz, and is optimized for tasks like speech separation. It offers high performance with metrics such as improved Signal-to-Distortion Ratio (SDR) and Speech Intelligibility (STOI).",
            "Tool 17: Hubert-Large is a self-supervised speech representation model that extracts high-quality features from 16kHz audio. It is designed to handle challenges in speech representation learning, including variable-length sound units, multiple sound units per utterance, and the absence of a predefined lexicon during pretraining. By using an offline clustering step to generate target labels for a BERT-style prediction loss, it produces representations that match or surpass state-of-the-art performance on speech benchmarks like Libri-Light, making it useful for tasks requiring robust speech feature extraction.",
            "Tool 18: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 19: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts."
        ],
        "tool_view": {
            "description": "A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "parameters": {
                "audio_file": {
                    "type": "string",
                    "description": "The file path to the local audio file that will be used as input for emotion prediction.",
                    "required": true,
                    "example": "male-crying.mp3"
                }
            },
            "response": "The tool’s response is a list of dictionaries, where each dictionary represents a predicted emotion for the input audio file along with its associated confidence score. Each element in the list contains the following components:\n\n1. **emo** – A string representing the predicted emotion label. These labels are derived from the model’s configuration mapping (`id2label`) and indicate the type of emotion detected in the audio.  \n\n2. **score** – A numeric value (typically rounded to one decimal place) representing the model’s confidence in that emotion prediction, expressed as a percentage. Higher scores indicate greater confidence.  \n\nThe list is sorted in descending order of confidence, and typically only the top predictions (e.g., the top two) with non-zero scores are returned. This ensures the response highlights the most likely emotional categories present in the audio.  \n\nIn summary, the response semantically conveys which emotions are present in the audio and how confident the model is in each prediction.",
            "examples": [
                {
                    "user_intent": "I want to classify the emotion from an audio recording of a crying person.",
                    "tool_call": {
                        "name": "Rajaram1996/Hubert_emotion",
                        "arguments": {
                            "audio_file": "male-crying.mp3"
                        }
                    }
                },
                {
                    "user_intent": "Can you tell me the emotion in this audio file of someone laughing?",
                    "tool_call": {
                        "name": "Rajaram1996/Hubert_emotion",
                        "arguments": {
                            "audio_file": "person-laughing.wav"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze the emotion in a voice message I received. Here's the file.",
                    "tool_call": {
                        "name": "Rajaram1996/Hubert_emotion",
                        "arguments": {
                            "audio_file": "voice-message.mp3"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 1: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 2: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 3: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.\nTool 4: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.\nTool 5: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 6: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\nTool 7: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 8: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 9: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 10: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 11: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 12: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 13: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 14: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 15: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 16: This tool uses the ConvTasNet model, trained on the sep_clean task of the Libri3Mix dataset, to perform audio source separation. It processes mixed audio signals into individual sources, leveraging Asteroid and Hugging Face Transformers frameworks. The model is designed to work with 3-source mixtures, operating at a sample rate of 8kHz, and is optimized for tasks like speech separation. It offers high performance with metrics such as improved Signal-to-Distortion Ratio (SDR) and Speech Intelligibility (STOI).\nTool 17: Hubert-Large is a self-supervised speech representation model that extracts high-quality features from 16kHz audio. It is designed to handle challenges in speech representation learning, including variable-length sound units, multiple sound units per utterance, and the absence of a predefined lexicon during pretraining. By using an offline clustering step to generate target labels for a BERT-style prediction loss, it produces representations that match or surpass state-of-the-art performance on speech benchmarks like Libri-Light, making it useful for tasks requiring robust speech feature extraction.\nTool 18: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 19: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\", \"parameters\": {\"audio_file\": {\"type\": \"string\", \"description\": \"The file path to the local audio file that will be used as input for emotion prediction.\", \"required\": true, \"example\": \"male-crying.mp3\"}}, \"response\": \"The tool\\u2019s response is a list of dictionaries, where each dictionary represents a predicted emotion for the input audio file along with its associated confidence score. Each element in the list contains the following components:\\n\\n1. **emo** \\u2013 A string representing the predicted emotion label. These labels are derived from the model\\u2019s configuration mapping (`id2label`) and indicate the type of emotion detected in the audio.  \\n\\n2. **score** \\u2013 A numeric value (typically rounded to one decimal place) representing the model\\u2019s confidence in that emotion prediction, expressed as a percentage. Higher scores indicate greater confidence.  \\n\\nThe list is sorted in descending order of confidence, and typically only the top predictions (e.g., the top two) with non-zero scores are returned. This ensures the response highlights the most likely emotional categories present in the audio.  \\n\\nIn summary, the response semantically conveys which emotions are present in the audio and how confident the model is in each prediction.\", \"examples\": [{\"user_intent\": \"I want to classify the emotion from an audio recording of a crying person.\", \"tool_call\": {\"name\": \"Rajaram1996/Hubert_emotion\", \"arguments\": {\"audio_file\": \"male-crying.mp3\"}}}, {\"user_intent\": \"Can you tell me the emotion in this audio file of someone laughing?\", \"tool_call\": {\"name\": \"Rajaram1996/Hubert_emotion\", \"arguments\": {\"audio_file\": \"person-laughing.wav\"}}}, {\"user_intent\": \"I need to analyze the emotion in a voice message I received. Here's the file.\", \"tool_call\": {\"name\": \"Rajaram1996/Hubert_emotion\", \"arguments\": {\"audio_file\": \"voice-message.mp3\"}}}]}\n\n# User Query\nOur company is working on an app that allows music producers to detect beats in a sample. We want to use the Hubert-large-ll60k model for this.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze an audio sample to detect beats using the Hubert-large-ll60k model.",
                    "tool_description": "Process a given audio file with the Hubert-large-ll60k model to identify beat positions and timing information within the music sample.",
                    "expected_response": "A structured list of detected beats, each with a timestamp or frame index indicating the beat location within the audio."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The file path to the music sample that will be analyzed for beats."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_96",
        "query": "We are developing a customer support platform for our telecommunication company in Spain. We want to know if they are happy or unhappy with our services, based on the content of their message.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that assess customer sentiment by processing textual content in their messages, identifying emotions or attitudes such as happiness or dissatisfaction, to provide insights into service perception.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')",
                    "api_arguments": "text",
                    "python_environment_requirements": "Hugging Face Transformers library",
                    "example_code": "",
                    "performance": {
                        "dataset": "TASS 2020 corpus",
                        "accuracy": ""
                    },
                    "description": "Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.",
                    "id": "huggingface_api_387",
                    "name": "finiteautomata/beto-sentiment-analysis"
                },
                "id": "gorilla_huggingface_tool_383",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 2: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 3: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 4: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 5: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.",
            "Tool 6: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.",
            "Tool 7: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "Tool 8: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.",
            "Tool 9: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 10: The \"distil-ast-audioset\" is a distilled audio classification model based on the Audio Spectrogram Transformer (AST) architecture, trained on the AudioSet dataset. It is designed for classifying audio input, and it leverages the Hugging Face Transformers framework for implementation. This model offers a lighter version of the original AST model, with a performance accuracy of 7.14% on the AudioSet dataset. It can be used to process audio data for classification tasks in environments that support specified Python dependencies like transformers, PyTorch, and datasets.",
            "Tool 11: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 12: The `facebook/opt-6.7b` model is a large pre-trained transformer designed for text generation using a causal language modeling approach. It can generate coherent and contextually relevant text from prompts, support evaluation of downstream tasks, and be fine-tuned for specific applications, making it suitable for a wide range of natural language processing tasks.",
            "Tool 13: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 14: A transformer-based image captioning tool that generates descriptive text for input images, enabling automated understanding and annotation of visual content.",
            "Tool 15: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 16: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 17: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 18: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 19: This tool provides automatic speech recognition using the fine-tuned Wav2Vec2 model from Hugging Face, specifically the \"jonatasgrosman/wav2vec2-large-xlsr-53-english\" version. It transcribes audio files (such as MP3 or WAV) into text, with support for English language inputs sampled at 16kHz. The model is optimized using the Common Voice 6.1 dataset and achieves competitive accuracy, with a Word Error Rate (WER) of 19.06%."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 2: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 3: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 4: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 5: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.\nTool 6: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.\nTool 7: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\nTool 8: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.\nTool 9: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 10: The \"distil-ast-audioset\" is a distilled audio classification model based on the Audio Spectrogram Transformer (AST) architecture, trained on the AudioSet dataset. It is designed for classifying audio input, and it leverages the Hugging Face Transformers framework for implementation. This model offers a lighter version of the original AST model, with a performance accuracy of 7.14% on the AudioSet dataset. It can be used to process audio data for classification tasks in environments that support specified Python dependencies like transformers, PyTorch, and datasets.\nTool 11: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 12: The `facebook/opt-6.7b` model is a large pre-trained transformer designed for text generation using a causal language modeling approach. It can generate coherent and contextually relevant text from prompts, support evaluation of downstream tasks, and be fine-tuned for specific applications, making it suitable for a wide range of natural language processing tasks.\nTool 13: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 14: A transformer-based image captioning tool that generates descriptive text for input images, enabling automated understanding and annotation of visual content.\nTool 15: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 16: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 17: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 18: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 19: This tool provides automatic speech recognition using the fine-tuned Wav2Vec2 model from Hugging Face, specifically the \"jonatasgrosman/wav2vec2-large-xlsr-53-english\" version. It transcribes audio files (such as MP3 or WAV) into text, with support for English language inputs sampled at 16kHz. The model is optimized using the Common Voice 6.1 dataset and achieves competitive accuracy, with a Word Error Rate (WER) of 19.06%.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nWe are developing a customer support platform for our telecommunication company in Spain. We want to know if they are happy or unhappy with our services, based on the content of their message.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the sentiment of a customer message in Spanish to determine if they are happy or unhappy with our services.",
                    "tool_description": "Perform multilingual sentiment analysis on customer messages, predicting a star rating (1 to 5) that reflects overall satisfaction or dissatisfaction with telecommunication services.",
                    "expected_response": "A sentiment analysis result indicating a star rating along with a confidence score, showing whether the customer is satisfied or dissatisfied."
                }
            ],
            "extracted_arguments": {
                "text": "The content of the customer's message in Spanish whose sentiment needs to be evaluated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_140",
        "query": "My boss wants me to extract captions from images of people in different settings.",
        "instruction": "Given an `image captioning` task, retrieve tools that can process image inputs to automatically generate descriptive captions by utilizing advanced multimodal image-to-text capabilities, specifically designed for extracting textual information from visual data.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image-to-Text",
                    "api_call": "VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')",
                    "api_arguments": {
                        "loc": "ydshieh/vit-gpt2-coco-en"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "requests",
                        "PIL",
                        "transformers"
                    ],
                    "example_code": "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\nloc = ydshieh/vit-gpt2-coco-en\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\ntokenizer = AutoTokenizer.from_pretrained(loc)\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\nmodel.eval()\ndef predict(image):\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\n with torch.no_grad():\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nwith Image.open(requests.get(url, stream=True).raw) as image:\n preds = predict(image)\nprint(preds)",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not specified"
                    },
                    "description": "A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.",
                    "id": "huggingface_api_73",
                    "name": "ydshieh/vit-gpt2-coco-en"
                },
                "id": "gorilla_huggingface_tool_73",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 1: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 2: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 3: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 4: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.",
            "Tool 5: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 6: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.",
            "Tool 7: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 8: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.",
            "Tool 9: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 10: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 11: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.",
            "Tool 12: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 13: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 14: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 15: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 16: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "Tool 17: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 18: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.",
            "Tool 19: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings."
        ],
        "tool_view": {
            "description": "A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image to run object detection on. Can be a URL pointing to an online image or a local file path on the system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns a structured object representing the results of object detection performed on the input image. The response typically includes the following components:\n\n1. **Detections / Results Array**  \n   - The main content is a list of detection results, where each element corresponds to one detected object in the image.  \n   - Each detection usually contains information such as:\n     - **Bounding Box (`boxes`)**: The coordinates of the detected object, often represented as `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.\n     - **Class / Label (`class` or `label`)**: The category of the detected object, e.g., \"forklift\" or \"person\".\n     - **Confidence Score (`confidence` or `score`)**: A numeric value indicating the model’s certainty about the detection.\n     - **Optional Additional Attributes**: Depending on the implementation, this may include class probabilities, object tracking IDs, or mask segmentation information.\n\n2. **Image-Level Metadata**  \n   - Some responses may include metadata related to the processed image, such as:\n     - Image dimensions (width, height)\n     - The path or URL of the input image\n     - Number of detections in the image\n\n3. **Post-Processing / Rendering Support**  \n   - The response is compatible with rendering utilities like `render_result`, which can visually overlay the detected bounding boxes and labels onto the image.  \n   - This implies the bounding boxes and labels are structured in a way that can be directly interpreted for visualization.\n\nOverall, the response is a comprehensive object that provides both raw detection data and structured information suitable for visualization, further analysis, or filtering based on confidence or class.",
            "examples": [
                {
                    "user_intent": "I want to detect forklifts in an image from a URL.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "https://example.com/forklift_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect forklifts in a local image file.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "/path/to/local/forklift_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Run forklift detection with custom confidence and IoU thresholds.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "https://example.com/forklift_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 1: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 2: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 3: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 4: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.\nTool 5: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 6: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.\nTool 7: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 8: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.\nTool 9: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 10: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 11: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.\nTool 12: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 13: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 14: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 15: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 16: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\nTool 17: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 18: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.\nTool 19: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image to run object detection on. Can be a URL pointing to an online image or a local file path on the system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns a structured object representing the results of object detection performed on the input image. The response typically includes the following components:\\n\\n1. **Detections / Results Array**  \\n   - The main content is a list of detection results, where each element corresponds to one detected object in the image.  \\n   - Each detection usually contains information such as:\\n     - **Bounding Box (`boxes`)**: The coordinates of the detected object, often represented as `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.\\n     - **Class / Label (`class` or `label`)**: The category of the detected object, e.g., \\\"forklift\\\" or \\\"person\\\".\\n     - **Confidence Score (`confidence` or `score`)**: A numeric value indicating the model\\u2019s certainty about the detection.\\n     - **Optional Additional Attributes**: Depending on the implementation, this may include class probabilities, object tracking IDs, or mask segmentation information.\\n\\n2. **Image-Level Metadata**  \\n   - Some responses may include metadata related to the processed image, such as:\\n     - Image dimensions (width, height)\\n     - The path or URL of the input image\\n     - Number of detections in the image\\n\\n3. **Post-Processing / Rendering Support**  \\n   - The response is compatible with rendering utilities like `render_result`, which can visually overlay the detected bounding boxes and labels onto the image.  \\n   - This implies the bounding boxes and labels are structured in a way that can be directly interpreted for visualization.\\n\\nOverall, the response is a comprehensive object that provides both raw detection data and structured information suitable for visualization, further analysis, or filtering based on confidence or class.\", \"examples\": [{\"user_intent\": \"I want to detect forklifts in an image from a URL.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"https://example.com/forklift_image.jpg\"}}}, {\"user_intent\": \"Detect forklifts in a local image file.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"/path/to/local/forklift_image.jpg\"}}}, {\"user_intent\": \"Run forklift detection with custom confidence and IoU thresholds.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"https://example.com/forklift_image.jpg\"}}}]}\n\n# User Query\nMy boss wants me to extract captions from images of people in different settings.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate descriptive captions for images containing people in various settings.",
                    "tool_description": "Use a generative image-to-text model to convert input images into detailed and accurate textual captions, capturing the content and context of people in different environments.",
                    "expected_response": "A collection of text captions corresponding to each input image, describing the people and their surroundings."
                }
            ],
            "extracted_arguments": {
                "image": "The input image file or URL containing people, for which captions need to be generated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_182",
        "query": "A client from real estate agency needs to get a list of objects present in a series of pictures to prepare their property listings.",
        "instruction": "Given an `object detection in images` task, retrieve tools designed for computer vision that can analyze images to list objects present by taking in image data and providing precise detections and attributes relevant to a real estate context.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "zero-shot-object-detection",
                    "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')",
                    "api_arguments": [
                        "texts",
                        "images"
                    ],
                    "python_environment_requirements": [
                        "requests",
                        "PIL",
                        "torch",
                        "transformers"
                    ],
                    "example_code": "processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not provided"
                    },
                    "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.",
                    "id": "huggingface_api_221",
                    "name": "google/owlvit-base-patch16"
                },
                "id": "gorilla_huggingface_tool_217",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 1: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 2: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.",
            "Tool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 4: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 5: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 6: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 7: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 8: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 9: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.",
            "Tool 10: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 11: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 12: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 13: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 14: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 15: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 16: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.",
            "Tool 17: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 18: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.",
            "Tool 19: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts."
        ],
        "tool_view": {
            "description": "BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "parameters": {
                "raw_image": {
                    "type": "Image",
                    "description": "The input image on which the visual question answering model will perform analysis. This should be provided as a PIL Image object.",
                    "required": true,
                    "example": "Image.open('path_to_image.jpg')"
                },
                "question": {
                    "type": "string",
                    "description": "The natural language question about the content of the image that the model should answer.",
                    "required": true,
                    "example": "How many dogs are in the picture?"
                }
            },
            "response": "The tool produces a textual answer to a visual question posed about an input image. The response is structured as a single string representing the model’s predicted answer. Semantically, this string conveys the model’s understanding of the visual content of the image in relation to the provided question. The response does not include additional metadata by default, such as confidence scores or token-level outputs; it is primarily the decoded natural language answer derived from the model’s internal generation.",
            "examples": [
                {
                    "user_intent": "How many people are standing in this photo?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1607746882042-944635dfe10e",
                            "question": "How many people are standing in this photo?"
                        }
                    }
                },
                {
                    "user_intent": "What color is the car in the image?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1612392061781-3f3b9a0f6c6b",
                            "question": "What color is the car in the image?"
                        }
                    }
                },
                {
                    "user_intent": "Is there any food on the table in this picture?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1556910103-1b1b3c0c4f4c",
                            "question": "Is there any food on the table in this picture?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 1: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 2: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.\nTool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 4: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 5: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 6: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 7: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 8: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 9: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\nTool 10: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 11: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 12: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 13: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 14: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 15: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 16: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.\nTool 17: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 18: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.\nTool 19: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\", \"parameters\": {\"raw_image\": {\"type\": \"Image\", \"description\": \"The input image on which the visual question answering model will perform analysis. This should be provided as a PIL Image object.\", \"required\": true, \"example\": \"Image.open('path_to_image.jpg')\"}, \"question\": {\"type\": \"string\", \"description\": \"The natural language question about the content of the image that the model should answer.\", \"required\": true, \"example\": \"How many dogs are in the picture?\"}}, \"response\": \"The tool produces a textual answer to a visual question posed about an input image. The response is structured as a single string representing the model\\u2019s predicted answer. Semantically, this string conveys the model\\u2019s understanding of the visual content of the image in relation to the provided question. The response does not include additional metadata by default, such as confidence scores or token-level outputs; it is primarily the decoded natural language answer derived from the model\\u2019s internal generation.\", \"examples\": [{\"user_intent\": \"How many people are standing in this photo?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1607746882042-944635dfe10e\", \"question\": \"How many people are standing in this photo?\"}}}, {\"user_intent\": \"What color is the car in the image?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1612392061781-3f3b9a0f6c6b\", \"question\": \"What color is the car in the image?\"}}}, {\"user_intent\": \"Is there any food on the table in this picture?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1556910103-1b1b3c0c4f4c\", \"question\": \"Is there any food on the table in this picture?\"}}}]}\n\n# User Query\nA client from real estate agency needs to get a list of objects present in a series of pictures to prepare their property listings.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect and list all objects present in these property images.",
                    "tool_description": "Perform object detection on input images to identify and classify all objects, providing their locations and labels.",
                    "expected_response": "A structured list of detected objects for each image, including class names and bounding box coordinates."
                }
            ],
            "extracted_arguments": {
                "image_list": "A collection of images of properties for which objects need to be identified, provided as image files or URLs."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_131",
        "query": "Develop a software to classify an image from a URL into a thousand categories.",
        "instruction": "Given an `image classification` task, retrieve tools that process image inputs from a URL to categorize them into multiple classes, leveraging advanced models that support feature map extraction and produce accurate classification results.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification, Feature Map Extraction, Image Embeddings",
                    "api_call": "timm.create_model('convnext_base.fb_in1k')",
                    "api_arguments": {
                        "pretrained": "True",
                        "features_only": "True",
                        "num_classes": "0"
                    },
                    "python_environment_requirements": [
                        "timm"
                    ],
                    "example_code": [
                        "from urllib.request import urlopen",
                        "from PIL import Image",
                        "import timm",
                        "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))",
                        "model = timm.create_model('convnext_base.fb_in1k', pretrained=True)",
                        "model = model.eval()",
                        "data_config = timm.data.resolve_model_data_config(model)",
                        "transforms = timm.data.create_transform(**data_config, is_training=False)",
                        "output = model(transforms(img).unsqueeze(0))"
                    ],
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "83.82%"
                    },
                    "description": "A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.",
                    "id": "huggingface_api_201",
                    "name": "convnext_base.fb_in1k"
                },
                "id": "gorilla_huggingface_tool_197",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 1: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 2: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.",
            "Tool 3: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 4: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 5: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.",
            "Tool 6: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.",
            "Tool 7: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.",
            "Tool 8: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 9: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 10: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.",
            "Tool 11: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.",
            "Tool 12: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 13: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 14: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 15: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 16: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.",
            "Tool 17: A CLIP ViT-bigG/14 model designed for zero-shot image classification, allowing users to classify images into arbitrary categories without task-specific training. Built with the LAION-2B dataset, it is useful for research and exploratory studies in computer vision, enabling analysis of model behavior and performance across diverse visual concepts. The model achieves 80.1% zero-shot top-1 accuracy on ImageNet-1k.",
            "Tool 18: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.",
            "Tool 19: A Hugging Face Transformers model for image classification that uses the SegFormer encoder fine-tuned on ImageNet-1k, enabling accurate prediction of image classes from input images. It is designed for efficient and effective image classification leveraging a hierarchical Transformer architecture and can be used to classify a wide variety of images in computer vision applications."
        ],
        "tool_view": {
            "description": "This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "parameters": {
                "image_url": {
                    "type": "string",
                    "description": "The URL of the image document from which the question will be answered.",
                    "required": true,
                    "example": "https://templates.invoicehome.com/invoice-template-us-neat-750px.png"
                },
                "question": {
                    "type": "string",
                    "description": "The question that needs to be answered based on the content of the document.",
                    "required": true,
                    "example": "What is the invoice number?"
                }
            },
            "response": "The tool is expected to return an answer to a specific question posed about a document, which may be in the form of an image. The response structure typically includes the following components:\n\n1. **answer**: This field contains the text that directly responds to the question asked. It represents the extracted information from the document that is relevant to the query.\n\n2. **score**: This field indicates the confidence level or probability associated with the provided answer. It reflects how certain the model is about the correctness of the answer based on the document content.\n\n3. **start**: This field specifies the starting index of the answer within the original text of the document. It helps to locate where the answer is derived from in the context of the document.\n\n4. **end**: This field indicates the ending index of the answer within the original text of the document. Together with the start index, it provides a precise range for the extracted answer.\n\n5. **context**: This field may include the relevant portion of the document text from which the answer was derived. It provides additional context to understand the answer better.\n\nOverall, the response encapsulates the answer to the question along with supporting information that indicates the reliability and source of the answer within the document.",
            "examples": [
                {
                    "user_intent": "Can you tell me what the invoice number is from this image?",
                    "tool_call": {
                        "name": "impira/layoutlm-document-qa",
                        "arguments": {
                            "image_url": "https://templates.invoicehome.com/invoice-template-us-neat-750px.png",
                            "question": "What is the invoice number?"
                        }
                    }
                },
                {
                    "user_intent": "I need to find out the total amount due from this receipt image.",
                    "tool_call": {
                        "name": "impira/layoutlm-document-qa",
                        "arguments": {
                            "image_url": "https://example.com/receipt.png",
                            "question": "What is the total amount due?"
                        }
                    }
                },
                {
                    "user_intent": "What date is mentioned on this document?",
                    "tool_call": {
                        "name": "impira/layoutlm-document-qa",
                        "arguments": {
                            "image_url": "https://example.com/document.png",
                            "question": "What is the date on the document?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 1: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 2: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.\nTool 3: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 4: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 5: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.\nTool 6: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.\nTool 7: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.\nTool 8: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 9: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 10: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.\nTool 11: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.\nTool 12: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 13: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 14: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 15: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 16: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.\nTool 17: A CLIP ViT-bigG/14 model designed for zero-shot image classification, allowing users to classify images into arbitrary categories without task-specific training. Built with the LAION-2B dataset, it is useful for research and exploratory studies in computer vision, enabling analysis of model behavior and performance across diverse visual concepts. The model achieves 80.1% zero-shot top-1 accuracy on ImageNet-1k.\nTool 18: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\nTool 19: A Hugging Face Transformers model for image classification that uses the SegFormer encoder fine-tuned on ImageNet-1k, enabling accurate prediction of image classes from input images. It is designed for efficient and effective image classification leveraging a hierarchical Transformer architecture and can be used to classify a wide variety of images in computer vision applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\", \"parameters\": {\"image_url\": {\"type\": \"string\", \"description\": \"The URL of the image document from which the question will be answered.\", \"required\": true, \"example\": \"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\"}, \"question\": {\"type\": \"string\", \"description\": \"The question that needs to be answered based on the content of the document.\", \"required\": true, \"example\": \"What is the invoice number?\"}}, \"response\": \"The tool is expected to return an answer to a specific question posed about a document, which may be in the form of an image. The response structure typically includes the following components:\\n\\n1. **answer**: This field contains the text that directly responds to the question asked. It represents the extracted information from the document that is relevant to the query.\\n\\n2. **score**: This field indicates the confidence level or probability associated with the provided answer. It reflects how certain the model is about the correctness of the answer based on the document content.\\n\\n3. **start**: This field specifies the starting index of the answer within the original text of the document. It helps to locate where the answer is derived from in the context of the document.\\n\\n4. **end**: This field indicates the ending index of the answer within the original text of the document. Together with the start index, it provides a precise range for the extracted answer.\\n\\n5. **context**: This field may include the relevant portion of the document text from which the answer was derived. It provides additional context to understand the answer better.\\n\\nOverall, the response encapsulates the answer to the question along with supporting information that indicates the reliability and source of the answer within the document.\", \"examples\": [{\"user_intent\": \"Can you tell me what the invoice number is from this image?\", \"tool_call\": {\"name\": \"impira/layoutlm-document-qa\", \"arguments\": {\"image_url\": \"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\", \"question\": \"What is the invoice number?\"}}}, {\"user_intent\": \"I need to find out the total amount due from this receipt image.\", \"tool_call\": {\"name\": \"impira/layoutlm-document-qa\", \"arguments\": {\"image_url\": \"https://example.com/receipt.png\", \"question\": \"What is the total amount due?\"}}}, {\"user_intent\": \"What date is mentioned on this document?\", \"tool_call\": {\"name\": \"impira/layoutlm-document-qa\", \"arguments\": {\"image_url\": \"https://example.com/document.png\", \"question\": \"What is the date on the document?\"}}}]}\n\n# User Query\nDevelop a software to classify an image from a URL into a thousand categories.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify an image from a URL into predefined categories.",
                    "tool_description": "Utilize a pre-trained image classification model to categorize an image based on its content.",
                    "expected_response": "A classification result indicating the predicted category of the image along with the associated confidence score."
                }
            ],
            "extracted_arguments": {
                "image_url": "The URL of the image to be classified.",
                "categories": "A list of predefined categories into which the image can be classified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_478",
        "query": "I want to build a system that can answer questions from users reading a book.",
        "instruction": "Given a `question answering` system development task, retrieve tools that leverage Natural Language Processing (NLP) frameworks to interpret text inputs, providing accurate responses to questions by utilizing pre-trained models designed for understanding and extracting information from book passages.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')",
                    "api_arguments": {
                        "context": "string",
                        "question": "string"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\nresult = qa_pipeline({'context': 'This is a context.', 'question': 'What is this?'})\nprint(result)",
                    "performance": {
                        "dataset": "squad_v2",
                        "exact": 79.8366040596311,
                        "f1": 83.916407079888
                    },
                    "description": "This model is a distilled version of deepset/roberta-large-squad2, trained on SQuAD 2.0 dataset for question answering tasks. It is based on the Roberta architecture and has been fine-tuned using Haystack's distillation feature.",
                    "id": "huggingface_api_480",
                    "name": "deepset/roberta-base-squad2-distilled"
                },
                "id": "gorilla_huggingface_tool_476",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 1: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 2: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 3: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 4: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 5: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 6: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 8: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 9: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 10: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 11: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 12: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 13: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 14: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 15: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 16: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 17: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 18: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 19: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions."
        ],
        "tool_view": {
            "description": "A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path to the image that contains the document to be processed for question answering.",
                    "required": true,
                    "example": "path/to/image"
                },
                "question": {
                    "type": "string",
                    "description": "The question that you want to ask regarding the content of the image.",
                    "required": true,
                    "example": "What is the name of the company in the document?"
                }
            },
            "response": "The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\n\nThe response would typically include:\n\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\n\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\n\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\n\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.",
            "examples": [
                {
                    "user_intent": "I have a scanned contract and I want to know the effective date mentioned in it.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/contract_scan_01.png",
                            "question": "What is the effective date of this contract?"
                        }
                    }
                },
                {
                    "user_intent": "Check this invoice image and tell me the total amount due.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/invoice_march2025.png",
                            "question": "What is the total amount to be paid?"
                        }
                    }
                },
                {
                    "user_intent": "I need to extract the name of the company from this PDF screenshot.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/company_profile_screenshot.png",
                            "question": "What is the name of the company in this document?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 1: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 2: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 3: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 4: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 5: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 6: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 8: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 9: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 10: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 11: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 12: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 13: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 14: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 15: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 16: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 17: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 18: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 19: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path to the image that contains the document to be processed for question answering.\", \"required\": true, \"example\": \"path/to/image\"}, \"question\": {\"type\": \"string\", \"description\": \"The question that you want to ask regarding the content of the image.\", \"required\": true, \"example\": \"What is the name of the company in the document?\"}}, \"response\": \"The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\\n\\nThe response would typically include:\\n\\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\\n\\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\\n\\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\\n\\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.\", \"examples\": [{\"user_intent\": \"I have a scanned contract and I want to know the effective date mentioned in it.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/contract_scan_01.png\", \"question\": \"What is the effective date of this contract?\"}}}, {\"user_intent\": \"Check this invoice image and tell me the total amount due.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/invoice_march2025.png\", \"question\": \"What is the total amount to be paid?\"}}}, {\"user_intent\": \"I need to extract the name of the company from this PDF screenshot.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/company_profile_screenshot.png\", \"question\": \"What is the name of the company in this document?\"}}}]}\n\n# User Query\nI want to build a system that can answer questions from users reading a book.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to build a system that can answer questions based on book content.",
                    "tool_description": "Create a system capable of answering natural language questions based on the content of a book, using a question-answering model fine-tuned for document processing.",
                    "expected_response": "The system should return answers to user queries based on the content of the book, potentially with context and confidence levels."
                }
            ],
            "extracted_arguments": {
                "document_content": "The textual content or scanned images of the book to be processed for answering questions.",
                "question": "The natural language question posed by the user related to the content of the book."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_475",
        "query": "I am building a social media app that requires people to write fascinating stories rather than boring sentences. Detect named entities in a sentence by using an NER model.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that perform natural language processing by detecting and classifying named entities such as names, dates, and locations from textual input using advanced models and embeddings.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Flair",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english-ontonotes-large')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\n\n# make example sentence\nsentence = Sentence('On September 1st George won 1 dollar while watching Game of Thrones.')\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)",
                    "performance": {
                        "dataset": "Ontonotes",
                        "accuracy": 90.93
                    },
                    "description": "English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.",
                    "id": "huggingface_api_436",
                    "name": "flair/ner-english-ontonotes-large"
                },
                "id": "gorilla_huggingface_tool_432",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 1: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 2: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 3: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 4: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 6: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 7: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 8: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 9: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 10: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.",
            "Tool 11: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 12: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 13: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 14: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 15: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 16: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 17: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.",
            "Tool 18: This tool uses the \"sentence-transformers/distilbert-base-nli-mean-tokens\" model to convert sentences and paragraphs into 768-dimensional dense vector representations, enabling tasks such as clustering and semantic search. It leverages the Hugging Face Transformers framework for feature extraction and requires the `sentence-transformers` library.",
            "Tool 19: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding."
        ],
        "tool_view": {
            "description": "Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pre-trained token classification model to load for Named Entity Recognition tasks. This model has been fine-tuned for food entity recognition in informal text.",
                    "required": true,
                    "example": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer associated with the pre-trained model, used to convert text into token IDs suitable for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                }
            },
            "response": "The tool is designed for Named Entity Recognition (NER) in the domain of food-related entities, particularly focusing on informal text such as social media posts. The response format from this tool would typically contain the following components:\n\n1. **Named Entities**: A list of identified entities in the input text, specifically food-related entities. Each entity would have:\n   - **Entity Type**: The type or category of the entity, which, in this case, would be 'FOOD' for food-related terms.\n   - **Start and End Position**: The positions in the input text where the entity starts and ends.\n   - **Entity Label**: A label that categorizes the entity, which would typically be the word 'FOOD' in this case.\n   - **Confidence/Score**: The model's confidence score for the classification of the entity.\n\n2. **Aggregation Strategy**: The results may be aggregated based on the specified strategy (e.g., 'simple'), which could determine whether the tool returns entities in a raw form or in a more processed, simplified format.\n\n3. **Performance Metrics**: Although the response to a specific input may not include these directly, performance statistics such as accuracy, precision, recall, and F1 score are part of the tool's performance metrics and could be referenced to give a sense of the model's quality and reliability. These include:\n   - **F1 Score**: A balance of precision and recall.\n   - **Precision**: The accuracy of positive predictions.\n   - **Recall**: The model’s ability to detect relevant entities.\n\nIn summary, the tool's response would typically consist of a list of recognized food entities from the input text along with relevant information such as their positions, labels, and confidence scores. The response would reflect the model's ability to identify food-related terms in informal language such as Instagram posts.",
            "examples": [
                {
                    "user_intent": "I need to identify food-related entities in a sentence about a meal.",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                },
                {
                    "user_intent": "Detect food items mentioned in a casual social media post about food.",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                },
                {
                    "user_intent": "Classify the food entities in this Instagram-style text: 'Today's meal: Fresh olive pok bowl topped with chia seeds.'",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 1: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 2: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 3: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 4: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 6: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 7: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 8: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 9: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 10: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.\nTool 11: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 12: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 13: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 14: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 15: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 16: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 17: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.\nTool 18: This tool uses the \"sentence-transformers/distilbert-base-nli-mean-tokens\" model to convert sentences and paragraphs into 768-dimensional dense vector representations, enabling tasks such as clustering and semantic search. It leverages the Hugging Face Transformers framework for feature extraction and requires the `sentence-transformers` library.\nTool 19: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pre-trained token classification model to load for Named Entity Recognition tasks. This model has been fine-tuned for food entity recognition in informal text.\", \"required\": true, \"example\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer associated with the pre-trained model, used to convert text into token IDs suitable for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}, \"response\": \"The tool is designed for Named Entity Recognition (NER) in the domain of food-related entities, particularly focusing on informal text such as social media posts. The response format from this tool would typically contain the following components:\\n\\n1. **Named Entities**: A list of identified entities in the input text, specifically food-related entities. Each entity would have:\\n   - **Entity Type**: The type or category of the entity, which, in this case, would be 'FOOD' for food-related terms.\\n   - **Start and End Position**: The positions in the input text where the entity starts and ends.\\n   - **Entity Label**: A label that categorizes the entity, which would typically be the word 'FOOD' in this case.\\n   - **Confidence/Score**: The model's confidence score for the classification of the entity.\\n\\n2. **Aggregation Strategy**: The results may be aggregated based on the specified strategy (e.g., 'simple'), which could determine whether the tool returns entities in a raw form or in a more processed, simplified format.\\n\\n3. **Performance Metrics**: Although the response to a specific input may not include these directly, performance statistics such as accuracy, precision, recall, and F1 score are part of the tool's performance metrics and could be referenced to give a sense of the model's quality and reliability. These include:\\n   - **F1 Score**: A balance of precision and recall.\\n   - **Precision**: The accuracy of positive predictions.\\n   - **Recall**: The model\\u2019s ability to detect relevant entities.\\n\\nIn summary, the tool's response would typically consist of a list of recognized food entities from the input text along with relevant information such as their positions, labels, and confidence scores. The response would reflect the model's ability to identify food-related terms in informal language such as Instagram posts.\", \"examples\": [{\"user_intent\": \"I need to identify food-related entities in a sentence about a meal.\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}, {\"user_intent\": \"Detect food items mentioned in a casual social media post about food.\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}, {\"user_intent\": \"Classify the food entities in this Instagram-style text: 'Today's meal: Fresh olive pok bowl topped with chia seeds.'\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}]}\n\n# User Query\nI am building a social media app that requires people to write fascinating stories rather than boring sentences. Detect named entities in a sentence by using an NER model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect named entities in a sentence to extract structured information.",
                    "tool_description": "Use a Named Entity Recognition (NER) model to identify and classify entities such as person names, locations, organizations, and miscellaneous terms in English text, enabling structured information extraction from unstructured sentences.",
                    "expected_response": "A list of identified entities including their types (PER, LOC, ORG, MISC), start and end positions in the text, and confidence scores."
                }
            ],
            "extracted_arguments": {
                "input_text": "The sentence or short passage in English from which named entities should be detected."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_59",
        "query": "How do I separate the speakers from an audio file using the pre-trained ConvTasNet_Libri2Mix_sepclean_8k model from Hugging Face?",
        "instruction": "Given a `speaker separation` task, retrieve tools that utilize pre-trained models to process audio files, specifically to distinguish and separate individual speakers, in alignment with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')",
                    "api_arguments": [
                        "repo_id",
                        "filename"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri2Mix",
                        "accuracy": {
                            "si_sdr": 14.764543634468069,
                            "si_sdr_imp": 14.764029375607246,
                            "sdr": 15.29337970745095,
                            "sdr_imp": 15.114146605113111,
                            "sir": 24.092904661115366,
                            "sir_imp": 23.913669683141528,
                            "sar": 16.06055906916849,
                            "sar_imp": -51.980784441287454,
                            "stoi": 0.9311142440593033,
                            "stoi_imp": 0.21817376142710482
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.",
                    "id": "huggingface_api_797",
                    "name": "ConvTasNet_Libri2Mix_sepclean_8k"
                },
                "id": "gorilla_huggingface_tool_792",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 1: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.",
            "Tool 2: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 3: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.",
            "Tool 4: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 7: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.",
            "Tool 8: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 9: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.",
            "Tool 10: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 11: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.",
            "Tool 12: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.",
            "Tool 13: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 14: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 15: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 16: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.",
            "Tool 17: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 18: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.",
            "Tool 19: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "parameters": {
                "num_speakers": {
                    "type": "int",
                    "description": "The exact number of speakers to be detected in the audio. This is an optional parameter that, when provided, will constrain the diarization process to detect exactly this number of speakers.",
                    "required": false,
                    "example": 2
                },
                "min_speakers": {
                    "type": "int",
                    "description": "The minimum number of speakers to be detected in the audio. This optional parameter can be used to set a lower bound on the number of speakers.",
                    "required": false,
                    "example": 1
                },
                "max_speakers": {
                    "type": "int",
                    "description": "The maximum number of speakers to be detected in the audio. This optional parameter can be used to set an upper bound on the number of speakers.",
                    "required": false,
                    "example": 4
                }
            },
            "response": "The tool returns a speaker diarization result for a given audio input. The response is typically structured as an object representing time-segmented speaker annotations, where each segment specifies the start and end times of speech and the corresponding speaker label. Key components of the response include:  \n\n1. **Segments**: A sequence of time intervals in the audio where speech is detected. Each segment contains:\n   - **Start time**: The timestamp (in seconds) marking the beginning of the speech segment.\n   - **End time**: The timestamp (in seconds) marking the end of the speech segment.\n   - **Speaker label**: A unique identifier for the speaker associated with that segment. Labels may be automatically assigned or inferred based on constraints (e.g., known number of speakers or min/max bounds).\n\n2. **Metadata (optional)**: May include additional information about the audio or diarization process, such as:\n   - **Confidence scores**: A measure of certainty for each segment’s speaker assignment.\n   - **Global information**: Total number of detected speakers or alignment with user-provided constraints (e.g., `num_speakers`, `min_speakers`, `max_speakers`).\n\n3. **Output formats**: The response can be accessed programmatically and is often exportable to standard formats such as RTTM, which encodes the same segment and speaker information in a textual representation.\n\nOverall, the response semantically provides a mapping from time intervals in the audio to speaker identities, enabling downstream tasks such as transcription labeling, speaker-specific analysis, or visualization of speaker turns.",
            "examples": [
                {
                    "user_intent": "I have a meeting recording and I want to separate the speech by speaker, but I don't know how many people are talking.",
                    "tool_call": {
                        "name": "pyannote/speaker-diarization",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "I have a 1-hour interview with exactly 3 participants and I want a diarization file showing who speaks when.",
                    "tool_call": {
                        "name": "pyannote/speaker-diarization",
                        "arguments": {
                            "num_speakers": 3
                        }
                    }
                },
                {
                    "user_intent": "I recorded a conference panel and I know there are at least 4 but no more than 6 speakers. I need to generate speaker segments.",
                    "tool_call": {
                        "name": "pyannote/speaker-diarization",
                        "arguments": {
                            "min_speakers": 4,
                            "max_speakers": 6
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 1: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.\nTool 2: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 3: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.\nTool 4: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 7: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.\nTool 8: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 9: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.\nTool 10: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 11: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.\nTool 12: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.\nTool 13: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 14: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 15: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 16: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.\nTool 17: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 18: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.\nTool 19: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\", \"parameters\": {\"num_speakers\": {\"type\": \"int\", \"description\": \"The exact number of speakers to be detected in the audio. This is an optional parameter that, when provided, will constrain the diarization process to detect exactly this number of speakers.\", \"required\": false, \"example\": 2}, \"min_speakers\": {\"type\": \"int\", \"description\": \"The minimum number of speakers to be detected in the audio. This optional parameter can be used to set a lower bound on the number of speakers.\", \"required\": false, \"example\": 1}, \"max_speakers\": {\"type\": \"int\", \"description\": \"The maximum number of speakers to be detected in the audio. This optional parameter can be used to set an upper bound on the number of speakers.\", \"required\": false, \"example\": 4}}, \"response\": \"The tool returns a speaker diarization result for a given audio input. The response is typically structured as an object representing time-segmented speaker annotations, where each segment specifies the start and end times of speech and the corresponding speaker label. Key components of the response include:  \\n\\n1. **Segments**: A sequence of time intervals in the audio where speech is detected. Each segment contains:\\n   - **Start time**: The timestamp (in seconds) marking the beginning of the speech segment.\\n   - **End time**: The timestamp (in seconds) marking the end of the speech segment.\\n   - **Speaker label**: A unique identifier for the speaker associated with that segment. Labels may be automatically assigned or inferred based on constraints (e.g., known number of speakers or min/max bounds).\\n\\n2. **Metadata (optional)**: May include additional information about the audio or diarization process, such as:\\n   - **Confidence scores**: A measure of certainty for each segment\\u2019s speaker assignment.\\n   - **Global information**: Total number of detected speakers or alignment with user-provided constraints (e.g., `num_speakers`, `min_speakers`, `max_speakers`).\\n\\n3. **Output formats**: The response can be accessed programmatically and is often exportable to standard formats such as RTTM, which encodes the same segment and speaker information in a textual representation.\\n\\nOverall, the response semantically provides a mapping from time intervals in the audio to speaker identities, enabling downstream tasks such as transcription labeling, speaker-specific analysis, or visualization of speaker turns.\", \"examples\": [{\"user_intent\": \"I have a meeting recording and I want to separate the speech by speaker, but I don't know how many people are talking.\", \"tool_call\": {\"name\": \"pyannote/speaker-diarization\", \"arguments\": {}}}, {\"user_intent\": \"I have a 1-hour interview with exactly 3 participants and I want a diarization file showing who speaks when.\", \"tool_call\": {\"name\": \"pyannote/speaker-diarization\", \"arguments\": {\"num_speakers\": 3}}}, {\"user_intent\": \"I recorded a conference panel and I know there are at least 4 but no more than 6 speakers. I need to generate speaker segments.\", \"tool_call\": {\"name\": \"pyannote/speaker-diarization\", \"arguments\": {\"min_speakers\": 4, \"max_speakers\": 6}}}]}\n\n# User Query\nHow do I separate the speakers from an audio file using the pre-trained ConvTasNet_Libri2Mix_sepclean_8k model from Hugging Face?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to separate individual speakers from a mixed audio recording using a pre-trained audio source separation model.",
                    "tool_description": "Use the ConvTasNet_Libri2Mix_sepclean_8k model from Hugging Face to perform audio source separation, isolating individual speech sources from a mixed audio file.",
                    "expected_response": "A collection of audio streams, each corresponding to a separated speaker, suitable for playback or further processing."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The input audio file containing mixed speech from multiple speakers.",
                "model_name": "The identifier of the pre-trained ConvTasNet model for speech separation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_294",
        "query": "A language service wants to incorporate a speech-to-speech translation feature that assists users in translating Hokkien to English on an audio file.",
        "instruction": "Given a `speech-to-speech translation` task, retrieve tools that facilitate the translation of audio inputs from one language to another, specifically focusing on converting Hokkien speech to English through automated processing and model generation techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "Speech-to-speech translation",
                    "api_call": "S2THubInterface()",
                    "api_arguments": {
                        "task": "speech_to_text",
                        "model": "facebook/xm_transformer_s2ut_hk-en",
                        "generator": "task.build_generator([model], cfg)",
                        "sample": "S2THubInterface.get_model_input(task, audio)"
                    },
                    "python_environment_requirements": {
                        "fairseq": "latest",
                        "torchaudio": "latest",
                        "huggingface_hub": "latest"
                    },
                    "example_code": "import json\nimport os\nfrom pathlib import Path\nimport IPython.display as ipd\nfrom fairseq import hub_utils\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\nfrom huggingface_hub import snapshot_download\nimport torchaudio\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/xm_transformer_s2ut_hk-en,\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\n cache_dir=cache_dir,\n)\nmodel = models[0].cpu()\ncfg[task].cpu = True\ngenerator = task.build_generator([model], cfg)\naudio, _ = torchaudio.load(/path/to/an/audio/file)\nsample = S2THubInterface.get_model_input(task, audio)\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\nlibrary_name = fairseq\ncache_dir = (\n cache_dir or (Path.home() / .cache / library_name).as_posix()\n)\ncache_dir = snapshot_download(\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\n)\nx = hub_utils.from_pretrained(\n cache_dir,\n model.pt,\n .,\n archive_map=CodeHiFiGANVocoder.hub_models(),\n config_yaml=config.json,\n fp16=False,\n is_vocoder=True,\n)\nwith open(f{x['args']['data']}/config.json) as f:\n vocoder_cfg = json.load(f)\nassert (\n len(x[args][model_path]) == 1\n), Too many vocoder models in the input\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\ntts_sample = tts_model.get_model_input(unit)\nwav, sr = tts_model.get_prediction(tts_sample)\nipd.Audio(wav, rate=sr)",
                    "performance": {
                        "dataset": "TED, drama, TAT domain",
                        "accuracy": "Not provided"
                    },
                    "description": "Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.",
                    "id": "huggingface_api_798",
                    "name": "xm_transformer_s2ut_hk-en"
                },
                "id": "gorilla_huggingface_tool_793",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.",
            "Tool 1: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 2: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.",
            "Tool 3: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 4: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 5: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 6: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.",
            "Tool 7: Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.",
            "Tool 8: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 9: Whisper-large is a pre-trained model for automatic speech recognition (ASR) and speech translation, capable of converting spoken audio into text and translating speech across languages. Trained on 680,000 hours of labeled data, it generalizes well across diverse datasets and domains, enabling robust transcription and translation without additional fine-tuning.",
            "Tool 10: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 11: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 12: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 13: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.",
            "Tool 14: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 15: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.",
            "Tool 16: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 17: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 18: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.",
            "Tool 19: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings."
        ],
        "tool_view": {
            "description": "The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.",
            "parameters": {
                "task": {
                    "type": "string",
                    "description": "The specific Fairseq task to perform, such as 'speech_to_text' for converting speech audio to text.",
                    "required": true,
                    "example": "speech_to_text"
                },
                "model": {
                    "type": "string",
                    "description": "The identifier or path of the Fairseq model to use for speech-to-speech translation.",
                    "required": true,
                    "example": "facebook/xm_transformer_s2ut_hk-en"
                },
                "generator": {
                    "type": "string",
                    "description": "The generator object built from the task and model, used to produce predictions from model inputs.",
                    "required": true,
                    "example": "task.build_generator([model], cfg)"
                },
                "sample": {
                    "type": "string",
                    "description": "The processed input sample obtained from the audio data, formatted for the model's input requirements.",
                    "required": true,
                    "example": "S2THubInterface.get_model_input(task, audio)"
                }
            },
            "response": "The tool produces a **speech-to-speech translation output** from an input audio signal. The response can be described in the following structure and components:\n\n1. **Unit representation of translated speech**  \n   - After processing the input audio through the speech-to-text and translation stages, the tool produces an intermediate representation (often discrete units or embeddings) of the target speech.  \n   - This representation is not raw audio but a symbolic or learned sequence that encodes the translated content while preserving prosody, timing, and speaker characteristics.\n\n2. **Synthesized waveform**  \n   - The intermediate unit representation is passed through a vocoder model (CodeHiFiGANVocoder in this case) to generate a **time-domain audio waveform** in the target language.  \n   - The waveform is returned as a numeric array (typically a 1D floating-point array representing audio samples).\n\n3. **Sampling rate**  \n   - Along with the waveform, the tool provides the **sampling rate** of the generated audio, which specifies how many samples per second the waveform contains. This allows correct playback and further audio processing.\n\n4. **Optional metadata (implicit or returned by the framework)**  \n   - May include information such as the duration of the translated audio, the units used in intermediate representation, or other task-specific details depending on the Fairseq `HubInterface` implementation.  \n\n**Summary:**  \nThe main content of the tool’s response is a tuple of the **synthesized speech waveform** and its **sampling rate**, generated from an input audio file. Internally, the process involves intermediate unit representations, but the final response is directly playable audio in the target language.  \n\n",
            "examples": [
                {
                    "user_intent": "Translate a Hokkien podcast episode to English audio.",
                    "tool_call": {
                        "name": "S2THubInterface",
                        "arguments": {
                            "task": "speech_to_text",
                            "model": "facebook/xm_transformer_s2ut_hk-en",
                            "generator": "task.build_generator([model], cfg)",
                            "sample": "S2THubInterface.get_model_input(task, '/path/to/hokkien_podcast.wav')"
                        }
                    }
                },
                {
                    "user_intent": "Convert a recorded Hokkien lecture into English speech for students.",
                    "tool_call": {
                        "name": "S2THubInterface",
                        "arguments": {
                            "task": "speech_to_text",
                            "model": "facebook/xm_transformer_s2ut_hk-en",
                            "generator": "task.build_generator([model], cfg)",
                            "sample": "S2THubInterface.get_model_input(task, '/path/to/lecture_recording.wav')"
                        }
                    }
                },
                {
                    "user_intent": "Translate short Hokkien audio messages into English voice clips.",
                    "tool_call": {
                        "name": "S2THubInterface",
                        "arguments": {
                            "task": "speech_to_text",
                            "model": "facebook/xm_transformer_s2ut_hk-en",
                            "generator": "task.build_generator([model], cfg)",
                            "sample": "S2THubInterface.get_model_input(task, '/path/to/short_messages.wav')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\nTool 1: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 2: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.\nTool 3: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 4: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 5: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 6: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.\nTool 7: Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.\nTool 8: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 9: Whisper-large is a pre-trained model for automatic speech recognition (ASR) and speech translation, capable of converting spoken audio into text and translating speech across languages. Trained on 680,000 hours of labeled data, it generalizes well across diverse datasets and domains, enabling robust transcription and translation without additional fine-tuning.\nTool 10: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 11: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 12: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 13: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.\nTool 14: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 15: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.\nTool 16: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 17: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 18: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.\nTool 19: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"xm_transformer_s2ut_hk-en\\\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\", \"parameters\": {\"task\": {\"type\": \"string\", \"description\": \"The specific Fairseq task to perform, such as 'speech_to_text' for converting speech audio to text.\", \"required\": true, \"example\": \"speech_to_text\"}, \"model\": {\"type\": \"string\", \"description\": \"The identifier or path of the Fairseq model to use for speech-to-speech translation.\", \"required\": true, \"example\": \"facebook/xm_transformer_s2ut_hk-en\"}, \"generator\": {\"type\": \"string\", \"description\": \"The generator object built from the task and model, used to produce predictions from model inputs.\", \"required\": true, \"example\": \"task.build_generator([model], cfg)\"}, \"sample\": {\"type\": \"string\", \"description\": \"The processed input sample obtained from the audio data, formatted for the model's input requirements.\", \"required\": true, \"example\": \"S2THubInterface.get_model_input(task, audio)\"}}, \"response\": \"The tool produces a **speech-to-speech translation output** from an input audio signal. The response can be described in the following structure and components:\\n\\n1. **Unit representation of translated speech**  \\n   - After processing the input audio through the speech-to-text and translation stages, the tool produces an intermediate representation (often discrete units or embeddings) of the target speech.  \\n   - This representation is not raw audio but a symbolic or learned sequence that encodes the translated content while preserving prosody, timing, and speaker characteristics.\\n\\n2. **Synthesized waveform**  \\n   - The intermediate unit representation is passed through a vocoder model (CodeHiFiGANVocoder in this case) to generate a **time-domain audio waveform** in the target language.  \\n   - The waveform is returned as a numeric array (typically a 1D floating-point array representing audio samples).\\n\\n3. **Sampling rate**  \\n   - Along with the waveform, the tool provides the **sampling rate** of the generated audio, which specifies how many samples per second the waveform contains. This allows correct playback and further audio processing.\\n\\n4. **Optional metadata (implicit or returned by the framework)**  \\n   - May include information such as the duration of the translated audio, the units used in intermediate representation, or other task-specific details depending on the Fairseq `HubInterface` implementation.  \\n\\n**Summary:**  \\nThe main content of the tool\\u2019s response is a tuple of the **synthesized speech waveform** and its **sampling rate**, generated from an input audio file. Internally, the process involves intermediate unit representations, but the final response is directly playable audio in the target language.  \\n\\n\", \"examples\": [{\"user_intent\": \"Translate a Hokkien podcast episode to English audio.\", \"tool_call\": {\"name\": \"S2THubInterface\", \"arguments\": {\"task\": \"speech_to_text\", \"model\": \"facebook/xm_transformer_s2ut_hk-en\", \"generator\": \"task.build_generator([model], cfg)\", \"sample\": \"S2THubInterface.get_model_input(task, '/path/to/hokkien_podcast.wav')\"}}}, {\"user_intent\": \"Convert a recorded Hokkien lecture into English speech for students.\", \"tool_call\": {\"name\": \"S2THubInterface\", \"arguments\": {\"task\": \"speech_to_text\", \"model\": \"facebook/xm_transformer_s2ut_hk-en\", \"generator\": \"task.build_generator([model], cfg)\", \"sample\": \"S2THubInterface.get_model_input(task, '/path/to/lecture_recording.wav')\"}}}, {\"user_intent\": \"Translate short Hokkien audio messages into English voice clips.\", \"tool_call\": {\"name\": \"S2THubInterface\", \"arguments\": {\"task\": \"speech_to_text\", \"model\": \"facebook/xm_transformer_s2ut_hk-en\", \"generator\": \"task.build_generator([model], cfg)\", \"sample\": \"S2THubInterface.get_model_input(task, '/path/to/short_messages.wav')\"}}}]}\n\n# User Query\nA language service wants to incorporate a speech-to-speech translation feature that assists users in translating Hokkien to English on an audio file.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate Hokkien audio recordings into English speech for users.",
                    "tool_description": "Use a Fairseq-based speech-to-speech translation model to convert spoken Hokkien audio into English speech output. The process involves extracting an intermediate unit representation from the input audio and synthesizing it into a target language waveform using a vocoder.",
                    "expected_response": "A synthesized English speech waveform along with its sampling rate, generated from the input Hokkien audio."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The Hokkien-language audio file or audio data to be translated.",
                "model": "The identifier of the speech-to-speech translation model capable of Hokkien-to-English conversion.",
                "task": "The Fairseq task indicating speech-to-speech translation processing.",
                "generator": "The generator object built from the task and model, used to produce predictions from the audio input.",
                "sample": "The processed input sample obtained from the audio data, formatted for model input requirements."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_168",
        "query": "There is an upcoming event called \"Space Party\" and we need a representative image for the event. Can you assist us in creating an image containing a party in space with astronauts and aliens having fun together?",
        "instruction": "Given an `image generation` task, retrieve tools that utilize text prompts to create visually appealing images based on the query's specifications, such as theme and elements to be included in the image.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Generation",
                    "api_call": "StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')",
                    "api_arguments": {
                        "prompt": "a photo of an astronaut riding a horse on mars"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "scipy",
                        "safetensors"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nmodel_id = stabilityai/stable-diffusion-2-1\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)",
                    "performance": {
                        "dataset": "COCO2017",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.",
                    "id": "huggingface_api_36",
                    "name": "stabilityai/stable-diffusion-2-1"
                },
                "id": "gorilla_huggingface_tool_36",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 1: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 2: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 3: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.",
            "Tool 4: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.",
            "Tool 5: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.",
            "Tool 6: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.",
            "Tool 7: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 8: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 9: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 10: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 11: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 12: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 13: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.",
            "Tool 14: The sentence-transformers/all-distilroberta-v1 model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering and semantic search. It uses the Hugging Face Transformers framework and is accessed through the SentenceTransformer API.",
            "Tool 15: The `sentence-transformers/distilbert-base-nli-stsb-mean-tokens` model maps sentences and paragraphs to a 768-dimensional dense vector space, enabling tasks such as clustering and semantic search. It uses Hugging Face's Transformers framework and can be accessed via the SentenceTransformer API, which generates sentence embeddings from input text.",
            "Tool 16: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 17: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 18: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 19: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity."
        ],
        "tool_view": {
            "description": "The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "parameters": {
                "images": {
                    "type": "array",
                    "description": "A list or sequence of video frames represented as arrays, where each frame is typically a 3D array (channels, height, width). This input is used by the model for video classification.",
                    "required": true,
                    "example": [
                        [
                            [
                                [
                                    0.1,
                                    0.2
                                ],
                                [
                                    0.3,
                                    0.4
                                ]
                            ],
                            [
                                [
                                    0.5,
                                    0.6
                                ],
                                [
                                    0.7,
                                    0.8
                                ]
                            ],
                            [
                                [
                                    0.9,
                                    1.0
                                ],
                                [
                                    1.1,
                                    1.2
                                ]
                            ]
                        ]
                    ]
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors the processor should return. Common values include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool’s response is an object representing the output of a video classification model. Its structure and components are as follows:\n\n1. **`logits`**  \n   - This is a tensor containing the raw, unnormalized scores produced by the model for each class in the classification task.  \n   - The shape typically corresponds to `[batch_size, num_classes]`, where `batch_size` is the number of input video samples and `num_classes` is the total number of target classes the model can predict.  \n   - Higher values indicate stronger model confidence for the corresponding class, but they are not probabilities.\n\n2. **`predicted_class_idx`** (derived from `logits`)  \n   - This is the integer index of the class with the highest score in `logits` for a given input video.  \n   - It represents the model’s most likely prediction for that video.\n\n3. **`predicted_class_label`** (derived using `model.config.id2label`)  \n   - This is a human-readable string corresponding to `predicted_class_idx`.  \n   - It maps the integer class index to the semantic label defined in the model’s configuration (e.g., “picking something up” or “throwing an object”).\n\n4. **Optional additional fields**  \n   - Depending on the framework version and configuration, the response object may also include intermediate outputs such as attention weights or hidden states, but these are typically optional and not returned by default.  \n\nIn summary, the primary semantic content of the response is the model’s **raw class scores (`logits`)** and the **predicted class** for the input video, with a mapping to a human-readable label for interpretation.",
            "examples": [
                {
                    "user_intent": "Classify a video using the TimeSformer model pre-trained on the Something Something v2 dataset.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I want to run video classification on a sample video input with the TimeSformer model and get the predicted class.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Process a set of random images as a video input and classify it using TimeSformer.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 1: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 2: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 3: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.\nTool 4: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.\nTool 5: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.\nTool 6: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.\nTool 7: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 8: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 9: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 10: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 11: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 12: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 13: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.\nTool 14: The sentence-transformers/all-distilroberta-v1 model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering and semantic search. It uses the Hugging Face Transformers framework and is accessed through the SentenceTransformer API.\nTool 15: The `sentence-transformers/distilbert-base-nli-stsb-mean-tokens` model maps sentences and paragraphs to a 768-dimensional dense vector space, enabling tasks such as clustering and semantic search. It uses Hugging Face's Transformers framework and can be accessed via the SentenceTransformer API, which generates sentence embeddings from input text.\nTool 16: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 17: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 18: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 19: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \\\"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\\\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\", \"parameters\": {\"images\": {\"type\": \"array\", \"description\": \"A list or sequence of video frames represented as arrays, where each frame is typically a 3D array (channels, height, width). This input is used by the model for video classification.\", \"required\": true, \"example\": [[[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[0.9, 1.0], [1.1, 1.2]]]]}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors the processor should return. Common values include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool\\u2019s response is an object representing the output of a video classification model. Its structure and components are as follows:\\n\\n1. **`logits`**  \\n   - This is a tensor containing the raw, unnormalized scores produced by the model for each class in the classification task.  \\n   - The shape typically corresponds to `[batch_size, num_classes]`, where `batch_size` is the number of input video samples and `num_classes` is the total number of target classes the model can predict.  \\n   - Higher values indicate stronger model confidence for the corresponding class, but they are not probabilities.\\n\\n2. **`predicted_class_idx`** (derived from `logits`)  \\n   - This is the integer index of the class with the highest score in `logits` for a given input video.  \\n   - It represents the model\\u2019s most likely prediction for that video.\\n\\n3. **`predicted_class_label`** (derived using `model.config.id2label`)  \\n   - This is a human-readable string corresponding to `predicted_class_idx`.  \\n   - It maps the integer class index to the semantic label defined in the model\\u2019s configuration (e.g., \\u201cpicking something up\\u201d or \\u201cthrowing an object\\u201d).\\n\\n4. **Optional additional fields**  \\n   - Depending on the framework version and configuration, the response object may also include intermediate outputs such as attention weights or hidden states, but these are typically optional and not returned by default.  \\n\\nIn summary, the primary semantic content of the response is the model\\u2019s **raw class scores (`logits`)** and the **predicted class** for the input video, with a mapping to a human-readable label for interpretation.\", \"examples\": [{\"user_intent\": \"Classify a video using the TimeSformer model pre-trained on the Something Something v2 dataset.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I want to run video classification on a sample video input with the TimeSformer model and get the predicted class.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Process a set of random images as a video input and classify it using TimeSformer.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nThere is an upcoming event called \"Space Party\" and we need a representative image for the event. Can you assist us in creating an image containing a party in space with astronauts and aliens having fun together?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Create an image of a party happening in space with astronauts and aliens.",
                    "tool_description": "Generate an image based on a description of a party taking place in space, featuring astronauts and aliens having fun together.",
                    "expected_response": "A generated image that visually represents the specified scene, including astronauts, aliens, and the space-themed setting."
                }
            ],
            "extracted_arguments": {
                "scene_description": "The narrative or visual description of the image to be generated, including astronauts, aliens, and space-themed elements.",
                "theme": "The specific theme or context of the event, such as a party or celebration in space."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_21",
        "query": "We are building an audio assistant. Apply noise suppression to our new voice commands.",
        "instruction": "Given an `audio enhancement` task, retrieve tools that specialize in processing audio inputs to suppress noise and enhance the quality of voice commands, aligning with the requirements for developing an audio assistant.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "AutoModelForAudioToAudio.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')",
                    "api_arguments": "pretrained_model_name_or_path",
                    "python_environment_requirements": [
                        "transformers",
                        "asteroid"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri1Mix",
                        "accuracy": {
                            "si_sdr": 13.329767398333798,
                            "si_sdr_imp": 9.879986092474098,
                            "sdr": 13.87279932997016,
                            "sdr_imp": 10.370136530757103,
                            "sir": "Infinity",
                            "sir_imp": "NaN",
                            "sar": 13.87279932997016,
                            "sar_imp": 10.370136530757103,
                            "stoi": 0.9140907015623948,
                            "stoi_imp": 0.11817087802185405
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.",
                    "id": "huggingface_api_776",
                    "name": "DCCRNet_Libri1Mix_enhsingle_16k"
                },
                "id": "gorilla_huggingface_tool_771",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 1: A fine-tuned Audio Spectrogram Transformer (AST) model for classifying audio commands, which converts audio into spectrogram images and applies a Vision Transformer to achieve high-accuracy results on the Speech Commands v2 dataset. It is used to perform audio classification tasks efficiently using the Hugging Face Transformers framework.",
            "Tool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 3: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 4: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 5: This tool uses a fine-tuned Hugging Face Transformers model to perform audio classification, specifically recognizing speech commands. It is highly accurate, achieving 99.79% accuracy on a benchmark speech commands dataset, making it suitable for applications that require reliable automated speech recognition and command detection.",
            "Tool 6: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 7: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.",
            "Tool 8: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.",
            "Tool 9: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 10: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 11: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 12: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 13: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.",
            "Tool 14: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 15: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 16: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 17: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 18: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 19: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency."
        ],
        "tool_view": {
            "description": "This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.",
                    "required": true,
                    "example": 0.25
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.",
                    "required": true,
                    "example": 0.45
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.",
                    "required": true,
                    "example": false
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections to return per image.",
                    "required": true,
                    "example": 1000
                },
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. Can be a local file path or a URL pointing to the image.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool’s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \n\n1. **`boxes`** – A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \n   - A confidence score indicating the model’s certainty that the object belongs to the target class.  \n   - Optional class label or ID identifying the object category (here, all are buildings).  \n\n2. **`masks`** – Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \n\n3. **`scores`** – Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \n\n4. **`class_ids` or `labels`** – Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \n\n5. **`metadata`** (optional) – May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \n\n6. **Visualization helpers** – Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \n\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.",
            "examples": [
                {
                    "user_intent": "I want to segment all buildings in a satellite image of a city block.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500,
                            "image": "https://example.com/satellite_city_block.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect and segment buildings in a rural area satellite image with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.6,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 200,
                            "image": "https://example.com/rural_area_satellite.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Process a satellite image to extract building masks for GIS analysis.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000,
                            "image": "https://example.com/gis_project_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 1: A fine-tuned Audio Spectrogram Transformer (AST) model for classifying audio commands, which converts audio into spectrogram images and applies a Vision Transformer to achieve high-accuracy results on the Speech Commands v2 dataset. It is used to perform audio classification tasks efficiently using the Hugging Face Transformers framework.\nTool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 3: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 4: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 5: This tool uses a fine-tuned Hugging Face Transformers model to perform audio classification, specifically recognizing speech commands. It is highly accurate, achieving 99.79% accuracy on a benchmark speech commands dataset, making it suitable for applications that require reliable automated speech recognition and command detection.\nTool 6: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 7: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.\nTool 8: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.\nTool 9: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 10: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 11: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 12: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 13: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.\nTool 14: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 15: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 16: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 17: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 18: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 19: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.\", \"required\": true, \"example\": 0.25}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.\", \"required\": true, \"example\": 0.45}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.\", \"required\": true, \"example\": false}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections to return per image.\", \"required\": true, \"example\": 1000}, \"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. Can be a local file path or a URL pointing to the image.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool\\u2019s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \\n\\n1. **`boxes`** \\u2013 A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \\n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \\n   - A confidence score indicating the model\\u2019s certainty that the object belongs to the target class.  \\n   - Optional class label or ID identifying the object category (here, all are buildings).  \\n\\n2. **`masks`** \\u2013 Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \\n\\n3. **`scores`** \\u2013 Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \\n\\n4. **`class_ids` or `labels`** \\u2013 Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \\n\\n5. **`metadata`** (optional) \\u2013 May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \\n\\n6. **Visualization helpers** \\u2013 Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \\n\\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.\", \"examples\": [{\"user_intent\": \"I want to segment all buildings in a satellite image of a city block.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500, \"image\": \"https://example.com/satellite_city_block.jpg\"}}}, {\"user_intent\": \"Detect and segment buildings in a rural area satellite image with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.6, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 200, \"image\": \"https://example.com/rural_area_satellite.jpg\"}}}, {\"user_intent\": \"Process a satellite image to extract building masks for GIS analysis.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://example.com/gis_project_image.jpg\"}}}]}\n\n# User Query\nWe are building an audio assistant. Apply noise suppression to our new voice commands.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Apply noise suppression to the new voice command audio recordings.",
                    "tool_description": "Perform speech enhancement by denoising audio using a pretrained SepFormer model to improve clarity of speech in noisy audio recordings.",
                    "expected_response": "An enhanced audio file or audio data with reduced background noise and improved speech clarity."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The audio data or audio file containing the voice commands requiring noise suppression."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_164",
        "query": "I am an English teacher. What kind of creative writing exercise could I give to my students that they could write a story using the words \"moon\", \"rabbit\", \"forest\", and \"magic\"?",
        "instruction": "Given a `creative writing exercise` task, retrieve tools that facilitate generative text tasks, such as forming coherent and imaginative stories, by processing input words or themes to produce creative narrative outputs aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Generative Commonsense Reasoning",
                    "api_call": "AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-common_gen')",
                    "api_arguments": [
                        "words",
                        "max_length"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelWithLMHead, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\ndef gen_sentence(words, max_length=32):\n input_text = words\n features = tokenizer([input_text], return_tensors='pt')\noutput = model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'], max_length=max_length)\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\nwords = tree plant ground hole dig\ngen_sentence(words)",
                    "performance": {
                        "dataset": "common_gen",
                        "accuracy": {
                            "ROUGE-2": 17.1,
                            "ROUGE-L": 39.47
                        }
                    },
                    "description": "Google's T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.",
                    "id": "huggingface_api_631",
                    "name": "mrm8488/t5-base-finetuned-common_gen"
                },
                "id": "gorilla_huggingface_tool_626",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 1: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.",
            "Tool 2: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 3: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "Tool 4: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.",
            "Tool 5: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.",
            "Tool 6: EleutherAI/gpt-j-6B is a 6-billion-parameter transformer model designed for natural language text generation. Trained on the large-scale Pile dataset, it leverages a 28-layer architecture with 16 attention heads and Rotary Position Embeddings, supporting high-quality generation across diverse tasks. It can be used via the Hugging Face Transformers library to generate coherent, context-aware text for applications such as creative writing, dialogue systems, and general-purpose language modeling.",
            "Tool 7: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 8: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 9: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 10: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "Tool 11: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.",
            "Tool 12: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 13: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 14: A pretrained BERT large (cased) model for English that performs masked language modeling, allowing users to predict missing words in text. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336 million parameters, making it suitable for tasks like text completion, language understanding, and NLP evaluation.",
            "Tool 15: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 16: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 17: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 18: Stable Diffusion Inpainting is a text-to-image model that generates photorealistic images from text prompts and allows targeted editing of existing images using a mask, making it useful for creative image manipulation and inpainting specific areas while preserving the rest of the image.",
            "Tool 19: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French."
        ],
        "tool_view": {
            "description": "GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text prompt that the model will continue or generate text from.",
                    "required": true,
                    "example": "Once upon a time"
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens to generate in the output sequence.",
                    "required": true,
                    "example": 50
                },
                "num_return_sequences": {
                    "type": "number",
                    "description": "The number of independent text sequences to generate for the given input prompt.",
                    "required": true,
                    "example": 3
                }
            },
            "response": "The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\n\nHere’s a description of the response structure:\n\n1. **Generated Texts**: \n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \n   - Each sequence will represent a continuation or completion of the input text (e.g., \"Hello, I'm a language model,\").\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\n\n2. **Content of Each Text Generation**: \n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\n\n### Key Components:\n\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\n   - Each element in the array is a string, representing a complete generated text.\n\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\n\n### Example of Expected Output Content (Abstract):\n\n- A list of 5 strings, each one being a text sequence that continues from the input (\"Hello, I'm a language model,\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\n\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.",
            "examples": [
                {
                    "user_intent": "Generate a short creative story starting with 'Once upon a time in a distant galaxy,'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "Once upon a time in a distant galaxy,",
                            "max_length": 50,
                            "num_return_sequences": 3
                        }
                    }
                },
                {
                    "user_intent": "Continue this sentence realistically: 'The stock market crashed yesterday because'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "The stock market crashed yesterday because",
                            "max_length": 40,
                            "num_return_sequences": 2
                        }
                    }
                },
                {
                    "user_intent": "Write multiple humorous completions for: 'I never expected my cat to'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "I never expected my cat to",
                            "max_length": 30,
                            "num_return_sequences": 5
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 1: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.\nTool 2: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 3: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\nTool 4: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.\nTool 5: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.\nTool 6: EleutherAI/gpt-j-6B is a 6-billion-parameter transformer model designed for natural language text generation. Trained on the large-scale Pile dataset, it leverages a 28-layer architecture with 16 attention heads and Rotary Position Embeddings, supporting high-quality generation across diverse tasks. It can be used via the Hugging Face Transformers library to generate coherent, context-aware text for applications such as creative writing, dialogue systems, and general-purpose language modeling.\nTool 7: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 8: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 9: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 10: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\nTool 11: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.\nTool 12: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 13: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 14: A pretrained BERT large (cased) model for English that performs masked language modeling, allowing users to predict missing words in text. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336 million parameters, making it suitable for tasks like text completion, language understanding, and NLP evaluation.\nTool 15: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 16: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 17: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 18: Stable Diffusion Inpainting is a text-to-image model that generates photorealistic images from text prompts and allows targeted editing of existing images using a mask, making it useful for creative image manipulation and inpainting specific areas while preserving the rest of the image.\nTool 19: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text prompt that the model will continue or generate text from.\", \"required\": true, \"example\": \"Once upon a time\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens to generate in the output sequence.\", \"required\": true, \"example\": 50}, \"num_return_sequences\": {\"type\": \"number\", \"description\": \"The number of independent text sequences to generate for the given input prompt.\", \"required\": true, \"example\": 3}}, \"response\": \"The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\\n\\nHere\\u2019s a description of the response structure:\\n\\n1. **Generated Texts**: \\n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \\n   - Each sequence will represent a continuation or completion of the input text (e.g., \\\"Hello, I'm a language model,\\\").\\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\\n\\n2. **Content of Each Text Generation**: \\n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\\n\\n### Key Components:\\n\\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \\n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\\n   - Each element in the array is a string, representing a complete generated text.\\n\\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\\n\\n### Example of Expected Output Content (Abstract):\\n\\n- A list of 5 strings, each one being a text sequence that continues from the input (\\\"Hello, I'm a language model,\\\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\\n\\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.\", \"examples\": [{\"user_intent\": \"Generate a short creative story starting with 'Once upon a time in a distant galaxy,'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"Once upon a time in a distant galaxy,\", \"max_length\": 50, \"num_return_sequences\": 3}}}, {\"user_intent\": \"Continue this sentence realistically: 'The stock market crashed yesterday because'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"The stock market crashed yesterday because\", \"max_length\": 40, \"num_return_sequences\": 2}}}, {\"user_intent\": \"Write multiple humorous completions for: 'I never expected my cat to'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"I never expected my cat to\", \"max_length\": 30, \"num_return_sequences\": 5}}}]}\n\n# User Query\nI am an English teacher. What kind of creative writing exercise could I give to my students that they could write a story using the words \"moon\", \"rabbit\", \"forest\", and \"magic\"?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a creative writing prompt for students that includes specific keywords.",
                    "tool_description": "Use a language model to generate a story prompt or exercise that encourages students to write creatively using specified words.",
                    "expected_response": "A short text describing a creative writing exercise or story prompt incorporating all the given keywords."
                },
                {
                    "user_intent": "Provide example story continuations using the keywords 'moon', 'rabbit', 'forest', and 'magic'.",
                    "tool_description": "Generate multiple possible story continuations that include the specified keywords, demonstrating ways students could write their stories.",
                    "expected_response": "A list of text sequences, each representing a story snippet or continuation that includes all the target keywords."
                }
            ],
            "extracted_arguments": {
                "keywords": "A list of target words that must appear in the creative writing output (string array).",
                "language": "The language in which the story should be written (English).",
                "exercise_type": "The type of writing activity to generate, e.g., short story, prompt, or continuation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_486",
        "query": "I am a journalist. I am writing an article about European start-ups. I need to see all the dates and company names in the text.",
        "instruction": "Given a `information extraction` task, retrieve tools that specialize in Named Entity Recognition (NER) to identify and extract specific entities such as dates and company names from the text, aligning with the query's content and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/camembert-ner')",
                    "api_arguments": {
                        "model": "model",
                        "tokenizer": "tokenizer",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForTokenClassification, pipeline"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\n\nfrom transformers import pipeline\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot « computer  est retir le 9 janvier 2015.)",
                    "performance": {
                        "dataset": "wikiner-fr",
                        "accuracy": {
                            "overall_f1": 0.8914,
                            "PER_f1": 0.9483,
                            "ORG_f1": 0.8181,
                            "LOC_f1": 0.8955,
                            "MISC_f1": 0.8146
                        }
                    },
                    "description": "camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.",
                    "id": "huggingface_api_405",
                    "name": "Jean-Baptiste/camembert-ner"
                },
                "id": "gorilla_huggingface_tool_401",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 1: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 2: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.",
            "Tool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 4: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 5: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 6: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 8: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 9: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 10: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.",
            "Tool 11: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 12: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 13: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 14: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 15: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 16: The sentence-transformers/all-distilroberta-v1 model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering and semantic search. It uses the Hugging Face Transformers framework and is accessed through the SentenceTransformer API.",
            "Tool 17: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 18: EleutherAI/gpt-j-6B is a 6-billion-parameter transformer model designed for natural language text generation. Trained on the large-scale Pile dataset, it leverages a 28-layer architecture with 16 attention heads and Rotary Position Embeddings, supporting high-quality generation across diverse tasks. It can be used via the Hugging Face Transformers library to generate coherent, context-aware text for applications such as creative writing, dialogue systems, and general-purpose language modeling.",
            "Tool 19: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use."
        ],
        "tool_view": {
            "description": "A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "parameters": {
                "inputs": {
                    "type": "string",
                    "description": "The text input that will be processed by the model to perform entity extraction. This is typically the raw text containing entities (like company names) that the model will extract.",
                    "required": true,
                    "example": "I love AutoTrain"
                }
            },
            "response": "The tool produces a **token classification response** typical of Hugging Face Transformers models for entity extraction. The response structure can be described as follows:\n\n1. **Token-level outputs**:  \n   The model returns a list of predictions corresponding to each token in the input text. Each element in this list represents a single token and contains information about the model’s classification of that token.\n\n2. **Typical fields per token**:  \n   - **`word` or `token`**: The actual text of the token from the input sentence.  \n   - **`entity` or `label`**: The predicted entity label assigned to this token (e.g., company name, organization, or non-entity). Labels usually follow the IOB or BIO tagging scheme.  \n   - **`score` or `confidence`**: A probability or confidence score indicating how confident the model is in its predicted label.  \n   - **`start` and `end`** (optional depending on implementation): Character offsets in the original input string that indicate the span of text covered by the token.\n\n3. **Aggregated entities (optional)**:  \n   Some implementations may post-process token-level outputs to group consecutive tokens with the same entity label into a single entity mention, returning:  \n   - **`entity_group` or `entity`**: The combined label for the entity span.  \n   - **`text`**: The reconstructed text of the entity from consecutive tokens.  \n   - **`start` and `end`**: Character offsets for the span of the entity in the original text.  \n   - **`score`**: Confidence score for the grouped entity, often computed as an aggregate of token-level scores.\n\n4. **Overall semantics**:  \n   The response allows a user to identify **which parts of the input text correspond to company names**, along with model confidence and positional information, enabling downstream tasks such as extraction, highlighting, or database population.  \n\nIn summary, the response is a **structured token-level prediction** that may optionally be aggregated into entity spans, providing the predicted label, confidence, and positional information for each recognized entity in the text.",
            "examples": [
                {
                    "user_intent": "Extract the company names mentioned in the sentence 'I recently started working at OpenAI and Microsoft.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "I recently started working at OpenAI and Microsoft."
                        }
                    }
                },
                {
                    "user_intent": "Identify entities representing companies in the text 'Tesla, SpaceX, and Neuralink are all led by Elon Musk.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "Tesla, SpaceX, and Neuralink are all led by Elon Musk."
                        }
                    }
                },
                {
                    "user_intent": "Find all company names in the paragraph: 'Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 1: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 2: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.\nTool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 4: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 5: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 6: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 8: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 9: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 10: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.\nTool 11: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 12: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 13: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 14: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 15: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 16: The sentence-transformers/all-distilroberta-v1 model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering and semantic search. It uses the Hugging Face Transformers framework and is accessed through the SentenceTransformer API.\nTool 17: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 18: EleutherAI/gpt-j-6B is a 6-billion-parameter transformer model designed for natural language text generation. Trained on the large-scale Pile dataset, it leverages a 28-layer architecture with 16 attention heads and Rotary Position Embeddings, supporting high-quality generation across diverse tasks. It can be used via the Hugging Face Transformers library to generate coherent, context-aware text for applications such as creative writing, dialogue systems, and general-purpose language modeling.\nTool 19: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\", \"parameters\": {\"inputs\": {\"type\": \"string\", \"description\": \"The text input that will be processed by the model to perform entity extraction. This is typically the raw text containing entities (like company names) that the model will extract.\", \"required\": true, \"example\": \"I love AutoTrain\"}}, \"response\": \"The tool produces a **token classification response** typical of Hugging Face Transformers models for entity extraction. The response structure can be described as follows:\\n\\n1. **Token-level outputs**:  \\n   The model returns a list of predictions corresponding to each token in the input text. Each element in this list represents a single token and contains information about the model\\u2019s classification of that token.\\n\\n2. **Typical fields per token**:  \\n   - **`word` or `token`**: The actual text of the token from the input sentence.  \\n   - **`entity` or `label`**: The predicted entity label assigned to this token (e.g., company name, organization, or non-entity). Labels usually follow the IOB or BIO tagging scheme.  \\n   - **`score` or `confidence`**: A probability or confidence score indicating how confident the model is in its predicted label.  \\n   - **`start` and `end`** (optional depending on implementation): Character offsets in the original input string that indicate the span of text covered by the token.\\n\\n3. **Aggregated entities (optional)**:  \\n   Some implementations may post-process token-level outputs to group consecutive tokens with the same entity label into a single entity mention, returning:  \\n   - **`entity_group` or `entity`**: The combined label for the entity span.  \\n   - **`text`**: The reconstructed text of the entity from consecutive tokens.  \\n   - **`start` and `end`**: Character offsets for the span of the entity in the original text.  \\n   - **`score`**: Confidence score for the grouped entity, often computed as an aggregate of token-level scores.\\n\\n4. **Overall semantics**:  \\n   The response allows a user to identify **which parts of the input text correspond to company names**, along with model confidence and positional information, enabling downstream tasks such as extraction, highlighting, or database population.  \\n\\nIn summary, the response is a **structured token-level prediction** that may optionally be aggregated into entity spans, providing the predicted label, confidence, and positional information for each recognized entity in the text.\", \"examples\": [{\"user_intent\": \"Extract the company names mentioned in the sentence 'I recently started working at OpenAI and Microsoft.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"I recently started working at OpenAI and Microsoft.\"}}}, {\"user_intent\": \"Identify entities representing companies in the text 'Tesla, SpaceX, and Neuralink are all led by Elon Musk.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"Tesla, SpaceX, and Neuralink are all led by Elon Musk.\"}}}, {\"user_intent\": \"Find all company names in the paragraph: 'Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.\"}}}]}\n\n# User Query\nI am a journalist. I am writing an article about European start-ups. I need to see all the dates and company names in the text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract all company names mentioned in the text.",
                    "tool_description": "A token classification model that identifies and extracts company names from unstructured text using a distilBERT-based NER architecture.",
                    "expected_response": "A structured list of detected company names with token-level information, including entity labels, confidence scores, and text spans."
                },
                {
                    "user_intent": "Extract all dates mentioned in the text.",
                    "tool_description": "A named entity recognition (NER) model that detects date expressions within text using a pretrained English NER framework.",
                    "expected_response": "A structured list of detected dates with token-level information, including entity labels, confidence scores, and text spans."
                }
            ],
            "extracted_arguments": {
                "text_input": "The full textual content in which company names and dates need to be identified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_326",
        "query": "I'm talking to a new person online. Can this API help keep me safe by filtering out any inappropriate messages they send me?",
        "instruction": "Given a `message filtering` task, retrieve tools that enhance online safety by processing text inputs to detect and filter out inappropriate messages, thus ensuring a secure communication environment.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": [
                            {
                                "name": "matched acc",
                                "accuracy": 88.1
                            },
                            {
                                "name": "mismatched acc",
                                "accuracy": 88.19
                            }
                        ]
                    },
                    "description": "distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is a simple and effective technique with very little performance drop.",
                    "id": "huggingface_api_497",
                    "name": "valhalla/distilbart-mnli-12-3"
                },
                "id": "gorilla_huggingface_tool_493",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 1: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 2: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 3: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 4: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 6: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 8: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 9: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 10: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 11: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 12: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 13: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 14: This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.",
            "Tool 15: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks such as object manipulation and indoor navigation. It can be loaded via Hugging Face Transformers to generate image embeddings, enabling applications that require understanding and interacting with visual environments from a first-person perspective.",
            "Tool 16: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 17: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.",
            "Tool 18: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 19: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically."
        ],
        "tool_view": {
            "description": "The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text to be analyzed for sentiment, specifically to classify whether it is NSFW (not safe for work) or SFW (safe for work).",
                    "required": true,
                    "example": "I see you’ve set aside this special time to humiliate yourself in public."
                }
            },
            "response": "The tool returns the sentiment classification of a given text with respect to NSFW (Not Safe For Work) content. The response is typically structured as a dictionary or object containing the following components:\n\n1. **`label`** – A string indicating the predicted class for the input text. There are two possible values:  \n   - `\"NSFW\"`: The text is considered inappropriate or unprofessional.  \n   - `\"SFW\"`: The text is considered safe for work.\n\n2. **`score`** – A numeric value representing the model’s confidence in the predicted label. This is usually a probability between 0 and 1, with higher values indicating greater confidence in the prediction.\n\nOverall, the response provides both the classification decision and a quantitative measure of certainty for that decision.",
            "examples": [
                {
                    "user_intent": "Analyze whether this sentence contains inappropriate content or is safe for work.",
                    "tool_call": {
                        "name": "michellejieli/NSFW_text_classifier",
                        "arguments": {
                            "text": "I see you’ve set aside this special time to humiliate yourself in public."
                        }
                    }
                },
                {
                    "user_intent": "Check if this comment from a Reddit post is safe for work or NSFW.",
                    "tool_call": {
                        "name": "michellejieli/NSFW_text_classifier",
                        "arguments": {
                            "text": "This is an explicit and graphic description of violence."
                        }
                    }
                },
                {
                    "user_intent": "I want to classify a sentence to determine if it's professional or inappropriate.",
                    "tool_call": {
                        "name": "michellejieli/NSFW_text_classifier",
                        "arguments": {
                            "text": "What a lovely day for a meeting!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 1: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 2: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 3: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 4: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 6: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 8: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 9: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 10: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 11: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 12: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 13: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 14: This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.\nTool 15: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks such as object manipulation and indoor navigation. It can be loaded via Hugging Face Transformers to generate image embeddings, enabling applications that require understanding and interacting with visual environments from a first-person perspective.\nTool 16: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 17: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.\nTool 18: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 19: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"michellejieli/NSFW_text_classifier\\\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \\\"NSFW\\\" (not safe for work) or \\\"SFW\\\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text to be analyzed for sentiment, specifically to classify whether it is NSFW (not safe for work) or SFW (safe for work).\", \"required\": true, \"example\": \"I see you\\u2019ve set aside this special time to humiliate yourself in public.\"}}, \"response\": \"The tool returns the sentiment classification of a given text with respect to NSFW (Not Safe For Work) content. The response is typically structured as a dictionary or object containing the following components:\\n\\n1. **`label`** \\u2013 A string indicating the predicted class for the input text. There are two possible values:  \\n   - `\\\"NSFW\\\"`: The text is considered inappropriate or unprofessional.  \\n   - `\\\"SFW\\\"`: The text is considered safe for work.\\n\\n2. **`score`** \\u2013 A numeric value representing the model\\u2019s confidence in the predicted label. This is usually a probability between 0 and 1, with higher values indicating greater confidence in the prediction.\\n\\nOverall, the response provides both the classification decision and a quantitative measure of certainty for that decision.\", \"examples\": [{\"user_intent\": \"Analyze whether this sentence contains inappropriate content or is safe for work.\", \"tool_call\": {\"name\": \"michellejieli/NSFW_text_classifier\", \"arguments\": {\"text\": \"I see you\\u2019ve set aside this special time to humiliate yourself in public.\"}}}, {\"user_intent\": \"Check if this comment from a Reddit post is safe for work or NSFW.\", \"tool_call\": {\"name\": \"michellejieli/NSFW_text_classifier\", \"arguments\": {\"text\": \"This is an explicit and graphic description of violence.\"}}}, {\"user_intent\": \"I want to classify a sentence to determine if it's professional or inappropriate.\", \"tool_call\": {\"name\": \"michellejieli/NSFW_text_classifier\", \"arguments\": {\"text\": \"What a lovely day for a meeting!\"}}}]}\n\n# User Query\nI'm talking to a new person online. Can this API help keep me safe by filtering out any inappropriate messages they send me?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze a message to determine if it contains inappropriate content or is safe for work.",
                    "tool_description": "Classify a given text as either 'NSFW' (Not Safe for Work) or 'SFW' (Safe for Work) using a sentiment analysis model.",
                    "expected_response": "A classification label indicating whether the message is 'NSFW' or 'SFW', along with a confidence score."
                }
            ],
            "extracted_arguments": {
                "text": "The content of the message to be analyzed for appropriateness, specified as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_153",
        "query": "The client is a real estate company working on virtual tours. We need to help them estimate depth in images of houses.",
        "instruction": "Given a `depth estimation` task, retrieve tools that specialize in computer vision applications by processing image inputs to accurately estimate depth information in virtual tours or real estate imagery.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.24.0",
                        "torch==1.12.1+cu116",
                        "tokenizers==0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3497,
                            "Mae": 0.2847,
                            "Rmse": 0.3977,
                            "Abs Rel": 0.3477,
                            "Log Mae": 0.1203,
                            "Log Rmse": 0.1726,
                            "Delta1": 0.5217,
                            "Delta2": 0.8246,
                            "Delta3": 0.9436
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.",
                    "id": "huggingface_api_169",
                    "name": "glpn-kitti-finetuned-diode-221214-123047"
                },
                "id": "gorilla_huggingface_tool_165",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 1: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 2: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 3: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 4: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 5: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 6: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 7: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 8: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 9: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.",
            "Tool 10: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.",
            "Tool 11: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 12: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.",
            "Tool 13: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 14: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 15: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.",
            "Tool 16: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 17: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.",
            "Tool 18: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 19: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene."
        ],
        "tool_view": {
            "description": "A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path to the pretrained Vision Transformer (ViT) model to be loaded. This specifies which model configuration and weights to use for feature extraction.",
                    "required": true,
                    "example": "facebook/dino-vitb16"
                }
            },
            "response": "The tool is designed for feature extraction using the Vision Transformer (ViT) model, specifically the 'facebook/dino-vitb16' variant, which has been pretrained using the DINO method. The response to an API call involving this tool will contain the following elements:\n\n1. **Last Hidden State**:  \n   This is the key output from the model. It represents the final hidden states of the transformer after processing the input image. The last hidden state typically contains the transformed feature representations of the input image at the model's final layer. These representations can be used for various downstream tasks, such as image classification, clustering, or similarity search.\n\n   - **Structure**: The last hidden state is likely represented as a tensor or array, containing features for each image patch (16x16 pixel segments) and potentially an additional feature vector for the [CLS] token, used for classification or summarization of the image.\n   - **Meaning**: This tensor encapsulates the learned representation of the image, capturing high-level features from the input image.\n\n2. **Model Configuration Information**:  \n   The response may optionally include information regarding the specific model configuration, like:\n   \n   - The architecture (ViT model) and its variant ('facebook/dino-vitb16').\n   - The pretrained nature of the model, indicating it was trained on ImageNet-1k with a specific resolution of 224x224 pixels and patch size of 16x16 pixels.\n   \n3. **Potential Error Messages or Status Information**:  \n   If there is an issue with the image processing, model loading, or feature extraction, the response might include error details or status messages to indicate what went wrong. This would help users troubleshoot issues like incorrect input format or model loading failures.\n\n4. **Additional Outputs**:  \n   Depending on the use case, there might be other outputs like:\n   - Intermediate features from earlier layers (if requested).\n   - Attention maps or visualizations related to the Transformer’s attention mechanism.\n\nThus, the response from this tool would primarily contain the **last hidden states** of the input image processed by the Vision Transformer model, which are the primary features to be used for further analysis or tasks. If any errors occur during the process, corresponding error messages would be included to assist in diagnosing the issue.",
            "examples": [
                {
                    "user_intent": "Extract feature embeddings from an online image of a cat for similarity search.",
                    "tool_call": {
                        "name": "ViTModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/dino-vitb16"
                        }
                    }
                },
                {
                    "user_intent": "I want to get the image features of a photo I uploaded locally to use in a clustering algorithm.",
                    "tool_call": {
                        "name": "ViTModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/dino-vitb16"
                        }
                    }
                },
                {
                    "user_intent": "Generate embeddings for multiple images from a URL dataset to use for content-based image retrieval.",
                    "tool_call": {
                        "name": "ViTModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/dino-vitb16"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 1: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 2: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 3: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 4: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 5: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 6: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 7: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 8: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 9: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.\nTool 10: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.\nTool 11: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 12: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\nTool 13: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 14: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 15: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.\nTool 16: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 17: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\nTool 18: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 19: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path to the pretrained Vision Transformer (ViT) model to be loaded. This specifies which model configuration and weights to use for feature extraction.\", \"required\": true, \"example\": \"facebook/dino-vitb16\"}}, \"response\": \"The tool is designed for feature extraction using the Vision Transformer (ViT) model, specifically the 'facebook/dino-vitb16' variant, which has been pretrained using the DINO method. The response to an API call involving this tool will contain the following elements:\\n\\n1. **Last Hidden State**:  \\n   This is the key output from the model. It represents the final hidden states of the transformer after processing the input image. The last hidden state typically contains the transformed feature representations of the input image at the model's final layer. These representations can be used for various downstream tasks, such as image classification, clustering, or similarity search.\\n\\n   - **Structure**: The last hidden state is likely represented as a tensor or array, containing features for each image patch (16x16 pixel segments) and potentially an additional feature vector for the [CLS] token, used for classification or summarization of the image.\\n   - **Meaning**: This tensor encapsulates the learned representation of the image, capturing high-level features from the input image.\\n\\n2. **Model Configuration Information**:  \\n   The response may optionally include information regarding the specific model configuration, like:\\n   \\n   - The architecture (ViT model) and its variant ('facebook/dino-vitb16').\\n   - The pretrained nature of the model, indicating it was trained on ImageNet-1k with a specific resolution of 224x224 pixels and patch size of 16x16 pixels.\\n   \\n3. **Potential Error Messages or Status Information**:  \\n   If there is an issue with the image processing, model loading, or feature extraction, the response might include error details or status messages to indicate what went wrong. This would help users troubleshoot issues like incorrect input format or model loading failures.\\n\\n4. **Additional Outputs**:  \\n   Depending on the use case, there might be other outputs like:\\n   - Intermediate features from earlier layers (if requested).\\n   - Attention maps or visualizations related to the Transformer\\u2019s attention mechanism.\\n\\nThus, the response from this tool would primarily contain the **last hidden states** of the input image processed by the Vision Transformer model, which are the primary features to be used for further analysis or tasks. If any errors occur during the process, corresponding error messages would be included to assist in diagnosing the issue.\", \"examples\": [{\"user_intent\": \"Extract feature embeddings from an online image of a cat for similarity search.\", \"tool_call\": {\"name\": \"ViTModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/dino-vitb16\"}}}, {\"user_intent\": \"I want to get the image features of a photo I uploaded locally to use in a clustering algorithm.\", \"tool_call\": {\"name\": \"ViTModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/dino-vitb16\"}}}, {\"user_intent\": \"Generate embeddings for multiple images from a URL dataset to use for content-based image retrieval.\", \"tool_call\": {\"name\": \"ViTModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/dino-vitb16\"}}}]}\n\n# User Query\nThe client is a real estate company working on virtual tours. We need to help them estimate depth in images of houses.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Estimate depth maps from images of houses for a virtual tour application.",
                    "tool_description": "Predict per-pixel depth from input images using a fine-tuned GLPN or DPT depth estimation model, providing geometric understanding of the scene for virtual tour visualization.",
                    "expected_response": "A depth map corresponding to each input image, with per-pixel depth values representing the distance from the camera to the scene surfaces."
                }
            ],
            "extracted_arguments": {
                "input_images": "A collection of images of houses for which depth estimation is required."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_395",
        "query": "We need to develop a model to classify sports clips by identifying the type of sports being played in the video.",
        "instruction": "Given a `video classification` task, retrieve tools that process video inputs to classify and identify the type of sports being played within the clips, utilizing models pre-trained for recognizing specific actions or categories.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k400')",
                    "api_arguments": "video, return_tensors",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import AutoImageProcessor, TimesformerForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(8, 3, 224, 224))\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\ninputs = processor(video, return_tensors=pt)\nwith torch.no_grad():\n  outputs = model(**inputs)\n  logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-400",
                        "accuracy": "Not provided"
                    },
                    "description": "TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.",
                    "id": "huggingface_api_316",
                    "name": "facebook/timesformer-base-finetuned-k400"
                },
                "id": "gorilla_huggingface_tool_312",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 1: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 2: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 4: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 5: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 6: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 7: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 8: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 9: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 10: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 11: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.",
            "Tool 12: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 13: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 14: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 15: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 16: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 17: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.",
            "Tool 18: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 19: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages."
        ],
        "tool_view": {
            "description": "The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pretrained VideoMAE model to load for video action recognition. This can be a model ID from the Hugging Face Hub or a local path.",
                    "required": true,
                    "example": "nateraw/videomae-base-finetuned-ucf101"
                }
            },
            "response": "The tool’s response is the output of the video action recognition model when given a video input. Its structure can be described as follows:\n\n1. **logits**:  \n   - A tensor containing raw, unnormalized scores for each possible action class in the model’s label set.  \n   - The dimension of this tensor corresponds to the number of classes the model was trained on (e.g., 101 for UCF101).  \n   - These scores represent the model’s confidence for each action before applying any normalization such as softmax.\n\n2. **predicted label (optional convenience mapping)**:  \n   - A single integer or string representing the action class with the highest score.  \n   - If an integer, it indexes into the model’s internal `id2label` mapping.  \n   - If a string, it is the human-readable class name corresponding to the highest-scoring class (e.g., `\"Archery\"`).  \n\n3. **id2label mapping (metadata)**:  \n   - A dictionary mapping integer class indices to human-readable labels.  \n   - Used to interpret the predicted label in a readable format.  \n\nIn practice, the typical response workflow is:\n\n- The model returns the `logits` tensor.  \n- The user selects the class with the maximum score to obtain the `predicted_label`.  \n- The `id2label` mapping allows converting the numeric prediction into a human-readable action category.  \n\nNo additional fields are defined by the tool; the response focuses on the model’s prediction scores and derived class label for the input video clip.",
            "examples": [
                {
                    "user_intent": "I want to classify a video of a person performing an action using a pre-trained model.",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                },
                {
                    "user_intent": "How can I use the VideoMAE model to recognize actions in a video clip from UCF101 dataset?",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                },
                {
                    "user_intent": "Can I recognize actions in my video using a Hugging Face model for action classification?",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 1: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 2: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 4: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 5: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 6: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 7: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 8: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 9: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 10: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 11: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.\nTool 12: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 13: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 14: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 15: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 16: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 17: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.\nTool 18: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 19: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"videomae-base-finetuned-ucf101\\\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained VideoMAE model to load for video action recognition. This can be a model ID from the Hugging Face Hub or a local path.\", \"required\": true, \"example\": \"nateraw/videomae-base-finetuned-ucf101\"}}, \"response\": \"The tool\\u2019s response is the output of the video action recognition model when given a video input. Its structure can be described as follows:\\n\\n1. **logits**:  \\n   - A tensor containing raw, unnormalized scores for each possible action class in the model\\u2019s label set.  \\n   - The dimension of this tensor corresponds to the number of classes the model was trained on (e.g., 101 for UCF101).  \\n   - These scores represent the model\\u2019s confidence for each action before applying any normalization such as softmax.\\n\\n2. **predicted label (optional convenience mapping)**:  \\n   - A single integer or string representing the action class with the highest score.  \\n   - If an integer, it indexes into the model\\u2019s internal `id2label` mapping.  \\n   - If a string, it is the human-readable class name corresponding to the highest-scoring class (e.g., `\\\"Archery\\\"`).  \\n\\n3. **id2label mapping (metadata)**:  \\n   - A dictionary mapping integer class indices to human-readable labels.  \\n   - Used to interpret the predicted label in a readable format.  \\n\\nIn practice, the typical response workflow is:\\n\\n- The model returns the `logits` tensor.  \\n- The user selects the class with the maximum score to obtain the `predicted_label`.  \\n- The `id2label` mapping allows converting the numeric prediction into a human-readable action category.  \\n\\nNo additional fields are defined by the tool; the response focuses on the model\\u2019s prediction scores and derived class label for the input video clip.\", \"examples\": [{\"user_intent\": \"I want to classify a video of a person performing an action using a pre-trained model.\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}, {\"user_intent\": \"How can I use the VideoMAE model to recognize actions in a video clip from UCF101 dataset?\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}, {\"user_intent\": \"Can I recognize actions in my video using a Hugging Face model for action classification?\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}]}\n\n# User Query\nWe need to develop a model to classify sports clips by identifying the type of sports being played in the video.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify a sports video clip to identify the type of sport being played.",
                    "tool_description": "Use a pre-trained VideoMAE model fine-tuned on a sports action dataset to analyze video inputs and predict the corresponding sport category.",
                    "expected_response": "A set of prediction scores (logits) for each sport category, along with the predicted sport label and an index-to-label mapping for interpretation."
                }
            ],
            "extracted_arguments": {
                "pretrained_model_name_or_path": "The name or path of the pre-trained video action recognition model to load, capable of classifying sports types."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_83",
        "query": "Detect the location of players in an image from a Counter-Strike: Global Offensive (CS:GO) game.",
        "instruction": "Given an `object detection` task, retrieve tools that are capable of detecting and identifying players in a game image by processing visual inputs and providing detailed location data of the targets.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8n-csgo-player-detection')",
                    "api_arguments": {
                        "image": "URL or local path to image"
                    },
                    "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21",
                    "example_code": "from ultralyticsplus import YOLO, render_result\n\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\n\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\n\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\nresults = model.predict(image)\n\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "csgo-object-detection",
                        "accuracy": 0.844
                    },
                    "description": "A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].",
                    "id": "huggingface_api_229",
                    "name": "keremberke/yolov8n-csgo-player-detection"
                },
                "id": "gorilla_huggingface_tool_225",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 1: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 2: This tool uses a YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players in images, supporting labels such as 'ct', 'cthead', 't', and 'thead'. It is designed for object detection tasks and can be utilized through the Hugging Face Transformers framework. The model provides high accuracy, with a performance score of 0.886 on the CS:GO object detection dataset.",
            "Tool 3: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 4: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 5: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 6: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 7: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 8: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 9: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 10: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 11: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 12: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 13: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 14: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.",
            "Tool 15: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 16: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 17: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 18: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 19: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream."
        ],
        "tool_view": {
            "description": "A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The URL or file path of the image on which to perform object detection. This image will be processed by the YOLOv8 CS:GO player detection model.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns an object detection result for the given input image. The response is structured as a list of detection outputs, typically with one entry per image processed. Each detection output contains the following components:  \n\n1. **`boxes`** – A collection of bounding boxes representing detected objects in the image. Each bounding box includes:  \n   - **Coordinates**: The position and size of the box, usually expressed as `[x_min, y_min, x_max, y_max]` or in a similar format.  \n   - **Confidence score**: A numeric value representing the model’s confidence that the detected object corresponds to the predicted class.  \n   - **Class label**: The predicted category of the detected object, such as `'ct'`, `'cthead'`, `'t'`, or `'thead'`.  \n\n2. **`masks` (optional)** – If segmentation is enabled, this may contain pixel-level masks for each detected object.  \n\n3. **`keypoints` (optional)** – If keypoint detection is enabled, this may include locations of relevant points for each detected object.  \n\n4. **`metadata` (optional)** – Additional information about the detection process, such as inference time, image size, or model configuration overrides.  \n\nThe response is designed to allow post-processing, visualization, or rendering. For example, it can be used with a rendering function to draw bounding boxes and labels directly on the input image. Each detection entry provides both the spatial location and classification information necessary to identify CS:GO players in the image.  \n\n",
            "examples": [
                {
                    "user_intent": "I want to detect CS:GO players in an image using the YOLOv8 model.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-csgo-player-detection",
                        "arguments": {
                            "image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Can you detect CS:GO players in this image and show the results?",
                    "tool_call": {
                        "name": "keremberke/yolov8m-csgo-player-detection",
                        "arguments": {
                            "image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Run an object detection on a CS:GO image and output the boxes for the detected players.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-csgo-player-detection",
                        "arguments": {
                            "image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 1: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 2: This tool uses a YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players in images, supporting labels such as 'ct', 'cthead', 't', and 'thead'. It is designed for object detection tasks and can be utilized through the Hugging Face Transformers framework. The model provides high accuracy, with a performance score of 0.886 on the CS:GO object detection dataset.\nTool 3: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 4: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 5: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 6: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 7: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 8: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 9: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 10: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 11: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 12: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 13: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 14: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.\nTool 15: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 16: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 17: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 18: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 19: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The URL or file path of the image on which to perform object detection. This image will be processed by the YOLOv8 CS:GO player detection model.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns an object detection result for the given input image. The response is structured as a list of detection outputs, typically with one entry per image processed. Each detection output contains the following components:  \\n\\n1. **`boxes`** \\u2013 A collection of bounding boxes representing detected objects in the image. Each bounding box includes:  \\n   - **Coordinates**: The position and size of the box, usually expressed as `[x_min, y_min, x_max, y_max]` or in a similar format.  \\n   - **Confidence score**: A numeric value representing the model\\u2019s confidence that the detected object corresponds to the predicted class.  \\n   - **Class label**: The predicted category of the detected object, such as `'ct'`, `'cthead'`, `'t'`, or `'thead'`.  \\n\\n2. **`masks` (optional)** \\u2013 If segmentation is enabled, this may contain pixel-level masks for each detected object.  \\n\\n3. **`keypoints` (optional)** \\u2013 If keypoint detection is enabled, this may include locations of relevant points for each detected object.  \\n\\n4. **`metadata` (optional)** \\u2013 Additional information about the detection process, such as inference time, image size, or model configuration overrides.  \\n\\nThe response is designed to allow post-processing, visualization, or rendering. For example, it can be used with a rendering function to draw bounding boxes and labels directly on the input image. Each detection entry provides both the spatial location and classification information necessary to identify CS:GO players in the image.  \\n\\n\", \"examples\": [{\"user_intent\": \"I want to detect CS:GO players in an image using the YOLOv8 model.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-csgo-player-detection\", \"arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}}, {\"user_intent\": \"Can you detect CS:GO players in this image and show the results?\", \"tool_call\": {\"name\": \"keremberke/yolov8m-csgo-player-detection\", \"arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}}, {\"user_intent\": \"Run an object detection on a CS:GO image and output the boxes for the detected players.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-csgo-player-detection\", \"arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}}]}\n\n# User Query\nDetect the location of players in an image from a Counter-Strike: Global Offensive (CS:GO) game.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect CS:GO players in an image using the YOLOv8 model.",
                    "tool_description": "Use a YOLOv8-based object detection model to identify and localize CS:GO players in an image, classifying them into labels such as 'ct', 'cthead', 't', and 'thead'.",
                    "expected_response": "A list of detected player objects with bounding boxes, confidence scores, and class labels, optionally including segmentation masks or keypoints."
                }
            ],
            "extracted_arguments": {
                "image": "The URL or file path of the CS:GO game screenshot to process for player detection."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_401",
        "query": "We are building a platform for developers and want to provide automatic code documentation generation for Python functions.",
        "instruction": "Given a `code documentation generation` task, retrieve tools that automatically generate documentation for Python functions by analyzing code inputs to produce descriptive summaries that align with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "Code Documentation Generation",
                    "api_call": "AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')",
                    "api_arguments": [
                        "tokenized_code"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\npipeline = SummarizationPipeline(\n model=AutoModelWithLMHead.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python),\n tokenizer=AutoTokenizer.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python, skip_special_tokens=True),\n device=0\n)\ntokenized_code = def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )\npipeline([tokenized_code])",
                    "performance": {
                        "dataset": "CodeSearchNet Corpus python dataset",
                        "accuracy": "20.26 BLEU score"
                    },
                    "description": "This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code.",
                    "id": "huggingface_api_553",
                    "name": "code_trans_t5_base_code_documentation_generation_python"
                },
                "id": "gorilla_huggingface_tool_549",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 1: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.",
            "Tool 2: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 3: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 4: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 5: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.",
            "Tool 6: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 7: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 8: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.",
            "Tool 9: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 10: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 11: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 12: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.",
            "Tool 13: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 14: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.",
            "Tool 15: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.",
            "Tool 16: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 17: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 18: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.",
            "Tool 19: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text."
        ],
        "tool_view": {
            "description": "This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "parameters": {
                "tokenized_code": {
                    "type": "string",
                    "description": "The Python code input that has been tokenized and preprocessed for the model. This code will be used by the model to generate natural-language documentation or descriptions.",
                    "required": true,
                    "example": "def e(message, exit_code=None): print_log(message, YELLOW, BOLD) if exit_code is not None: sys.exit(exit_code)"
                }
            },
            "response": "The tool produces a textual response that consists of a natural language description or documentation of the input Python code. The response structure can be understood as follows:\n\n- **Type**: Text string.  \n- **Content**: A concise and coherent summary of the functionality and purpose of the given Python code or function. This typically includes:\n  - The role of the function or code snippet.\n  - The key operations it performs.\n  - Any inputs, outputs, or side effects relevant to understanding the code.  \n- **Granularity**: The description is generally at the function or snippet level, explaining behavior rather than implementation details line by line.  \n- **Format**: Plain text intended to be human-readable, suitable for use as code documentation, docstrings, or explanatory comments.  \n\nThe response does not return structured metadata, code annotations, or token-level outputs; it is purely a generated textual summary derived from the tokenized Python code provided as input.",
            "examples": [
                {
                    "user_intent": "Generate a detailed docstring for this Python function that processes user input and handles errors.",
                    "tool_call": {
                        "name": "code_trans_t5_base_code_documentation_generation_python",
                        "arguments": {
                            "tokenized_code": "def process_input(user_input, max_length=50):\n    if len(user_input) > max_length:\n        raise ValueError('Input too long')\n    return user_input.strip().lower()"
                        }
                    }
                },
                {
                    "user_intent": "I want to automatically create documentation for a function that calculates factorials.",
                    "tool_call": {
                        "name": "code_trans_t5_base_code_documentation_generation_python",
                        "arguments": {
                            "tokenized_code": "def factorial(n):\n    if n == 0:\n        return 1\n    return n * factorial(n-1)"
                        }
                    }
                },
                {
                    "user_intent": "Create a descriptive comment for this code snippet that logs system messages with different levels.",
                    "tool_call": {
                        "name": "code_trans_t5_base_code_documentation_generation_python",
                        "arguments": {
                            "tokenized_code": "def log_message(message, level='INFO'):\n    print(f'[{level}] {message}')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 1: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.\nTool 2: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 3: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 4: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 5: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.\nTool 6: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 7: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 8: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.\nTool 9: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 10: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 11: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 12: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\nTool 13: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 14: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.\nTool 15: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.\nTool 16: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 17: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 18: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.\nTool 19: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\", \"parameters\": {\"tokenized_code\": {\"type\": \"string\", \"description\": \"The Python code input that has been tokenized and preprocessed for the model. This code will be used by the model to generate natural-language documentation or descriptions.\", \"required\": true, \"example\": \"def e(message, exit_code=None): print_log(message, YELLOW, BOLD) if exit_code is not None: sys.exit(exit_code)\"}}, \"response\": \"The tool produces a textual response that consists of a natural language description or documentation of the input Python code. The response structure can be understood as follows:\\n\\n- **Type**: Text string.  \\n- **Content**: A concise and coherent summary of the functionality and purpose of the given Python code or function. This typically includes:\\n  - The role of the function or code snippet.\\n  - The key operations it performs.\\n  - Any inputs, outputs, or side effects relevant to understanding the code.  \\n- **Granularity**: The description is generally at the function or snippet level, explaining behavior rather than implementation details line by line.  \\n- **Format**: Plain text intended to be human-readable, suitable for use as code documentation, docstrings, or explanatory comments.  \\n\\nThe response does not return structured metadata, code annotations, or token-level outputs; it is purely a generated textual summary derived from the tokenized Python code provided as input.\", \"examples\": [{\"user_intent\": \"Generate a detailed docstring for this Python function that processes user input and handles errors.\", \"tool_call\": {\"name\": \"code_trans_t5_base_code_documentation_generation_python\", \"arguments\": {\"tokenized_code\": \"def process_input(user_input, max_length=50):\\n    if len(user_input) > max_length:\\n        raise ValueError('Input too long')\\n    return user_input.strip().lower()\"}}}, {\"user_intent\": \"I want to automatically create documentation for a function that calculates factorials.\", \"tool_call\": {\"name\": \"code_trans_t5_base_code_documentation_generation_python\", \"arguments\": {\"tokenized_code\": \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    return n * factorial(n-1)\"}}}, {\"user_intent\": \"Create a descriptive comment for this code snippet that logs system messages with different levels.\", \"tool_call\": {\"name\": \"code_trans_t5_base_code_documentation_generation_python\", \"arguments\": {\"tokenized_code\": \"def log_message(message, level='INFO'):\\n    print(f'[{level}] {message}')\"}}}]}\n\n# User Query\nWe are building a platform for developers and want to provide automatic code documentation generation for Python functions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Automatically generate natural-language documentation for Python functions.",
                    "tool_description": "Use a pretrained model to produce clear, human-readable descriptions of Python code functionality from tokenized code input, suitable for docstrings or explanatory comments.",
                    "expected_response": "A text string summarizing the purpose, operations, inputs, and outputs of the provided Python code or function."
                }
            ],
            "extracted_arguments": {
                "tokenized_code": "The Python function or code snippet that has been tokenized and preprocessed for input into the documentation generation model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_196",
        "query": "Create a tool that receives a table and a question in natural language, and returns an answer to the question based on the inputted table.",
        "instruction": "Given a `table-based question answering` task, retrieve tools that process natural language queries and tabular data inputs to provide accurate responses based on the information within the tables.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')",
                    "api_arguments": [
                        "model_name",
                        "question",
                        "table"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\noutputs = model(**inputs)\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())",
                    "performance": {
                        "dataset": "SQA",
                        "accuracy": "Not provided"
                    },
                    "description": "TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.",
                    "id": "huggingface_api_455",
                    "name": "lysandre/tapas-temporary-repo"
                },
                "id": "gorilla_huggingface_tool_451",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 1: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 2: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 3: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 4: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 5: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 6: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 7: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 8: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 9: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 10: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 11: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 12: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 13: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 14: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.",
            "Tool 15: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.",
            "Tool 16: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.",
            "Tool 17: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 18: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.",
            "Tool 19: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding."
        ],
        "tool_view": {
            "description": "A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "parameters": {
                "table": {
                    "type": "object",
                    "description": "A structured table represented as a nested data structure (e.g., a list of dictionaries or a pandas DataFrame) containing the data to be queried.",
                    "required": true,
                    "example": [
                        {
                            "Name": "Alice",
                            "Age": 30,
                            "City": "New York"
                        },
                        {
                            "Name": "Bob",
                            "Age": 25,
                            "City": "Los Angeles"
                        }
                    ]
                },
                "query": {
                    "type": "string",
                    "description": "A natural language question or query to be answered based on the content of the provided table.",
                    "required": true,
                    "example": "What is the age of Bob?"
                }
            },
            "response": "The response returned by this tool would likely contain the answer to the user's query based on the table provided. Given the functionality of the model, the response would contain:\n\n1. **Answer**: This is the direct response to the query posed by the user, derived by the TAPAS model from the given table. It could be a textual answer, a number, or a set of values depending on the nature of the question and the table data.\n\nThe response structure is simple but crucial for extracting useful information from a table:\n\n- **Answer**: The specific answer to the query asked, typically extracted from the context of the table. The answer can be based on cells, rows, columns, or any relevant information within the provided table.\n  \nAdditionally, depending on the system's implementation or specific configuration, there might be other auxiliary components such as:\n\n- **Confidence score**: A numerical value indicating the model’s confidence in its answer, though this is not explicitly described in the provided tool documentation.\n  \nHowever, based on the provided tool documentation, only the **answer** is the key component of the response.",
            "examples": [
                {
                    "user_intent": "I have a table of Olympic medal counts by country. I want to know which country won the most gold medals in 2020.",
                    "tool_call": {
                        "name": "google/tapas-medium-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Country": [
                                    "USA",
                                    "China",
                                    "Japan"
                                ],
                                "Gold": [
                                    39,
                                    38,
                                    27
                                ],
                                "Silver": [
                                    41,
                                    32,
                                    14
                                ],
                                "Bronze": [
                                    33,
                                    18,
                                    17
                                ]
                            },
                            "query": "Which country won the most gold medals?"
                        }
                    }
                },
                {
                    "user_intent": "Given a sales table for Q1, I need to know the total revenue generated by Product B.",
                    "tool_call": {
                        "name": "google/tapas-medium-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Product": [
                                    "A",
                                    "B",
                                    "C"
                                ],
                                "Units Sold": [
                                    120,
                                    200,
                                    150
                                ],
                                "Unit Price": [
                                    10,
                                    15,
                                    20
                                ]
                            },
                            "query": "What is the total revenue for Product B?"
                        }
                    }
                },
                {
                    "user_intent": "I have a table of movies with release years and box office earnings. I want to know which movie made the highest box office in 2022.",
                    "tool_call": {
                        "name": "google/tapas-medium-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Movie": [
                                    "Movie X",
                                    "Movie Y",
                                    "Movie Z"
                                ],
                                "Release Year": [
                                    2021,
                                    2022,
                                    2022
                                ],
                                "Box Office (Millions)": [
                                    100,
                                    250,
                                    180
                                ]
                            },
                            "query": "Which movie earned the highest box office in 2022?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 1: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 2: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 3: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 4: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 5: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 6: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 7: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 8: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 9: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 10: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 11: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 12: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 13: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 14: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.\nTool 15: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.\nTool 16: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.\nTool 17: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 18: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.\nTool 19: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table\\u2019s content, making it useful for extracting structured information from tables in a straightforward, automated way.\", \"parameters\": {\"table\": {\"type\": \"object\", \"description\": \"A structured table represented as a nested data structure (e.g., a list of dictionaries or a pandas DataFrame) containing the data to be queried.\", \"required\": true, \"example\": [{\"Name\": \"Alice\", \"Age\": 30, \"City\": \"New York\"}, {\"Name\": \"Bob\", \"Age\": 25, \"City\": \"Los Angeles\"}]}, \"query\": {\"type\": \"string\", \"description\": \"A natural language question or query to be answered based on the content of the provided table.\", \"required\": true, \"example\": \"What is the age of Bob?\"}}, \"response\": \"The response returned by this tool would likely contain the answer to the user's query based on the table provided. Given the functionality of the model, the response would contain:\\n\\n1. **Answer**: This is the direct response to the query posed by the user, derived by the TAPAS model from the given table. It could be a textual answer, a number, or a set of values depending on the nature of the question and the table data.\\n\\nThe response structure is simple but crucial for extracting useful information from a table:\\n\\n- **Answer**: The specific answer to the query asked, typically extracted from the context of the table. The answer can be based on cells, rows, columns, or any relevant information within the provided table.\\n  \\nAdditionally, depending on the system's implementation or specific configuration, there might be other auxiliary components such as:\\n\\n- **Confidence score**: A numerical value indicating the model\\u2019s confidence in its answer, though this is not explicitly described in the provided tool documentation.\\n  \\nHowever, based on the provided tool documentation, only the **answer** is the key component of the response.\", \"examples\": [{\"user_intent\": \"I have a table of Olympic medal counts by country. I want to know which country won the most gold medals in 2020.\", \"tool_call\": {\"name\": \"google/tapas-medium-finetuned-wtq\", \"arguments\": {\"table\": {\"Country\": [\"USA\", \"China\", \"Japan\"], \"Gold\": [39, 38, 27], \"Silver\": [41, 32, 14], \"Bronze\": [33, 18, 17]}, \"query\": \"Which country won the most gold medals?\"}}}, {\"user_intent\": \"Given a sales table for Q1, I need to know the total revenue generated by Product B.\", \"tool_call\": {\"name\": \"google/tapas-medium-finetuned-wtq\", \"arguments\": {\"table\": {\"Product\": [\"A\", \"B\", \"C\"], \"Units Sold\": [120, 200, 150], \"Unit Price\": [10, 15, 20]}, \"query\": \"What is the total revenue for Product B?\"}}}, {\"user_intent\": \"I have a table of movies with release years and box office earnings. I want to know which movie made the highest box office in 2022.\", \"tool_call\": {\"name\": \"google/tapas-medium-finetuned-wtq\", \"arguments\": {\"table\": {\"Movie\": [\"Movie X\", \"Movie Y\", \"Movie Z\"], \"Release Year\": [2021, 2022, 2022], \"Box Office (Millions)\": [100, 250, 180]}, \"query\": \"Which movie earned the highest box office in 2022?\"}}}]}\n\n# User Query\nCreate a tool that receives a table and a question in natural language, and returns an answer to the question based on the inputted table.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a tool that can answer questions based on a table of data.",
                    "tool_description": "Design a model that processes a structured table and a natural language query, providing an accurate answer derived from the table's content.",
                    "expected_response": "The specific answer to the question posed, extracted from the provided table. The answer could be a number, text, or a set of values depending on the question and the data."
                }
            ],
            "extracted_arguments": {
                "table": "A structured table represented as a nested data structure (e.g., a list of dictionaries or a pandas DataFrame) containing the data to be queried.",
                "query": "A natural language question or query to be answered based on the content of the provided table."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_130",
        "query": "A podcast producer is looking to improve the quality of their audio files by removing background noise. What can they do?",
        "instruction": "Given an `audio enhancement` task, retrieve tools that specialize in improving the quality of audio files by removing background noise through advanced speech enhancement techniques, processing audio inputs to deliver clearer outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Enhancement",
                    "api_call": "separator.from_hparams(source=speechbrain/sepformer-wham16k-enhancement)",
                    "api_arguments": {
                        "path": "path to the input audio file"
                    },
                    "python_environment_requirements": "pip install speechbrain",
                    "example_code": "from speechbrain.pretrained import SepformerSeparation as separator\nimport torchaudio\nmodel = separator.from_hparams(source=speechbrain/sepformer-wham16k-enhancement, savedir='pretrained_models/sepformer-wham16k-enhancement')\nest_sources = model.separate_file(path='speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav')\ntorchaudio.save(enhanced_wham16k.wav, est_sources[:, :, 0].detach().cpu(), 16000)",
                    "performance": {
                        "dataset": "WHAM!",
                        "accuracy": {
                            "Test-Set SI-SNR": "14.3 dB",
                            "Test-Set PESQ": "2.20"
                        }
                    },
                    "description": "This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 16k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.",
                    "id": "huggingface_api_788",
                    "name": "speechbrain/sepformer-wham16k-enhancement"
                },
                "id": "gorilla_huggingface_tool_783",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 1: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 2: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.",
            "Tool 3: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 4: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 5: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 6: This tool performs speech recognition on Portuguese audio files using the fine-tuned Wav2Vec 2.0 model (jonatasgrosman/wav2vec2-large-xlsr-53-portuguese). It transcribes audio inputs, such as MP3 or WAV files, into text and is optimized for speech sampled at 16kHz. The model is based on the Common Voice 6.1 dataset and can be used within a Python environment with libraries like Hugging Face Transformers, Torch, and Librosa.",
            "Tool 7: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 8: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 9: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 10: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 11: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 12: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "Tool 13: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.",
            "Tool 14: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 15: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.",
            "Tool 16: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 17: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 18: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.",
            "Tool 19: The \"google/ddpm-ema-church-256\" is a Denoising Diffusion Probabilistic Model (DDPM) used for high-quality image generation. Based on latent variable models inspired by nonequilibrium thermodynamics, it can synthesize images effectively using various noise schedulers and pipelines for optimized performance. This model, available through Hugging Face's Diffusers library, is designed for high-speed and high-quality image generation, with notable performance on datasets like CIFAR10, achieving an Inception score of 9.46 and an FID score of 3.17."
        ],
        "tool_view": {
            "description": "This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "parameters": {
                "path": {
                    "type": "string",
                    "description": "The file path to the input audio file that needs to be enhanced.",
                    "required": true,
                    "example": "speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav"
                }
            },
            "response": "The tool produces an enhanced (denoised) version of an input audio signal. The typical response is an audio data structure representing the separated or enhanced speech. Specifically, the response includes the following components:  \n\n1. **Enhanced Audio Tensor**:  \n   - A multi-dimensional array (tensor) representing the waveform of the enhanced speech signal.  \n   - Dimensions usually correspond to `[channels, samples]` or `[batch, channels, samples]` depending on the model configuration.  \n   - The values are floating-point numbers representing the audio amplitude.  \n\n2. **Sample Rate**:  \n   - The audio sampling frequency used to interpret the waveform correctly. For this model, it is 16 kHz.  \n\n3. **Optional Metadata**:  \n   - Some implementations may include additional metadata such as the number of sources separated, channel layout, or processing parameters.  \n\nIn practice, the response is used to save or play back the enhanced audio. For example, one would typically pass the tensor to an audio library (like `torchaudio`) along with the sample rate to store it as a WAV file. The response does **not** include textual output; its primary content is the denoised audio waveform.",
            "examples": [
                {
                    "user_intent": "I have a noisy recording of a meeting and want to clean it up for transcription.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/audio/meeting_noisy.wav"
                        }
                    }
                },
                {
                    "user_intent": "Enhance the audio quality of my podcast episode recorded in a cafe.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/podcasts/episode5_cafe.wav"
                        }
                    }
                },
                {
                    "user_intent": "I want to denoise a voice memo recorded with background traffic noise.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/voice_memos/traffic_memo.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 1: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 2: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\nTool 3: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 4: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 5: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 6: This tool performs speech recognition on Portuguese audio files using the fine-tuned Wav2Vec 2.0 model (jonatasgrosman/wav2vec2-large-xlsr-53-portuguese). It transcribes audio inputs, such as MP3 or WAV files, into text and is optimized for speech sampled at 16kHz. The model is based on the Common Voice 6.1 dataset and can be used within a Python environment with libraries like Hugging Face Transformers, Torch, and Librosa.\nTool 7: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 8: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 9: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 10: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 11: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 12: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\nTool 13: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.\nTool 14: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 15: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.\nTool 16: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 17: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 18: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.\nTool 19: The \"google/ddpm-ema-church-256\" is a Denoising Diffusion Probabilistic Model (DDPM) used for high-quality image generation. Based on latent variable models inspired by nonequilibrium thermodynamics, it can synthesize images effectively using various noise schedulers and pipelines for optimized performance. This model, available through Hugging Face's Diffusers library, is designed for high-speed and high-quality image generation, with notable performance on datasets like CIFAR10, achieving an Inception score of 9.46 and an FID score of 3.17.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\", \"parameters\": {\"path\": {\"type\": \"string\", \"description\": \"The file path to the input audio file that needs to be enhanced.\", \"required\": true, \"example\": \"speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav\"}}, \"response\": \"The tool produces an enhanced (denoised) version of an input audio signal. The typical response is an audio data structure representing the separated or enhanced speech. Specifically, the response includes the following components:  \\n\\n1. **Enhanced Audio Tensor**:  \\n   - A multi-dimensional array (tensor) representing the waveform of the enhanced speech signal.  \\n   - Dimensions usually correspond to `[channels, samples]` or `[batch, channels, samples]` depending on the model configuration.  \\n   - The values are floating-point numbers representing the audio amplitude.  \\n\\n2. **Sample Rate**:  \\n   - The audio sampling frequency used to interpret the waveform correctly. For this model, it is 16 kHz.  \\n\\n3. **Optional Metadata**:  \\n   - Some implementations may include additional metadata such as the number of sources separated, channel layout, or processing parameters.  \\n\\nIn practice, the response is used to save or play back the enhanced audio. For example, one would typically pass the tensor to an audio library (like `torchaudio`) along with the sample rate to store it as a WAV file. The response does **not** include textual output; its primary content is the denoised audio waveform.\", \"examples\": [{\"user_intent\": \"I have a noisy recording of a meeting and want to clean it up for transcription.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/audio/meeting_noisy.wav\"}}}, {\"user_intent\": \"Enhance the audio quality of my podcast episode recorded in a cafe.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/podcasts/episode5_cafe.wav\"}}}, {\"user_intent\": \"I want to denoise a voice memo recorded with background traffic noise.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/voice_memos/traffic_memo.wav\"}}}]}\n\n# User Query\nA podcast producer is looking to improve the quality of their audio files by removing background noise. What can they do?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to clean up my podcast recordings by removing background noise and reverberation.",
                    "tool_description": "Perform speech enhancement on audio files using a pretrained SepFormer model implemented in SpeechBrain to denoise and improve clarity of speech recordings.",
                    "expected_response": "An enhanced audio waveform tensor representing the denoised speech, including sample rate and optional metadata."
                }
            ],
            "extracted_arguments": {
                "audio_file_path": "The file path to the input podcast audio file that needs noise reduction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_297",
        "query": "I am a novel writer. I plan to write some paragraphs, but I cannot find the exact word placeholder or missing word. Can you help me make a tool to complete the given text below?",
        "instruction": "Given a `text completion` task, retrieve tools that utilize masked language models to fill in missing words or placeholders in text, thereby assisting in developing coherent and complete text passages.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='roberta-base')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='roberta-base')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "MNLI",
                                "accuracy": 87.6
                            },
                            {
                                "name": "QQP",
                                "accuracy": 91.9
                            },
                            {
                                "name": "QNLI",
                                "accuracy": 92.8
                            },
                            {
                                "name": "SST-2",
                                "accuracy": 94.8
                            },
                            {
                                "name": "CoLA",
                                "accuracy": 63.6
                            },
                            {
                                "name": "STS-B",
                                "accuracy": 91.2
                            },
                            {
                                "name": "MRPC",
                                "accuracy": 90.2
                            },
                            {
                                "name": "RTE",
                                "accuracy": 78.7
                            }
                        ]
                    },
                    "description": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task.",
                    "id": "huggingface_api_659",
                    "name": "roberta-base"
                },
                "id": "gorilla_huggingface_tool_654",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 1: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 2: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 3: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 4: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 5: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 6: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.",
            "Tool 7: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 8: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.",
            "Tool 9: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 10: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 11: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 12: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.",
            "Tool 13: The `microsoft/deberta-v3-base` model is a state-of-the-art DeBERTa V3 transformer designed for fill-mask tasks, allowing users to predict missing words in text. It enhances BERT and RoBERTa architectures through disentangled attention and an improved mask decoder, offering efficient and accurate language understanding. This model is suitable for NLP applications that require contextual word prediction, leveraging pre-trained knowledge to generate high-quality masked token completions.",
            "Tool 14: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.",
            "Tool 15: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 16: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 17: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 18: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.",
            "Tool 19: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021."
        ],
        "tool_view": {
            "description": "This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The identifier or path of the pretrained model to load. This can be a model name from the Hugging Face Hub or a local path to a model directory.",
                    "required": true,
                    "example": "neuralmind/bert-base-portuguese-cased"
                }
            },
            "response": "The tool is designed for the **Fill-Mask** task in the domain of Natural Language Processing (NLP). The response from this tool would likely contain the following components:\n\n1. **Filled Text**: The primary output would be the input text with the `[MASK]` token replaced by the model's best guess based on context. This represents the predicted word or phrase that completes the sentence.\n\n2. **Probabilities (optional)**: The tool might also provide the probability or confidence score associated with the predicted word or phrase. This would indicate how confident the model is about the chosen prediction.\n\n3. **Top Predictions (optional)**: Depending on the tool's configuration, the response might include a list of the top N most likely words or phrases that could replace the `[MASK]`, along with their corresponding probabilities. This provides multiple possible completions for the masked portion of the sentence.\n\nThe structure would likely include:\n\n- **filled_text**: The input sentence with the `[MASK]` token filled in.\n- **predicted_token**: The model's prediction for the masked token.\n- **confidence_score** (optional): The model's confidence in its prediction, expressed as a probability or likelihood.\n- **top_predictions** (optional): A list of alternative predictions for the masked token, each associated with a probability score.\n\nThis response would allow users to see both the model's best guess and other plausible completions.",
            "examples": [
                {
                    "user_intent": "I need a fill-mask model to predict missing words in a Portuguese sentence.",
                    "tool_call": {
                        "name": "neuralmind/bert-base-portuguese-cased",
                        "arguments": {
                            "pretrained_model_name_or_path": "neuralmind/bert-base-portuguese-cased"
                        }
                    }
                },
                {
                    "user_intent": "Can you suggest the missing word in this sentence: 'Tinha uma [MASK] no meio do caminho.'",
                    "tool_call": {
                        "name": "neuralmind/bert-base-portuguese-cased",
                        "arguments": {
                            "pretrained_model_name_or_path": "neuralmind/bert-base-portuguese-cased"
                        }
                    }
                },
                {
                    "user_intent": "Run a Portuguese sentence completion task using a BERT model.",
                    "tool_call": {
                        "name": "neuralmind/bert-base-portuguese-cased",
                        "arguments": {
                            "pretrained_model_name_or_path": "neuralmind/bert-base-portuguese-cased"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 1: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 2: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 3: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 4: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 5: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 6: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.\nTool 7: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 8: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.\nTool 9: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 10: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 11: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 12: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.\nTool 13: The `microsoft/deberta-v3-base` model is a state-of-the-art DeBERTa V3 transformer designed for fill-mask tasks, allowing users to predict missing words in text. It enhances BERT and RoBERTa architectures through disentangled attention and an improved mask decoder, offering efficient and accurate language understanding. This model is suitable for NLP applications that require contextual word prediction, leveraging pre-trained knowledge to generate high-quality masked token completions.\nTool 14: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.\nTool 15: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 16: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 17: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 18: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.\nTool 19: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The identifier or path of the pretrained model to load. This can be a model name from the Hugging Face Hub or a local path to a model directory.\", \"required\": true, \"example\": \"neuralmind/bert-base-portuguese-cased\"}}, \"response\": \"The tool is designed for the **Fill-Mask** task in the domain of Natural Language Processing (NLP). The response from this tool would likely contain the following components:\\n\\n1. **Filled Text**: The primary output would be the input text with the `[MASK]` token replaced by the model's best guess based on context. This represents the predicted word or phrase that completes the sentence.\\n\\n2. **Probabilities (optional)**: The tool might also provide the probability or confidence score associated with the predicted word or phrase. This would indicate how confident the model is about the chosen prediction.\\n\\n3. **Top Predictions (optional)**: Depending on the tool's configuration, the response might include a list of the top N most likely words or phrases that could replace the `[MASK]`, along with their corresponding probabilities. This provides multiple possible completions for the masked portion of the sentence.\\n\\nThe structure would likely include:\\n\\n- **filled_text**: The input sentence with the `[MASK]` token filled in.\\n- **predicted_token**: The model's prediction for the masked token.\\n- **confidence_score** (optional): The model's confidence in its prediction, expressed as a probability or likelihood.\\n- **top_predictions** (optional): A list of alternative predictions for the masked token, each associated with a probability score.\\n\\nThis response would allow users to see both the model's best guess and other plausible completions.\", \"examples\": [{\"user_intent\": \"I need a fill-mask model to predict missing words in a Portuguese sentence.\", \"tool_call\": {\"name\": \"neuralmind/bert-base-portuguese-cased\", \"arguments\": {\"pretrained_model_name_or_path\": \"neuralmind/bert-base-portuguese-cased\"}}}, {\"user_intent\": \"Can you suggest the missing word in this sentence: 'Tinha uma [MASK] no meio do caminho.'\", \"tool_call\": {\"name\": \"neuralmind/bert-base-portuguese-cased\", \"arguments\": {\"pretrained_model_name_or_path\": \"neuralmind/bert-base-portuguese-cased\"}}}, {\"user_intent\": \"Run a Portuguese sentence completion task using a BERT model.\", \"tool_call\": {\"name\": \"neuralmind/bert-base-portuguese-cased\", \"arguments\": {\"pretrained_model_name_or_path\": \"neuralmind/bert-base-portuguese-cased\"}}}]}\n\n# User Query\nI am a novel writer. I plan to write some paragraphs, but I cannot find the exact word placeholder or missing word. Can you help me make a tool to complete the given text below?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I need a tool to help me complete a given text by predicting missing words.",
                    "tool_description": "This tool performs fill-in-the-blank (masked word) predictions in text, allowing users to complete sentences by predicting the most likely word for a missing token in the text.",
                    "expected_response": "The filled-in text with the missing word replaced by the model's prediction, along with potential alternative predictions and their confidence scores."
                }
            ],
            "extracted_arguments": {
                "pretrained_model_name_or_path": "The identifier or path of the pretrained model to load, which will be used for the masked word prediction task."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_258",
        "query": "Our app offers assistance to people with hearing problems by enhancing the clarity of speech. We need a feature to clean and enhance the audio.",
        "instruction": "Given an `audio enhancement` task, retrieve tools that process audio inputs to improve the clarity and quality of speech, such as noise reduction and speech enhancement algorithms, in order to assist individuals with hearing impairments.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "AutoModelForAudioToAudio.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')",
                    "api_arguments": "pretrained_model_name_or_path",
                    "python_environment_requirements": [
                        "transformers",
                        "asteroid"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri1Mix",
                        "accuracy": {
                            "si_sdr": 13.329767398333798,
                            "si_sdr_imp": 9.879986092474098,
                            "sdr": 13.87279932997016,
                            "sdr_imp": 10.370136530757103,
                            "sir": "Infinity",
                            "sir_imp": "NaN",
                            "sar": 13.87279932997016,
                            "sar_imp": 10.370136530757103,
                            "stoi": 0.9140907015623948,
                            "stoi_imp": 0.11817087802185405
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.",
                    "id": "huggingface_api_776",
                    "name": "DCCRNet_Libri1Mix_enhsingle_16k"
                },
                "id": "gorilla_huggingface_tool_771",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "Tool 1: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 2: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.",
            "Tool 3: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.",
            "Tool 4: DCCRNet_Libri1Mix_enhsingle_16k is an audio-to-audio model built with Hugging Face Transformers and Asteroid, designed for speech enhancement. Trained on the Libri1Mix dataset using the enh_single task, it improves speech quality by separating and enhancing single-speaker audio, achieving high performance on standard metrics such as SI-SDR and STOI.",
            "Tool 5: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 7: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.",
            "Tool 8: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 9: A Keras-based MAXIM model pre-trained for image deblurring, designed to restore sharpness to blurred images. It is optimized on the GoPro dataset and achieves high-quality results, making it suitable for enhancing image clarity in photography and computer vision applications.",
            "Tool 10: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.",
            "Tool 11: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.",
            "Tool 12: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 13: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.",
            "Tool 14: This tool uses the ConvTasNet model, trained on the sep_clean task of the Libri3Mix dataset, to perform audio source separation. It processes mixed audio signals into individual sources, leveraging Asteroid and Hugging Face Transformers frameworks. The model is designed to work with 3-source mixtures, operating at a sample rate of 8kHz, and is optimized for tasks like speech separation. It offers high performance with metrics such as improved Signal-to-Distortion Ratio (SDR) and Speech Intelligibility (STOI).",
            "Tool 15: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 16: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.",
            "Tool 17: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 18: Hubert-Large is a self-supervised speech representation model that extracts high-quality features from 16kHz audio. It is designed to handle challenges in speech representation learning, including variable-length sound units, multiple sound units per utterance, and the absence of a predefined lexicon during pretraining. By using an offline clustering step to generate target labels for a BERT-style prediction loss, it produces representations that match or surpass state-of-the-art performance on speech benchmarks like Libri-Light, making it useful for tasks requiring robust speech feature extraction.",
            "Tool 19: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis."
        ],
        "tool_view": {
            "description": "DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "parameters": {
                "audio": {
                    "type": "array",
                    "description": "An array representing the input audio waveform to be processed by the model. The audio should be preprocessed to match the model's expected sample rate of 16 kHz.",
                    "required": true,
                    "example": [
                        0.0,
                        0.1,
                        -0.1,
                        0.05,
                        -0.05
                    ]
                }
            },
            "response": "The tool produces an audio-to-audio output representing the enhanced or separated version of the input audio. Specifically, given an input audio signal, the model applies source separation or speech enhancement to produce a corresponding output audio signal with reduced interference or noise.  \n\nThe response can be described as follows:  \n\n- **Type**: Audio waveform or array  \n- **Format**: Typically a numeric array or tensor representing the waveform samples of the processed audio. The sampling rate matches the model’s training configuration (e.g., 16 kHz for this model).  \n- **Content**:  \n  - The same temporal length as the input audio (or slightly adjusted due to internal processing).  \n  - Enhanced or separated audio where target sources (e.g., speech) are more isolated, and noise or interfering sources are reduced.  \n- **Semantic role**: Each element of the array corresponds to an audio sample amplitude at a specific time point, representing the model’s attempt to reconstruct clean speech from the input mixture.  \n\nNo additional metadata is explicitly returned by the tool; the primary output is the processed audio waveform itself, suitable for playback or further audio processing tasks.",
            "examples": [
                {
                    "user_intent": "I want to use a pre-trained model to enhance audio. Can I try the DCUNet model for that?",
                    "tool_call": {
                        "name": "DCUNet_Libri1Mix_enhsingle_16k",
                        "arguments": {
                            "audio": "path_to_audio_file.wav"
                        }
                    }
                },
                {
                    "user_intent": "How can I apply audio enhancement using the DCUNet model from the Hugging Face Transformers framework?",
                    "tool_call": {
                        "name": "DCUNet_Libri1Mix_enhsingle_16k",
                        "arguments": {
                            "audio": "audio_input_data"
                        }
                    }
                },
                {
                    "user_intent": "I need to enhance a speech recording using a pre-trained model. Can I use the DCUNet model for this task?",
                    "tool_call": {
                        "name": "DCUNet_Libri1Mix_enhsingle_16k",
                        "arguments": {
                            "audio": "speech_recording.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\nTool 1: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 2: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.\nTool 3: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.\nTool 4: DCCRNet_Libri1Mix_enhsingle_16k is an audio-to-audio model built with Hugging Face Transformers and Asteroid, designed for speech enhancement. Trained on the Libri1Mix dataset using the enh_single task, it improves speech quality by separating and enhancing single-speaker audio, achieving high performance on standard metrics such as SI-SDR and STOI.\nTool 5: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 7: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.\nTool 8: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 9: A Keras-based MAXIM model pre-trained for image deblurring, designed to restore sharpness to blurred images. It is optimized on the GoPro dataset and achieves high-quality results, making it suitable for enhancing image clarity in photography and computer vision applications.\nTool 10: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\nTool 11: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.\nTool 12: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 13: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.\nTool 14: This tool uses the ConvTasNet model, trained on the sep_clean task of the Libri3Mix dataset, to perform audio source separation. It processes mixed audio signals into individual sources, leveraging Asteroid and Hugging Face Transformers frameworks. The model is designed to work with 3-source mixtures, operating at a sample rate of 8kHz, and is optimized for tasks like speech separation. It offers high performance with metrics such as improved Signal-to-Distortion Ratio (SDR) and Speech Intelligibility (STOI).\nTool 15: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 16: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.\nTool 17: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 18: Hubert-Large is a self-supervised speech representation model that extracts high-quality features from 16kHz audio. It is designed to handle challenges in speech representation learning, including variable-length sound units, multiple sound units per utterance, and the absence of a predefined lexicon during pretraining. By using an offline clustering step to generate target labels for a BERT-style prediction loss, it produces representations that match or surpass state-of-the-art performance on speech benchmarks like Libri-Light, making it useful for tasks requiring robust speech feature extraction.\nTool 19: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\", \"parameters\": {\"audio\": {\"type\": \"array\", \"description\": \"An array representing the input audio waveform to be processed by the model. The audio should be preprocessed to match the model's expected sample rate of 16 kHz.\", \"required\": true, \"example\": [0.0, 0.1, -0.1, 0.05, -0.05]}}, \"response\": \"The tool produces an audio-to-audio output representing the enhanced or separated version of the input audio. Specifically, given an input audio signal, the model applies source separation or speech enhancement to produce a corresponding output audio signal with reduced interference or noise.  \\n\\nThe response can be described as follows:  \\n\\n- **Type**: Audio waveform or array  \\n- **Format**: Typically a numeric array or tensor representing the waveform samples of the processed audio. The sampling rate matches the model\\u2019s training configuration (e.g., 16 kHz for this model).  \\n- **Content**:  \\n  - The same temporal length as the input audio (or slightly adjusted due to internal processing).  \\n  - Enhanced or separated audio where target sources (e.g., speech) are more isolated, and noise or interfering sources are reduced.  \\n- **Semantic role**: Each element of the array corresponds to an audio sample amplitude at a specific time point, representing the model\\u2019s attempt to reconstruct clean speech from the input mixture.  \\n\\nNo additional metadata is explicitly returned by the tool; the primary output is the processed audio waveform itself, suitable for playback or further audio processing tasks.\", \"examples\": [{\"user_intent\": \"I want to use a pre-trained model to enhance audio. Can I try the DCUNet model for that?\", \"tool_call\": {\"name\": \"DCUNet_Libri1Mix_enhsingle_16k\", \"arguments\": {\"audio\": \"path_to_audio_file.wav\"}}}, {\"user_intent\": \"How can I apply audio enhancement using the DCUNet model from the Hugging Face Transformers framework?\", \"tool_call\": {\"name\": \"DCUNet_Libri1Mix_enhsingle_16k\", \"arguments\": {\"audio\": \"audio_input_data\"}}}, {\"user_intent\": \"I need to enhance a speech recording using a pre-trained model. Can I use the DCUNet model for this task?\", \"tool_call\": {\"name\": \"DCUNet_Libri1Mix_enhsingle_16k\", \"arguments\": {\"audio\": \"speech_recording.wav\"}}}]}\n\n# User Query\nOur app offers assistance to people with hearing problems by enhancing the clarity of speech. We need a feature to clean and enhance the audio.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Enhance the clarity of speech in audio recordings for users with hearing difficulties.",
                    "tool_description": "Apply a pre-trained audio-to-audio model to perform speech enhancement, reducing noise and improving the intelligibility of single-speaker audio.",
                    "expected_response": "An enhanced audio waveform with improved speech clarity, reduced noise, and better separation of the target speaker from any background interference."
                }
            ],
            "extracted_arguments": {
                "audio": "An array representing the input audio waveform to be enhanced, preprocessed to match the model's expected sample rate."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_280",
        "query": "We want our System to generate possible user queries for a document provided as a text input.",
        "instruction": "Given a `query generation` task, retrieve tools that leverage text-to-text transformation capabilities by processing textual inputs in order to generate user queries aligned with the document's content and context.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text2Text Generation",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')",
                    "api_arguments": "text, max_length",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "MS MARCO",
                        "accuracy": "Not specified"
                    },
                    "description": "A T5 model trained on the MS MARCO dataset for generating queries from documents.",
                    "id": "huggingface_api_648",
                    "name": "castorini/doc2query-t5-base-msmarco"
                },
                "id": "gorilla_huggingface_tool_643",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 1: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 2: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 3: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 4: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 5: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 6: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 7: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 8: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 9: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.",
            "Tool 10: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 11: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 12: This tool provides a fine-tuned model for document question answering, built on the LayoutLMv2 architecture. It allows for extracting answers from documents based on textual queries, utilizing a pretrained version of the model specifically fine-tuned for information extraction tasks.",
            "Tool 13: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.",
            "Tool 14: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 15: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.",
            "Tool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 17: TAPEX is a transformer-based model designed for answering natural language questions over structured tables by generating and executing SQL queries. Built on the BART architecture, it enables table reasoning by learning from a synthetic corpus of executable SQL queries, allowing users to input a table and a query in natural language and receive accurate, structured responses.",
            "Tool 18: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 19: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information."
        ],
        "tool_view": {
            "description": "A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "parameters": {
                "tokenizer": {
                    "type": "object",
                    "description": "A tokenizer object from Hugging Face Transformers used to preprocess input text for the sequence classification model. It converts raw text into token IDs suitable for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                }
            },
            "response": "The tool described is a text classification model designed for distinguishing between question queries and statement queries. The response from this tool would likely consist of:\n\n1. **Predicted Label**: The primary output of the tool will be the predicted label for the input text. This label will correspond to whether the input text is classified as a \"Question Query\" or a \"Statement Query.\" \n\n2. **Model Confidence**: The tool may also provide a confidence score or probability associated with the prediction. This indicates the model’s certainty about the classification.\n\n3. **Model Metadata**: There could be metadata related to the model, such as the name of the model (\"shahrukhx01/question-vs-statement-classifier\") and the tokenizer used (which is specified in the API arguments). However, this information is more likely to be used for technical debugging or auditing rather than part of the core response.\n\n4. **Model Output Format**: The output could follow a typical classification result format, which would include:\n   - **Class Label**: Identifies whether the input text is classified as a \"Question Query\" or a \"Statement Query.\"\n   - **Confidence Score**: A numerical value (if available) representing how confident the model is about the classification.\n  \n5. **Error or Exception Handling**: If there’s an issue with the input or the processing, the response might include an error message or exception details explaining what went wrong (e.g., invalid input, processing error, or model failure). \n\nIn summary, the response from the tool will primarily include the classification label (whether the input text is a question or a statement) and possibly a confidence score, alongside any necessary error information if applicable.",
            "examples": [
                {
                    "user_intent": "I want to automatically detect if a user input is a question or just a statement.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "What time does the meeting start?",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                },
                {
                    "user_intent": "Classify this customer feedback to see if it contains questions or general statements.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "I really like the new layout of your website.",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                },
                {
                    "user_intent": "Analyze multiple chatbot messages to separate questions from statements.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "Can you help me reset my password?",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 1: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 2: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 3: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 4: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 5: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 6: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 7: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 8: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 9: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.\nTool 10: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 11: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 12: This tool provides a fine-tuned model for document question answering, built on the LayoutLMv2 architecture. It allows for extracting answers from documents based on textual queries, utilizing a pretrained version of the model specifically fine-tuned for information extraction tasks.\nTool 13: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.\nTool 14: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 15: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.\nTool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 17: TAPEX is a transformer-based model designed for answering natural language questions over structured tables by generating and executing SQL queries. Built on the BART architecture, it enables table reasoning by learning from a synthetic corpus of executable SQL queries, allowing users to input a table and a query in natural language and receive accurate, structured responses.\nTool 18: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 19: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\", \"parameters\": {\"tokenizer\": {\"type\": \"object\", \"description\": \"A tokenizer object from Hugging Face Transformers used to preprocess input text for the sequence classification model. It converts raw text into token IDs suitable for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}, \"response\": \"The tool described is a text classification model designed for distinguishing between question queries and statement queries. The response from this tool would likely consist of:\\n\\n1. **Predicted Label**: The primary output of the tool will be the predicted label for the input text. This label will correspond to whether the input text is classified as a \\\"Question Query\\\" or a \\\"Statement Query.\\\" \\n\\n2. **Model Confidence**: The tool may also provide a confidence score or probability associated with the prediction. This indicates the model\\u2019s certainty about the classification.\\n\\n3. **Model Metadata**: There could be metadata related to the model, such as the name of the model (\\\"shahrukhx01/question-vs-statement-classifier\\\") and the tokenizer used (which is specified in the API arguments). However, this information is more likely to be used for technical debugging or auditing rather than part of the core response.\\n\\n4. **Model Output Format**: The output could follow a typical classification result format, which would include:\\n   - **Class Label**: Identifies whether the input text is classified as a \\\"Question Query\\\" or a \\\"Statement Query.\\\"\\n   - **Confidence Score**: A numerical value (if available) representing how confident the model is about the classification.\\n  \\n5. **Error or Exception Handling**: If there\\u2019s an issue with the input or the processing, the response might include an error message or exception details explaining what went wrong (e.g., invalid input, processing error, or model failure). \\n\\nIn summary, the response from the tool will primarily include the classification label (whether the input text is a question or a statement) and possibly a confidence score, alongside any necessary error information if applicable.\", \"examples\": [{\"user_intent\": \"I want to automatically detect if a user input is a question or just a statement.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"What time does the meeting start?\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}, {\"user_intent\": \"Classify this customer feedback to see if it contains questions or general statements.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"I really like the new layout of your website.\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}, {\"user_intent\": \"Analyze multiple chatbot messages to separate questions from statements.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"Can you help me reset my password?\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}]}\n\n# User Query\nWe want our System to generate possible user queries for a document provided as a text input.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate potential user search queries from a given document.",
                    "tool_description": "A transformer-based model that analyzes a text document and produces suggested natural language queries to improve information retrieval and document search.",
                    "expected_response": "A list of text strings representing possible user queries derived from the content of the input document."
                }
            ],
            "extracted_arguments": {
                "document_text": "The full textual content of the document for which query suggestions are to be generated, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_282",
        "query": "We need to analyze user reviews for our app to determine whether they are positive, negative, or neutral.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that assess the emotional tone of text data by processing user reviews to classify them as positive, negative, or neutral, consistent with the query’s requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')",
                    "api_arguments": "text",
                    "python_environment_requirements": "Hugging Face Transformers library",
                    "example_code": "",
                    "performance": {
                        "dataset": "TASS 2020 corpus",
                        "accuracy": ""
                    },
                    "description": "Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.",
                    "id": "huggingface_api_387",
                    "name": "finiteautomata/beto-sentiment-analysis"
                },
                "id": "gorilla_huggingface_tool_383",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 1: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 2: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 3: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 4: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 5: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 6: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 7: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 8: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 9: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 10: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 11: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 12: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 13: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 14: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 15: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 16: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 18: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 19: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question."
        ],
        "tool_view": {
            "description": "A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "parameters": {
                "data": {
                    "type": "dataframe",
                    "description": "A pandas DataFrame containing the input features for the model. Each column should correspond to a feature used during training, and the columns must be preprocessed as expected by the model.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_0': [0.1, 0.5], 'feat_1': [1.2, 3.4]})"
                }
            },
            "response": "The tool response is expected to contain the following elements:\n\n1. **Predictions**: This will be the main output of the model's inference. It is expected to include the predicted labels for the input data provided in the `data` argument. Each label corresponds to a binary classification result (e.g., 0 or 1, representing categories such as \"negative\" or \"positive\" sentiment in the context of sentiment analysis).\n\n2. **Confidence Scores (optional)**: While not explicitly required by the documentation, a typical binary classification model may return confidence scores or probabilities associated with each prediction. These would reflect the model's certainty about each classification decision, usually in the form of probabilities between 0 and 1 for each class (e.g., the likelihood that a given review is positive or negative).\n\n3. **Metadata (optional)**: The response may include additional metadata about the prediction process. This could encompass things like the model version, processing time, or other relevant information that may assist with interpreting the results. However, this is not explicitly mentioned in the documentation, so it would be inferred as a common practice.\n\n4. **Model Information (optional)**: Although not required, there might be an inclusion of model-related information like the model's name (`desertdev/autotrain-imdb-sentiment-analysis-44994113085`), accuracy (`0.565`), and possibly any other relevant details about the training dataset (e.g., `desertdev/autotrain-data-imdb-sentiment-analysis`) to provide context for the predictions made.\n\nIn summary, the response would contain the binary classification results (predicted labels for the input data), potentially accompanied by confidence scores and additional metadata for the user to understand the predictions and their context.",
            "examples": [
                {
                    "user_intent": "I want to predict whether the movie reviews in my CSV file are positive or negative.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "movie_reviews.csv"
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of a batch of new reviews we just collected from our website.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "new_reviews_batch.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV file with user comments, and I need the model to classify each as positive or negative.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "user_comments.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 1: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 2: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 3: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 4: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 5: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 6: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 7: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 8: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 9: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 10: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 11: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 12: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 13: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 14: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 15: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 16: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 18: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 19: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\", \"parameters\": {\"data\": {\"type\": \"dataframe\", \"description\": \"A pandas DataFrame containing the input features for the model. Each column should correspond to a feature used during training, and the columns must be preprocessed as expected by the model.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_0': [0.1, 0.5], 'feat_1': [1.2, 3.4]})\"}}, \"response\": \"The tool response is expected to contain the following elements:\\n\\n1. **Predictions**: This will be the main output of the model's inference. It is expected to include the predicted labels for the input data provided in the `data` argument. Each label corresponds to a binary classification result (e.g., 0 or 1, representing categories such as \\\"negative\\\" or \\\"positive\\\" sentiment in the context of sentiment analysis).\\n\\n2. **Confidence Scores (optional)**: While not explicitly required by the documentation, a typical binary classification model may return confidence scores or probabilities associated with each prediction. These would reflect the model's certainty about each classification decision, usually in the form of probabilities between 0 and 1 for each class (e.g., the likelihood that a given review is positive or negative).\\n\\n3. **Metadata (optional)**: The response may include additional metadata about the prediction process. This could encompass things like the model version, processing time, or other relevant information that may assist with interpreting the results. However, this is not explicitly mentioned in the documentation, so it would be inferred as a common practice.\\n\\n4. **Model Information (optional)**: Although not required, there might be an inclusion of model-related information like the model's name (`desertdev/autotrain-imdb-sentiment-analysis-44994113085`), accuracy (`0.565`), and possibly any other relevant details about the training dataset (e.g., `desertdev/autotrain-data-imdb-sentiment-analysis`) to provide context for the predictions made.\\n\\nIn summary, the response would contain the binary classification results (predicted labels for the input data), potentially accompanied by confidence scores and additional metadata for the user to understand the predictions and their context.\", \"examples\": [{\"user_intent\": \"I want to predict whether the movie reviews in my CSV file are positive or negative.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"movie_reviews.csv\"}}}, {\"user_intent\": \"Check the sentiment of a batch of new reviews we just collected from our website.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"new_reviews_batch.csv\"}}}, {\"user_intent\": \"I have a CSV file with user comments, and I need the model to classify each as positive or negative.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"user_comments.csv\"}}}]}\n\n# User Query\nWe need to analyze user reviews for our app to determine whether they are positive, negative, or neutral.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze user reviews to determine if they are positive, negative, or neutral.",
                    "tool_description": "Perform sentiment analysis on a set of user reviews to classify each review as positive, negative, or neutral.",
                    "expected_response": "A collection of sentiment classifications for each review, with labels indicating positive, negative, or neutral sentiment."
                }
            ],
            "extracted_arguments": {
                "data": "A dataset containing the user reviews to be analyzed, typically in a tabular format such as a CSV or DataFrame."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_488",
        "query": "I have received a PDF document and a question. My task is to find the answer part in the document.",
        "instruction": "Given a `document question answering` task, retrieve tools that utilize content extraction and comprehension techniques on PDF documents to accurately find answers related to specific queries in the document.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "pipeline('question-answering')",
                    "api_arguments": [
                        "image_url",
                        "question"
                    ],
                    "python_environment_requirements": [
                        "PIL",
                        "pytesseract",
                        "PyTorch",
                        "transformers"
                    ],
                    "example_code": "nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)",
                    "performance": {
                        "dataset": [
                            "SQuAD2.0",
                            "DocVQA"
                        ],
                        "accuracy": "Not provided"
                    },
                    "description": "A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.",
                    "id": "huggingface_api_930",
                    "name": "impira/layoutlm-document-qa"
                },
                "id": "gorilla_huggingface_tool_113",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 1: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 2: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 4: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 5: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 6: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 8: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 9: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 11: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 12: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 13: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 14: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.",
            "Tool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 16: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 17: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.",
            "Tool 18: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information."
        ],
        "tool_view": {
            "description": "This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "A text string containing the question that is to be answered based on the provided context.",
                    "required": true,
                    "example": "What is the address listed on the document?"
                },
                "context": {
                    "type": "string",
                    "description": "A text string containing the context or content of the document from which the answer should be extracted.",
                    "required": true,
                    "example": "The document mentions that the office address is 123 Business Ave, Suite 4B."
                }
            },
            "response": "The tool produces a response that contains the answer to a question posed about a given document context. Specifically, the response typically includes:\n\n- **Answer Text:** A string representing the extracted or generated answer to the input question based on the provided document context.\n- **Answer Span or Location (optional):** Some implementations may also return the position or span within the document text or layout where the answer was found or predicted.\n- **Confidence Scores (optional):** The response might include a confidence score or probability indicating the model’s certainty about the answer.\n\nOverall, the response delivers a concise, contextually grounded answer to the user's question by leveraging the LayoutLMv2 model’s ability to understand both textual content and document layout information.",
            "examples": [
                {
                    "user_intent": "I have a PDF of a financial report and want to find out the total revenue for 2024.",
                    "tool_call": {
                        "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023",
                        "arguments": {
                            "question": "What is the total revenue for 2024?",
                            "context": "Attached is the PDF content of the 2024 financial report, including tables and text."
                        }
                    }
                },
                {
                    "user_intent": "Check the contract document and tell me who is responsible for maintenance.",
                    "tool_call": {
                        "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023",
                        "arguments": {
                            "question": "Who is responsible for maintenance according to this contract?",
                            "context": "Full text of the signed maintenance contract with all clauses and sections."
                        }
                    }
                },
                {
                    "user_intent": "I want to find the expiry date listed in this insurance policy document.",
                    "tool_call": {
                        "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023",
                        "arguments": {
                            "question": "What is the expiry date of the insurance policy?",
                            "context": "The insurance policy document containing the policy number, coverage details, and terms."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 1: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 2: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 4: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 5: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 6: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 8: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 9: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 11: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 12: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 13: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 14: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.\nTool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 16: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 17: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.\nTool 18: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"A text string containing the question that is to be answered based on the provided context.\", \"required\": true, \"example\": \"What is the address listed on the document?\"}, \"context\": {\"type\": \"string\", \"description\": \"A text string containing the context or content of the document from which the answer should be extracted.\", \"required\": true, \"example\": \"The document mentions that the office address is 123 Business Ave, Suite 4B.\"}}, \"response\": \"The tool produces a response that contains the answer to a question posed about a given document context. Specifically, the response typically includes:\\n\\n- **Answer Text:** A string representing the extracted or generated answer to the input question based on the provided document context.\\n- **Answer Span or Location (optional):** Some implementations may also return the position or span within the document text or layout where the answer was found or predicted.\\n- **Confidence Scores (optional):** The response might include a confidence score or probability indicating the model\\u2019s certainty about the answer.\\n\\nOverall, the response delivers a concise, contextually grounded answer to the user's question by leveraging the LayoutLMv2 model\\u2019s ability to understand both textual content and document layout information.\", \"examples\": [{\"user_intent\": \"I have a PDF of a financial report and want to find out the total revenue for 2024.\", \"tool_call\": {\"name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023\", \"arguments\": {\"question\": \"What is the total revenue for 2024?\", \"context\": \"Attached is the PDF content of the 2024 financial report, including tables and text.\"}}}, {\"user_intent\": \"Check the contract document and tell me who is responsible for maintenance.\", \"tool_call\": {\"name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023\", \"arguments\": {\"question\": \"Who is responsible for maintenance according to this contract?\", \"context\": \"Full text of the signed maintenance contract with all clauses and sections.\"}}}, {\"user_intent\": \"I want to find the expiry date listed in this insurance policy document.\", \"tool_call\": {\"name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023\", \"arguments\": {\"question\": \"What is the expiry date of the insurance policy?\", \"context\": \"The insurance policy document containing the policy number, coverage details, and terms.\"}}}]}\n\n# User Query\nI have received a PDF document and a question. My task is to find the answer part in the document.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find the answer to a question within a PDF document.",
                    "tool_description": "Use a document question-answering model based on LayoutLMv2 to extract an answer from the provided document content using the input question.",
                    "expected_response": "A response containing the extracted answer text from the document, optionally including the answer span or location within the document and a confidence score."
                }
            ],
            "extracted_arguments": {
                "question": "A text string containing the question to be answered based on the document.",
                "context": "A text string containing the content of the PDF document from which the answer should be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_133",
        "query": "Write a story about a spaceship journey to a distant planet in search of a new home for humanity.",
        "instruction": "Given a `creative writing` task, retrieve tools that facilitate text generation by utilizing advanced language models to craft narrative stories based on thematic prompts and content requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-j-6B')",
                    "api_arguments": {
                        "pretrained_model": "EleutherAI/gpt-j-6B"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForCausalLM"
                    },
                    "example_code": {
                        "loading_model": "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(EleutherAI/gpt-j-6B)\nmodel = AutoModelForCausalLM.from_pretrained(EleutherAI/gpt-j-6B)"
                    },
                    "performance": {
                        "dataset": "the_pile",
                        "accuracy": {
                            "LAMBADA_PPL": 3.99,
                            "LAMBADA_Acc": "69.7%",
                            "Winogrande": "65.3%",
                            "Hellaswag": "66.1%",
                            "PIQA": "76.5%"
                        }
                    },
                    "description": "GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. It consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI.",
                    "id": "huggingface_api_611",
                    "name": "EleutherAI/gpt-j-6B"
                },
                "id": "gorilla_huggingface_tool_606",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 1: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 2: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 5: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 6: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 7: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 8: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 9: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 10: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 11: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 12: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 13: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 14: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.",
            "Tool 15: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 16: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 17: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 18: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.",
            "Tool 19: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages."
        ],
        "tool_view": {
            "description": "GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text prompt that the model will continue or generate text from.",
                    "required": true,
                    "example": "Once upon a time"
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens to generate in the output sequence.",
                    "required": true,
                    "example": 50
                },
                "num_return_sequences": {
                    "type": "number",
                    "description": "The number of independent text sequences to generate for the given input prompt.",
                    "required": true,
                    "example": 3
                }
            },
            "response": "The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\n\nHere’s a description of the response structure:\n\n1. **Generated Texts**: \n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \n   - Each sequence will represent a continuation or completion of the input text (e.g., \"Hello, I'm a language model,\").\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\n\n2. **Content of Each Text Generation**: \n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\n\n### Key Components:\n\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\n   - Each element in the array is a string, representing a complete generated text.\n\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\n\n### Example of Expected Output Content (Abstract):\n\n- A list of 5 strings, each one being a text sequence that continues from the input (\"Hello, I'm a language model,\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\n\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.",
            "examples": [
                {
                    "user_intent": "Generate a short creative story starting with 'Once upon a time in a distant galaxy,'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "Once upon a time in a distant galaxy,",
                            "max_length": 50,
                            "num_return_sequences": 3
                        }
                    }
                },
                {
                    "user_intent": "Continue this sentence realistically: 'The stock market crashed yesterday because'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "The stock market crashed yesterday because",
                            "max_length": 40,
                            "num_return_sequences": 2
                        }
                    }
                },
                {
                    "user_intent": "Write multiple humorous completions for: 'I never expected my cat to'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "I never expected my cat to",
                            "max_length": 30,
                            "num_return_sequences": 5
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 1: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 2: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 5: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 6: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 7: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 8: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 9: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 10: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 11: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 12: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 13: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 14: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\nTool 15: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 16: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 17: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 18: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.\nTool 19: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text prompt that the model will continue or generate text from.\", \"required\": true, \"example\": \"Once upon a time\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens to generate in the output sequence.\", \"required\": true, \"example\": 50}, \"num_return_sequences\": {\"type\": \"number\", \"description\": \"The number of independent text sequences to generate for the given input prompt.\", \"required\": true, \"example\": 3}}, \"response\": \"The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\\n\\nHere\\u2019s a description of the response structure:\\n\\n1. **Generated Texts**: \\n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \\n   - Each sequence will represent a continuation or completion of the input text (e.g., \\\"Hello, I'm a language model,\\\").\\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\\n\\n2. **Content of Each Text Generation**: \\n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\\n\\n### Key Components:\\n\\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \\n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\\n   - Each element in the array is a string, representing a complete generated text.\\n\\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\\n\\n### Example of Expected Output Content (Abstract):\\n\\n- A list of 5 strings, each one being a text sequence that continues from the input (\\\"Hello, I'm a language model,\\\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\\n\\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.\", \"examples\": [{\"user_intent\": \"Generate a short creative story starting with 'Once upon a time in a distant galaxy,'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"Once upon a time in a distant galaxy,\", \"max_length\": 50, \"num_return_sequences\": 3}}}, {\"user_intent\": \"Continue this sentence realistically: 'The stock market crashed yesterday because'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"The stock market crashed yesterday because\", \"max_length\": 40, \"num_return_sequences\": 2}}}, {\"user_intent\": \"Write multiple humorous completions for: 'I never expected my cat to'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"I never expected my cat to\", \"max_length\": 30, \"num_return_sequences\": 5}}}]}\n\n# User Query\nWrite a story about a spaceship journey to a distant planet in search of a new home for humanity.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a story about a spaceship journey to a distant planet searching for a new home for humanity.",
                    "tool_description": "Use a transformer-based language model to create a coherent narrative based on a space exploration prompt, producing multiple creative text continuations.",
                    "expected_response": "A set of text sequences, each representing a story or narrative continuation of the spaceship journey scenario."
                }
            ],
            "extracted_arguments": {
                "text": "The input prompt describing the spaceship journey and mission to find a new home for humanity.",
                "max_length": "The maximum number of tokens to generate in the story.",
                "num_return_sequences": "The number of independent story variations to generate."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_265",
        "query": "My company develops drones for agriculture purposes, and we need a model to segment aerial images accurately.",
        "instruction": "Given an `image segmentation` task, retrieve tools that process aerial images for accurate segmentation, focused on distinguishing features relevant to agricultural applications by implementing advanced computer vision models.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')",
                    "api_arguments": {
                        "from_pretrained": "facebook/maskformer-swin-base-ade"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "PIL": "latest",
                        "requests": "latest"
                    },
                    "example_code": "from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\ninputs = feature_extractor(images=image, return_tensors=pt)\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\noutputs = model(**inputs)\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]",
                    "performance": {
                        "dataset": "ADE20k",
                        "accuracy": "Not provided"
                    },
                    "description": "MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.",
                    "id": "huggingface_api_251",
                    "name": "facebook/maskformer-swin-base-ade"
                },
                "id": "gorilla_huggingface_tool_247",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 1: A fine-tuned CLIP model optimized for zero-shot image classification on remote sensing images, enabling users to match images with descriptive text labels without additional training and facilitating text-to-image and image-to-image retrieval in aerial and satellite imagery contexts.",
            "Tool 2: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 3: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 4: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 5: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 6: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 7: A ConvNeXt-V2 image classification model pretrained with a fully convolutional masked autoencoder (FCMAE) and fine-tuned on ImageNet-1k, capable of accurately identifying and classifying images with high precision.",
            "Tool 8: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 9: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 10: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.",
            "Tool 11: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.",
            "Tool 12: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 13: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 14: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 15: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.",
            "Tool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 17: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.",
            "Tool 18: A YOLOv8-based model for detecting and segmenting defects on printed circuit boards (PCBs), including issues like dry joints, incorrect installation, PCB damage, and short circuits, allowing precise localization and mask generation for quality control and inspection purposes.",
            "Tool 19: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes."
        ],
        "tool_view": {
            "description": "This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.",
                    "required": true,
                    "example": 0.25
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.",
                    "required": true,
                    "example": 0.45
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.",
                    "required": true,
                    "example": false
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections to return per image.",
                    "required": true,
                    "example": 1000
                },
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. Can be a local file path or a URL pointing to the image.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool’s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \n\n1. **`boxes`** – A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \n   - A confidence score indicating the model’s certainty that the object belongs to the target class.  \n   - Optional class label or ID identifying the object category (here, all are buildings).  \n\n2. **`masks`** – Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \n\n3. **`scores`** – Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \n\n4. **`class_ids` or `labels`** – Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \n\n5. **`metadata`** (optional) – May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \n\n6. **Visualization helpers** – Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \n\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.",
            "examples": [
                {
                    "user_intent": "I want to segment all buildings in a satellite image of a city block.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500,
                            "image": "https://example.com/satellite_city_block.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect and segment buildings in a rural area satellite image with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.6,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 200,
                            "image": "https://example.com/rural_area_satellite.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Process a satellite image to extract building masks for GIS analysis.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000,
                            "image": "https://example.com/gis_project_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 1: A fine-tuned CLIP model optimized for zero-shot image classification on remote sensing images, enabling users to match images with descriptive text labels without additional training and facilitating text-to-image and image-to-image retrieval in aerial and satellite imagery contexts.\nTool 2: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 3: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 4: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 5: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 6: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 7: A ConvNeXt-V2 image classification model pretrained with a fully convolutional masked autoencoder (FCMAE) and fine-tuned on ImageNet-1k, capable of accurately identifying and classifying images with high precision.\nTool 8: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 9: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 10: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.\nTool 11: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.\nTool 12: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 13: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 14: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 15: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.\nTool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 17: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.\nTool 18: A YOLOv8-based model for detecting and segmenting defects on printed circuit boards (PCBs), including issues like dry joints, incorrect installation, PCB damage, and short circuits, allowing precise localization and mask generation for quality control and inspection purposes.\nTool 19: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.\", \"required\": true, \"example\": 0.25}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.\", \"required\": true, \"example\": 0.45}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.\", \"required\": true, \"example\": false}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections to return per image.\", \"required\": true, \"example\": 1000}, \"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. Can be a local file path or a URL pointing to the image.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool\\u2019s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \\n\\n1. **`boxes`** \\u2013 A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \\n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \\n   - A confidence score indicating the model\\u2019s certainty that the object belongs to the target class.  \\n   - Optional class label or ID identifying the object category (here, all are buildings).  \\n\\n2. **`masks`** \\u2013 Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \\n\\n3. **`scores`** \\u2013 Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \\n\\n4. **`class_ids` or `labels`** \\u2013 Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \\n\\n5. **`metadata`** (optional) \\u2013 May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \\n\\n6. **Visualization helpers** \\u2013 Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \\n\\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.\", \"examples\": [{\"user_intent\": \"I want to segment all buildings in a satellite image of a city block.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500, \"image\": \"https://example.com/satellite_city_block.jpg\"}}}, {\"user_intent\": \"Detect and segment buildings in a rural area satellite image with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.6, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 200, \"image\": \"https://example.com/rural_area_satellite.jpg\"}}}, {\"user_intent\": \"Process a satellite image to extract building masks for GIS analysis.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://example.com/gis_project_image.jpg\"}}}]}\n\n# User Query\nMy company develops drones for agriculture purposes, and we need a model to segment aerial images accurately.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to segment buildings or structures in aerial images for agricultural analysis.",
                    "tool_description": "Use a YOLOv8-based model trained on satellite or aerial imagery to detect and segment relevant structures within the image. The model allows configurable parameters for confidence threshold, non-maxima suppression, and maximum detections, providing accurate segmentation outputs suitable for analysis.",
                    "expected_response": "A structured object containing bounding boxes, segmentation masks, confidence scores, and optional class labels for each detected object in the aerial image."
                }
            ],
            "extracted_arguments": {
                "image": "The input aerial or satellite image to be segmented, provided as a file path or URL.",
                "conf": "The confidence threshold for detecting objects; predictions below this value will be discarded (numeric).",
                "iou": "The Intersection over Union threshold for non-maximum suppression to filter overlapping detections (numeric).",
                "agnostic_nms": "A boolean indicating whether non-maximum suppression should be class-agnostic.",
                "max_det": "The maximum number of detections to return per image (numeric)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_364",
        "query": "We are working on creating an audiobook. Convert this text: \"The sun was shining brightly, and the birds were singing sweetly\" into speech.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that process textual inputs to generate synthesized speech outputs, transforming written content into an audio format suitable for audiobooks and similar applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "SpeechBrain",
                    "functionality": "Text-to-Speech",
                    "api_call": "Tacotron2.from_hparams(source='speechbrain/tts-tacotron2-ljspeech')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "speechbrain"
                    ],
                    "example_code": [
                        "import torchaudio",
                        "from speechbrain.pretrained import Tacotron2",
                        "from speechbrain.pretrained import HIFIGAN",
                        "tacotron2 = Tacotron2.from_hparams(source=speechbrain/tts-tacotron2-ljspeech, savedir=tmpdir_tts)",
                        "hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)",
                        "mel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)",
                        "waveforms = hifi_gan.decode_batch(mel_output)",
                        "torchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)"
                    ],
                    "performance": {
                        "dataset": "LJSpeech",
                        "accuracy": "Not specified"
                    },
                    "description": "This repository provides all the necessary tools for Text-to-Speech (TTS) with SpeechBrain using a Tacotron2 pretrained on LJSpeech. The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.",
                    "id": "huggingface_api_723",
                    "name": "speechbrain/tts-tacotron2-ljspeech"
                },
                "id": "gorilla_huggingface_tool_718",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 1: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 2: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 3: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.",
            "Tool 4: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 5: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 6: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.",
            "Tool 7: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.",
            "Tool 8: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 9: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 10: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 11: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.",
            "Tool 12: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.",
            "Tool 13: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 14: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.",
            "Tool 15: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 16: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.",
            "Tool 17: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 18: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 19: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis."
        ],
        "tool_view": {
            "description": "The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "parameters": {
                "arg_overrides": {
                    "type": "object",
                    "description": "A dictionary of arguments that override default model or task configurations. For example, you can specify a custom configuration YAML file or set the task type.",
                    "required": true,
                    "example": {
                        "config_yaml": "config.yaml",
                        "task": "speech_to_text"
                    }
                },
                "cache_dir": {
                    "type": "string",
                    "description": "The directory path where downloaded model files and other cached data will be stored. If not provided, defaults to the environment variable 'HUGGINGFACE_HUB_CACHE' or a standard cache location.",
                    "required": true,
                    "example": "/home/user/.cache/huggingface"
                }
            },
            "response": "The response from this tool would likely contain several key components, organized around the functionality of performing speech-to-speech translation from English to Hokkien using a model from Fairseq. Below is a description of the expected response structure and its semantic components:\n\n1. **Model and Task Information**:\n   - The response would confirm the successful loading of the **speech-to-speech translation model** and associated **task**. It may include:\n     - The model type used (in this case, \"xm_transformer_s2ut_en-hk\").\n     - The task used (likely \"speech_to_text\" or similar).\n\n2. **Input and Output Audio Data**:\n   - **Input Audio**: The response would include some form of **audio input data**, such as an audio waveform or an indication of successful input loading (e.g., confirmation of loading the audio file).\n   - **Predicted Translation**: The **predicted output** from the speech-to-speech translation would be provided, which might include:\n     - Translated speech in Hokkien corresponding to the English audio input.\n     - The output audio could be in the form of a **waveform** or **audio file** with the corresponding sampling rate.\n\n3. **Generated Prediction**:\n   - There would likely be information about the model's **prediction generation process**, which may include:\n     - Details about how the input audio was processed (e.g., through the Fairseq `S2THubInterface`).\n     - The output **prediction unit** (likely a phonetic or linguistic unit after translation).\n\n4. **Speech Synthesis Output**:\n   - The response may describe or include details about the **speech synthesis** process used to generate the output audio. It might include:\n     - Confirmation of the **vocoder model** used for speech synthesis (e.g., \"CodeHiFiGANVocoder\").\n     - **Waveform data** (`wav`) along with the **sampling rate** (`sr`), allowing the synthesized audio to be played back or saved.\n\n5. **Performance Details**:\n   - The response would mention the performance of the model, which in this case includes:\n     - A reference to the **MuST-C dataset** as the training data.\n     - **Accuracy** details may not be explicitly specified but might be inferred from the dataset used (TED domain, Audiobook domain).\n\n6. **Configuration and Environment**:\n   - The response would mention the **configuration** used for the model, referencing files like `config.yaml` or `config.json` and possibly specifying that certain configurations were loaded for tasks like speech-to-text.\n   - **Python environment requirements** such as `fairseq`, `huggingface_hub`, and `torchaudio` would be implicitly required to execute the function, but would not be part of the direct output.\n\nIn summary, the response would contain:\n- A confirmation of successful model loading and task initialization.\n- Details of the input audio (possibly the waveform data) and output (translated speech in Hokkien).\n- The generated prediction (linguistic or phonetic unit).\n- Audio synthesis output (waveform and sampling rate).\n- Details about the performance dataset and any relevant configurations or model specifications used.\n\n",
            "examples": [
                {
                    "user_intent": "Translate an English TED talk audio into Hokkien speech.",
                    "tool_call": {
                        "name": "xm_transformer_s2ut_en-hk",
                        "arguments": {
                            "input_audio_path": "/path/to/ted_talk_en.wav",
                            "output_audio_path": "/path/to/ted_talk_hk.wav",
                            "device": "cpu"
                        }
                    }
                },
                {
                    "user_intent": "Convert an audiobook chapter from English to Hokkien audio for language learning.",
                    "tool_call": {
                        "name": "xm_transformer_s2ut_en-hk",
                        "arguments": {
                            "input_audio_path": "/path/to/audiobook_chapter_en.wav",
                            "output_audio_path": "/path/to/audiobook_chapter_hk.wav",
                            "device": "cpu"
                        }
                    }
                },
                {
                    "user_intent": "Quickly generate a Hokkien version of a short English podcast segment.",
                    "tool_call": {
                        "name": "xm_transformer_s2ut_en-hk",
                        "arguments": {
                            "input_audio_path": "/path/to/podcast_segment_en.wav",
                            "output_audio_path": "/path/to/podcast_segment_hk.wav",
                            "device": "cpu"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 1: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 2: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 3: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.\nTool 4: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 5: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 6: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.\nTool 7: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.\nTool 8: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 9: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 10: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 11: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\nTool 12: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.\nTool 13: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 14: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.\nTool 15: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 16: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\nTool 17: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 18: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 19: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"xm_transformer_s2ut_en-hk\\\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\", \"parameters\": {\"arg_overrides\": {\"type\": \"object\", \"description\": \"A dictionary of arguments that override default model or task configurations. For example, you can specify a custom configuration YAML file or set the task type.\", \"required\": true, \"example\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\"}}, \"cache_dir\": {\"type\": \"string\", \"description\": \"The directory path where downloaded model files and other cached data will be stored. If not provided, defaults to the environment variable 'HUGGINGFACE_HUB_CACHE' or a standard cache location.\", \"required\": true, \"example\": \"/home/user/.cache/huggingface\"}}, \"response\": \"The response from this tool would likely contain several key components, organized around the functionality of performing speech-to-speech translation from English to Hokkien using a model from Fairseq. Below is a description of the expected response structure and its semantic components:\\n\\n1. **Model and Task Information**:\\n   - The response would confirm the successful loading of the **speech-to-speech translation model** and associated **task**. It may include:\\n     - The model type used (in this case, \\\"xm_transformer_s2ut_en-hk\\\").\\n     - The task used (likely \\\"speech_to_text\\\" or similar).\\n\\n2. **Input and Output Audio Data**:\\n   - **Input Audio**: The response would include some form of **audio input data**, such as an audio waveform or an indication of successful input loading (e.g., confirmation of loading the audio file).\\n   - **Predicted Translation**: The **predicted output** from the speech-to-speech translation would be provided, which might include:\\n     - Translated speech in Hokkien corresponding to the English audio input.\\n     - The output audio could be in the form of a **waveform** or **audio file** with the corresponding sampling rate.\\n\\n3. **Generated Prediction**:\\n   - There would likely be information about the model's **prediction generation process**, which may include:\\n     - Details about how the input audio was processed (e.g., through the Fairseq `S2THubInterface`).\\n     - The output **prediction unit** (likely a phonetic or linguistic unit after translation).\\n\\n4. **Speech Synthesis Output**:\\n   - The response may describe or include details about the **speech synthesis** process used to generate the output audio. It might include:\\n     - Confirmation of the **vocoder model** used for speech synthesis (e.g., \\\"CodeHiFiGANVocoder\\\").\\n     - **Waveform data** (`wav`) along with the **sampling rate** (`sr`), allowing the synthesized audio to be played back or saved.\\n\\n5. **Performance Details**:\\n   - The response would mention the performance of the model, which in this case includes:\\n     - A reference to the **MuST-C dataset** as the training data.\\n     - **Accuracy** details may not be explicitly specified but might be inferred from the dataset used (TED domain, Audiobook domain).\\n\\n6. **Configuration and Environment**:\\n   - The response would mention the **configuration** used for the model, referencing files like `config.yaml` or `config.json` and possibly specifying that certain configurations were loaded for tasks like speech-to-text.\\n   - **Python environment requirements** such as `fairseq`, `huggingface_hub`, and `torchaudio` would be implicitly required to execute the function, but would not be part of the direct output.\\n\\nIn summary, the response would contain:\\n- A confirmation of successful model loading and task initialization.\\n- Details of the input audio (possibly the waveform data) and output (translated speech in Hokkien).\\n- The generated prediction (linguistic or phonetic unit).\\n- Audio synthesis output (waveform and sampling rate).\\n- Details about the performance dataset and any relevant configurations or model specifications used.\\n\\n\", \"examples\": [{\"user_intent\": \"Translate an English TED talk audio into Hokkien speech.\", \"tool_call\": {\"name\": \"xm_transformer_s2ut_en-hk\", \"arguments\": {\"input_audio_path\": \"/path/to/ted_talk_en.wav\", \"output_audio_path\": \"/path/to/ted_talk_hk.wav\", \"device\": \"cpu\"}}}, {\"user_intent\": \"Convert an audiobook chapter from English to Hokkien audio for language learning.\", \"tool_call\": {\"name\": \"xm_transformer_s2ut_en-hk\", \"arguments\": {\"input_audio_path\": \"/path/to/audiobook_chapter_en.wav\", \"output_audio_path\": \"/path/to/audiobook_chapter_hk.wav\", \"device\": \"cpu\"}}}, {\"user_intent\": \"Quickly generate a Hokkien version of a short English podcast segment.\", \"tool_call\": {\"name\": \"xm_transformer_s2ut_en-hk\", \"arguments\": {\"input_audio_path\": \"/path/to/podcast_segment_en.wav\", \"output_audio_path\": \"/path/to/podcast_segment_hk.wav\", \"device\": \"cpu\"}}}]}\n\n# User Query\nWe are working on creating an audiobook. Convert this text: \"The sun was shining brightly, and the birds were singing sweetly\" into speech.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert written text from an audiobook into natural-sounding speech.",
                    "tool_description": "Use a text-to-speech model to synthesize spoken audio from provided written text, generating realistic speech suitable for audiobooks or narration.",
                    "expected_response": "An audio file or waveform containing the spoken version of the input text."
                }
            ],
            "extracted_arguments": {
                "input_text": "The written text content to be converted into speech.",
                "voice_type": "Optional parameter specifying desired voice characteristics (e.g., male, female, accent).",
                "output_audio_path": "File path or location where the generated speech audio will be saved.",
                "device": "Optional parameter specifying the computing device for processing (e.g., CPU or GPU)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_278",
        "query": "Our task is to complete a given sentence with a missing word. The sentence is from an electronic health record.",
        "instruction": "Given a `text completion` task in the clinical domain, retrieve tools that facilitate Natural Language Processing by leveraging fill-mask techniques to accurately complete sentences based on contextual inputs from electronic health records.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')",
                    "api_arguments": [
                        "AutoTokenizer",
                        "AutoModel",
                        "from_pretrained"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\nmodel = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')",
                    "performance": {
                        "dataset": "MIMIC III",
                        "accuracy": "Not provided"
                    },
                    "description": "Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI).",
                    "id": "huggingface_api_674",
                    "name": "emilyalsentzer/Bio_ClinicalBERT"
                },
                "id": "gorilla_huggingface_tool_669",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 1: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 2: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 3: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 4: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 5: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.",
            "Tool 6: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 7: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 8: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 9: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 10: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 11: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 12: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.",
            "Tool 13: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.",
            "Tool 14: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.",
            "Tool 15: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.",
            "Tool 16: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 17: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 18: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 19: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity."
        ],
        "tool_view": {
            "description": "CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.",
                    "required": true,
                    "example": "camembert-base"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.",
                    "required": true,
                    "example": "camembert-base"
                }
            },
            "response": "The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \n\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \n- **score**: A numerical value representing the model’s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \n- **token**: The identifier of the predicted token according to the model’s vocabulary.  \n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \n\nThe response is usually ordered by descending score, so the first entry is the model’s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.",
            "examples": [
                {
                    "user_intent": "Predict the missing word in the French sentence about cheese.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Le camembert est <mask> :)"
                        }
                    }
                },
                {
                    "user_intent": "Complete this French sentence describing the weather.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Aujourd'hui, il fait très <mask> dehors."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for a French sentence about happiness.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Je me sens vraiment <mask> aujourd'hui."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 1: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 2: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 3: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 4: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 5: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.\nTool 6: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 7: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 8: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 9: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 10: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 11: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 12: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.\nTool 13: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.\nTool 14: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.\nTool 15: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.\nTool 16: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 17: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 18: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 19: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.\", \"required\": true, \"example\": \"camembert-base\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.\", \"required\": true, \"example\": \"camembert-base\"}}, \"response\": \"The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \\n\\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \\n- **score**: A numerical value representing the model\\u2019s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \\n- **token**: The identifier of the predicted token according to the model\\u2019s vocabulary.  \\n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \\n\\nThe response is usually ordered by descending score, so the first entry is the model\\u2019s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.\", \"examples\": [{\"user_intent\": \"Predict the missing word in the French sentence about cheese.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Le camembert est <mask> :)\"}}}, {\"user_intent\": \"Complete this French sentence describing the weather.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Aujourd'hui, il fait tr\\u00e8s <mask> dehors.\"}}}, {\"user_intent\": \"Fill in the blank for a French sentence about happiness.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Je me sens vraiment <mask> aujourd'hui.\"}}}]}\n\n# User Query\nOur task is to complete a given sentence with a missing word. The sentence is from an electronic health record.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Complete the missing word in a sentence from an electronic health record.",
                    "tool_description": "Use a masked language model to predict and fill in missing words in medical or clinical text, enabling contextual completion of sentences.",
                    "expected_response": "A ranked list of candidate words to replace the masked token, including the completed sentence, predicted token, token ID, and confidence score."
                }
            ],
            "extracted_arguments": {
                "text": "The input sentence from the electronic health record containing a masked token representing the missing word.",
                "model": "The specific masked language model to use for predicting the missing word, e.g., a medical or general-purpose model.",
                "tokenizer": "The tokenizer corresponding to the chosen model to properly process the input text."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_240",
        "query": "Take a look at this document image and tell me the answer to my question: \"What is the total amount due?\".",
        "instruction": "Given an `image-based document question answering` task, retrieve tools that utilize multimodal capabilities to interpret and process images containing text and layout information in order to extract specific answers to questions, such as the total amount due from a document image.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')",
                    "api_arguments": {},
                    "python_environment_requirements": {
                        "transformers": ">=4.11.0"
                    },
                    "example_code": {},
                    "performance": {
                        "dataset": {},
                        "accuracy": {}
                    },
                    "description": "A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.",
                    "id": "huggingface_api_134",
                    "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023"
                },
                "id": "gorilla_huggingface_tool_132",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 1: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 2: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 3: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 4: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 5: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 6: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 7: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 8: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 9: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 10: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 11: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 12: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 13: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 14: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.",
            "Tool 15: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 16: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.",
            "Tool 17: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 18: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 19: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture."
        ],
        "tool_view": {
            "description": "A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path or URL of the document image on which the question will be asked. This image will be processed by the vision encoder to extract features for answering the question.",
                    "required": true,
                    "example": "path/to/document.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The question you want to ask about the content of the provided document image. The model will generate an answer based on the image content.",
                    "required": true,
                    "example": "What is the total amount on the invoice?"
                }
            },
            "response": "The response from this tool would provide an answer to the question posed, based on the content of the provided image. \n\nThe response structure would typically contain the following components:\n\n1. **Answer**: The textual answer to the question based on the content extracted from the image.\n   - **Type**: String\n   - **Description**: This is the direct answer to the user's question, which the model generates based on analyzing the image.\n\n2. **Confidence (Optional)**: A measure of how confident the model is in its answer, typically a numerical value or a qualitative assessment.\n   - **Type**: Float or String (e.g., 'high', 'medium', 'low')\n   - **Description**: Indicates the reliability of the generated answer. It may be provided by some implementations of the pipeline, but is not always guaranteed.\n\n3. **Context or Additional Information (Optional)**: Some models may also include relevant context or sources used to generate the answer, such as the text extracted from the image.\n   - **Type**: String or List of Strings\n   - **Description**: Provides additional details or references from the document that informed the answer, though this is not always part of the response.\n\nThis structure provides the user with a concise, direct answer, potentially supplemented by information on how confident the model is in its answer and context that might explain how the answer was derived from the image.",
            "examples": [
                {
                    "user_intent": "I have a scanned invoice and I want to know the total amount due.",
                    "tool_call": {
                        "name": "naver-clova-ix/donut-base-finetuned-docvqa",
                        "arguments": {
                            "image": "/home/user/documents/invoice_july.png",
                            "question": "What is the total amount due?"
                        }
                    }
                },
                {
                    "user_intent": "Check this receipt and tell me the date of purchase.",
                    "tool_call": {
                        "name": "naver-clova-ix/donut-base-finetuned-docvqa",
                        "arguments": {
                            "image": "/home/user/receipts/receipt_2025-12-01.jpg",
                            "question": "What is the date of purchase?"
                        }
                    }
                },
                {
                    "user_intent": "I scanned a contract and need to extract the name of the signing party.",
                    "tool_call": {
                        "name": "naver-clova-ix/donut-base-finetuned-docvqa",
                        "arguments": {
                            "image": "/home/user/contracts/contract_alpha.png",
                            "question": "Who is the signing party on this contract?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 1: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 2: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 3: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 4: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 5: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 6: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 7: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 8: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 9: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 10: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 11: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 12: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 13: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 14: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.\nTool 15: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 16: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.\nTool 17: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 18: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 19: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path or URL of the document image on which the question will be asked. This image will be processed by the vision encoder to extract features for answering the question.\", \"required\": true, \"example\": \"path/to/document.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The question you want to ask about the content of the provided document image. The model will generate an answer based on the image content.\", \"required\": true, \"example\": \"What is the total amount on the invoice?\"}}, \"response\": \"The response from this tool would provide an answer to the question posed, based on the content of the provided image. \\n\\nThe response structure would typically contain the following components:\\n\\n1. **Answer**: The textual answer to the question based on the content extracted from the image.\\n   - **Type**: String\\n   - **Description**: This is the direct answer to the user's question, which the model generates based on analyzing the image.\\n\\n2. **Confidence (Optional)**: A measure of how confident the model is in its answer, typically a numerical value or a qualitative assessment.\\n   - **Type**: Float or String (e.g., 'high', 'medium', 'low')\\n   - **Description**: Indicates the reliability of the generated answer. It may be provided by some implementations of the pipeline, but is not always guaranteed.\\n\\n3. **Context or Additional Information (Optional)**: Some models may also include relevant context or sources used to generate the answer, such as the text extracted from the image.\\n   - **Type**: String or List of Strings\\n   - **Description**: Provides additional details or references from the document that informed the answer, though this is not always part of the response.\\n\\nThis structure provides the user with a concise, direct answer, potentially supplemented by information on how confident the model is in its answer and context that might explain how the answer was derived from the image.\", \"examples\": [{\"user_intent\": \"I have a scanned invoice and I want to know the total amount due.\", \"tool_call\": {\"name\": \"naver-clova-ix/donut-base-finetuned-docvqa\", \"arguments\": {\"image\": \"/home/user/documents/invoice_july.png\", \"question\": \"What is the total amount due?\"}}}, {\"user_intent\": \"Check this receipt and tell me the date of purchase.\", \"tool_call\": {\"name\": \"naver-clova-ix/donut-base-finetuned-docvqa\", \"arguments\": {\"image\": \"/home/user/receipts/receipt_2025-12-01.jpg\", \"question\": \"What is the date of purchase?\"}}}, {\"user_intent\": \"I scanned a contract and need to extract the name of the signing party.\", \"tool_call\": {\"name\": \"naver-clova-ix/donut-base-finetuned-docvqa\", \"arguments\": {\"image\": \"/home/user/contracts/contract_alpha.png\", \"question\": \"Who is the signing party on this contract?\"}}}]}\n\n# User Query\nTake a look at this document image and tell me the answer to my question: \"What is the total amount due?\".\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract information from a document image by asking a specific question.",
                    "tool_description": "Process a document image with a question to retrieve an answer based on the visual content of the image.",
                    "expected_response": "A textual answer to the question based on the content of the provided image."
                }
            ],
            "extracted_arguments": {
                "image": "The file path or URL of the document image to be processed.",
                "question": "The question to be asked about the content of the provided document image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_172",
        "query": "I need a method to compare the similarity between two sentences to be used within a meme generator, so we can produce a meme with a similar caption.",
        "instruction": "Given a `sentence similarity` task, retrieve tools that evaluate and compare the semantic similarity between two sentences by processing textual inputs and generating embeddings or similarity scores to assist in creating related content, such as memes.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "snli, multi_nli, ms_marco",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_707",
                    "name": "sentence-transformers/paraphrase-MiniLM-L3-v2"
                },
                "id": "gorilla_huggingface_tool_702",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.",
            "Tool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 2: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 3: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 4: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 5: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.",
            "Tool 6: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 7: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.",
            "Tool 8: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 9: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 10: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 11: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 12: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 13: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.",
            "Tool 14: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.",
            "Tool 15: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.",
            "Tool 16: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 17: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 18: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 19: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning."
        ],
        "tool_view": {
            "description": "A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.",
            "parameters": {
                "images": {
                    "type": "list",
                    "description": "A list of images that will be processed by the Vision Transformer (ViT) model for feature extraction. The images can be provided in various formats (e.g., PIL Image objects or URLs pointing to image files).",
                    "required": true,
                    "example": [
                        "<PIL Image Object>"
                    ]
                },
                "return_tensors": {
                    "type": "string",
                    "description": "The type of tensor to return. This parameter determines the format of the output tensors. It is typically set to 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool is designed to perform feature extraction using the Vision Transformer (ViT) model from Hugging Face's Transformers library, specifically utilizing the \"facebook/dino-vits8\" pretrained model.\n\n### Expected Response Structure:\n1. **Extracted Features (last_hidden_state)**:  \n   The response contains the output of the feature extraction process, typically in the form of a tensor representing the last hidden state from the model. This tensor encodes features of the input image as learned by the Vision Transformer model.  \n   - **Shape/Dimensions**: The tensor's shape will depend on the model and the input image. It generally represents the image in a high-dimensional feature space.\n\n2. **Model Outputs**:  \n   The output will include other relevant model information, such as intermediate representations, logits, or any other details depending on the configuration. However, in this case, the focus is primarily on the **last_hidden_state**, which is a key feature for most downstream tasks.\n\n3. **Metadata (Optional)**:  \n   Additional metadata about the processing or the specific configuration used, such as the version of the model or other runtime details, may be included. However, this is not explicitly defined in the tool documentation and may not be present in the core output.\n\n4. **Error/Status Information (if any)**:  \n   In case of issues with the input data (e.g., invalid image format, tensor shape mismatches), the response might also include error messages or status codes, although error handling is not explicitly defined in the tool documentation.\n\n### Overall, the tool’s response primarily contains the extracted features of the input image, focusing on the **last_hidden_state** as the core output.",
            "examples": [
                {
                    "user_intent": "I want to extract features from an image using the Facebook DINO ViT model.",
                    "tool_call": {
                        "name": "facebook/dino-vits8",
                        "arguments": {
                            "images": "http://images.cocodataset.org/val2017/000000039769.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Use the ViT model to extract features from a local image file.",
                    "tool_call": {
                        "name": "facebook/dino-vits8",
                        "arguments": {
                            "images": "path/to/local/image.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Extract features from a batch of images for model processing.",
                    "tool_call": {
                        "name": "facebook/dino-vits8",
                        "arguments": {
                            "images": [
                                "image1.jpg",
                                "image2.jpg",
                                "image3.jpg"
                            ],
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.\nTool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 2: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 3: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 4: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 5: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.\nTool 6: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 7: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.\nTool 8: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 9: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 10: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 11: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 12: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 13: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.\nTool 14: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.\nTool 15: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.\nTool 16: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 17: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 18: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 19: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.\", \"parameters\": {\"images\": {\"type\": \"list\", \"description\": \"A list of images that will be processed by the Vision Transformer (ViT) model for feature extraction. The images can be provided in various formats (e.g., PIL Image objects or URLs pointing to image files).\", \"required\": true, \"example\": [\"<PIL Image Object>\"]}, \"return_tensors\": {\"type\": \"string\", \"description\": \"The type of tensor to return. This parameter determines the format of the output tensors. It is typically set to 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool is designed to perform feature extraction using the Vision Transformer (ViT) model from Hugging Face's Transformers library, specifically utilizing the \\\"facebook/dino-vits8\\\" pretrained model.\\n\\n### Expected Response Structure:\\n1. **Extracted Features (last_hidden_state)**:  \\n   The response contains the output of the feature extraction process, typically in the form of a tensor representing the last hidden state from the model. This tensor encodes features of the input image as learned by the Vision Transformer model.  \\n   - **Shape/Dimensions**: The tensor's shape will depend on the model and the input image. It generally represents the image in a high-dimensional feature space.\\n\\n2. **Model Outputs**:  \\n   The output will include other relevant model information, such as intermediate representations, logits, or any other details depending on the configuration. However, in this case, the focus is primarily on the **last_hidden_state**, which is a key feature for most downstream tasks.\\n\\n3. **Metadata (Optional)**:  \\n   Additional metadata about the processing or the specific configuration used, such as the version of the model or other runtime details, may be included. However, this is not explicitly defined in the tool documentation and may not be present in the core output.\\n\\n4. **Error/Status Information (if any)**:  \\n   In case of issues with the input data (e.g., invalid image format, tensor shape mismatches), the response might also include error messages or status codes, although error handling is not explicitly defined in the tool documentation.\\n\\n### Overall, the tool\\u2019s response primarily contains the extracted features of the input image, focusing on the **last_hidden_state** as the core output.\", \"examples\": [{\"user_intent\": \"I want to extract features from an image using the Facebook DINO ViT model.\", \"tool_call\": {\"name\": \"facebook/dino-vits8\", \"arguments\": {\"images\": \"http://images.cocodataset.org/val2017/000000039769.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Use the ViT model to extract features from a local image file.\", \"tool_call\": {\"name\": \"facebook/dino-vits8\", \"arguments\": {\"images\": \"path/to/local/image.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Extract features from a batch of images for model processing.\", \"tool_call\": {\"name\": \"facebook/dino-vits8\", \"arguments\": {\"images\": [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"], \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nI need a method to compare the similarity between two sentences to be used within a meme generator, so we can produce a meme with a similar caption.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to compare the similarity between two sentences to find the most similar ones.",
                    "tool_description": "Use a sentence-transformers model to compare the similarity between two sentences by encoding them into vector embeddings and calculating their semantic similarity.",
                    "expected_response": "A similarity score between the two sentences indicating how closely related they are in meaning."
                }
            ],
            "extracted_arguments": {
                "sentence_1": "The first sentence to be compared for similarity.",
                "sentence_2": "The second sentence to be compared for similarity."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_225",
        "query": "We have a robotic arm in our warehouse that needs to be trained to optimize loading and unloading tasks. The robotic arm is based on the CartPole environment.",
        "instruction": "Given a `robotic arm training` task, retrieve tools that utilize reinforcement learning frameworks, specifically targeting the optimization of tasks in a CartPole environment, by leveraging pre-trained models and appropriate algorithms to enhance task efficiency and performance.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "deep-reinforcement-learning",
                    "api_call": "load_from_hub(repo_id='HumanCompatibleAI/ppo-seals-CartPole-v0')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/",
                    "performance": {
                        "dataset": "seals/CartPole-v0",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_891",
                    "name": "ppo-seals-CartPole-v0"
                },
                "id": "gorilla_huggingface_tool_884",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.",
            "Tool 1: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.",
            "Tool 2: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.",
            "Tool 3: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.",
            "Tool 4: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.",
            "Tool 5: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.",
            "Tool 6: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "Tool 7: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 8: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 9: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.",
            "Tool 10: A tabular classification model built with Scikit-learn and saved using Joblib, designed for loading and performing classification tasks on-demand with customizable hyperparameters.",
            "Tool 11: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.",
            "Tool 12: This tool uses a YOLOv8 model for pothole segmentation in images, achieving high accuracy in detecting potholes. It leverages the Hugging Face Transformers framework and can be used with specific parameters such as confidence threshold, Intersection over Union (IoU) values, and non-maximum suppression settings to optimize performance. The model is pre-trained on the pothole-segmentation dataset, making it particularly suitable for applications in road maintenance and infrastructure monitoring.",
            "Tool 13: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 14: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.",
            "Tool 15: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 16: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 17: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 18: This tool provides a Decision Transformer model trained on expert trajectories from the Gym Hopper environment, enabling reinforcement learning tasks using Hugging Face Transformers. It is designed to leverage pretrained models for decision-making and action prediction in simulated control environments.",
            "Tool 19: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data."
        ],
        "tool_view": {
            "description": "Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or identifier of the reinforcement learning model to use for robotics tasks. This specifies which pretrained model the pipeline will load for inference.",
                    "required": true,
                    "example": "Antheia/Hanna"
                }
            },
            "response": "The tool provides an inference API for a reinforcement learning robotics model named Antheia/Hanna. The response from this tool would typically consist of the output produced by the model when given an input representing a robotics task or environment state. While the exact response structure is not explicitly defined in the documentation, a reasonable expectation is that the response would include the following components:  \n\n1. **Action or Control Output**:  \n   - The primary content of the response would be the action(s) or control signals the model predicts for the robot. This could be a discrete action choice or continuous control values depending on the robotics task.  \n\n2. **Confidence or Probability Scores (optional)**:  \n   - If the model provides probabilistic outputs, the response may include confidence levels or probability distributions over possible actions.  \n\n3. **Metadata (optional)**:  \n   - Additional information about the inference, such as timestamps, model version, or information about the input state, could be included to help interpret the results.  \n\n4. **Structured Format**:  \n   - The response would likely be structured as a dictionary or object, with keys identifying the action(s), optional probabilities, and any metadata.  \n\nIn essence, the response represents the model’s decision for a given robotics state, formatted in a way that can be directly used to control or evaluate the robot’s behavior in simulation or real-world environments.",
            "examples": [
                {
                    "user_intent": "I want to simulate a robotic arm picking up objects using a pre-trained RL model.",
                    "tool_call": {
                        "name": "Antheia/Hanna",
                        "arguments": {
                            "model": "Antheia/Hanna"
                        }
                    }
                },
                {
                    "user_intent": "Run a reinforcement learning inference to test a quadruped robot navigating obstacles.",
                    "tool_call": {
                        "name": "Antheia/Hanna",
                        "arguments": {
                            "model": "Antheia/Hanna"
                        }
                    }
                },
                {
                    "user_intent": "Use a robotics RL model to evaluate a drone performing complex aerial maneuvers.",
                    "tool_call": {
                        "name": "Antheia/Hanna",
                        "arguments": {
                            "model": "Antheia/Hanna"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\nTool 1: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.\nTool 2: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.\nTool 3: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.\nTool 4: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.\nTool 5: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\nTool 6: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\nTool 7: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 8: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 9: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.\nTool 10: A tabular classification model built with Scikit-learn and saved using Joblib, designed for loading and performing classification tasks on-demand with customizable hyperparameters.\nTool 11: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.\nTool 12: This tool uses a YOLOv8 model for pothole segmentation in images, achieving high accuracy in detecting potholes. It leverages the Hugging Face Transformers framework and can be used with specific parameters such as confidence threshold, Intersection over Union (IoU) values, and non-maximum suppression settings to optimize performance. The model is pre-trained on the pothole-segmentation dataset, making it particularly suitable for applications in road maintenance and infrastructure monitoring.\nTool 13: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 14: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.\nTool 15: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 16: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 17: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 18: This tool provides a Decision Transformer model trained on expert trajectories from the Gym Hopper environment, enabling reinforcement learning tasks using Hugging Face Transformers. It is designed to leverage pretrained models for decision-making and action prediction in simulated control environments.\nTool 19: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or identifier of the reinforcement learning model to use for robotics tasks. This specifies which pretrained model the pipeline will load for inference.\", \"required\": true, \"example\": \"Antheia/Hanna\"}}, \"response\": \"The tool provides an inference API for a reinforcement learning robotics model named Antheia/Hanna. The response from this tool would typically consist of the output produced by the model when given an input representing a robotics task or environment state. While the exact response structure is not explicitly defined in the documentation, a reasonable expectation is that the response would include the following components:  \\n\\n1. **Action or Control Output**:  \\n   - The primary content of the response would be the action(s) or control signals the model predicts for the robot. This could be a discrete action choice or continuous control values depending on the robotics task.  \\n\\n2. **Confidence or Probability Scores (optional)**:  \\n   - If the model provides probabilistic outputs, the response may include confidence levels or probability distributions over possible actions.  \\n\\n3. **Metadata (optional)**:  \\n   - Additional information about the inference, such as timestamps, model version, or information about the input state, could be included to help interpret the results.  \\n\\n4. **Structured Format**:  \\n   - The response would likely be structured as a dictionary or object, with keys identifying the action(s), optional probabilities, and any metadata.  \\n\\nIn essence, the response represents the model\\u2019s decision for a given robotics state, formatted in a way that can be directly used to control or evaluate the robot\\u2019s behavior in simulation or real-world environments.\", \"examples\": [{\"user_intent\": \"I want to simulate a robotic arm picking up objects using a pre-trained RL model.\", \"tool_call\": {\"name\": \"Antheia/Hanna\", \"arguments\": {\"model\": \"Antheia/Hanna\"}}}, {\"user_intent\": \"Run a reinforcement learning inference to test a quadruped robot navigating obstacles.\", \"tool_call\": {\"name\": \"Antheia/Hanna\", \"arguments\": {\"model\": \"Antheia/Hanna\"}}}, {\"user_intent\": \"Use a robotics RL model to evaluate a drone performing complex aerial maneuvers.\", \"tool_call\": {\"name\": \"Antheia/Hanna\", \"arguments\": {\"model\": \"Antheia/Hanna\"}}}]}\n\n# User Query\nWe have a robotic arm in our warehouse that needs to be trained to optimize loading and unloading tasks. The robotic arm is based on the CartPole environment.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained reinforcement learning agent for the CartPole environment to control a robotic arm.",
                    "tool_description": "Provide a pre-trained PPO agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling immediate deployment and testing of reinforcement learning agents.",
                    "expected_response": "A loaded RL agent object ready for inference in the CartPole environment, including access to action selection methods."
                },
                {
                    "user_intent": "Simulate and optimize the robotic arm's loading and unloading tasks using the loaded CartPole RL agent.",
                    "tool_description": "Run inference with the pre-trained CartPole RL agent to evaluate and optimize robotic arm actions for loading and unloading tasks, providing predicted control signals or actions based on the environment state.",
                    "expected_response": "A sequence of recommended actions or control signals for the robotic arm, optionally including confidence scores or metadata about each action."
                }
            ],
            "extracted_arguments": {
                "environment": "The reinforcement learning environment representing the robotic arm's dynamics and tasks, in this case based on CartPole-v1.",
                "task_description": "The high-level objective for the robotic arm, specifying the type of operations to optimize, such as loading and unloading."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_166",
        "query": "Can you teach me to build a program that will analyze video and describe what's happening in natural language?",
        "instruction": "Given a `video analysis and description` task, retrieve tools capable of processing video inputs to analyze and generate descriptive natural language text that explains the content and actions occurring in the video, aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "XClipModel.from_pretrained('microsoft/xclip-base-patch32')",
                    "api_arguments": "N/A",
                    "python_environment_requirements": "transformers",
                    "example_code": "For code examples, we refer to the documentation.",
                    "performance": {
                        "dataset": "Kinetics 400",
                        "accuracy": {
                            "top-1": 80.4,
                            "top-5": 95.0
                        }
                    },
                    "description": "X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.",
                    "id": "huggingface_api_312",
                    "name": "microsoft/xclip-base-patch32"
                },
                "id": "gorilla_huggingface_tool_308",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 1: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 2: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 3: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 4: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 6: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 7: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 8: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 9: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 10: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 11: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.",
            "Tool 12: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 13: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 14: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 15: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 16: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 17: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.",
            "Tool 18: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 19: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language."
        ],
        "tool_view": {
            "description": "The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "parameters": {
                "images": {
                    "type": "array",
                    "description": "A list or sequence of video frames represented as arrays, where each frame is typically a 3D array (channels, height, width). This input is used by the model for video classification.",
                    "required": true,
                    "example": [
                        [
                            [
                                [
                                    0.1,
                                    0.2
                                ],
                                [
                                    0.3,
                                    0.4
                                ]
                            ],
                            [
                                [
                                    0.5,
                                    0.6
                                ],
                                [
                                    0.7,
                                    0.8
                                ]
                            ],
                            [
                                [
                                    0.9,
                                    1.0
                                ],
                                [
                                    1.1,
                                    1.2
                                ]
                            ]
                        ]
                    ]
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors the processor should return. Common values include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool’s response is an object representing the output of a video classification model. Its structure and components are as follows:\n\n1. **`logits`**  \n   - This is a tensor containing the raw, unnormalized scores produced by the model for each class in the classification task.  \n   - The shape typically corresponds to `[batch_size, num_classes]`, where `batch_size` is the number of input video samples and `num_classes` is the total number of target classes the model can predict.  \n   - Higher values indicate stronger model confidence for the corresponding class, but they are not probabilities.\n\n2. **`predicted_class_idx`** (derived from `logits`)  \n   - This is the integer index of the class with the highest score in `logits` for a given input video.  \n   - It represents the model’s most likely prediction for that video.\n\n3. **`predicted_class_label`** (derived using `model.config.id2label`)  \n   - This is a human-readable string corresponding to `predicted_class_idx`.  \n   - It maps the integer class index to the semantic label defined in the model’s configuration (e.g., “picking something up” or “throwing an object”).\n\n4. **Optional additional fields**  \n   - Depending on the framework version and configuration, the response object may also include intermediate outputs such as attention weights or hidden states, but these are typically optional and not returned by default.  \n\nIn summary, the primary semantic content of the response is the model’s **raw class scores (`logits`)** and the **predicted class** for the input video, with a mapping to a human-readable label for interpretation.",
            "examples": [
                {
                    "user_intent": "Classify a video using the TimeSformer model pre-trained on the Something Something v2 dataset.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I want to run video classification on a sample video input with the TimeSformer model and get the predicted class.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Process a set of random images as a video input and classify it using TimeSformer.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 1: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 2: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 3: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 4: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 6: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 7: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 8: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 9: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 10: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 11: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.\nTool 12: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 13: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 14: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 15: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 16: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 17: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\nTool 18: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 19: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \\\"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\\\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\", \"parameters\": {\"images\": {\"type\": \"array\", \"description\": \"A list or sequence of video frames represented as arrays, where each frame is typically a 3D array (channels, height, width). This input is used by the model for video classification.\", \"required\": true, \"example\": [[[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[0.9, 1.0], [1.1, 1.2]]]]}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors the processor should return. Common values include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool\\u2019s response is an object representing the output of a video classification model. Its structure and components are as follows:\\n\\n1. **`logits`**  \\n   - This is a tensor containing the raw, unnormalized scores produced by the model for each class in the classification task.  \\n   - The shape typically corresponds to `[batch_size, num_classes]`, where `batch_size` is the number of input video samples and `num_classes` is the total number of target classes the model can predict.  \\n   - Higher values indicate stronger model confidence for the corresponding class, but they are not probabilities.\\n\\n2. **`predicted_class_idx`** (derived from `logits`)  \\n   - This is the integer index of the class with the highest score in `logits` for a given input video.  \\n   - It represents the model\\u2019s most likely prediction for that video.\\n\\n3. **`predicted_class_label`** (derived using `model.config.id2label`)  \\n   - This is a human-readable string corresponding to `predicted_class_idx`.  \\n   - It maps the integer class index to the semantic label defined in the model\\u2019s configuration (e.g., \\u201cpicking something up\\u201d or \\u201cthrowing an object\\u201d).\\n\\n4. **Optional additional fields**  \\n   - Depending on the framework version and configuration, the response object may also include intermediate outputs such as attention weights or hidden states, but these are typically optional and not returned by default.  \\n\\nIn summary, the primary semantic content of the response is the model\\u2019s **raw class scores (`logits`)** and the **predicted class** for the input video, with a mapping to a human-readable label for interpretation.\", \"examples\": [{\"user_intent\": \"Classify a video using the TimeSformer model pre-trained on the Something Something v2 dataset.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I want to run video classification on a sample video input with the TimeSformer model and get the predicted class.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Process a set of random images as a video input and classify it using TimeSformer.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nCan you teach me to build a program that will analyze video and describe what's happening in natural language?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify the actions occurring in a video using a pre-trained video classification model.",
                    "tool_description": "Use a TimeSformer or VideoMAE model to process video frames and predict the most likely action classes, leveraging space-time attention or masked autoencoder mechanisms.",
                    "expected_response": "An object containing raw class scores (`logits`), the predicted class index, and a human-readable predicted class label describing the action in the video."
                },
                {
                    "user_intent": "Convert the predicted video actions into a natural language description.",
                    "tool_description": "Take the predicted class labels from the video classification model and generate coherent natural language sentences that describe the observed actions using a language generation model.",
                    "expected_response": "A textual description summarizing the video content in natural language, capturing the identified actions and events."
                }
            ],
            "extracted_arguments": {
                "video_frames": "A sequence of frames extracted from the target video, represented as arrays suitable for model input.",
                "tensor_format": "The desired tensor type for processing, e.g., PyTorch or TensorFlow tensors."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_344",
        "query": "We want to code a loading spinner to display when our system is undergoing maintenance. Can you generate the code for us?",
        "instruction": "Given a `program synthesis` task, retrieve tools that generate executable code by interpreting natural language prompts to provide code solutions that adhere to the query’s specifications and requirements, particularly for tasks like creating a loading spinner.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Program Synthesis",
                    "api_call": "AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\ntext = def hello_world():\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))",
                    "performance": {
                        "dataset": "HumanEval and MTPB",
                        "accuracy": "Refer to the paper for accuracy details"
                    },
                    "description": "CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.",
                    "id": "huggingface_api_619",
                    "name": "Salesforce/codegen-350M-multi"
                },
                "id": "gorilla_huggingface_tool_614",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 1: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 2: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 3: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 4: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.",
            "Tool 5: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 6: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.",
            "Tool 7: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 8: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.",
            "Tool 9: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.",
            "Tool 10: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 11: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 12: A BERT-based model pre-trained on 152 million sentences from StackOverflow, designed for token-level classification tasks such as code and named entity recognition within StackOverflow posts.",
            "Tool 13: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 14: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 15: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 16: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 17: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.",
            "Tool 18: This tool uses a YOLOv8 model for pothole segmentation in images, achieving high accuracy in detecting potholes. It leverages the Hugging Face Transformers framework and can be used with specific parameters such as confidence threshold, Intersection over Union (IoU) values, and non-maximum suppression settings to optimize performance. The model is pre-trained on the pothole-segmentation dataset, making it particularly suitable for applications in road maintenance and infrastructure monitoring.",
            "Tool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets."
        ],
        "tool_view": {
            "description": "This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "parameters": {
                "tokenized_code": {
                    "type": "string",
                    "description": "The Python code input that has been tokenized and preprocessed for the model. This code will be used by the model to generate natural-language documentation or descriptions.",
                    "required": true,
                    "example": "def e(message, exit_code=None): print_log(message, YELLOW, BOLD) if exit_code is not None: sys.exit(exit_code)"
                }
            },
            "response": "The tool produces a textual response that consists of a natural language description or documentation of the input Python code. The response structure can be understood as follows:\n\n- **Type**: Text string.  \n- **Content**: A concise and coherent summary of the functionality and purpose of the given Python code or function. This typically includes:\n  - The role of the function or code snippet.\n  - The key operations it performs.\n  - Any inputs, outputs, or side effects relevant to understanding the code.  \n- **Granularity**: The description is generally at the function or snippet level, explaining behavior rather than implementation details line by line.  \n- **Format**: Plain text intended to be human-readable, suitable for use as code documentation, docstrings, or explanatory comments.  \n\nThe response does not return structured metadata, code annotations, or token-level outputs; it is purely a generated textual summary derived from the tokenized Python code provided as input.",
            "examples": [
                {
                    "user_intent": "Generate a detailed docstring for this Python function that processes user input and handles errors.",
                    "tool_call": {
                        "name": "code_trans_t5_base_code_documentation_generation_python",
                        "arguments": {
                            "tokenized_code": "def process_input(user_input, max_length=50):\n    if len(user_input) > max_length:\n        raise ValueError('Input too long')\n    return user_input.strip().lower()"
                        }
                    }
                },
                {
                    "user_intent": "I want to automatically create documentation for a function that calculates factorials.",
                    "tool_call": {
                        "name": "code_trans_t5_base_code_documentation_generation_python",
                        "arguments": {
                            "tokenized_code": "def factorial(n):\n    if n == 0:\n        return 1\n    return n * factorial(n-1)"
                        }
                    }
                },
                {
                    "user_intent": "Create a descriptive comment for this code snippet that logs system messages with different levels.",
                    "tool_call": {
                        "name": "code_trans_t5_base_code_documentation_generation_python",
                        "arguments": {
                            "tokenized_code": "def log_message(message, level='INFO'):\n    print(f'[{level}] {message}')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 1: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 2: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 3: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 4: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.\nTool 5: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 6: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.\nTool 7: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 8: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.\nTool 9: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\nTool 10: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 11: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 12: A BERT-based model pre-trained on 152 million sentences from StackOverflow, designed for token-level classification tasks such as code and named entity recognition within StackOverflow posts.\nTool 13: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 14: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 15: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 16: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 17: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.\nTool 18: This tool uses a YOLOv8 model for pothole segmentation in images, achieving high accuracy in detecting potholes. It leverages the Hugging Face Transformers framework and can be used with specific parameters such as confidence threshold, Intersection over Union (IoU) values, and non-maximum suppression settings to optimize performance. The model is pre-trained on the pothole-segmentation dataset, making it particularly suitable for applications in road maintenance and infrastructure monitoring.\nTool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\", \"parameters\": {\"tokenized_code\": {\"type\": \"string\", \"description\": \"The Python code input that has been tokenized and preprocessed for the model. This code will be used by the model to generate natural-language documentation or descriptions.\", \"required\": true, \"example\": \"def e(message, exit_code=None): print_log(message, YELLOW, BOLD) if exit_code is not None: sys.exit(exit_code)\"}}, \"response\": \"The tool produces a textual response that consists of a natural language description or documentation of the input Python code. The response structure can be understood as follows:\\n\\n- **Type**: Text string.  \\n- **Content**: A concise and coherent summary of the functionality and purpose of the given Python code or function. This typically includes:\\n  - The role of the function or code snippet.\\n  - The key operations it performs.\\n  - Any inputs, outputs, or side effects relevant to understanding the code.  \\n- **Granularity**: The description is generally at the function or snippet level, explaining behavior rather than implementation details line by line.  \\n- **Format**: Plain text intended to be human-readable, suitable for use as code documentation, docstrings, or explanatory comments.  \\n\\nThe response does not return structured metadata, code annotations, or token-level outputs; it is purely a generated textual summary derived from the tokenized Python code provided as input.\", \"examples\": [{\"user_intent\": \"Generate a detailed docstring for this Python function that processes user input and handles errors.\", \"tool_call\": {\"name\": \"code_trans_t5_base_code_documentation_generation_python\", \"arguments\": {\"tokenized_code\": \"def process_input(user_input, max_length=50):\\n    if len(user_input) > max_length:\\n        raise ValueError('Input too long')\\n    return user_input.strip().lower()\"}}}, {\"user_intent\": \"I want to automatically create documentation for a function that calculates factorials.\", \"tool_call\": {\"name\": \"code_trans_t5_base_code_documentation_generation_python\", \"arguments\": {\"tokenized_code\": \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    return n * factorial(n-1)\"}}}, {\"user_intent\": \"Create a descriptive comment for this code snippet that logs system messages with different levels.\", \"tool_call\": {\"name\": \"code_trans_t5_base_code_documentation_generation_python\", \"arguments\": {\"tokenized_code\": \"def log_message(message, level='INFO'):\\n    print(f'[{level}] {message}')\"}}}]}\n\n# User Query\nWe want to code a loading spinner to display when our system is undergoing maintenance. Can you generate the code for us?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate Python code for a loading spinner to display during system maintenance.",
                    "tool_description": "Use a code generation model to produce Python code that implements a visual loading spinner for console or GUI display, suitable for indicating system maintenance or ongoing processes.",
                    "expected_response": "A Python code snippet that defines a loading spinner function or class, including necessary loops, timing, and display logic."
                }
            ],
            "extracted_arguments": {
                "spinner_type": "The style or type of loading spinner to generate (e.g., console-based, GUI-based, or ASCII animation).",
                "display_context": "The environment where the spinner will appear, such as a terminal, web interface, or desktop application."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_155",
        "query": "We want to classify images supplied by our users into categories such as cats, dogs, birds, and more.",
        "instruction": "Given an `image classification` task, retrieve tools that are capable of categorizing images into predefined classes, such as animals, by processing visual inputs and leveraging zero-shot learning techniques to generalize classifications without needing task-specific training data.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('openai/clip-vit-base-patch16')",
                    "api_arguments": [
                        "text",
                        "images",
                        "return_tensors",
                        "padding"
                    ],
                    "python_environment_requirements": [
                        "PIL",
                        "requests",
                        "transformers"
                    ],
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(openai/clip-vit-base-patch16)\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": [
                            "Food101",
                            "CIFAR10",
                            "CIFAR100",
                            "Birdsnap",
                            "SUN397",
                            "Stanford Cars",
                            "FGVC Aircraft",
                            "VOC2007",
                            "DTD",
                            "Oxford-IIIT Pet dataset",
                            "Caltech101",
                            "Flowers102",
                            "MNIST",
                            "SVHN",
                            "IIIT5K",
                            "Hateful Memes",
                            "SST-2",
                            "UCF101",
                            "Kinetics700",
                            "Country211",
                            "CLEVR Counting",
                            "KITTI Distance",
                            "STL-10",
                            "RareAct",
                            "Flickr30",
                            "MSCOCO",
                            "ImageNet",
                            "ImageNet-A",
                            "ImageNet-R",
                            "ImageNet Sketch",
                            "ObjectNet (ImageNet Overlap)",
                            "Youtube-BB",
                            "ImageNet-Vid"
                        ],
                        "accuracy": "varies depending on the dataset"
                    },
                    "description": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.",
                    "id": "huggingface_api_351",
                    "name": "openai/clip-vit-base-patch16"
                },
                "id": "gorilla_huggingface_tool_347",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 1: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 2: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 3: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 4: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.",
            "Tool 5: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 6: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.",
            "Tool 7: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 8: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 9: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.",
            "Tool 10: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.",
            "Tool 11: This tool uses the SqueezeBERT transformer model, fine-tuned on the MultiNLI dataset, to perform zero-shot text classification. It allows users to classify input text into one or more candidate labels without prior task-specific training, enabling flexible and efficient natural language understanding, including on edge devices.",
            "Tool 12: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 13: A CLIP ViT-bigG/14 model designed for zero-shot image classification, allowing users to classify images into arbitrary categories without task-specific training. Built with the LAION-2B dataset, it is useful for research and exploratory studies in computer vision, enabling analysis of model behavior and performance across diverse visual concepts. The model achieves 80.1% zero-shot top-1 accuracy on ImageNet-1k.",
            "Tool 14: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 15: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 16: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.",
            "Tool 17: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 18: A multilingual zero-shot text classification tool that uses the XLM-RoBERTa large model fine-tuned on natural language inference data across 15 languages. It allows users to classify text into custom categories without prior task-specific training by providing candidate labels and an optional hypothesis template, making it useful for quickly categorizing text in multiple languages.",
            "Tool 19: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations."
        ],
        "tool_view": {
            "description": "A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path or URL of the image to be classified by the model.",
                    "required": true,
                    "example": "images/dog.jpg"
                }
            },
            "response": "The tool returns a structured classification result for a given input image. Each response typically includes the following components:\n\n1. **Label / Class Name**: A string indicating the predicted category of the image (e.g., \"dog\" or \"food\"). This represents the model’s best guess for the image class.\n\n2. **Score / Confidence**: A numerical value, usually between 0 and 1, representing the model's confidence in the predicted label. Higher values indicate greater certainty in the classification.\n\n3. **Optional Additional Metadata**: Depending on the Hugging Face pipeline implementation, the response may include multiple predictions sorted by confidence, allowing inspection of secondary likely classes along with their respective scores.\n\nOverall, the response is a list of one or more objects, each containing a label and its associated confidence score, providing both the classification result and the model's certainty about that result.",
            "examples": [
                {
                    "user_intent": "I have a photo of my pet and I want to check if it's a dog or something else.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/photos/pet1.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Classify this picture to see if it's a meal or a dog.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/images/dinner_plate.png"
                        }
                    }
                },
                {
                    "user_intent": "I want to automatically sort my images folder into dogs and food photos.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/photos/vacation/photo_2025.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 1: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 2: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 3: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 4: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.\nTool 5: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 6: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.\nTool 7: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 8: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 9: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.\nTool 10: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.\nTool 11: This tool uses the SqueezeBERT transformer model, fine-tuned on the MultiNLI dataset, to perform zero-shot text classification. It allows users to classify input text into one or more candidate labels without prior task-specific training, enabling flexible and efficient natural language understanding, including on edge devices.\nTool 12: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 13: A CLIP ViT-bigG/14 model designed for zero-shot image classification, allowing users to classify images into arbitrary categories without task-specific training. Built with the LAION-2B dataset, it is useful for research and exploratory studies in computer vision, enabling analysis of model behavior and performance across diverse visual concepts. The model achieves 80.1% zero-shot top-1 accuracy on ImageNet-1k.\nTool 14: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 15: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 16: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.\nTool 17: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 18: A multilingual zero-shot text classification tool that uses the XLM-RoBERTa large model fine-tuned on natural language inference data across 15 languages. It allows users to classify text into custom categories without prior task-specific training by providing candidate labels and an optional hypothesis template, making it useful for quickly categorizing text in multiple languages.\nTool 19: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path or URL of the image to be classified by the model.\", \"required\": true, \"example\": \"images/dog.jpg\"}}, \"response\": \"The tool returns a structured classification result for a given input image. Each response typically includes the following components:\\n\\n1. **Label / Class Name**: A string indicating the predicted category of the image (e.g., \\\"dog\\\" or \\\"food\\\"). This represents the model\\u2019s best guess for the image class.\\n\\n2. **Score / Confidence**: A numerical value, usually between 0 and 1, representing the model's confidence in the predicted label. Higher values indicate greater certainty in the classification.\\n\\n3. **Optional Additional Metadata**: Depending on the Hugging Face pipeline implementation, the response may include multiple predictions sorted by confidence, allowing inspection of secondary likely classes along with their respective scores.\\n\\nOverall, the response is a list of one or more objects, each containing a label and its associated confidence score, providing both the classification result and the model's certainty about that result.\", \"examples\": [{\"user_intent\": \"I have a photo of my pet and I want to check if it's a dog or something else.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/photos/pet1.jpg\"}}}, {\"user_intent\": \"Classify this picture to see if it's a meal or a dog.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/images/dinner_plate.png\"}}}, {\"user_intent\": \"I want to automatically sort my images folder into dogs and food photos.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/photos/vacation/photo_2025.jpg\"}}}]}\n\n# User Query\nWe want to classify images supplied by our users into categories such as cats, dogs, birds, and more.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify user-submitted images into multiple animal categories like cats, dogs, birds, and more.",
                    "tool_description": "Perform zero-shot image classification on input images, allowing categorization into arbitrary user-defined categories without prior task-specific training.",
                    "expected_response": "A list of classification results for each image, each containing predicted labels and associated confidence scores for the specified categories."
                }
            ],
            "extracted_arguments": {
                "image_path": "The file path or URL of the image to be classified.",
                "categories": "A list of target category names provided by the user for classification."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_179",
        "query": "I work at an art school and our professor wants to create an AI chatbot that can study an image of a painting and answer questions about it.",
        "instruction": "Given a `visual question answering` task, retrieve tools that leverage multimodal capabilities to analyze images and generate informative textual responses based on the visual content and accompanying questions.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                    "api_arguments": {
                        "img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg",
                        "question": "how many dogs are in the picture?"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": {
                        "import_requests": "import requests",
                        "import_PIL": "from PIL import Image",
                        "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration",
                        "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')",
                        "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')",
                        "generate_output": "out = model.generate(**inputs)",
                        "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"
                    },
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_64",
                    "name": "blip2-opt-2.7b"
                },
                "id": "gorilla_huggingface_tool_64",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 1: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 2: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 3: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 4: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 6: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 8: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 9: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 10: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 11: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 12: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 13: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 14: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 15: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 16: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 17: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 18: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 19: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data."
        ],
        "tool_view": {
            "description": "A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path to the image that contains the document to be processed for question answering.",
                    "required": true,
                    "example": "path/to/image"
                },
                "question": {
                    "type": "string",
                    "description": "The question that you want to ask regarding the content of the image.",
                    "required": true,
                    "example": "What is the name of the company in the document?"
                }
            },
            "response": "The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\n\nThe response would typically include:\n\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\n\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\n\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\n\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.",
            "examples": [
                {
                    "user_intent": "I have a scanned contract and I want to know the effective date mentioned in it.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/contract_scan_01.png",
                            "question": "What is the effective date of this contract?"
                        }
                    }
                },
                {
                    "user_intent": "Check this invoice image and tell me the total amount due.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/invoice_march2025.png",
                            "question": "What is the total amount to be paid?"
                        }
                    }
                },
                {
                    "user_intent": "I need to extract the name of the company from this PDF screenshot.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/company_profile_screenshot.png",
                            "question": "What is the name of the company in this document?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 1: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 2: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 3: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 4: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 6: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 8: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 9: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 10: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 11: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 12: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 13: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 14: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 15: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 16: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 17: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 18: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 19: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path to the image that contains the document to be processed for question answering.\", \"required\": true, \"example\": \"path/to/image\"}, \"question\": {\"type\": \"string\", \"description\": \"The question that you want to ask regarding the content of the image.\", \"required\": true, \"example\": \"What is the name of the company in the document?\"}}, \"response\": \"The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\\n\\nThe response would typically include:\\n\\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\\n\\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\\n\\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\\n\\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.\", \"examples\": [{\"user_intent\": \"I have a scanned contract and I want to know the effective date mentioned in it.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/contract_scan_01.png\", \"question\": \"What is the effective date of this contract?\"}}}, {\"user_intent\": \"Check this invoice image and tell me the total amount due.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/invoice_march2025.png\", \"question\": \"What is the total amount to be paid?\"}}}, {\"user_intent\": \"I need to extract the name of the company from this PDF screenshot.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/company_profile_screenshot.png\", \"question\": \"What is the name of the company in this document?\"}}}]}\n\n# User Query\nI work at an art school and our professor wants to create an AI chatbot that can study an image of a painting and answer questions about it.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze an image of a painting and ask questions about its content.",
                    "tool_description": "A Visual Question Answering (VQA) model that processes images and natural language questions to provide answers about the visual content.",
                    "expected_response": "A textual answer describing or explaining the content of the painting, potentially including contextual details or confidence scores."
                }
            ],
            "extracted_arguments": {
                "image": "The file path or image data representing the painting to be analyzed.",
                "question": "A natural language question about the painting's content or details."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_472",
        "query": "Our hospital needs to analyze digital blood samples in order to detect and count platelets, red blood cells, and white blood cells.",
        "instruction": "Given a `medical image analysis` task, retrieve tools that employ computer vision techniques to analyze digital images for detecting and quantifying specific cellular components such as platelets, red blood cells, and white blood cells.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8m-blood-cell-detection')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic_nms": false,
                        "max_det": 1000
                    },
                    "python_environment_requirements": [
                        "ultralyticsplus==0.0.24",
                        "ultralytics==8.0.23"
                    ],
                    "example_code": [
                        "from ultralyticsplus import YOLO, render_result",
                        "model = YOLO('keremberke/yolov8m-blood-cell-detection')",
                        "model.overrides['conf'] = 0.25",
                        "model.overrides['iou'] = 0.45",
                        "model.overrides['agnostic_nms'] = False",
                        "model.overrides['max_det'] = 1000",
                        "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'",
                        "results = model.predict(image)",
                        "print(results[0].boxes)",
                        "render = render_result(model=model, image=image, result=results[0])",
                        "render.show()"
                    ],
                    "performance": {
                        "dataset": "blood-cell-object-detection",
                        "accuracy": 0.927
                    },
                    "description": "A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.",
                    "id": "huggingface_api_224",
                    "name": "keremberke/yolov8m-blood-cell-detection"
                },
                "id": "gorilla_huggingface_tool_220",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "Tool 1: This tool uses a YOLOv8 model for detecting blood cells, including platelets, red blood cells (RBC), and white blood cells (WBC), trained on the blood-cell-object-detection dataset. It allows users to perform object detection with customizable settings such as confidence threshold, Intersection over Union (IoU), and maximum detection limits. This tool is designed to be used within the Hugging Face Transformers framework and requires specific Python packages like `ultralyticsplus` and `ultralytics`. It provides high accuracy (92.7%) for blood cell detection tasks.",
            "Tool 2: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 5: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 6: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.",
            "Tool 7: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 8: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.",
            "Tool 9: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 10: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "Tool 11: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 12: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 13: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 14: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 15: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 16: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 17: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 18: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 19: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context."
        ],
        "tool_view": {
            "description": "This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting blood cells. Detections with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection Over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping detections.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "If set to true, NMS will be class-agnostic, meaning overlapping detections will be suppressed regardless of class. If false, NMS respects class labels.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Any detections beyond this number will be ignored.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The response from the tool would likely contain the following components, based on the provided documentation:\n\n1. **Detections**: This is the primary output of the tool, listing the detected objects within the input image. The detected objects are expected to be blood cells, specifically platelets, RBCs (Red Blood Cells), and WBCs (White Blood Cells). Each detection would typically contain information such as:\n   - **Object Class**: Identifies the type of blood cell detected (e.g., Platelet, RBC, WBC).\n   - **Bounding Box Coordinates**: The coordinates that describe the location of the detected object within the image, usually in the form of a rectangle (e.g., xmin, ymin, xmax, ymax).\n   - **Confidence Score**: A value indicating the model's certainty in the detection, typically constrained by the `conf` parameter (in this case, 0.25).\n   - **IoU (Intersection over Union) Score**: Used to filter out poor-quality detections, based on the `iou` threshold (0.45).\n   - **Label or Index**: A unique identifier or label associated with each detection.\n\n2. **Metadata**: Information regarding the detection process, which might include:\n   - **Model Version**: The version of the YOLO model used (YOLOv8 in this case).\n   - **Inference Time**: The time taken to process the image and produce results.\n   - **Input Image Information**: Details about the image, such as the image URL or dimensions.\n   - **Non-Maximum Suppression (NMS) Information**: Depending on the `agnostic_nms` flag, this could indicate whether non-maximum suppression was applied during the inference process.\n\n3. **Visual Output (if applicable)**: A rendered image that shows the detected blood cells overlaid on the original image, possibly including:\n   - **Bounding Boxes**: Graphical representation of the detected blood cells, drawn on the input image.\n   - **Confidence Labels**: Numerical labels indicating the confidence score for each detection, potentially displayed near the bounding boxes.\n\n4. **Performance Information (optional)**: Information on the model's performance, including:\n   - **Accuracy**: The accuracy of the model, which is 0.893 as mentioned in the documentation. This would typically be presented as a percentage or ratio, indicating how often the model's predictions match the ground truth.\n\nIn summary, the response would likely be a structured set of detected objects (blood cells) with associated bounding boxes, class labels, confidence scores, and possibly a visual overlay of the detections on the original image, along with metadata and performance details.",
            "examples": [
                {
                    "user_intent": "Detect and count all red blood cells in this microscope image.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500,
                            "image": "https://example.com/microscope_rbc_sample.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a blood smear image to identify WBCs, RBCs, and platelets.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000,
                            "image": "https://example.com/blood_smear_full.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Quickly detect blood cells in a low-resolution image with a higher confidence threshold.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 300,
                            "image": "https://example.com/low_res_blood_sample.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\nTool 1: This tool uses a YOLOv8 model for detecting blood cells, including platelets, red blood cells (RBC), and white blood cells (WBC), trained on the blood-cell-object-detection dataset. It allows users to perform object detection with customizable settings such as confidence threshold, Intersection over Union (IoU), and maximum detection limits. This tool is designed to be used within the Hugging Face Transformers framework and requires specific Python packages like `ultralyticsplus` and `ultralytics`. It provides high accuracy (92.7%) for blood cell detection tasks.\nTool 2: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 5: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 6: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.\nTool 7: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 8: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.\nTool 9: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 10: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\nTool 11: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 12: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 13: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 14: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 15: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 16: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 17: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 18: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 19: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a YOLOv8-based model to detect blood cells\\u2014including platelets, red blood cells, and white blood cells\\u2014in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting blood cells. Detections with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection Over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping detections.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"If set to true, NMS will be class-agnostic, meaning overlapping detections will be suppressed regardless of class. If false, NMS respects class labels.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Any detections beyond this number will be ignored.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The response from the tool would likely contain the following components, based on the provided documentation:\\n\\n1. **Detections**: This is the primary output of the tool, listing the detected objects within the input image. The detected objects are expected to be blood cells, specifically platelets, RBCs (Red Blood Cells), and WBCs (White Blood Cells). Each detection would typically contain information such as:\\n   - **Object Class**: Identifies the type of blood cell detected (e.g., Platelet, RBC, WBC).\\n   - **Bounding Box Coordinates**: The coordinates that describe the location of the detected object within the image, usually in the form of a rectangle (e.g., xmin, ymin, xmax, ymax).\\n   - **Confidence Score**: A value indicating the model's certainty in the detection, typically constrained by the `conf` parameter (in this case, 0.25).\\n   - **IoU (Intersection over Union) Score**: Used to filter out poor-quality detections, based on the `iou` threshold (0.45).\\n   - **Label or Index**: A unique identifier or label associated with each detection.\\n\\n2. **Metadata**: Information regarding the detection process, which might include:\\n   - **Model Version**: The version of the YOLO model used (YOLOv8 in this case).\\n   - **Inference Time**: The time taken to process the image and produce results.\\n   - **Input Image Information**: Details about the image, such as the image URL or dimensions.\\n   - **Non-Maximum Suppression (NMS) Information**: Depending on the `agnostic_nms` flag, this could indicate whether non-maximum suppression was applied during the inference process.\\n\\n3. **Visual Output (if applicable)**: A rendered image that shows the detected blood cells overlaid on the original image, possibly including:\\n   - **Bounding Boxes**: Graphical representation of the detected blood cells, drawn on the input image.\\n   - **Confidence Labels**: Numerical labels indicating the confidence score for each detection, potentially displayed near the bounding boxes.\\n\\n4. **Performance Information (optional)**: Information on the model's performance, including:\\n   - **Accuracy**: The accuracy of the model, which is 0.893 as mentioned in the documentation. This would typically be presented as a percentage or ratio, indicating how often the model's predictions match the ground truth.\\n\\nIn summary, the response would likely be a structured set of detected objects (blood cells) with associated bounding boxes, class labels, confidence scores, and possibly a visual overlay of the detections on the original image, along with metadata and performance details.\", \"examples\": [{\"user_intent\": \"Detect and count all red blood cells in this microscope image.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500, \"image\": \"https://example.com/microscope_rbc_sample.jpg\"}}}, {\"user_intent\": \"Analyze a blood smear image to identify WBCs, RBCs, and platelets.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://example.com/blood_smear_full.jpg\"}}}, {\"user_intent\": \"Quickly detect blood cells in a low-resolution image with a higher confidence threshold.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 300, \"image\": \"https://example.com/low_res_blood_sample.jpg\"}}}]}\n\n# User Query\nOur hospital needs to analyze digital blood samples in order to detect and count platelets, red blood cells, and white blood cells.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze digital blood sample images to detect and count platelets, red blood cells, and white blood cells.",
                    "tool_description": "Use a YOLOv8-based object detection model to identify and count blood cells—including platelets, RBCs, and WBCs—in digital microscope images, with configurable confidence thresholds, IoU for non-maximum suppression, and maximum detection limits.",
                    "expected_response": "A structured list of detected blood cells with class labels (Platelet, RBC, WBC), bounding box coordinates, confidence scores, IoU values, and optional visual overlays on the input images."
                }
            ],
            "extracted_arguments": {
                "image": "The digital blood sample image to be analyzed, provided as a file path or URL.",
                "conf": "Numeric value representing the confidence threshold for detecting blood cells.",
                "iou": "Numeric value defining the Intersection over Union threshold for Non-Maximum Suppression.",
                "agnostic_nms": "Boolean flag indicating whether NMS should ignore class labels when filtering overlapping detections.",
                "max_det": "Numeric value specifying the maximum number of detections allowed per image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_436",
        "query": "I wish to edit my images by detecting straight lines and controlling the diffusion models in the image's diffusion process.",
        "instruction": "Given an `image editing` task focused on straight line detection and diffusion model control, retrieve tools that can process and modify images by detecting geometric features like straight lines and integrating these into the diffusion process to achieve the desired edits.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "ControlNet - M-LSD Straight Line Version",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": {
                        "diffusers": "pip install diffusers",
                        "transformers": "pip install transformers",
                        "accelerate": "pip install accelerate",
                        "controlnet_aux": "pip install controlnet_aux"
                    },
                    "example_code": {
                        "import": [
                            "from PIL import Image",
                            "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler",
                            "import torch",
                            "from controlnet_aux import MLSDdetector",
                            "from diffusers.utils import load_image"
                        ],
                        "setup": [
                            "mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')",
                            "image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)",
                            "image = mlsd(image)",
                            "controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
                        ],
                        "execution": [
                            "pipe.enable_xformers_memory_efficient_attention()",
                            "pipe.enable_model_cpu_offload()",
                            "image = pipe(room, image, num_inference_steps=20).images[0]",
                            "image.save('images/room_mlsd_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "600k edge-image, caption pairs generated from Places2",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_268",
                    "name": "lllyasviel/sd-controlnet-mlsd"
                },
                "id": "gorilla_huggingface_tool_264",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 1: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.",
            "Tool 2: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.",
            "Tool 3: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.",
            "Tool 4: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.",
            "Tool 5: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.",
            "Tool 6: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.",
            "Tool 7: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.",
            "Tool 8: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.",
            "Tool 9: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 10: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.",
            "Tool 11: This tool provides a ControlNet-based neural network model designed to guide diffusion-based text-to-image generation by conditioning on instruct pix2pix images, enabling more controlled and precise image synthesis within the Stable Diffusion framework.",
            "Tool 12: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 13: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 14: This tool provides an unconditional image generation model based on Denoising Diffusion Probabilistic Models (DDPM), allowing the creation of high-quality synthetic images. It is implemented with Hugging Face’s Transformers framework and can generate images from random noise using various diffusion schedulers. The model achieves strong performance on standard benchmarks, including an Inception score of 9.46 and an FID score of 3.17 on the CIFAR10 dataset, making it suitable for research or creative applications in generative computer vision.",
            "Tool 15: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 16: ControlNet is a neural network designed to enhance diffusion models by adding additional conditioning factors. This specific checkpoint, \"lllyasviel/control_v11p_sd15_lineart,\" is tailored to work with lineart images, allowing for more precise control in image generation tasks. It can be integrated with Hugging Face’s frameworks, particularly for tasks involving image-to-image transformations, and requires specific Python dependencies for setup.",
            "Tool 17: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 18: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 19: Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation."
        ],
        "tool_view": {
            "description": "This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "parameters": {
                "torch_dtype": {
                    "type": "string",
                    "description": "The data type to use for PyTorch tensors when loading the ControlNet model, which affects memory usage and computation precision.",
                    "required": true,
                    "example": "torch.float16"
                }
            },
            "response": "The tool produces an image-to-image transformation using a diffusion model conditioned on straight-line detection via M-LSD. The response of the tool is an object representing the generated image(s) after processing the input. Specifically:  \n\n- **Primary output**: A collection (typically a list) of generated images. Each image represents the result of applying the ControlNet-conditioned Stable Diffusion pipeline to the input image under the specified prompt or guidance.  \n- **Image format**: Each element is an image object (e.g., a PIL Image) that can be saved or further processed.  \n- **Optional metadata**: While not explicitly defined in the tool JSON, typical pipelines may include associated metadata such as:\n  - The input image used for conditioning.\n  - The prompt or textual description guiding the generation.\n  - Parameters used during inference (e.g., number of inference steps, scheduler type, seed).\n  - Any intermediate outputs or attention maps if requested (though not standard in the basic setup).  \n\nIn essence, the core semantic content of the response is the fully generated image(s) reflecting the diffusion process guided by the straight-line features detected in the input.",
            "examples": [
                {
                    "user_intent": "Enhance a photo of my living room by generating a stylized version guided by detected straight lines in the room.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained",
                        "arguments": {
                            "model_id": "lllyasviel/sd-controlnet-mlsd",
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "Create a clean architectural sketch of a building from a photo using straight line detection.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained",
                        "arguments": {
                            "model_id": "lllyasviel/sd-controlnet-mlsd",
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "Generate a futuristic room concept from an existing room image, while preserving all straight edges and lines.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained",
                        "arguments": {
                            "model_id": "lllyasviel/sd-controlnet-mlsd",
                            "torch_dtype": "torch.float16"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 1: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.\nTool 2: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.\nTool 3: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.\nTool 4: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.\nTool 5: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.\nTool 6: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.\nTool 7: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.\nTool 8: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.\nTool 9: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 10: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.\nTool 11: This tool provides a ControlNet-based neural network model designed to guide diffusion-based text-to-image generation by conditioning on instruct pix2pix images, enabling more controlled and precise image synthesis within the Stable Diffusion framework.\nTool 12: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 13: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 14: This tool provides an unconditional image generation model based on Denoising Diffusion Probabilistic Models (DDPM), allowing the creation of high-quality synthetic images. It is implemented with Hugging Face’s Transformers framework and can generate images from random noise using various diffusion schedulers. The model achieves strong performance on standard benchmarks, including an Inception score of 9.46 and an FID score of 3.17 on the CIFAR10 dataset, making it suitable for research or creative applications in generative computer vision.\nTool 15: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 16: ControlNet is a neural network designed to enhance diffusion models by adding additional conditioning factors. This specific checkpoint, \"lllyasviel/control_v11p_sd15_lineart,\" is tailored to work with lineart images, allowing for more precise control in image generation tasks. It can be integrated with Hugging Face’s frameworks, particularly for tasks involving image-to-image transformations, and requires specific Python dependencies for setup.\nTool 17: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 18: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 19: Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\", \"parameters\": {\"torch_dtype\": {\"type\": \"string\", \"description\": \"The data type to use for PyTorch tensors when loading the ControlNet model, which affects memory usage and computation precision.\", \"required\": true, \"example\": \"torch.float16\"}}, \"response\": \"The tool produces an image-to-image transformation using a diffusion model conditioned on straight-line detection via M-LSD. The response of the tool is an object representing the generated image(s) after processing the input. Specifically:  \\n\\n- **Primary output**: A collection (typically a list) of generated images. Each image represents the result of applying the ControlNet-conditioned Stable Diffusion pipeline to the input image under the specified prompt or guidance.  \\n- **Image format**: Each element is an image object (e.g., a PIL Image) that can be saved or further processed.  \\n- **Optional metadata**: While not explicitly defined in the tool JSON, typical pipelines may include associated metadata such as:\\n  - The input image used for conditioning.\\n  - The prompt or textual description guiding the generation.\\n  - Parameters used during inference (e.g., number of inference steps, scheduler type, seed).\\n  - Any intermediate outputs or attention maps if requested (though not standard in the basic setup).  \\n\\nIn essence, the core semantic content of the response is the fully generated image(s) reflecting the diffusion process guided by the straight-line features detected in the input.\", \"examples\": [{\"user_intent\": \"Enhance a photo of my living room by generating a stylized version guided by detected straight lines in the room.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained\", \"arguments\": {\"model_id\": \"lllyasviel/sd-controlnet-mlsd\", \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"Create a clean architectural sketch of a building from a photo using straight line detection.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained\", \"arguments\": {\"model_id\": \"lllyasviel/sd-controlnet-mlsd\", \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"Generate a futuristic room concept from an existing room image, while preserving all straight edges and lines.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained\", \"arguments\": {\"model_id\": \"lllyasviel/sd-controlnet-mlsd\", \"torch_dtype\": \"torch.float16\"}}}]}\n\n# User Query\nI wish to edit my images by detecting straight lines and controlling the diffusion models in the image's diffusion process.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Edit images by detecting straight lines and guiding their transformation.",
                    "tool_description": "Use the ControlNet model conditioned on M-LSD straight line detection to control a diffusion-based image generation process, enabling precise editing based on detected geometric features.",
                    "expected_response": "A collection of generated images where the diffusion process has been guided by the detected straight lines, including optional metadata such as input image, prompt, and inference parameters."
                }
            ],
            "extracted_arguments": {
                "torch_dtype": "The data type for PyTorch tensors used when loading the ControlNet model, influencing memory and computation precision."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_118",
        "query": "Build a system for detecting hate speech from social media comments in Korean.",
        "instruction": "Given a `hate speech detection` system development task, retrieve tools that enable natural language processing for identifying harmful content in social media comments, specifically utilizing models capable of processing Korean text to extract relevant features and assist in classification tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Feature Extraction",
                    "framework": "PyTorch Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BartModel.from_pretrained('gogamza/kobart-base-v2')",
                    "api_arguments": {
                        "tokenizer": "PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "tokenizers": "latest"
                    },
                    "example_code": "from transformers import PreTrainedTokenizerFast, BartModel\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')",
                    "performance": {
                        "dataset": "NSMC",
                        "accuracy": 0.901
                    },
                    "description": "KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.",
                    "id": "huggingface_api_8",
                    "name": "kobart-base-v2"
                },
                "id": "gorilla_huggingface_tool_8",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 1: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 2: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 3: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 4: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 5: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 6: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 7: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 8: A Korean BERT-based model designed for text-to-text generation tasks, trained on a 70GB Korean text dataset with 42,000 lower-cased subwords, suitable for encoding and decoding Korean text using the Hugging Face Transformers framework.",
            "Tool 9: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 10: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 11: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 12: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 13: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 14: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 15: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 16: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 17: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 18: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.",
            "Tool 19: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages."
        ],
        "tool_view": {
            "description": "Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pre-trained token classification model to load for Named Entity Recognition tasks. This model has been fine-tuned for food entity recognition in informal text.",
                    "required": true,
                    "example": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer associated with the pre-trained model, used to convert text into token IDs suitable for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                }
            },
            "response": "The tool is designed for Named Entity Recognition (NER) in the domain of food-related entities, particularly focusing on informal text such as social media posts. The response format from this tool would typically contain the following components:\n\n1. **Named Entities**: A list of identified entities in the input text, specifically food-related entities. Each entity would have:\n   - **Entity Type**: The type or category of the entity, which, in this case, would be 'FOOD' for food-related terms.\n   - **Start and End Position**: The positions in the input text where the entity starts and ends.\n   - **Entity Label**: A label that categorizes the entity, which would typically be the word 'FOOD' in this case.\n   - **Confidence/Score**: The model's confidence score for the classification of the entity.\n\n2. **Aggregation Strategy**: The results may be aggregated based on the specified strategy (e.g., 'simple'), which could determine whether the tool returns entities in a raw form or in a more processed, simplified format.\n\n3. **Performance Metrics**: Although the response to a specific input may not include these directly, performance statistics such as accuracy, precision, recall, and F1 score are part of the tool's performance metrics and could be referenced to give a sense of the model's quality and reliability. These include:\n   - **F1 Score**: A balance of precision and recall.\n   - **Precision**: The accuracy of positive predictions.\n   - **Recall**: The model’s ability to detect relevant entities.\n\nIn summary, the tool's response would typically consist of a list of recognized food entities from the input text along with relevant information such as their positions, labels, and confidence scores. The response would reflect the model's ability to identify food-related terms in informal language such as Instagram posts.",
            "examples": [
                {
                    "user_intent": "I need to identify food-related entities in a sentence about a meal.",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                },
                {
                    "user_intent": "Detect food items mentioned in a casual social media post about food.",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                },
                {
                    "user_intent": "Classify the food entities in this Instagram-style text: 'Today's meal: Fresh olive pok bowl topped with chia seeds.'",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 1: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 2: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 3: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 4: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 5: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 6: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 7: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 8: A Korean BERT-based model designed for text-to-text generation tasks, trained on a 70GB Korean text dataset with 42,000 lower-cased subwords, suitable for encoding and decoding Korean text using the Hugging Face Transformers framework.\nTool 9: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 10: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 11: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 12: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 13: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 14: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 15: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 16: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 17: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 18: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.\nTool 19: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pre-trained token classification model to load for Named Entity Recognition tasks. This model has been fine-tuned for food entity recognition in informal text.\", \"required\": true, \"example\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer associated with the pre-trained model, used to convert text into token IDs suitable for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}, \"response\": \"The tool is designed for Named Entity Recognition (NER) in the domain of food-related entities, particularly focusing on informal text such as social media posts. The response format from this tool would typically contain the following components:\\n\\n1. **Named Entities**: A list of identified entities in the input text, specifically food-related entities. Each entity would have:\\n   - **Entity Type**: The type or category of the entity, which, in this case, would be 'FOOD' for food-related terms.\\n   - **Start and End Position**: The positions in the input text where the entity starts and ends.\\n   - **Entity Label**: A label that categorizes the entity, which would typically be the word 'FOOD' in this case.\\n   - **Confidence/Score**: The model's confidence score for the classification of the entity.\\n\\n2. **Aggregation Strategy**: The results may be aggregated based on the specified strategy (e.g., 'simple'), which could determine whether the tool returns entities in a raw form or in a more processed, simplified format.\\n\\n3. **Performance Metrics**: Although the response to a specific input may not include these directly, performance statistics such as accuracy, precision, recall, and F1 score are part of the tool's performance metrics and could be referenced to give a sense of the model's quality and reliability. These include:\\n   - **F1 Score**: A balance of precision and recall.\\n   - **Precision**: The accuracy of positive predictions.\\n   - **Recall**: The model\\u2019s ability to detect relevant entities.\\n\\nIn summary, the tool's response would typically consist of a list of recognized food entities from the input text along with relevant information such as their positions, labels, and confidence scores. The response would reflect the model's ability to identify food-related terms in informal language such as Instagram posts.\", \"examples\": [{\"user_intent\": \"I need to identify food-related entities in a sentence about a meal.\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}, {\"user_intent\": \"Detect food items mentioned in a casual social media post about food.\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}, {\"user_intent\": \"Classify the food entities in this Instagram-style text: 'Today's meal: Fresh olive pok bowl topped with chia seeds.'\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}]}\n\n# User Query\nBuild a system for detecting hate speech from social media comments in Korean.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect hate speech in Korean social media comments.",
                    "tool_description": "Apply a fine-tuned Korean language model to classify text comments as containing hate speech or not, enabling moderation and content analysis in social media contexts.",
                    "expected_response": "A classification result for each input comment indicating whether it contains hate speech, including confidence scores or probability estimates."
                }
            ],
            "extracted_arguments": {
                "text_input": "The Korean social media comment or text to analyze for hate speech."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_138",
        "query": "We have a surveillance camera in our backyard. We would like to analyze the captured videos to recognize the activities taking place in the backyard.",
        "instruction": "Given a `video analysis` task, retrieve tools that perform activity recognition by processing video inputs to identify and classify actions occurring within the footage according to the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')",
                    "api_arguments": {
                        "learning_rate": 5e-05,
                        "train_batch_size": 8,
                        "eval_batch_size": 8,
                        "seed": 42,
                        "optimizer": "Adam with betas=(0.9,0.999) and epsilon=1e-08",
                        "lr_scheduler_type": "linear",
                        "lr_scheduler_warmup_ratio": 0.1,
                        "training_steps": 148
                    },
                    "python_environment_requirements": {
                        "Transformers": "4.24.0",
                        "Pytorch": "1.12.1+cu113",
                        "Datasets": "2.6.1",
                        "Tokenizers": "0.13.2"
                    },
                    "example_code": "from transformers import AutoModelForVideoClassification, AutoTokenizer\nmodel = AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')\ntokenizer = AutoTokenizer.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": 0.8645
                    },
                    "description": "This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645.",
                    "id": "huggingface_api_343",
                    "name": "sayakpaul/videomae-base-finetuned-ucf101-subset"
                },
                "id": "gorilla_huggingface_tool_339",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 1: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 2: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 3: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.",
            "Tool 4: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 5: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 6: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 7: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 8: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 9: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 10: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 11: Graphormer-base-pcqm4mv1 is a pretrained graph Transformer model designed for graph classification and representation tasks, particularly in molecular modeling. It was trained on the PCQM4M-LSC dataset and achieved first place in the KDD CUP 2021 quantum prediction track, making it highly effective for learning from graph-structured data. The model can be used directly or fine-tuned for specific downstream tasks using the Hugging Face Transformers framework.",
            "Tool 12: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 13: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 14: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 15: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.",
            "Tool 16: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 17: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 18: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "Tool 19: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications."
        ],
        "tool_view": {
            "description": "A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "parameters": {
                "images": {
                    "type": "array",
                    "description": "A list of video frames represented as arrays, where each frame is typically a 3D array corresponding to (channels, height, width). This input represents the video to be classified.",
                    "required": true,
                    "example": [
                        [
                            [
                                [
                                    0.1,
                                    0.2
                                ],
                                [
                                    0.3,
                                    0.4
                                ]
                            ],
                            [
                                [
                                    0.5,
                                    0.6
                                ],
                                [
                                    0.7,
                                    0.8
                                ]
                            ],
                            [
                                [
                                    0.9,
                                    1.0
                                ],
                                [
                                    1.1,
                                    1.2
                                ]
                            ]
                        ]
                    ]
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors to return, such as 'pt' for PyTorch or 'tf' for TensorFlow, which determines the format of the model inputs.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool produces a structured output representing the classification results of a video input. The response typically contains the following components:  \n\n1. **logits**:  \n   - A tensor of raw, unnormalized scores produced by the model for each possible video class.  \n   - Its dimensionality corresponds to the number of classes the model can predict.  \n   - Higher values indicate greater confidence that the input video belongs to the corresponding class, but they are not probabilities.  \n\n2. **predicted_class_idx** (optional, often derived from logits):  \n   - An integer representing the index of the class with the highest logit score.  \n   - This index can be mapped to a human-readable label using the model’s `id2label` mapping.  \n\n3. **id2label mapping** (from `model.config`):  \n   - A dictionary that maps class indices to descriptive class names.  \n   - Allows conversion of `predicted_class_idx` into a human-readable label.  \n\n4. **Additional model outputs** (optional, depending on the configuration of `TimesformerForVideoClassification`):  \n   - May include intermediate hidden states, attention weights, or other auxiliary outputs if the model is configured to return them.  \n\nIn practice, the typical workflow is: the video is preprocessed into a tensor, fed into the model, the model outputs the `logits`, and the highest-scoring class index is converted to a label using `id2label`. This provides the predicted action or category for the input video.",
            "examples": [
                {
                    "user_intent": "I want to classify a short video clip of someone stirring a cup of coffee to see what action it contains.",
                    "tool_call": {
                        "name": "TimesformerForVideoClassification",
                        "arguments": {
                            "images": "np.array of 16 frames of size 448x448x3 representing the coffee stirring video",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this 10-second sports video to automatically identify the type of sport being played.",
                    "tool_call": {
                        "name": "TimesformerForVideoClassification",
                        "arguments": {
                            "images": "np.array of 16 frames of size 448x448x3 extracted evenly across the sports video",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Classify an office meeting video to detect the primary action, like typing, presenting, or talking.",
                    "tool_call": {
                        "name": "TimesformerForVideoClassification",
                        "arguments": {
                            "images": "np.array of 16 frames of size 448x448x3 sampled from the meeting video",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 1: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 2: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 3: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.\nTool 4: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 5: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 6: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 7: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 8: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 9: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 10: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 11: Graphormer-base-pcqm4mv1 is a pretrained graph Transformer model designed for graph classification and representation tasks, particularly in molecular modeling. It was trained on the PCQM4M-LSC dataset and achieved first place in the KDD CUP 2021 quantum prediction track, making it highly effective for learning from graph-structured data. The model can be used directly or fine-tuned for specific downstream tasks using the Hugging Face Transformers framework.\nTool 12: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 13: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 14: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 15: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.\nTool 16: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 17: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 18: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\nTool 19: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\", \"parameters\": {\"images\": {\"type\": \"array\", \"description\": \"A list of video frames represented as arrays, where each frame is typically a 3D array corresponding to (channels, height, width). This input represents the video to be classified.\", \"required\": true, \"example\": [[[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[0.9, 1.0], [1.1, 1.2]]]]}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors to return, such as 'pt' for PyTorch or 'tf' for TensorFlow, which determines the format of the model inputs.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool produces a structured output representing the classification results of a video input. The response typically contains the following components:  \\n\\n1. **logits**:  \\n   - A tensor of raw, unnormalized scores produced by the model for each possible video class.  \\n   - Its dimensionality corresponds to the number of classes the model can predict.  \\n   - Higher values indicate greater confidence that the input video belongs to the corresponding class, but they are not probabilities.  \\n\\n2. **predicted_class_idx** (optional, often derived from logits):  \\n   - An integer representing the index of the class with the highest logit score.  \\n   - This index can be mapped to a human-readable label using the model\\u2019s `id2label` mapping.  \\n\\n3. **id2label mapping** (from `model.config`):  \\n   - A dictionary that maps class indices to descriptive class names.  \\n   - Allows conversion of `predicted_class_idx` into a human-readable label.  \\n\\n4. **Additional model outputs** (optional, depending on the configuration of `TimesformerForVideoClassification`):  \\n   - May include intermediate hidden states, attention weights, or other auxiliary outputs if the model is configured to return them.  \\n\\nIn practice, the typical workflow is: the video is preprocessed into a tensor, fed into the model, the model outputs the `logits`, and the highest-scoring class index is converted to a label using `id2label`. This provides the predicted action or category for the input video.\", \"examples\": [{\"user_intent\": \"I want to classify a short video clip of someone stirring a cup of coffee to see what action it contains.\", \"tool_call\": {\"name\": \"TimesformerForVideoClassification\", \"arguments\": {\"images\": \"np.array of 16 frames of size 448x448x3 representing the coffee stirring video\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Analyze this 10-second sports video to automatically identify the type of sport being played.\", \"tool_call\": {\"name\": \"TimesformerForVideoClassification\", \"arguments\": {\"images\": \"np.array of 16 frames of size 448x448x3 extracted evenly across the sports video\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Classify an office meeting video to detect the primary action, like typing, presenting, or talking.\", \"tool_call\": {\"name\": \"TimesformerForVideoClassification\", \"arguments\": {\"images\": \"np.array of 16 frames of size 448x448x3 sampled from the meeting video\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nWe have a surveillance camera in our backyard. We would like to analyze the captured videos to recognize the activities taking place in the backyard.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze surveillance videos from my backyard to automatically identify the activities occurring.",
                    "tool_description": "Use a pre-trained TimeSformer model to classify actions in video footage by leveraging space-time attention over the video frames.",
                    "expected_response": "A structured output containing predicted action classes for the video, including logits, predicted class index, and a mapping from indices to human-readable labels."
                }
            ],
            "extracted_arguments": {
                "video_frames": "A list of video frames representing the backyard surveillance footage, where each frame is a 3D array of channels, height, and width.",
                "return_tensors": "The format of tensors to return from the model, such as PyTorch or TensorFlow, determining how the video input is processed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_490",
        "query": "I have a collection of audio recordings from interviews that I need transcribed into text. How can I achieve this using automatic speech recognition?",
        "instruction": "Given an `audio transcription` task, retrieve tools that specialize in converting audio recordings, such as interviews, into text using automatic speech recognition (ASR) technologies. These tools should be capable of handling different audio formats and providing accurate transcriptions in alignment with the query's needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transcription",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')",
                    "api_arguments": {
                        "model_name": "openai/whisper-tiny.en"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets",
                        "torch"
                    ],
                    "example_code": [
                        "from transformers import WhisperProcessor, WhisperForConditionalGeneration",
                        "from datasets import load_dataset",
                        "processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)",
                        "model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)",
                        "ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)",
                        "sample = ds[0][audio]",
                        "input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features",
                        "predicted_ids = model.generate(input_features)",
                        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
                    ],
                    "performance": {
                        "dataset": "LibriSpeech (clean)",
                        "accuracy": 8.437
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.",
                    "id": "huggingface_api_754",
                    "name": "openai/whisper-tiny.en"
                },
                "id": "gorilla_huggingface_tool_749",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 1: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 2: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 3: A compact wav2vec2 model for automatic speech recognition that converts spoken audio into text using the Hugging Face Transformers framework.",
            "Tool 4: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.",
            "Tool 5: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.",
            "Tool 6: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.",
            "Tool 7: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.",
            "Tool 8: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.",
            "Tool 9: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 10: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 11: This tool is a fine-tuned speech recognition model for Arabic, based on the Wav2Vec2 architecture from Hugging Face Transformers. It transcribes audio files into text, using a model that was trained on the Common Voice 6.1 and Arabic Speech Corpus datasets. The tool is designed to handle various audio formats and provides accurate transcriptions, achieving a word error rate (WER) of 39.59% and a character error rate (CER) of 18.18%. It is ideal for automatic speech recognition tasks in Arabic.",
            "Tool 12: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 13: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.",
            "Tool 14: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "Tool 15: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 16: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.",
            "Tool 17: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 18: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.",
            "Tool 19: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs."
        ],
        "tool_view": {
            "description": "The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path to the pretrained model to be used for automatic speech recognition. This should be a valid model identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "facebook/s2t-medium-librispeech-asr"
                }
            },
            "response": "The tool produces a transcription of spoken audio, converting speech input into text. The response is typically a structured object or string containing the following components:\n\n1. **Generated Text**:  \n   - The primary output is the textual transcription of the input audio.  \n   - This may be returned as a single string if the input is a single audio sample or as a list of strings if multiple audio samples are processed in a batch.\n\n2. **Optional Metadata** (depending on implementation):  \n   - Some wrappers or higher-level pipelines may include additional information, such as:\n     - **Audio input identifier**: a reference or index corresponding to the input audio sample.  \n     - **Token-level details**: the sequence of predicted tokens or token IDs before decoding.  \n     - **Confidence scores**: probability scores or likelihoods associated with the generated transcription.  \n     - **Processing information**: model configuration, sampling rate, or batch size used.\n\n3. **Structure in typical Hugging Face workflow**:  \n   - The model generates token IDs via `model.generate(input_features=…)`.  \n   - These token IDs are then converted to human-readable text using `processor.batch_decode(generated_ids)`.  \n   - Therefore, the final response is most commonly a **list of strings**, where each string represents the decoded transcription for one input audio segment.\n\nIn summary, the tool’s response delivers the speech-to-text transcription, optionally accompanied by token-level or processing metadata, structured either as a single string or a list of strings corresponding to the audio inputs.",
            "examples": [
                {
                    "user_intent": "Transcribe a short English audio clip of someone reading a paragraph from a book.",
                    "tool_call": {
                        "name": "Speech2TextForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/s2t-medium-librispeech-asr"
                        }
                    }
                },
                {
                    "user_intent": "Convert a podcast episode's audio file into text for content analysis.",
                    "tool_call": {
                        "name": "Speech2TextForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/s2t-medium-librispeech-asr"
                        }
                    }
                },
                {
                    "user_intent": "Generate a transcript for an English-language interview recorded in a quiet studio.",
                    "tool_call": {
                        "name": "Speech2TextForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/s2t-medium-librispeech-asr"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 1: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 2: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 3: A compact wav2vec2 model for automatic speech recognition that converts spoken audio into text using the Hugging Face Transformers framework.\nTool 4: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\nTool 5: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.\nTool 6: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.\nTool 7: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.\nTool 8: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.\nTool 9: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 10: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 11: This tool is a fine-tuned speech recognition model for Arabic, based on the Wav2Vec2 architecture from Hugging Face Transformers. It transcribes audio files into text, using a model that was trained on the Common Voice 6.1 and Arabic Speech Corpus datasets. The tool is designed to handle various audio formats and provides accurate transcriptions, achieving a word error rate (WER) of 39.59% and a character error rate (CER) of 18.18%. It is ideal for automatic speech recognition tasks in Arabic.\nTool 12: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 13: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.\nTool 14: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\nTool 15: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 16: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.\nTool 17: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 18: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.\nTool 19: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path to the pretrained model to be used for automatic speech recognition. This should be a valid model identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"facebook/s2t-medium-librispeech-asr\"}}, \"response\": \"The tool produces a transcription of spoken audio, converting speech input into text. The response is typically a structured object or string containing the following components:\\n\\n1. **Generated Text**:  \\n   - The primary output is the textual transcription of the input audio.  \\n   - This may be returned as a single string if the input is a single audio sample or as a list of strings if multiple audio samples are processed in a batch.\\n\\n2. **Optional Metadata** (depending on implementation):  \\n   - Some wrappers or higher-level pipelines may include additional information, such as:\\n     - **Audio input identifier**: a reference or index corresponding to the input audio sample.  \\n     - **Token-level details**: the sequence of predicted tokens or token IDs before decoding.  \\n     - **Confidence scores**: probability scores or likelihoods associated with the generated transcription.  \\n     - **Processing information**: model configuration, sampling rate, or batch size used.\\n\\n3. **Structure in typical Hugging Face workflow**:  \\n   - The model generates token IDs via `model.generate(input_features=\\u2026)`.  \\n   - These token IDs are then converted to human-readable text using `processor.batch_decode(generated_ids)`.  \\n   - Therefore, the final response is most commonly a **list of strings**, where each string represents the decoded transcription for one input audio segment.\\n\\nIn summary, the tool\\u2019s response delivers the speech-to-text transcription, optionally accompanied by token-level or processing metadata, structured either as a single string or a list of strings corresponding to the audio inputs.\", \"examples\": [{\"user_intent\": \"Transcribe a short English audio clip of someone reading a paragraph from a book.\", \"tool_call\": {\"name\": \"Speech2TextForConditionalGeneration.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/s2t-medium-librispeech-asr\"}}}, {\"user_intent\": \"Convert a podcast episode's audio file into text for content analysis.\", \"tool_call\": {\"name\": \"Speech2TextForConditionalGeneration.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/s2t-medium-librispeech-asr\"}}}, {\"user_intent\": \"Generate a transcript for an English-language interview recorded in a quiet studio.\", \"tool_call\": {\"name\": \"Speech2TextForConditionalGeneration.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/s2t-medium-librispeech-asr\"}}}]}\n\n# User Query\nI have a collection of audio recordings from interviews that I need transcribed into text. How can I achieve this using automatic speech recognition?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to transcribe a collection of audio recordings into text using automatic speech recognition.",
                    "tool_description": "Use an automatic speech recognition model to process a batch of audio recordings and transcribe them into text.",
                    "expected_response": "A list of strings, each representing the transcription of one audio recording."
                }
            ],
            "extracted_arguments": {
                "audio_recordings": "The collection of audio files or audio samples to be transcribed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_12",
        "query": "Translate the following text from French to English: “Le système éducatif français est composé d'écoles maternelles, d'écoles élémentaires, de collèges et de lycées.”",
        "instruction": "Given a `translation` task, retrieve tools that convert text from French to English by processing bilingual textual inputs to produce accurate translations aligned with the query's linguistic and contextual requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "translation_pipeline('Bonjour, comment ça va?')",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": {
                                "newsdiscussdev2015-enfr.fr.en": 33.1,
                                "newsdiscusstest2015-enfr.fr.en": 38.7,
                                "newssyscomb2009.fr.en": 30.3,
                                "news-test2008.fr.en": 26.2,
                                "newstest2009.fr.en": 30.2,
                                "newstest2010.fr.en": 32.2,
                                "newstest2011.fr.en": 33.0,
                                "newstest2012.fr.en": 32.8,
                                "newstest2013.fr.en": 33.9,
                                "newstest2014-fren.fr.en": 37.8,
                                "Tatoeba.fr.en": 57.5
                            }
                        }
                    },
                    "description": "Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.",
                    "id": "huggingface_api_524",
                    "name": "opus-mt-fr-en"
                },
                "id": "gorilla_huggingface_tool_520",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 1: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 2: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 3: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.",
            "Tool 4: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 5: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 6: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 7: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 8: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 9: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 10: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 11: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 12: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 13: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 14: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 15: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 16: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 17: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.",
            "Tool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 19: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French."
        ],
        "tool_view": {
            "description": "This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "parameters": {},
            "response": "The tool produces a response that contains the translated text from German to English. The response is typically structured as a list of dictionaries, where each dictionary represents one translation output. Each dictionary usually includes the following components:  \n\n- **`translation_text`**: A string containing the English translation of the input German text. This is the primary content of the response.  \n\nIn principle, for each input string, the tool returns one corresponding translation dictionary, but when multiple inputs are provided, the response contains multiple dictionaries in a list, maintaining the order of the input texts. The response may also include additional metadata depending on the implementation, but the core component is the translated text under `translation_text`.",
            "examples": [
                {
                    "user_intent": "Translate this German sentence into English: 'Das Wetter heute ist schön.'",
                    "tool_call": {
                        "name": "opus-mt-de-en",
                        "arguments": {
                            "text": "Das Wetter heute ist schön."
                        }
                    }
                },
                {
                    "user_intent": "I have a paragraph in German and need it translated to English for my report.",
                    "tool_call": {
                        "name": "opus-mt-de-en",
                        "arguments": {
                            "text": "Deutschland ist bekannt für seine malerischen Landschaften, historischen Städte und reiche Kultur."
                        }
                    }
                },
                {
                    "user_intent": "Convert this German news headline into English: 'Neue wirtschaftliche Maßnahmen wurden angekündigt.'",
                    "tool_call": {
                        "name": "opus-mt-de-en",
                        "arguments": {
                            "text": "Neue wirtschaftliche Maßnahmen wurden angekündigt."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 1: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 2: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 3: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.\nTool 4: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 5: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 6: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 7: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 8: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 9: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 10: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 11: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 12: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 13: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 14: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 15: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 16: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 17: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.\nTool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 19: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\", \"parameters\": {}, \"response\": \"The tool produces a response that contains the translated text from German to English. The response is typically structured as a list of dictionaries, where each dictionary represents one translation output. Each dictionary usually includes the following components:  \\n\\n- **`translation_text`**: A string containing the English translation of the input German text. This is the primary content of the response.  \\n\\nIn principle, for each input string, the tool returns one corresponding translation dictionary, but when multiple inputs are provided, the response contains multiple dictionaries in a list, maintaining the order of the input texts. The response may also include additional metadata depending on the implementation, but the core component is the translated text under `translation_text`.\", \"examples\": [{\"user_intent\": \"Translate this German sentence into English: 'Das Wetter heute ist sch\\u00f6n.'\", \"tool_call\": {\"name\": \"opus-mt-de-en\", \"arguments\": {\"text\": \"Das Wetter heute ist sch\\u00f6n.\"}}}, {\"user_intent\": \"I have a paragraph in German and need it translated to English for my report.\", \"tool_call\": {\"name\": \"opus-mt-de-en\", \"arguments\": {\"text\": \"Deutschland ist bekannt f\\u00fcr seine malerischen Landschaften, historischen St\\u00e4dte und reiche Kultur.\"}}}, {\"user_intent\": \"Convert this German news headline into English: 'Neue wirtschaftliche Ma\\u00dfnahmen wurden angek\\u00fcndigt.'\", \"tool_call\": {\"name\": \"opus-mt-de-en\", \"arguments\": {\"text\": \"Neue wirtschaftliche Ma\\u00dfnahmen wurden angek\\u00fcndigt.\"}}}]}\n\n# User Query\nTranslate the following text from French to English: “Le système éducatif français est composé d'écoles maternelles, d'écoles élémentaires, de collèges et de lycées.”\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate the given French text into English.",
                    "tool_description": "Use a French-to-English machine translation model to convert input French text into fluent English, preserving meaning and context.",
                    "expected_response": "A dictionary or list containing the English translation of the input French text under the key `translation_text`."
                }
            ],
            "extracted_arguments": {
                "text_to_translate": "The French text string that needs to be translated into English."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_346",
        "query": "We have received an invoice document, and would like to extract the total amount from it.",
        "instruction": "Given a `document analysis` task, retrieve tools that are specialized in extracting specific information, such as total amounts, from documents like invoices by processing textual and visual content to provide accurate data extraction.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')",
                    "api_arguments": "question, context",
                    "python_environment_requirements": "transformers",
                    "example_code": "nlp(question='What is the total amount?', context='your_invoice_text')",
                    "performance": {
                        "dataset": "proprietary dataset of invoices, SQuAD2.0, and DocVQA",
                        "accuracy": "Not provided"
                    },
                    "description": "A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.",
                    "id": "huggingface_api_122",
                    "name": "layoutlm-invoices"
                },
                "id": "gorilla_huggingface_tool_121",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 1: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 2: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 3: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 4: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 5: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 6: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 7: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 8: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 9: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 11: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 12: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 13: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 14: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 15: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.",
            "Tool 16: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 17: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 18: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 19: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text."
        ],
        "tool_view": {
            "description": "A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The question you want to ask about the document or invoice. This is the input query that the model will try to answer based on the provided context.",
                    "required": true,
                    "example": "What is the total amount?"
                },
                "context": {
                    "type": "string",
                    "description": "The textual content of the document or invoice where the model will search for the answer to the question. This can include the full text extracted from the document.",
                    "required": true,
                    "example": "Invoice #12345 dated 2025-11-01 with total amount $1,250.00 for services rendered."
                }
            },
            "response": "The tool returns a structured answer to a user-provided question based on the content of a document. The response typically contains the following components:\n\n1. **Answer Text**: The main textual output of the model, representing the answer extracted or generated from the document context. This may include non-consecutive sequences of tokens, not limited to contiguous spans.\n\n2. **Score/Confidence (optional)**: A numerical value indicating the model’s confidence in the predicted answer. This may be included if the model provides a probability or confidence score for the extracted answer.\n\n3. **Answer Spans/Offsets (optional)**: Information about the positions of the relevant tokens in the input document, which may be provided when the model identifies non-contiguous spans contributing to the answer.\n\n4. **Additional Metadata (optional)**: Any auxiliary information such as document sections used, attention weights, or reasoning indicators, depending on the underlying implementation.\n\nOverall, the response conveys the extracted or predicted answer to the input question, potentially augmented with confidence measures and positional metadata, enabling users to interpret and verify the answer within the context of the document.",
            "examples": [
                {
                    "user_intent": "I have a PDF invoice and I want to know the total amount billed.",
                    "tool_call": {
                        "name": "layoutlm-invoices",
                        "arguments": {
                            "question": "What is the total amount?",
                            "context": "Invoice from ABC Corp dated 2025-12-01 with line items for office supplies and services totaling $1,245.50."
                        }
                    }
                },
                {
                    "user_intent": "Extract the invoice number from this scanned document.",
                    "tool_call": {
                        "name": "layoutlm-invoices",
                        "arguments": {
                            "question": "What is the invoice number?",
                            "context": "Scanned invoice showing invoice number INV-2025-045, issued to XYZ Ltd., dated 2025-11-20."
                        }
                    }
                },
                {
                    "user_intent": "Check who is the vendor on this receipt.",
                    "tool_call": {
                        "name": "layoutlm-invoices",
                        "arguments": {
                            "question": "Who is the vendor?",
                            "context": "Receipt from OfficeMart, 123 Main St., purchased office chairs and desks on 2025-12-10, total $678.90."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 1: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 2: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 3: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 4: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 5: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 6: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 7: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 8: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 9: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 11: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 12: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 13: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 14: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 15: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.\nTool 16: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 17: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 18: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 19: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The question you want to ask about the document or invoice. This is the input query that the model will try to answer based on the provided context.\", \"required\": true, \"example\": \"What is the total amount?\"}, \"context\": {\"type\": \"string\", \"description\": \"The textual content of the document or invoice where the model will search for the answer to the question. This can include the full text extracted from the document.\", \"required\": true, \"example\": \"Invoice #12345 dated 2025-11-01 with total amount $1,250.00 for services rendered.\"}}, \"response\": \"The tool returns a structured answer to a user-provided question based on the content of a document. The response typically contains the following components:\\n\\n1. **Answer Text**: The main textual output of the model, representing the answer extracted or generated from the document context. This may include non-consecutive sequences of tokens, not limited to contiguous spans.\\n\\n2. **Score/Confidence (optional)**: A numerical value indicating the model\\u2019s confidence in the predicted answer. This may be included if the model provides a probability or confidence score for the extracted answer.\\n\\n3. **Answer Spans/Offsets (optional)**: Information about the positions of the relevant tokens in the input document, which may be provided when the model identifies non-contiguous spans contributing to the answer.\\n\\n4. **Additional Metadata (optional)**: Any auxiliary information such as document sections used, attention weights, or reasoning indicators, depending on the underlying implementation.\\n\\nOverall, the response conveys the extracted or predicted answer to the input question, potentially augmented with confidence measures and positional metadata, enabling users to interpret and verify the answer within the context of the document.\", \"examples\": [{\"user_intent\": \"I have a PDF invoice and I want to know the total amount billed.\", \"tool_call\": {\"name\": \"layoutlm-invoices\", \"arguments\": {\"question\": \"What is the total amount?\", \"context\": \"Invoice from ABC Corp dated 2025-12-01 with line items for office supplies and services totaling $1,245.50.\"}}}, {\"user_intent\": \"Extract the invoice number from this scanned document.\", \"tool_call\": {\"name\": \"layoutlm-invoices\", \"arguments\": {\"question\": \"What is the invoice number?\", \"context\": \"Scanned invoice showing invoice number INV-2025-045, issued to XYZ Ltd., dated 2025-11-20.\"}}}, {\"user_intent\": \"Check who is the vendor on this receipt.\", \"tool_call\": {\"name\": \"layoutlm-invoices\", \"arguments\": {\"question\": \"Who is the vendor?\", \"context\": \"Receipt from OfficeMart, 123 Main St., purchased office chairs and desks on 2025-12-10, total $678.90.\"}}}]}\n\n# User Query\nWe have received an invoice document, and would like to extract the total amount from it.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I have an invoice document and I want to know the total amount billed.",
                    "tool_description": "Use a multimodal LayoutLM-based document question-answering model to extract the total amount from an invoice document by analyzing its text and layout.",
                    "expected_response": "A structured answer containing the total billed amount, potentially including confidence score and positional metadata from the document."
                }
            ],
            "extracted_arguments": {
                "question": "The specific inquiry to extract from the document, in this case the total amount billed, represented as a string.",
                "context": "The textual content of the invoice document where the answer should be found, represented as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_485",
        "query": "I'm working on a biomedical research project and need to extract features from a set of entity names to help me understand their relationships.",
        "instruction": "Given a `biomedical feature extraction` task, retrieve tools that facilitate the extraction of features from biomedical entities by leveraging pre-trained models tailored for entity relationship analysis, ensuring they handle inputs of entity names and yield meaningful embeddings or representations.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')",
                    "api_arguments": "input_ids, attention_mask",
                    "python_environment_requirements": "transformers",
                    "example_code": "inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]",
                    "performance": {
                        "dataset": "UMLS",
                        "accuracy": "N/A"
                    },
                    "description": "SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.",
                    "id": "huggingface_api_3",
                    "name": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
                },
                "id": "gorilla_huggingface_tool_3",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 1: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 2: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 3: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 4: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 5: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 6: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 8: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 9: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 10: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 11: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 12: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 13: A Keras-based TabTransformer model for supervised and semi-supervised learning on tabular data, capable of handling both numerical and categorical features. Categorical features are embedded and processed through Transformer blocks to capture contextual relationships, then combined with numerical features and passed through a final feed-forward network for prediction, with a SoftMax output for classification. This model is designed to leverage self-attention mechanisms to improve learning from structured datasets.",
            "Tool 14: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.",
            "Tool 15: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 16: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.",
            "Tool 17: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 18: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently."
        ],
        "tool_view": {
            "description": "SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "parameters": {
                "input_ids": {
                    "type": "tensor",
                    "description": "A tensor containing the token IDs of the input biomedical entity names, typically produced by a tokenizer.",
                    "required": true,
                    "example": "[101, 5467, 8901, 102]"
                },
                "attention_mask": {
                    "type": "tensor",
                    "description": "A tensor indicating which tokens should be attended to (1) and which should be ignored/padded (0) during model processing.",
                    "required": true,
                    "example": "[1, 1, 1, 1]"
                }
            },
            "response": "The tool performs feature extraction using the **SapBERT** model, which is based on a biomedical pretraining scheme. This tool takes a string input consisting of biomedical entity names and processes it using the **cambridgeltl/SapBERT-from-PubMedBERT-fulltext** model. The response generated by the tool is expected to contain the **[CLS] embedding** from the last hidden layer of the model for the given input.\n\nHere’s a breakdown of the expected response structure:\n\n1. **Embedding**:  \n   The primary output of the tool is the **[CLS] embedding** from the last hidden state of the model. This embedding is a tensor representing the feature vector of the input text, typically used for downstream tasks like classification or clustering.\n\n   - This **embedding** is expected to be a **tensor** (numerical representation) corresponding to the input string.\n   - It will likely be a multidimensional array (e.g., shape `(1, embedding_size)`), where `embedding_size` refers to the number of dimensions in the model's hidden layer output (e.g., 768 dimensions in a typical BERT-based model).\n\n2. **Input Parameters**:  \n   The model receives two key inputs as arguments:\n   - **input_ids**: These are tokenized representations of the input string (in this case, biomedical entity names).\n   - **attention_mask**: This is typically a binary mask that tells the model which tokens to pay attention to and which to ignore (e.g., padding tokens).\n   \n   These parameters are passed into the model during inference and guide how the model processes the input.\n\n3. **Performance Details**:  \n   The tool does not provide accuracy data in this specific use case (indicated as \"N/A\"). This may be due to the tool's use being specific to feature extraction, where performance is more focused on generating embeddings rather than on task-specific accuracy measures.\n\n4. **Model Information**:  \n   - The model is based on **SapBERT**, which is pre-trained using the **UMLS 2020AA** dataset and uses **microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext** as the base model.\n   - The model specifically aligns the representation space of biomedical entities, making it suitable for extracting features from biomedical texts.\n\n### Summary of the Response:\nThe response is expected to contain the **[CLS] embedding** from the last hidden state of the model, representing the given biomedical text input as a numerical tensor. This embedding can be used for further analysis or as input to other downstream tasks.",
            "examples": [
                {
                    "user_intent": "I want to get the feature embedding for the biomedical term 'diabetes mellitus'.",
                    "tool_call": {
                        "name": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext",
                        "arguments": {
                            "input_ids": "[101, 2212, 1999, 3081, 1052, 102]",
                            "attention_mask": "[1, 1, 1, 1, 1, 1]"
                        }
                    }
                },
                {
                    "user_intent": "Extract the CLS embedding for 'COVID-19 infection' using SapBERT.",
                    "tool_call": {
                        "name": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext",
                        "arguments": {
                            "input_ids": "[101, 2240, 1011, 2030, 3691, 102]",
                            "attention_mask": "[1, 1, 1, 1, 1, 1]"
                        }
                    }
                },
                {
                    "user_intent": "Generate embeddings for multiple biomedical entities: 'hypertension' and 'myocardial infarction'.",
                    "tool_call": {
                        "name": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext",
                        "arguments": {
                            "input_ids": "[[101, 8233, 102], [101, 1040, 4521, 2345, 102]]",
                            "attention_mask": "[[1, 1, 1], [1, 1, 1, 1, 1]]"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 1: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 2: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 3: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 4: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 5: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 6: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 8: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 9: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 10: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 11: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 12: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 13: A Keras-based TabTransformer model for supervised and semi-supervised learning on tabular data, capable of handling both numerical and categorical features. Categorical features are embedded and processed through Transformer blocks to capture contextual relationships, then combined with numerical features and passed through a final feed-forward network for prediction, with a SoftMax output for classification. This model is designed to leverage self-attention mechanisms to improve learning from structured datasets.\nTool 14: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.\nTool 15: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 16: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.\nTool 17: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 18: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\", \"parameters\": {\"input_ids\": {\"type\": \"tensor\", \"description\": \"A tensor containing the token IDs of the input biomedical entity names, typically produced by a tokenizer.\", \"required\": true, \"example\": \"[101, 5467, 8901, 102]\"}, \"attention_mask\": {\"type\": \"tensor\", \"description\": \"A tensor indicating which tokens should be attended to (1) and which should be ignored/padded (0) during model processing.\", \"required\": true, \"example\": \"[1, 1, 1, 1]\"}}, \"response\": \"The tool performs feature extraction using the **SapBERT** model, which is based on a biomedical pretraining scheme. This tool takes a string input consisting of biomedical entity names and processes it using the **cambridgeltl/SapBERT-from-PubMedBERT-fulltext** model. The response generated by the tool is expected to contain the **[CLS] embedding** from the last hidden layer of the model for the given input.\\n\\nHere\\u2019s a breakdown of the expected response structure:\\n\\n1. **Embedding**:  \\n   The primary output of the tool is the **[CLS] embedding** from the last hidden state of the model. This embedding is a tensor representing the feature vector of the input text, typically used for downstream tasks like classification or clustering.\\n\\n   - This **embedding** is expected to be a **tensor** (numerical representation) corresponding to the input string.\\n   - It will likely be a multidimensional array (e.g., shape `(1, embedding_size)`), where `embedding_size` refers to the number of dimensions in the model's hidden layer output (e.g., 768 dimensions in a typical BERT-based model).\\n\\n2. **Input Parameters**:  \\n   The model receives two key inputs as arguments:\\n   - **input_ids**: These are tokenized representations of the input string (in this case, biomedical entity names).\\n   - **attention_mask**: This is typically a binary mask that tells the model which tokens to pay attention to and which to ignore (e.g., padding tokens).\\n   \\n   These parameters are passed into the model during inference and guide how the model processes the input.\\n\\n3. **Performance Details**:  \\n   The tool does not provide accuracy data in this specific use case (indicated as \\\"N/A\\\"). This may be due to the tool's use being specific to feature extraction, where performance is more focused on generating embeddings rather than on task-specific accuracy measures.\\n\\n4. **Model Information**:  \\n   - The model is based on **SapBERT**, which is pre-trained using the **UMLS 2020AA** dataset and uses **microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext** as the base model.\\n   - The model specifically aligns the representation space of biomedical entities, making it suitable for extracting features from biomedical texts.\\n\\n### Summary of the Response:\\nThe response is expected to contain the **[CLS] embedding** from the last hidden state of the model, representing the given biomedical text input as a numerical tensor. This embedding can be used for further analysis or as input to other downstream tasks.\", \"examples\": [{\"user_intent\": \"I want to get the feature embedding for the biomedical term 'diabetes mellitus'.\", \"tool_call\": {\"name\": \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", \"arguments\": {\"input_ids\": \"[101, 2212, 1999, 3081, 1052, 102]\", \"attention_mask\": \"[1, 1, 1, 1, 1, 1]\"}}}, {\"user_intent\": \"Extract the CLS embedding for 'COVID-19 infection' using SapBERT.\", \"tool_call\": {\"name\": \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", \"arguments\": {\"input_ids\": \"[101, 2240, 1011, 2030, 3691, 102]\", \"attention_mask\": \"[1, 1, 1, 1, 1, 1]\"}}}, {\"user_intent\": \"Generate embeddings for multiple biomedical entities: 'hypertension' and 'myocardial infarction'.\", \"tool_call\": {\"name\": \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", \"arguments\": {\"input_ids\": \"[[101, 8233, 102], [101, 1040, 4521, 2345, 102]]\", \"attention_mask\": \"[[1, 1, 1], [1, 1, 1, 1, 1]]\"}}}]}\n\n# User Query\nI'm working on a biomedical research project and need to extract features from a set of entity names to help me understand their relationships.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract feature embeddings for a set of biomedical entity names to analyze their relationships.",
                    "tool_description": "Generate [CLS] embeddings for biomedical entity names using the SapBERT model, which aligns entity representations in a shared feature space for downstream analysis.",
                    "expected_response": "A tensor containing the [CLS] embeddings from the last hidden layer of SapBERT for each input entity name, suitable for similarity analysis or relationship modeling."
                }
            ],
            "extracted_arguments": {
                "entity_names": "A list of biomedical entity names as strings whose feature embeddings need to be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_106",
        "query": "We are developing an AI-powered code review system. Our model should provide a short summary of the provided code snippet.",
        "instruction": "Given a `code summarization` task, retrieve tools that specialize in code understanding and text generation by processing input code snippets to produce concise summaries that capture the essential semantics and purposes of the code.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Code Understanding and Generation",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')",
                    "api_arguments": [
                        "text",
                        "return_tensors",
                        "input_ids",
                        "max_length"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import RobertaTokenizer, T5ForConditionalGeneration\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\ntext = def greet(user): print(f'hello <extra_id_0>!')\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\ngenerated_ids = model.generate(input_ids, max_length=8)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))",
                    "performance": {
                        "dataset": "code_search_net",
                        "accuracy": "Refer to the paper for evaluation results on several downstream benchmarks"
                    },
                    "description": "CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection.",
                    "id": "huggingface_api_656",
                    "name": "Salesforce/codet5-base"
                },
                "id": "gorilla_huggingface_tool_651",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.",
            "Tool 1: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 2: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 3: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 4: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 5: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 6: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.",
            "Tool 7: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.",
            "Tool 8: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 9: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 10: A BERT-based model pre-trained on 152 million sentences from StackOverflow, designed for token-level classification tasks such as code and named entity recognition within StackOverflow posts.",
            "Tool 11: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 12: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.",
            "Tool 13: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 14: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.",
            "Tool 15: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 16: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 17: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 18: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 19: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text."
        ],
        "tool_view": {
            "description": "SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.",
            "parameters": {
                "inputs": {
                    "type": "string",
                    "description": "The input text or code that is passed into the model for generation. This can be any text prompt or code snippet that the model will use to generate a continuation or response.",
                    "required": true,
                    "example": "def print_hello_world():"
                }
            },
            "response": "The tool response provides text generated by a model trained for code generation tasks using a causal language model architecture, specifically the \"SantaCoder\" model from the \"bigcode/santacoder\" checkpoint. \n\nThe response typically includes the following components:\n\n1. **Generated Text**: \n   - The primary component of the response is the text generated by the model. This text is based on the input prompt provided in the API call (encoded through the tokenizer).\n   - The generated text is often code, such as Python, Java, or JavaScript, based on the model's training data.\n\n2. **Text Decoding**:\n   - After generating the tokenized output, the model decodes the token sequence back into human-readable text, such as source code or natural language.\n\n3. **Metadata (optional)**:\n   - Depending on the specific implementation or response configuration, the response may include metadata related to the generation process. This could include:\n     - **Generation Parameters**: Information like the length of the generated output, temperature, and other hyperparameters that control how the model generates text.\n     - **Model Details**: The model name (e.g., \"bigcode/santacoder\") and version could also be included for context, especially in debugging or logging scenarios.\n\nIn summary, the response will contain the generated text, typically in the form of code, that corresponds to the input prompt, with potential decoding details and metadata for context.",
            "examples": [
                {
                    "user_intent": "Generate a Python function that calculates the factorial of a number.",
                    "tool_call": {
                        "name": "bigcode/santacoder",
                        "arguments": {
                            "inputs": "def factorial(n):"
                        }
                    }
                },
                {
                    "user_intent": "Create a JavaScript function to reverse a string.",
                    "tool_call": {
                        "name": "bigcode/santacoder",
                        "arguments": {
                            "inputs": "function reverseString(str) {"
                        }
                    }
                },
                {
                    "user_intent": "Write a Java method that checks if a number is prime.",
                    "tool_call": {
                        "name": "bigcode/santacoder",
                        "arguments": {
                            "inputs": "public boolean isPrime(int n) {"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.\nTool 1: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 2: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 3: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 4: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 5: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 6: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.\nTool 7: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.\nTool 8: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 9: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 10: A BERT-based model pre-trained on 152 million sentences from StackOverflow, designed for token-level classification tasks such as code and named entity recognition within StackOverflow posts.\nTool 11: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 12: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\nTool 13: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 14: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.\nTool 15: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 16: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 17: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 18: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 19: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.\", \"parameters\": {\"inputs\": {\"type\": \"string\", \"description\": \"The input text or code that is passed into the model for generation. This can be any text prompt or code snippet that the model will use to generate a continuation or response.\", \"required\": true, \"example\": \"def print_hello_world():\"}}, \"response\": \"The tool response provides text generated by a model trained for code generation tasks using a causal language model architecture, specifically the \\\"SantaCoder\\\" model from the \\\"bigcode/santacoder\\\" checkpoint. \\n\\nThe response typically includes the following components:\\n\\n1. **Generated Text**: \\n   - The primary component of the response is the text generated by the model. This text is based on the input prompt provided in the API call (encoded through the tokenizer).\\n   - The generated text is often code, such as Python, Java, or JavaScript, based on the model's training data.\\n\\n2. **Text Decoding**:\\n   - After generating the tokenized output, the model decodes the token sequence back into human-readable text, such as source code or natural language.\\n\\n3. **Metadata (optional)**:\\n   - Depending on the specific implementation or response configuration, the response may include metadata related to the generation process. This could include:\\n     - **Generation Parameters**: Information like the length of the generated output, temperature, and other hyperparameters that control how the model generates text.\\n     - **Model Details**: The model name (e.g., \\\"bigcode/santacoder\\\") and version could also be included for context, especially in debugging or logging scenarios.\\n\\nIn summary, the response will contain the generated text, typically in the form of code, that corresponds to the input prompt, with potential decoding details and metadata for context.\", \"examples\": [{\"user_intent\": \"Generate a Python function that calculates the factorial of a number.\", \"tool_call\": {\"name\": \"bigcode/santacoder\", \"arguments\": {\"inputs\": \"def factorial(n):\"}}}, {\"user_intent\": \"Create a JavaScript function to reverse a string.\", \"tool_call\": {\"name\": \"bigcode/santacoder\", \"arguments\": {\"inputs\": \"function reverseString(str) {\"}}}, {\"user_intent\": \"Write a Java method that checks if a number is prime.\", \"tool_call\": {\"name\": \"bigcode/santacoder\", \"arguments\": {\"inputs\": \"public boolean isPrime(int n) {\"}}}]}\n\n# User Query\nWe are developing an AI-powered code review system. Our model should provide a short summary of the provided code snippet.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a concise summary or description for a given code snippet.",
                    "tool_description": "Use a pre-trained code understanding model to analyze a code snippet and produce a brief natural-language summary of its functionality.",
                    "expected_response": "A short textual summary describing the purpose or behavior of the input code snippet."
                }
            ],
            "extracted_arguments": {
                "code_snippet": "The source code input that needs to be analyzed and summarized, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_147",
        "query": "We are a medical research team working on a COVID-19 project. We need assistance in question answering related to the pandemic and related research papers.",
        "instruction": "Given a `question answering` task related to COVID-19, retrieve tools that specialize in processing natural language queries and providing detailed answers from a database of pandemic-related research documents. The tools should use pre-trained models optimized for extractive question answering to ensure accurate and contextually relevant responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model=RobertaForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2-covid'), tokenizer=RobertaTokenizer.from_pretrained('deepset/roberta-base-squad2-covid'))",
                    "api_arguments": {
                        "model_name": "deepset/roberta-base-squad2-covid",
                        "tokenizer": "deepset/roberta-base-squad2-covid"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "QA_input": {
                            "question": "Why is model conversion important?",
                            "context": "The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks."
                        },
                        "res": "nlp(QA_input)"
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "XVAL_EM": 0.17890995260663506,
                            "XVAL_f1": 0.49925444207319924,
                            "XVAL_top_3_recall": 0.8021327014218009
                        }
                    },
                    "description": "This model is a Roberta-based model fine-tuned on SQuAD-style CORD-19 annotations for the task of extractive question answering in the context of COVID-19. It can be used with the Hugging Face Transformers library for question answering tasks.",
                    "id": "huggingface_api_472",
                    "name": "deepset/roberta-base-squad2-covid"
                },
                "id": "gorilla_huggingface_tool_468",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.",
            "Tool 1: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 2: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "Tool 3: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.",
            "Tool 4: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 5: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 6: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 7: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 8: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 9: FLAN-T5 is a multilingual language model fine-tuned on over 1,000 diverse tasks, designed for text-to-text generation in zero-shot and few-shot NLP settings. It can perform tasks such as reasoning, translation, and question answering, achieving strong benchmark performance and supporting research in versatile natural language understanding and generation applications.",
            "Tool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 11: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.",
            "Tool 12: FLAN-T5 XXL is a large, fine-tuned T5 language model designed for text-to-text generation across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, translation, and question answering, and achieves strong performance on benchmarks like MMLU. This model is suited for research and applications requiring advanced natural language understanding and generation.",
            "Tool 13: LLaMA-7B is a transformer-based, auto-regressive language model designed for generating text and conducting research in natural language understanding, question answering, and reading comprehension. It is trained on diverse English-language sources, including web pages, books, and academic content, and can be used via the Hugging Face Transformers library to produce coherent, context-aware text for a variety of NLP applications.",
            "Tool 14: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 15: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.",
            "Tool 16: FLAN-T5 XL is a large-scale language model fine-tuned on over 1000 tasks across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, question answering, and language understanding. The model, accessible via the Hugging Face Transformers framework, achieves state-of-the-art performance on benchmarks like MMLU, with an accuracy of 75.2%. It is designed for research on the capabilities and limitations of large language models.",
            "Tool 17: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 18: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 19: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models."
        ],
        "tool_view": {
            "description": "A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "Why is model conversion important?"
                },
                "context": {
                    "type": "string",
                    "description": "The text passage or context in which the model should find the answer to the question.",
                    "required": true,
                    "example": "The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks."
                }
            },
            "response": "The tool's response will contain the answer to a given question, based on the context provided. This response will be structured to include the following components:\n\n1. **Answer**: The specific answer to the question posed, extracted from the context. This will be a text string, representing the portion of the context that best addresses the question.\n\n2. **Score**: A numerical value representing the confidence level or likelihood that the answer provided is correct, based on the model’s internal computations. This score is typically expressed as a floating-point value.\n\n3. **Start and End Positions**: These are the indices (in the provided context text) that indicate where the answer begins and ends. This allows users to trace the exact segment of the context from which the answer is derived.\n\nIn principle, the response will contain:\n- A text string as the **answer** to the user’s question.\n- A **score** representing the model's confidence in the answer.\n- **Start and end positions** within the context text indicating where the model identified the answer.\n\nThis structure is typical for extractive question-answering tasks, where the goal is to extract a relevant portion of the provided context as the answer to the question.",
            "examples": [
                {
                    "user_intent": "I want to know the recommended isolation period for COVID-19 patients according to recent guidelines.",
                    "tool_call": {
                        "name": "deepset/roberta-base-squad2-covid",
                        "arguments": {
                            "question": "What is the recommended isolation period for COVID-19 patients?",
                            "context": "According to CDC guidelines, individuals who test positive for COVID-19 should isolate for at least 5 days from the onset of symptoms, followed by 5 days of mask-wearing around others if symptoms are improving."
                        }
                    }
                },
                {
                    "user_intent": "What symptoms indicate severe COVID-19 that require immediate medical attention?",
                    "tool_call": {
                        "name": "deepset/roberta-base-squad2-covid",
                        "arguments": {
                            "question": "Which symptoms indicate severe COVID-19 requiring hospitalization?",
                            "context": "Severe COVID-19 symptoms include difficulty breathing, persistent chest pain, confusion, inability to stay awake, and bluish lips or face. Immediate medical attention is advised in such cases."
                        }
                    }
                },
                {
                    "user_intent": "Can COVID-19 be transmitted through surfaces?",
                    "tool_call": {
                        "name": "deepset/roberta-base-squad2-covid",
                        "arguments": {
                            "question": "Is surface transmission of COVID-19 possible?",
                            "context": "Studies suggest that while SARS-CoV-2 can survive on surfaces for hours to days, transmission through surface contact is considered less common compared to respiratory droplets and aerosols."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.\nTool 1: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 2: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\nTool 3: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\nTool 4: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 5: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 6: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 7: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 8: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 9: FLAN-T5 is a multilingual language model fine-tuned on over 1,000 diverse tasks, designed for text-to-text generation in zero-shot and few-shot NLP settings. It can perform tasks such as reasoning, translation, and question answering, achieving strong benchmark performance and supporting research in versatile natural language understanding and generation applications.\nTool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 11: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.\nTool 12: FLAN-T5 XXL is a large, fine-tuned T5 language model designed for text-to-text generation across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, translation, and question answering, and achieves strong performance on benchmarks like MMLU. This model is suited for research and applications requiring advanced natural language understanding and generation.\nTool 13: LLaMA-7B is a transformer-based, auto-regressive language model designed for generating text and conducting research in natural language understanding, question answering, and reading comprehension. It is trained on diverse English-language sources, including web pages, books, and academic content, and can be used via the Hugging Face Transformers library to produce coherent, context-aware text for a variety of NLP applications.\nTool 14: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 15: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.\nTool 16: FLAN-T5 XL is a large-scale language model fine-tuned on over 1000 tasks across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, question answering, and language understanding. The model, accessible via the Hugging Face Transformers framework, achieves state-of-the-art performance on benchmarks like MMLU, with an accuracy of 75.2%. It is designed for research on the capabilities and limitations of large language models.\nTool 17: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 18: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 19: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"Why is model conversion important?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text passage or context in which the model should find the answer to the question.\", \"required\": true, \"example\": \"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\"}}, \"response\": \"The tool's response will contain the answer to a given question, based on the context provided. This response will be structured to include the following components:\\n\\n1. **Answer**: The specific answer to the question posed, extracted from the context. This will be a text string, representing the portion of the context that best addresses the question.\\n\\n2. **Score**: A numerical value representing the confidence level or likelihood that the answer provided is correct, based on the model\\u2019s internal computations. This score is typically expressed as a floating-point value.\\n\\n3. **Start and End Positions**: These are the indices (in the provided context text) that indicate where the answer begins and ends. This allows users to trace the exact segment of the context from which the answer is derived.\\n\\nIn principle, the response will contain:\\n- A text string as the **answer** to the user\\u2019s question.\\n- A **score** representing the model's confidence in the answer.\\n- **Start and end positions** within the context text indicating where the model identified the answer.\\n\\nThis structure is typical for extractive question-answering tasks, where the goal is to extract a relevant portion of the provided context as the answer to the question.\", \"examples\": [{\"user_intent\": \"I want to know the recommended isolation period for COVID-19 patients according to recent guidelines.\", \"tool_call\": {\"name\": \"deepset/roberta-base-squad2-covid\", \"arguments\": {\"question\": \"What is the recommended isolation period for COVID-19 patients?\", \"context\": \"According to CDC guidelines, individuals who test positive for COVID-19 should isolate for at least 5 days from the onset of symptoms, followed by 5 days of mask-wearing around others if symptoms are improving.\"}}}, {\"user_intent\": \"What symptoms indicate severe COVID-19 that require immediate medical attention?\", \"tool_call\": {\"name\": \"deepset/roberta-base-squad2-covid\", \"arguments\": {\"question\": \"Which symptoms indicate severe COVID-19 requiring hospitalization?\", \"context\": \"Severe COVID-19 symptoms include difficulty breathing, persistent chest pain, confusion, inability to stay awake, and bluish lips or face. Immediate medical attention is advised in such cases.\"}}}, {\"user_intent\": \"Can COVID-19 be transmitted through surfaces?\", \"tool_call\": {\"name\": \"deepset/roberta-base-squad2-covid\", \"arguments\": {\"question\": \"Is surface transmission of COVID-19 possible?\", \"context\": \"Studies suggest that while SARS-CoV-2 can survive on surfaces for hours to days, transmission through surface contact is considered less common compared to respiratory droplets and aerosols.\"}}}]}\n\n# User Query\nWe are a medical research team working on a COVID-19 project. We need assistance in question answering related to the pandemic and related research papers.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to ask questions about COVID-19 research and guidelines and get precise answers from relevant texts.",
                    "tool_description": "Use a RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data to perform extractive question answering, identifying precise answers from provided COVID-19 related texts.",
                    "expected_response": "A structured answer containing the extracted text that answers the question, a confidence score, and start and end positions within the context."
                }
            ],
            "extracted_arguments": {
                "question": "The natural language question regarding COVID-19 or related research, as a string.",
                "context": "The text or passage from research papers or guidelines in which the answer should be found, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_494",
        "query": "We are a group of teachers looking to create an automated summary based on the student's essays.",
        "instruction": "Given an `automated summarization` task, retrieve tools that process text inputs, like student essays, to generate concise and coherent summaries, aligning with the requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BartModel.from_pretrained('facebook/bart-base')",
                    "api_arguments": [
                        "inputs"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import BartTokenizer, BartModel\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\nmodel = BartModel.from_pretrained('facebook/bart-base')\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "arxiv",
                        "accuracy": "Not provided"
                    },
                    "description": "BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).",
                    "id": "huggingface_api_4",
                    "name": "facebook/bart-base"
                },
                "id": "gorilla_huggingface_tool_4",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 1: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 2: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 3: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 4: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 5: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 6: A transformer-based image captioning tool that generates descriptive text for input images, enabling automated understanding and annotation of visual content.",
            "Tool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 8: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 9: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 10: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 11: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 12: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 13: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 14: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 15: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 16: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 17: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 18: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 19: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages."
        ],
        "tool_view": {
            "description": "Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input Chinese text that you want the model to summarize. This should be a complete text string.",
                    "required": true,
                    "example": "今天天气很好，我们去公园散步吧。"
                },
                "max_length": {
                    "type": "integer",
                    "description": "The maximum number of tokens to generate in the summary. This limits the length of the output summary.",
                    "required": true,
                    "example": 150
                }
            },
            "response": "The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\n\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\n\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\"summary_text\"`) pointing to the summarized string. The content of the summary reflects the model’s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \n\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.",
            "examples": [
                {
                    "user_intent": "我有一篇关于人工智能发展的中文文章，想生成一段简短摘要。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "人工智能近年来取得了显著的发展，从自然语言处理到计算机视觉，各个领域都有突破性进展。技术的进步推动了产业升级，同时也带来了伦理和隐私等挑战。",
                            "max_length": 60
                        }
                    }
                },
                {
                    "user_intent": "需要对一条微博内容生成中文摘要，方便快速浏览。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "今天在上海举办的科技大会上，多家AI公司展示了最新的智能产品，包括自动驾驶、语音助手和智能家居设备，引发了广泛关注。",
                            "max_length": 40
                        }
                    }
                },
                {
                    "user_intent": "我有一篇教育领域的长篇文章，希望生成概括性摘要用于报告。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "近年来，中国教育领域进行了多项改革，尤其在中小学课程和教学方法上有所调整。在线教育和智能教育工具的应用提高了学习效率，同时也引发了教育公平性的讨论。",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 1: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 2: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 3: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 4: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 5: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 6: A transformer-based image captioning tool that generates descriptive text for input images, enabling automated understanding and annotation of visual content.\nTool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 8: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 9: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 10: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 11: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 12: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 13: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 14: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 15: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 16: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 17: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 18: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 19: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input Chinese text that you want the model to summarize. This should be a complete text string.\", \"required\": true, \"example\": \"\\u4eca\\u5929\\u5929\\u6c14\\u5f88\\u597d\\uff0c\\u6211\\u4eec\\u53bb\\u516c\\u56ed\\u6563\\u6b65\\u5427\\u3002\"}, \"max_length\": {\"type\": \"integer\", \"description\": \"The maximum number of tokens to generate in the summary. This limits the length of the output summary.\", \"required\": true, \"example\": 150}}, \"response\": \"The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\\n\\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\\n\\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\\\"summary_text\\\"`) pointing to the summarized string. The content of the summary reflects the model\\u2019s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \\n\\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.\", \"examples\": [{\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u5173\\u4e8e\\u4eba\\u5de5\\u667a\\u80fd\\u53d1\\u5c55\\u7684\\u4e2d\\u6587\\u6587\\u7ae0\\uff0c\\u60f3\\u751f\\u6210\\u4e00\\u6bb5\\u7b80\\u77ed\\u6458\\u8981\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eba\\u5de5\\u667a\\u80fd\\u8fd1\\u5e74\\u6765\\u53d6\\u5f97\\u4e86\\u663e\\u8457\\u7684\\u53d1\\u5c55\\uff0c\\u4ece\\u81ea\\u7136\\u8bed\\u8a00\\u5904\\u7406\\u5230\\u8ba1\\u7b97\\u673a\\u89c6\\u89c9\\uff0c\\u5404\\u4e2a\\u9886\\u57df\\u90fd\\u6709\\u7a81\\u7834\\u6027\\u8fdb\\u5c55\\u3002\\u6280\\u672f\\u7684\\u8fdb\\u6b65\\u63a8\\u52a8\\u4e86\\u4ea7\\u4e1a\\u5347\\u7ea7\\uff0c\\u540c\\u65f6\\u4e5f\\u5e26\\u6765\\u4e86\\u4f26\\u7406\\u548c\\u9690\\u79c1\\u7b49\\u6311\\u6218\\u3002\", \"max_length\": 60}}}, {\"user_intent\": \"\\u9700\\u8981\\u5bf9\\u4e00\\u6761\\u5fae\\u535a\\u5185\\u5bb9\\u751f\\u6210\\u4e2d\\u6587\\u6458\\u8981\\uff0c\\u65b9\\u4fbf\\u5feb\\u901f\\u6d4f\\u89c8\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eca\\u5929\\u5728\\u4e0a\\u6d77\\u4e3e\\u529e\\u7684\\u79d1\\u6280\\u5927\\u4f1a\\u4e0a\\uff0c\\u591a\\u5bb6AI\\u516c\\u53f8\\u5c55\\u793a\\u4e86\\u6700\\u65b0\\u7684\\u667a\\u80fd\\u4ea7\\u54c1\\uff0c\\u5305\\u62ec\\u81ea\\u52a8\\u9a7e\\u9a76\\u3001\\u8bed\\u97f3\\u52a9\\u624b\\u548c\\u667a\\u80fd\\u5bb6\\u5c45\\u8bbe\\u5907\\uff0c\\u5f15\\u53d1\\u4e86\\u5e7f\\u6cdb\\u5173\\u6ce8\\u3002\", \"max_length\": 40}}}, {\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u6559\\u80b2\\u9886\\u57df\\u7684\\u957f\\u7bc7\\u6587\\u7ae0\\uff0c\\u5e0c\\u671b\\u751f\\u6210\\u6982\\u62ec\\u6027\\u6458\\u8981\\u7528\\u4e8e\\u62a5\\u544a\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u8fd1\\u5e74\\u6765\\uff0c\\u4e2d\\u56fd\\u6559\\u80b2\\u9886\\u57df\\u8fdb\\u884c\\u4e86\\u591a\\u9879\\u6539\\u9769\\uff0c\\u5c24\\u5176\\u5728\\u4e2d\\u5c0f\\u5b66\\u8bfe\\u7a0b\\u548c\\u6559\\u5b66\\u65b9\\u6cd5\\u4e0a\\u6709\\u6240\\u8c03\\u6574\\u3002\\u5728\\u7ebf\\u6559\\u80b2\\u548c\\u667a\\u80fd\\u6559\\u80b2\\u5de5\\u5177\\u7684\\u5e94\\u7528\\u63d0\\u9ad8\\u4e86\\u5b66\\u4e60\\u6548\\u7387\\uff0c\\u540c\\u65f6\\u4e5f\\u5f15\\u53d1\\u4e86\\u6559\\u80b2\\u516c\\u5e73\\u6027\\u7684\\u8ba8\\u8bba\\u3002\", \"max_length\": 80}}}]}\n\n# User Query\nWe are a group of teachers looking to create an automated summary based on the student's essays.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate an automated summary of student essays.",
                    "tool_description": "Generate a concise summary from an essay text to condense the main points and information for easier review.",
                    "expected_response": "A string containing the summary of the student's essay, highlighting key points in a concise manner."
                }
            ],
            "extracted_arguments": {
                "text": "The full text of the student's essay to be summarized.",
                "max_length": "The maximum number of tokens for the generated summary."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_186",
        "query": "You have been provided with the dataset of plants, and your task is to identify the species of plants among Iris Setosa, Iris Versicolor, and Iris Virginica.",
        "instruction": "Given a `plant species classification` task, retrieve tools that can identify plant species using a pre-trained machine learning model by processing tabular data inputs to provide predictions of species such as Iris Setosa, Iris Versicolor, and Iris Virginica.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Transformers",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "scikit-learn/iris",
                        "accuracy": 0.9
                    },
                    "description": "A K-Nearest Neighbors (KNN) model trained on the Iris dataset for multi-class classification. The model is trained using AutoTrain and has an accuracy of 0.9.",
                    "id": "huggingface_api_859",
                    "name": "abhishek/autotrain-iris-knn"
                },
                "id": "gorilla_huggingface_tool_852",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a multi-class tabular classification model, trained on the Iris dataset using XGBoost and AutoTrain. It is designed for classifying tabular data and can be used with the Joblib framework to make predictions. The model achieves an accuracy of 86.67% on the Iris dataset, and requires the `joblib` and `pandas` libraries in the Python environment.",
            "Tool 1: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.",
            "Tool 2: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.",
            "Tool 3: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 4: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 5: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 6: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.",
            "Tool 7: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.",
            "Tool 8: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 9: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 10: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.",
            "Tool 11: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 12: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 13: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 14: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.",
            "Tool 15: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 16: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 17: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.",
            "Tool 18: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.",
            "Tool 19: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required."
        ],
        "tool_view": {
            "description": "This tool is a multi-class tabular classification model, trained on the Iris dataset using XGBoost and AutoTrain. It is designed for classifying tabular data and can be used with the Joblib framework to make predictions. The model achieves an accuracy of 86.67% on the Iris dataset, and requires the `joblib` and `pandas` libraries in the Python environment.",
            "parameters": {
                "data": {
                    "type": "string",
                    "description": "The file path to the CSV file containing the input data for the classification model. The data should include the features required by the model.",
                    "required": true,
                    "example": "data.csv"
                }
            },
            "response": "The tool response would contain the following components:\n\n1. **Predictions**: This will be a collection of predicted class labels for the input data. Each prediction corresponds to an instance in the dataset provided as input (`data`), with the predicted label reflecting the model's classification result for that instance.\n\n2. **Model Information**: The response might include basic details about the model used for prediction, such as:\n   - **Model Name**: This could be the identifier or name of the model, in this case, \"abhishek/autotrain-iris-xgboost.\"\n   - **Model Accuracy**: The accuracy of the model, here it is 86.67% as reported, providing a measure of how well the model performs on a validation set.\n\n3. **Features**: The features used for prediction will be reflected in the output, either by the columns of the input data (`data`) or explicitly described if the response includes metadata about the model. In the example, the features are loaded from a configuration file (`config.json`), which can help provide additional context about the model’s input expectations.\n\n4. **Input Data Information**: The response may describe how the input data should be structured. It includes the feature names (`features`), as well as any transformations or preprocessing steps applied to the data (e.g., renaming columns like 'feat_0', 'feat_1', etc.).\n\n5. **Performance Metrics**: If available, performance information such as accuracy, precision, recall, or other evaluation metrics could be included, with accuracy being one of the key metrics for the model’s performance.\n\nThis response structure serves to provide both the classification results and information about the underlying model, its expected input, and its performance characteristics.",
            "examples": [
                {
                    "user_intent": "I have a CSV file with iris flower measurements and I want to predict the species for each row using the pre-trained model.",
                    "tool_call": {
                        "name": "abhishek/autotrain-iris-xgboost",
                        "arguments": {
                            "data": "iris_test_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "Classify new iris flower samples from my dataset and get the predicted class labels.",
                    "tool_call": {
                        "name": "abhishek/autotrain-iris-xgboost",
                        "arguments": {
                            "data": "new_iris_samples.csv"
                        }
                    }
                },
                {
                    "user_intent": "I want to run the multi-class classification model on a small subset of my iris dataset to check its predictions.",
                    "tool_call": {
                        "name": "abhishek/autotrain-iris-xgboost",
                        "arguments": {
                            "data": "iris_subset.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a multi-class tabular classification model, trained on the Iris dataset using XGBoost and AutoTrain. It is designed for classifying tabular data and can be used with the Joblib framework to make predictions. The model achieves an accuracy of 86.67% on the Iris dataset, and requires the `joblib` and `pandas` libraries in the Python environment.\nTool 1: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.\nTool 2: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.\nTool 3: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 4: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 5: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 6: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\nTool 7: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.\nTool 8: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 9: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 10: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\nTool 11: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 12: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 13: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 14: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.\nTool 15: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 16: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 17: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.\nTool 18: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.\nTool 19: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a multi-class tabular classification model, trained on the Iris dataset using XGBoost and AutoTrain. It is designed for classifying tabular data and can be used with the Joblib framework to make predictions. The model achieves an accuracy of 86.67% on the Iris dataset, and requires the `joblib` and `pandas` libraries in the Python environment.\", \"parameters\": {\"data\": {\"type\": \"string\", \"description\": \"The file path to the CSV file containing the input data for the classification model. The data should include the features required by the model.\", \"required\": true, \"example\": \"data.csv\"}}, \"response\": \"The tool response would contain the following components:\\n\\n1. **Predictions**: This will be a collection of predicted class labels for the input data. Each prediction corresponds to an instance in the dataset provided as input (`data`), with the predicted label reflecting the model's classification result for that instance.\\n\\n2. **Model Information**: The response might include basic details about the model used for prediction, such as:\\n   - **Model Name**: This could be the identifier or name of the model, in this case, \\\"abhishek/autotrain-iris-xgboost.\\\"\\n   - **Model Accuracy**: The accuracy of the model, here it is 86.67% as reported, providing a measure of how well the model performs on a validation set.\\n\\n3. **Features**: The features used for prediction will be reflected in the output, either by the columns of the input data (`data`) or explicitly described if the response includes metadata about the model. In the example, the features are loaded from a configuration file (`config.json`), which can help provide additional context about the model\\u2019s input expectations.\\n\\n4. **Input Data Information**: The response may describe how the input data should be structured. It includes the feature names (`features`), as well as any transformations or preprocessing steps applied to the data (e.g., renaming columns like 'feat_0', 'feat_1', etc.).\\n\\n5. **Performance Metrics**: If available, performance information such as accuracy, precision, recall, or other evaluation metrics could be included, with accuracy being one of the key metrics for the model\\u2019s performance.\\n\\nThis response structure serves to provide both the classification results and information about the underlying model, its expected input, and its performance characteristics.\", \"examples\": [{\"user_intent\": \"I have a CSV file with iris flower measurements and I want to predict the species for each row using the pre-trained model.\", \"tool_call\": {\"name\": \"abhishek/autotrain-iris-xgboost\", \"arguments\": {\"data\": \"iris_test_data.csv\"}}}, {\"user_intent\": \"Classify new iris flower samples from my dataset and get the predicted class labels.\", \"tool_call\": {\"name\": \"abhishek/autotrain-iris-xgboost\", \"arguments\": {\"data\": \"new_iris_samples.csv\"}}}, {\"user_intent\": \"I want to run the multi-class classification model on a small subset of my iris dataset to check its predictions.\", \"tool_call\": {\"name\": \"abhishek/autotrain-iris-xgboost\", \"arguments\": {\"data\": \"iris_subset.csv\"}}}]}\n\n# User Query\nYou have been provided with the dataset of plants, and your task is to identify the species of plants among Iris Setosa, Iris Versicolor, and Iris Virginica.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify plant samples into Iris species using a pre-trained model.",
                    "tool_description": "Use a multi-class tabular classification model trained on the Iris dataset with XGBoost to predict the species of each plant in the input dataset.",
                    "expected_response": "A collection of predicted class labels for each plant instance, including model information, features used, and performance metrics such as accuracy."
                }
            ],
            "extracted_arguments": {
                "data": "The file path to the CSV file containing plant measurements with features required by the Iris classification model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_400",
        "query": "I am running a wine store, and I am looking for a machine learning model that can help me classify the quality of wine based on some given features.",
        "instruction": "Given a `wine quality classification` task, retrieve tools that leverage machine learning models to classify wine quality based on input features, ensuring alignment with the query's requirements for classification accuracy and model implementation.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Wine Quality classification",
                    "api_call": "joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))",
                    "api_arguments": "X",
                    "python_environment_requirements": [
                        "huggingface_hub",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = julien-c/wine-quality\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\ndata_file = cached_download(\n hf_hub_url(REPO_ID, winequality-red.csv)\n)\nwinedf = pd.read_csv(data_file, sep=;)\nX = winedf.drop([quality], axis=1)\nY = winedf[quality]\nprint(X[:3])\nlabels = model.predict(X[:3])",
                    "performance": {
                        "dataset": "winequality-red.csv",
                        "accuracy": 0.6616635397123202
                    },
                    "description": "A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.",
                    "id": "huggingface_api_847",
                    "name": "osanseviero/wine-quality"
                },
                "id": "gorilla_huggingface_tool_841",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "Tool 1: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.",
            "Tool 2: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 3: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.",
            "Tool 4: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 5: A Keras-based TabTransformer model for supervised and semi-supervised learning on tabular data, capable of handling both numerical and categorical features. Categorical features are embedded and processed through Transformer blocks to capture contextual relationships, then combined with numerical features and passed through a final feed-forward network for prediction, with a SoftMax output for classification. This model is designed to leverage self-attention mechanisms to improve learning from structured datasets.",
            "Tool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 7: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.",
            "Tool 8: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.",
            "Tool 9: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 10: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 11: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 12: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 13: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 14: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.",
            "Tool 15: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 16: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 17: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.",
            "Tool 18: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 19: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness."
        ],
        "tool_view": {
            "description": "This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "parameters": {
                "X": {
                    "type": "DataFrame",
                    "description": "A pandas DataFrame containing the input features for predicting wine quality. Each row represents a wine sample, and each column represents a feature.",
                    "required": true,
                    "example": "pd.DataFrame({'fixed_acidity': [7.4, 7.8, 7.8], 'volatile_acidity': [0.70, 0.88, 0.76], 'citric_acid': [0.00, 0.00, 0.04]})"
                }
            },
            "response": "The tool returns a prediction output from a trained Scikit-learn classification model for wine quality. The response structure is as follows:\n\n- **Type**: Array or list of discrete labels.  \n- **Length**: Matches the number of input feature vectors provided in `X`. Each element corresponds to the predicted class for one input sample.  \n- **Element meaning**: Each label represents the predicted quality category of a wine sample, as determined by the model trained on the wine quality dataset. The labels are typically integer values corresponding to quality scores from the original dataset.  \n\nIn principle, the response provides a one-to-one mapping between input feature rows and predicted wine quality classes, enabling downstream evaluation or usage in further processing.",
            "examples": [
                {
                    "user_intent": "I want to classify wine quality using a pre-trained Scikit-learn model.",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "wine data to classify"
                        }
                    }
                },
                {
                    "user_intent": "Can I load a model and use it to predict the quality of wines from a CSV file?",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "data from winequality-red.csv"
                        }
                    }
                },
                {
                    "user_intent": "How do I load a pre-trained wine quality classifier and make predictions on a sample dataset?",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "sample wine data from winequality-red.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\nTool 1: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.\nTool 2: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 3: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.\nTool 4: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 5: A Keras-based TabTransformer model for supervised and semi-supervised learning on tabular data, capable of handling both numerical and categorical features. Categorical features are embedded and processed through Transformer blocks to capture contextual relationships, then combined with numerical features and passed through a final feed-forward network for prediction, with a SoftMax output for classification. This model is designed to leverage self-attention mechanisms to improve learning from structured datasets.\nTool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 7: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.\nTool 8: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.\nTool 9: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 10: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 11: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 12: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 13: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 14: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.\nTool 15: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 16: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 17: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.\nTool 18: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 19: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\", \"parameters\": {\"X\": {\"type\": \"DataFrame\", \"description\": \"A pandas DataFrame containing the input features for predicting wine quality. Each row represents a wine sample, and each column represents a feature.\", \"required\": true, \"example\": \"pd.DataFrame({'fixed_acidity': [7.4, 7.8, 7.8], 'volatile_acidity': [0.70, 0.88, 0.76], 'citric_acid': [0.00, 0.00, 0.04]})\"}}, \"response\": \"The tool returns a prediction output from a trained Scikit-learn classification model for wine quality. The response structure is as follows:\\n\\n- **Type**: Array or list of discrete labels.  \\n- **Length**: Matches the number of input feature vectors provided in `X`. Each element corresponds to the predicted class for one input sample.  \\n- **Element meaning**: Each label represents the predicted quality category of a wine sample, as determined by the model trained on the wine quality dataset. The labels are typically integer values corresponding to quality scores from the original dataset.  \\n\\nIn principle, the response provides a one-to-one mapping between input feature rows and predicted wine quality classes, enabling downstream evaluation or usage in further processing.\", \"examples\": [{\"user_intent\": \"I want to classify wine quality using a pre-trained Scikit-learn model.\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"wine data to classify\"}}}, {\"user_intent\": \"Can I load a model and use it to predict the quality of wines from a CSV file?\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"data from winequality-red.csv\"}}}, {\"user_intent\": \"How do I load a pre-trained wine quality classifier and make predictions on a sample dataset?\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"sample wine data from winequality-red.csv\"}}}]}\n\n# User Query\nI am running a wine store, and I am looking for a machine learning model that can help me classify the quality of wine based on some given features.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify wine quality using a pre-trained Scikit-learn model.",
                    "tool_description": "Use a Scikit-learn classification model to predict the quality of wines based on input tabular features. The model is pre-trained on wine datasets and can be loaded from the Hugging Face Hub.",
                    "expected_response": "An array of predicted quality labels, where each element corresponds to the predicted class for one wine sample."
                }
            ],
            "extracted_arguments": {
                "X": "A pandas DataFrame containing the input features for predicting wine quality, with each row representing a wine sample and each column representing a feature."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_438",
        "query": "Our delivery drones need to detect and avoid obstacles while flying. Develop a solution for them to detect objects in their path.",
        "instruction": "Given an `object detection` task, retrieve tools that implement computer vision techniques to analyze visual inputs and accurately identify and locate objects in an environment, aiding in the development of obstacle detection solutions for drone navigation.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YolosForObjectDetection.from_pretrained('hustvl/yolos-small')",
                    "api_arguments": {
                        "model_name": "hustvl/yolos-small"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": {
                        "import": [
                            "from transformers import YolosFeatureExtractor, YolosForObjectDetection",
                            "from PIL import Image",
                            "import requests"
                        ],
                        "url": "http://images.cocodataset.org/val2017/000000039769.jpg",
                        "image": "Image.open(requests.get(url, stream=True).raw)",
                        "feature_extractor": "YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')",
                        "model": "YolosForObjectDetection.from_pretrained('hustvl/yolos-small')",
                        "inputs": "feature_extractor(images=image, return_tensors='pt')",
                        "outputs": "model(**inputs)",
                        "logits": "outputs.logits",
                        "bboxes": "outputs.pred_boxes"
                    },
                    "performance": {
                        "dataset": "COCO 2017 validation",
                        "accuracy": "36.1 AP"
                    },
                    "description": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).",
                    "id": "huggingface_api_210",
                    "name": "hustvl/yolos-small"
                },
                "id": "gorilla_huggingface_tool_206",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 1: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 2: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 3: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 4: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 5: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 6: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 7: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 8: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 9: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 10: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 11: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 12: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 13: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 14: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 16: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 17: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "Tool 18: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 19: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums."
        ],
        "tool_view": {
            "description": "A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image to run object detection on. Can be a URL pointing to an online image or a local file path on the system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns a structured object representing the results of object detection performed on the input image. The response typically includes the following components:\n\n1. **Detections / Results Array**  \n   - The main content is a list of detection results, where each element corresponds to one detected object in the image.  \n   - Each detection usually contains information such as:\n     - **Bounding Box (`boxes`)**: The coordinates of the detected object, often represented as `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.\n     - **Class / Label (`class` or `label`)**: The category of the detected object, e.g., \"forklift\" or \"person\".\n     - **Confidence Score (`confidence` or `score`)**: A numeric value indicating the model’s certainty about the detection.\n     - **Optional Additional Attributes**: Depending on the implementation, this may include class probabilities, object tracking IDs, or mask segmentation information.\n\n2. **Image-Level Metadata**  \n   - Some responses may include metadata related to the processed image, such as:\n     - Image dimensions (width, height)\n     - The path or URL of the input image\n     - Number of detections in the image\n\n3. **Post-Processing / Rendering Support**  \n   - The response is compatible with rendering utilities like `render_result`, which can visually overlay the detected bounding boxes and labels onto the image.  \n   - This implies the bounding boxes and labels are structured in a way that can be directly interpreted for visualization.\n\nOverall, the response is a comprehensive object that provides both raw detection data and structured information suitable for visualization, further analysis, or filtering based on confidence or class.",
            "examples": [
                {
                    "user_intent": "I want to detect forklifts in an image from a URL.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "https://example.com/forklift_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect forklifts in a local image file.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "/path/to/local/forklift_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Run forklift detection with custom confidence and IoU thresholds.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "https://example.com/forklift_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 1: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 2: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 3: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 4: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 5: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 6: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 7: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 8: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 9: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 10: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 11: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 12: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 13: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 14: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 16: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 17: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\nTool 18: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 19: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image to run object detection on. Can be a URL pointing to an online image or a local file path on the system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns a structured object representing the results of object detection performed on the input image. The response typically includes the following components:\\n\\n1. **Detections / Results Array**  \\n   - The main content is a list of detection results, where each element corresponds to one detected object in the image.  \\n   - Each detection usually contains information such as:\\n     - **Bounding Box (`boxes`)**: The coordinates of the detected object, often represented as `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.\\n     - **Class / Label (`class` or `label`)**: The category of the detected object, e.g., \\\"forklift\\\" or \\\"person\\\".\\n     - **Confidence Score (`confidence` or `score`)**: A numeric value indicating the model\\u2019s certainty about the detection.\\n     - **Optional Additional Attributes**: Depending on the implementation, this may include class probabilities, object tracking IDs, or mask segmentation information.\\n\\n2. **Image-Level Metadata**  \\n   - Some responses may include metadata related to the processed image, such as:\\n     - Image dimensions (width, height)\\n     - The path or URL of the input image\\n     - Number of detections in the image\\n\\n3. **Post-Processing / Rendering Support**  \\n   - The response is compatible with rendering utilities like `render_result`, which can visually overlay the detected bounding boxes and labels onto the image.  \\n   - This implies the bounding boxes and labels are structured in a way that can be directly interpreted for visualization.\\n\\nOverall, the response is a comprehensive object that provides both raw detection data and structured information suitable for visualization, further analysis, or filtering based on confidence or class.\", \"examples\": [{\"user_intent\": \"I want to detect forklifts in an image from a URL.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"https://example.com/forklift_image.jpg\"}}}, {\"user_intent\": \"Detect forklifts in a local image file.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"/path/to/local/forklift_image.jpg\"}}}, {\"user_intent\": \"Run forklift detection with custom confidence and IoU thresholds.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"https://example.com/forklift_image.jpg\"}}}]}\n\n# User Query\nOur delivery drones need to detect and avoid obstacles while flying. Develop a solution for them to detect objects in their path.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect objects in the drone's path during flight.",
                    "tool_description": "Use an object detection model to identify and localize obstacles in images captured by the drone's camera in real-time or from pre-recorded footage.",
                    "expected_response": "A collection of detected objects in the form of bounding boxes, object labels, and confidence scores."
                }
            ],
            "extracted_arguments": {
                "image": "The input image from the drone's camera feed, which can be a URL or a local file path."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_15",
        "query": "I want to analyze the text and images in a document and extract answers to questions based on the content.",
        "instruction": "Given a `multimodal document analysis` task, retrieve tools that analyze both text and images within a document to extract answers to questions, utilizing advanced machine learning models designed for document question answering.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')",
                    "api_arguments": {
                        "question": "string",
                        "context": "string"
                    },
                    "python_environment_requirements": {
                        "transformers": ">=4.0.0"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A LayoutLMv2 model for document question answering.",
                    "id": "huggingface_api_129",
                    "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023"
                },
                "id": "gorilla_huggingface_tool_128",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 1: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 2: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 3: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 4: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 5: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 6: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 7: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 8: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 9: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 10: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 11: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 12: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 14: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 15: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 16: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 17: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 18: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 19: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2."
        ],
        "tool_view": {
            "description": "A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "parameters": {
                "api_call": {
                    "type": "string",
                    "description": "The API call to load the pretrained model using the Hugging Face Transformers library. This is used to initialize the model for document question answering.",
                    "required": true,
                    "example": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')"
                },
                "python_environment_requirements": {
                    "type": "object",
                    "description": "A dictionary specifying the required Python packages and their versions to run the model. In this case, the model requires the 'transformers' package version 4.11.0 or higher.",
                    "required": true,
                    "example": {
                        "transformers": ">=4.11.0"
                    }
                }
            },
            "response": "The tool produces responses that contain answers extracted from documents or images based on a user-provided question. In principle, the response structure includes the following components:\n\n1. **Answer Text**: The primary content of the response, representing the text extracted from the document that best answers the input question. This is the main output the model is designed to provide.  \n\n2. **Answer Coordinates / Bounding Box (optional)**: For multimodal inputs where layout information is considered, the response may include spatial coordinates indicating the location of the answer within the document image. This typically consists of bounding box coordinates (e.g., x_min, y_min, x_max, y_max).  \n\n3. **Confidence Score (optional)**: Some implementations may provide a numerical score indicating the model’s confidence in the predicted answer. This helps assess the reliability of the extracted answer.  \n\n4. **Contextual Information (optional)**: Depending on the model’s configuration, the response could include additional context or surrounding text extracted from the document to support or clarify the answer.  \n\nOverall, the response is a structured representation of the answer to a question posed over a document, combining textual content with optional layout and confidence metadata to fully capture multimodal document understanding.",
            "examples": [
                {
                    "user_intent": "I want to extract information from an image document using a pre-trained model.",
                    "tool_call": {
                        "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Can I use this model to answer questions based on a document image?",
                    "tool_call": {
                        "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "How can I use the LayoutLM model to answer questions from a document image with layout and text?",
                    "tool_call": {
                        "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023",
                        "arguments": {}
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 1: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 2: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 3: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 4: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 5: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 6: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 7: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 8: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 9: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 10: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 11: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 12: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 14: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 15: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 16: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 17: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 18: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 19: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\", \"parameters\": {\"api_call\": {\"type\": \"string\", \"description\": \"The API call to load the pretrained model using the Hugging Face Transformers library. This is used to initialize the model for document question answering.\", \"required\": true, \"example\": \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\"}, \"python_environment_requirements\": {\"type\": \"object\", \"description\": \"A dictionary specifying the required Python packages and their versions to run the model. In this case, the model requires the 'transformers' package version 4.11.0 or higher.\", \"required\": true, \"example\": {\"transformers\": \">=4.11.0\"}}}, \"response\": \"The tool produces responses that contain answers extracted from documents or images based on a user-provided question. In principle, the response structure includes the following components:\\n\\n1. **Answer Text**: The primary content of the response, representing the text extracted from the document that best answers the input question. This is the main output the model is designed to provide.  \\n\\n2. **Answer Coordinates / Bounding Box (optional)**: For multimodal inputs where layout information is considered, the response may include spatial coordinates indicating the location of the answer within the document image. This typically consists of bounding box coordinates (e.g., x_min, y_min, x_max, y_max).  \\n\\n3. **Confidence Score (optional)**: Some implementations may provide a numerical score indicating the model\\u2019s confidence in the predicted answer. This helps assess the reliability of the extracted answer.  \\n\\n4. **Contextual Information (optional)**: Depending on the model\\u2019s configuration, the response could include additional context or surrounding text extracted from the document to support or clarify the answer.  \\n\\nOverall, the response is a structured representation of the answer to a question posed over a document, combining textual content with optional layout and confidence metadata to fully capture multimodal document understanding.\", \"examples\": [{\"user_intent\": \"I want to extract information from an image document using a pre-trained model.\", \"tool_call\": {\"name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\", \"arguments\": {}}}, {\"user_intent\": \"Can I use this model to answer questions based on a document image?\", \"tool_call\": {\"name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\", \"arguments\": {}}}, {\"user_intent\": \"How can I use the LayoutLM model to answer questions from a document image with layout and text?\", \"tool_call\": {\"name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\", \"arguments\": {}}}]}\n\n# User Query\nI want to analyze the text and images in a document and extract answers to questions based on the content.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract answers from a document using both text and image content.",
                    "tool_description": "A document question-answering model that processes document images and their layout information to extract precise answers from the content using a pretrained transformer model.",
                    "expected_response": "A structured response containing the answer text extracted from the document, optionally including answer coordinates within the document, confidence score, and contextual information."
                }
            ],
            "extracted_arguments": {
                "document_input": "The document content to be analyzed, which may include images and text, provided in a suitable format for the model.",
                "questions": "A set of text-based questions for which answers are to be extracted from the document."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_418",
        "query": "Extract the named entities from a given text snippet.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that process textual inputs to identify and classify entities within the text, based on categories such as names, dates, and locations, following the specifications of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english-ontonotes')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\ntagger.predict(sentence)\nprint(sentence)\nfor entity in sentence.get_spans('ner'):\n    print(entity)",
                    "performance": {
                        "dataset": "Ontonotes",
                        "accuracy": "89.27"
                    },
                    "description": "This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_417",
                    "name": "flair/ner-english-ontonotes"
                },
                "id": "gorilla_huggingface_tool_413",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 1: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 2: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 3: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 4: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 5: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 6: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 7: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 8: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 9: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 10: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 11: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 12: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 13: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 14: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 15: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 16: This tool provides a pretrained Hugging Face Transformers model for token-level entity extraction, enabling identification of named entities in text. It uses the DistilBERT architecture, was trained with AutoTrain, and achieves high accuracy on relevant datasets, making it suitable for natural language processing tasks that require precise entity recognition.",
            "Tool 17: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 18: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 19: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer."
        ],
        "tool_view": {
            "description": "A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pretrained token classification model to load for named entity recognition, specified by its model identifier.",
                    "required": true,
                    "example": "d4data/biomedical-ner-all"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.",
                    "required": true,
                    "example": "d4data/biomedical-ner-all"
                },
                "aggregation_strategy": {
                    "type": "string",
                    "description": "The strategy to aggregate token-level predictions into entity-level results. Common strategies include 'simple', 'first', 'average', etc.",
                    "required": true,
                    "example": "simple"
                }
            },
            "response": "The tool produces a **Named Entity Recognition (NER) response** for a given input text. The response is typically structured as a sequence of entities detected in the text, with each entity containing several key pieces of information:\n\n1. **Entity text or span** – The exact substring from the input text that has been recognized as an entity.  \n2. **Entity type or label** – The category or class of the entity, corresponding to one of the 107 biomedical entity types the model was trained to recognize. Examples include medical conditions, medications, procedures, or anatomical terms.  \n3. **Start and end positions** – Integer indices indicating the position of the entity text within the input string. The `start` index marks the first character of the entity, and the `end` index marks the position immediately after the last character of the entity.  \n4. **Score or confidence** – A numeric value representing the model’s confidence in the predicted entity label. This is usually a probability between 0 and 1.  \n\nBecause the tool is used with an **aggregation strategy of \"simple\"**, overlapping or subword tokens may be merged so that each entity in the response corresponds to a coherent, human-readable span rather than fragmented subword tokens.\n\nOverall, the response is a list of such entity dictionaries, providing a structured identification of biomedical entities present in the input text.",
            "examples": [
                {
                    "user_intent": "I want to extract all biomedical entities like diseases, medications, and procedures from a patient's clinical note.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "The patient was prescribed 5 mg of amlodipine daily and showed improvement in hypertension symptoms within 2 weeks."
                        }
                    }
                },
                {
                    "user_intent": "Analyze a set of clinical trial abstracts to automatically identify genes, proteins, and other biomedical entities.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "BRCA1 mutations are linked to a higher risk of breast cancer, and patients often undergo genetic counseling before starting therapy."
                        }
                    }
                },
                {
                    "user_intent": "Detect and highlight any medical procedures and conditions mentioned in a cardiology case report.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "After undergoing catheter ablation, the patient reported no recurrence of palpitations at the 6-month follow-up."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 1: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 2: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 3: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 4: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 5: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 6: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 7: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 8: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 9: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 10: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 11: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 12: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 13: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 14: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 15: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 16: This tool provides a pretrained Hugging Face Transformers model for token-level entity extraction, enabling identification of named entities in text. It uses the DistilBERT architecture, was trained with AutoTrain, and achieves high accuracy on relevant datasets, making it suitable for natural language processing tasks that require precise entity recognition.\nTool 17: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 18: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 19: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pretrained token classification model to load for named entity recognition, specified by its model identifier.\", \"required\": true, \"example\": \"d4data/biomedical-ner-all\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.\", \"required\": true, \"example\": \"d4data/biomedical-ner-all\"}, \"aggregation_strategy\": {\"type\": \"string\", \"description\": \"The strategy to aggregate token-level predictions into entity-level results. Common strategies include 'simple', 'first', 'average', etc.\", \"required\": true, \"example\": \"simple\"}}, \"response\": \"The tool produces a **Named Entity Recognition (NER) response** for a given input text. The response is typically structured as a sequence of entities detected in the text, with each entity containing several key pieces of information:\\n\\n1. **Entity text or span** \\u2013 The exact substring from the input text that has been recognized as an entity.  \\n2. **Entity type or label** \\u2013 The category or class of the entity, corresponding to one of the 107 biomedical entity types the model was trained to recognize. Examples include medical conditions, medications, procedures, or anatomical terms.  \\n3. **Start and end positions** \\u2013 Integer indices indicating the position of the entity text within the input string. The `start` index marks the first character of the entity, and the `end` index marks the position immediately after the last character of the entity.  \\n4. **Score or confidence** \\u2013 A numeric value representing the model\\u2019s confidence in the predicted entity label. This is usually a probability between 0 and 1.  \\n\\nBecause the tool is used with an **aggregation strategy of \\\"simple\\\"**, overlapping or subword tokens may be merged so that each entity in the response corresponds to a coherent, human-readable span rather than fragmented subword tokens.\\n\\nOverall, the response is a list of such entity dictionaries, providing a structured identification of biomedical entities present in the input text.\", \"examples\": [{\"user_intent\": \"I want to extract all biomedical entities like diseases, medications, and procedures from a patient's clinical note.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"The patient was prescribed 5 mg of amlodipine daily and showed improvement in hypertension symptoms within 2 weeks.\"}}}, {\"user_intent\": \"Analyze a set of clinical trial abstracts to automatically identify genes, proteins, and other biomedical entities.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"BRCA1 mutations are linked to a higher risk of breast cancer, and patients often undergo genetic counseling before starting therapy.\"}}}, {\"user_intent\": \"Detect and highlight any medical procedures and conditions mentioned in a cardiology case report.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"After undergoing catheter ablation, the patient reported no recurrence of palpitations at the 6-month follow-up.\"}}}]}\n\n# User Query\nExtract the named entities from a given text snippet.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract all named entities from a given text snippet.",
                    "tool_description": "Apply a Named Entity Recognition (NER) model to identify and classify entities such as persons, organizations, locations, and other predefined categories within the input text.",
                    "expected_response": "A structured list of detected entities, each containing the entity text, entity type or label, start and end character positions in the text, and a confidence score."
                }
            ],
            "extracted_arguments": {
                "text": "The input text snippet from which named entities are to be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_103",
        "query": "Use a zero-shot classifier to classify news headlines into three categories: sports, technology, and politics.",
        "instruction": "Given a `zero-shot classification` task, retrieve tools that leverage natural language processing capabilities to categorize text into specified categories without prior training, using a defined set of candidate labels to process the input query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')",
                    "api_arguments": [
                        "sent",
                        "candidate_labels"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\nsent = 'Apple just announced the newest iPhone X'\ncandidate_labels = ['technology', 'sports', 'politics']\nres = classifier(sent, candidate_labels)\nprint(res)",
                    "performance": {
                        "dataset": {
                            "SNLI-test": "91.64",
                            "MNLI_mismatched": "87.77"
                        }
                    },
                    "description": "This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks.",
                    "id": "huggingface_api_513",
                    "name": "cross-encoder/nli-deberta-v3-xsmall"
                },
                "id": "gorilla_huggingface_tool_509",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 1: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 2: This tool performs zero-shot classification using a Cross-Encoder model trained on the SNLI and MultiNLI datasets. It can classify text into predefined categories without requiring task-specific training, making it useful for categorizing text based on its content. The model can be accessed through the `zero-shot-classification` pipeline in the Transformers framework.",
            "Tool 3: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 4: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 5: A CLIP ViT-bigG/14 model designed for zero-shot image classification, allowing users to classify images into arbitrary categories without task-specific training. Built with the LAION-2B dataset, it is useful for research and exploratory studies in computer vision, enabling analysis of model behavior and performance across diverse visual concepts. The model achieves 80.1% zero-shot top-1 accuracy on ImageNet-1k.",
            "Tool 6: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.",
            "Tool 7: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 9: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 10: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 11: A multilingual zero-shot text classification tool that uses the XLM-RoBERTa large model fine-tuned on natural language inference data across 15 languages. It allows users to classify text into custom categories without prior task-specific training by providing candidate labels and an optional hypothesis template, making it useful for quickly categorizing text in multiple languages.",
            "Tool 12: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.",
            "Tool 13: A zero-shot text classification model that allows labeling text with arbitrary categories without task-specific training, using a distilled version of BART fine-tuned on MNLI for efficient and accurate natural language inference.",
            "Tool 14: Perform zero-shot text classification using the MobileBERT model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, enabling classification of text into user-defined categories without prior task-specific training.",
            "Tool 15: A distilled BART model designed for zero-shot text classification, allowing users to classify text into candidate labels without prior task-specific training, offering high accuracy on the MNLI benchmark.",
            "Tool 16: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.",
            "Tool 17: A CLIP ViT L/14 model trained on the LAION-2B English subset, designed for zero-shot image classification. It enables classification of images into arbitrary categories without task-specific training, making it useful for research and interdisciplinary studies of image understanding and model impact.",
            "Tool 18: The \"vicgalle/xlm-roberta-large-xnli-anli\" tool is a pre-trained XLM-RoBERTa model fine-tuned for zero-shot classification. It enables classifying text sequences into predefined categories without needing task-specific training data. This model, based on multiple natural language inference datasets, can be used to classify text in different languages, with high accuracy on datasets like XNLI. It is integrated into the Hugging Face Transformers framework and requires the \"transformers\" library in a Python environment.",
            "Tool 19: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template."
        ],
        "tool_view": {
            "description": "This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "parameters": {
                "sequence": {
                    "type": "string",
                    "description": "The input text sequence that you want to classify using the zero-shot model.",
                    "required": true,
                    "example": "El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo"
                },
                "candidate_labels": {
                    "type": "array",
                    "description": "A list of labels or categories that the model will consider when classifying the input sequence.",
                    "required": true,
                    "example": [
                        "cultura",
                        "sociedad",
                        "economia",
                        "salud",
                        "deportes"
                    ]
                },
                "hypothesis_template": {
                    "type": "string",
                    "description": "A template string used to frame the classification hypothesis, where '{}' will be replaced with each candidate label.",
                    "required": true,
                    "example": "Este ejemplo es {}."
                }
            },
            "response": "The tool performs zero-shot classification using a pre-trained Spanish BERT model fine-tuned on the XNLI dataset. The response from this tool typically includes the following components:\n\n1. **Labels**: A list of candidate labels that the model has evaluated as the most relevant or likely classifications for the input text.\n   \n2. **Scores**: A corresponding list of numeric scores indicating the model's confidence or probability for each label. These scores typically represent the model's prediction confidence in the range [0, 1] for each label.\n\n3. **Sequence**: The input sequence of text that was classified. This may be included to confirm the processed input.\n\n4. **Chosen Label(s)**: Depending on the configuration, the model might also return the label with the highest confidence as the primary classification.\n\nThe response will reflect the likelihood of each candidate label in relation to the given text and hypothesis template, allowing users to interpret the predicted classification for the sequence. The exact structure of the output depends on the specific implementation but will generally include these key components.",
            "examples": [
                {
                    "user_intent": "I want to classify this Spanish sentence into categories: 'El gobierno anunció nuevas medidas económicas para combatir la inflación.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "El gobierno anunció nuevas medidas económicas para combatir la inflación.",
                            "candidate_labels": [
                                "economia",
                                "sociedad",
                                "salud",
                                "cultura",
                                "deportes"
                            ],
                            "hypothesis_template": "Este texto trata sobre {}."
                        }
                    }
                },
                {
                    "user_intent": "Determine whether this Spanish sentence is about health, culture, or sports: 'El Maratón de la Ciudad atrajo a miles de corredores de todo el mundo.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "El Maratón de la Ciudad atrajo a miles de corredores de todo el mundo.",
                            "candidate_labels": [
                                "deportes",
                                "salud",
                                "cultura",
                                "economia",
                                "sociedad"
                            ],
                            "hypothesis_template": "Este texto se refiere a {}."
                        }
                    }
                },
                {
                    "user_intent": "Classify this Spanish review about a museum visit: 'La exposición de arte contemporáneo fue fascinante y educativa.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "La exposición de arte contemporáneo fue fascinante y educativa.",
                            "candidate_labels": [
                                "cultura",
                                "sociedad",
                                "economia",
                                "deportes",
                                "salud"
                            ],
                            "hypothesis_template": "Este texto es sobre {}."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 1: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 2: This tool performs zero-shot classification using a Cross-Encoder model trained on the SNLI and MultiNLI datasets. It can classify text into predefined categories without requiring task-specific training, making it useful for categorizing text based on its content. The model can be accessed through the `zero-shot-classification` pipeline in the Transformers framework.\nTool 3: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 4: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 5: A CLIP ViT-bigG/14 model designed for zero-shot image classification, allowing users to classify images into arbitrary categories without task-specific training. Built with the LAION-2B dataset, it is useful for research and exploratory studies in computer vision, enabling analysis of model behavior and performance across diverse visual concepts. The model achieves 80.1% zero-shot top-1 accuracy on ImageNet-1k.\nTool 6: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.\nTool 7: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 9: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 10: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 11: A multilingual zero-shot text classification tool that uses the XLM-RoBERTa large model fine-tuned on natural language inference data across 15 languages. It allows users to classify text into custom categories without prior task-specific training by providing candidate labels and an optional hypothesis template, making it useful for quickly categorizing text in multiple languages.\nTool 12: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\nTool 13: A zero-shot text classification model that allows labeling text with arbitrary categories without task-specific training, using a distilled version of BART fine-tuned on MNLI for efficient and accurate natural language inference.\nTool 14: Perform zero-shot text classification using the MobileBERT model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, enabling classification of text into user-defined categories without prior task-specific training.\nTool 15: A distilled BART model designed for zero-shot text classification, allowing users to classify text into candidate labels without prior task-specific training, offering high accuracy on the MNLI benchmark.\nTool 16: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\nTool 17: A CLIP ViT L/14 model trained on the LAION-2B English subset, designed for zero-shot image classification. It enables classification of images into arbitrary categories without task-specific training, making it useful for research and interdisciplinary studies of image understanding and model impact.\nTool 18: The \"vicgalle/xlm-roberta-large-xnli-anli\" tool is a pre-trained XLM-RoBERTa model fine-tuned for zero-shot classification. It enables classifying text sequences into predefined categories without needing task-specific training data. This model, based on multiple natural language inference datasets, can be used to classify text in different languages, with high accuracy on datasets like XNLI. It is integrated into the Hugging Face Transformers framework and requires the \"transformers\" library in a Python environment.\nTool 19: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\", \"parameters\": {\"sequence\": {\"type\": \"string\", \"description\": \"The input text sequence that you want to classify using the zero-shot model.\", \"required\": true, \"example\": \"El autor se perfila, a los 50 a\\u00f1os de su muerte, como uno de los grandes de su siglo\"}, \"candidate_labels\": {\"type\": \"array\", \"description\": \"A list of labels or categories that the model will consider when classifying the input sequence.\", \"required\": true, \"example\": [\"cultura\", \"sociedad\", \"economia\", \"salud\", \"deportes\"]}, \"hypothesis_template\": {\"type\": \"string\", \"description\": \"A template string used to frame the classification hypothesis, where '{}' will be replaced with each candidate label.\", \"required\": true, \"example\": \"Este ejemplo es {}.\"}}, \"response\": \"The tool performs zero-shot classification using a pre-trained Spanish BERT model fine-tuned on the XNLI dataset. The response from this tool typically includes the following components:\\n\\n1. **Labels**: A list of candidate labels that the model has evaluated as the most relevant or likely classifications for the input text.\\n   \\n2. **Scores**: A corresponding list of numeric scores indicating the model's confidence or probability for each label. These scores typically represent the model's prediction confidence in the range [0, 1] for each label.\\n\\n3. **Sequence**: The input sequence of text that was classified. This may be included to confirm the processed input.\\n\\n4. **Chosen Label(s)**: Depending on the configuration, the model might also return the label with the highest confidence as the primary classification.\\n\\nThe response will reflect the likelihood of each candidate label in relation to the given text and hypothesis template, allowing users to interpret the predicted classification for the sequence. The exact structure of the output depends on the specific implementation but will generally include these key components.\", \"examples\": [{\"user_intent\": \"I want to classify this Spanish sentence into categories: 'El gobierno anunci\\u00f3 nuevas medidas econ\\u00f3micas para combatir la inflaci\\u00f3n.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"El gobierno anunci\\u00f3 nuevas medidas econ\\u00f3micas para combatir la inflaci\\u00f3n.\", \"candidate_labels\": [\"economia\", \"sociedad\", \"salud\", \"cultura\", \"deportes\"], \"hypothesis_template\": \"Este texto trata sobre {}.\"}}}, {\"user_intent\": \"Determine whether this Spanish sentence is about health, culture, or sports: 'El Marat\\u00f3n de la Ciudad atrajo a miles de corredores de todo el mundo.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"El Marat\\u00f3n de la Ciudad atrajo a miles de corredores de todo el mundo.\", \"candidate_labels\": [\"deportes\", \"salud\", \"cultura\", \"economia\", \"sociedad\"], \"hypothesis_template\": \"Este texto se refiere a {}.\"}}}, {\"user_intent\": \"Classify this Spanish review about a museum visit: 'La exposici\\u00f3n de arte contempor\\u00e1neo fue fascinante y educativa.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"La exposici\\u00f3n de arte contempor\\u00e1neo fue fascinante y educativa.\", \"candidate_labels\": [\"cultura\", \"sociedad\", \"economia\", \"deportes\", \"salud\"], \"hypothesis_template\": \"Este texto es sobre {}.\"}}}]}\n\n# User Query\nUse a zero-shot classifier to classify news headlines into three categories: sports, technology, and politics.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify news headlines into sports, technology, and politics using zero-shot classification.",
                    "tool_description": "Perform zero-shot text classification on input news headlines by leveraging a pre-trained model to assign them to predefined categories without task-specific training.",
                    "expected_response": "A set of labels and corresponding confidence scores for each headline, indicating the most likely category (sports, technology, or politics) for each input."
                }
            ],
            "extracted_arguments": {
                "sequence": "The news headline text to be classified, as a string.",
                "candidate_labels": "A list of categories to classify the text into (sports, technology, politics), as an array of strings.",
                "hypothesis_template": "A template string used to frame the classification hypothesis, where '{}' is replaced with each candidate label."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_474",
        "query": "I'm working on a project that requires generating high-quality images of faces for a set of characters in a video game.",
        "instruction": "Given an `image generation` task, retrieve tools that perform high-quality image synthesis, particularly focusing on generating facial images by implementing advanced image generation models to produce visual representations of characters.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')",
                    "api_arguments": [
                        "model_id"
                    ],
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "!pip install diffusers\nfrom diffusers import DiffusionPipeline\nmodel_id = CompVis/ldm-celebahq-256\npipeline = DiffusionPipeline.from_pretrained(model_id)\nimage = pipeline(num_inference_steps=200)[sample]\nimage[0].save(ldm_generated_image.png)",
                    "performance": {
                        "dataset": "CelebA-HQ",
                        "accuracy": "N/A"
                    },
                    "description": "Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.",
                    "id": "huggingface_api_291",
                    "name": "CompVis/ldm-celebahq-256"
                },
                "id": "gorilla_huggingface_tool_287",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 1: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 2: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.",
            "Tool 3: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 4: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.",
            "Tool 5: A ConvNeXt image classification model pretrained on ImageNet-1k, designed for classifying images, extracting feature maps, and generating image embeddings, suitable for computer vision tasks requiring high-quality visual representations.",
            "Tool 6: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 7: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 8: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 9: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 10: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 11: This tool uses Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images, trained on the unconditional CIFAR10 dataset. It supports various noise schedulers like DDPM, DDIM, and PNDM, and can be used through the Hugging Face Transformers framework. The model delivers impressive performance with an Inception score of 9.46 and an FID score of 3.17, making it suitable for generating realistic images in computer vision tasks.",
            "Tool 12: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.",
            "Tool 13: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 14: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 15: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 16: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 17: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 18: This tool provides a fine-tuned video classification model, `sayakpaul/videomae-base-finetuned-ucf101-subset`, based on the Hugging Face Transformers framework. It allows users to perform video classification using pre-trained models, with customizable training parameters such as learning rate, batch size, optimizer, and training steps. The model is optimized for UCF101 dataset subsets, achieving an accuracy of 86.45% on an unknown evaluation set. It is compatible with Python environments that include libraries like Transformers, PyTorch, and Datasets.",
            "Tool 19: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration."
        ],
        "tool_view": {
            "description": "Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "parameters": {
                "model_id": {
                    "type": "string",
                    "description": "The identifier of the pretrained Latent Diffusion Model (LDM) to load from Hugging Face. This determines which model weights and configuration will be used for image generation.",
                    "required": true,
                    "example": "CompVis/ldm-celebahq-256"
                }
            },
            "response": "The tool produces a generated image as its response. The response structure can be described as follows:\n\n- **Primary Output**: A single image or a batch of images generated unconditionally by the diffusion model. Each image is represented as an in-memory image object compatible with standard image handling libraries (e.g., PIL Image in Python).  \n\n- **Optional Metadata**: Depending on the pipeline implementation, the response may include additional information such as:\n  - **`sample` or `images` field**: A list or array of generated images.\n  - **Generation parameters**: Information about the inference settings used, such as `num_inference_steps` or random seed values.\n  \n- **Semantic Role**: The output image(s) represent the model’s unconditional synthesis of realistic human faces, consistent with the training distribution (CelebA-HQ dataset). No textual prompt or conditional input is required for generation.  \n\nIn summary, the tool’s response is fundamentally a high-resolution image object (or a collection thereof), optionally accompanied by metadata describing the generation process, intended for saving, display, or further processing.",
            "examples": [
                {
                    "user_intent": "Generate a high-resolution realistic portrait of a person using AI.",
                    "tool_call": {
                        "name": "DiffusionPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "CompVis/ldm-celebahq-256"
                        }
                    }
                },
                {
                    "user_intent": "I want to create a batch of AI-generated celebrity-style images for testing a facial recognition model.",
                    "tool_call": {
                        "name": "DiffusionPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "CompVis/ldm-celebahq-256"
                        }
                    }
                },
                {
                    "user_intent": "Generate a realistic face image for use in an artwork project without providing any initial image or prompt.",
                    "tool_call": {
                        "name": "DiffusionPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "CompVis/ldm-celebahq-256"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 1: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 2: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.\nTool 3: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 4: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.\nTool 5: A ConvNeXt image classification model pretrained on ImageNet-1k, designed for classifying images, extracting feature maps, and generating image embeddings, suitable for computer vision tasks requiring high-quality visual representations.\nTool 6: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 7: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 8: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 9: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 10: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 11: This tool uses Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images, trained on the unconditional CIFAR10 dataset. It supports various noise schedulers like DDPM, DDIM, and PNDM, and can be used through the Hugging Face Transformers framework. The model delivers impressive performance with an Inception score of 9.46 and an FID score of 3.17, making it suitable for generating realistic images in computer vision tasks.\nTool 12: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.\nTool 13: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 14: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 15: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 16: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 17: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 18: This tool provides a fine-tuned video classification model, `sayakpaul/videomae-base-finetuned-ucf101-subset`, based on the Hugging Face Transformers framework. It allows users to perform video classification using pre-trained models, with customizable training parameters such as learning rate, batch size, optimizer, and training steps. The model is optimized for UCF101 dataset subsets, achieving an accuracy of 86.45% on an unknown evaluation set. It is compatible with Python environments that include libraries like Transformers, PyTorch, and Datasets.\nTool 19: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\", \"parameters\": {\"model_id\": {\"type\": \"string\", \"description\": \"The identifier of the pretrained Latent Diffusion Model (LDM) to load from Hugging Face. This determines which model weights and configuration will be used for image generation.\", \"required\": true, \"example\": \"CompVis/ldm-celebahq-256\"}}, \"response\": \"The tool produces a generated image as its response. The response structure can be described as follows:\\n\\n- **Primary Output**: A single image or a batch of images generated unconditionally by the diffusion model. Each image is represented as an in-memory image object compatible with standard image handling libraries (e.g., PIL Image in Python).  \\n\\n- **Optional Metadata**: Depending on the pipeline implementation, the response may include additional information such as:\\n  - **`sample` or `images` field**: A list or array of generated images.\\n  - **Generation parameters**: Information about the inference settings used, such as `num_inference_steps` or random seed values.\\n  \\n- **Semantic Role**: The output image(s) represent the model\\u2019s unconditional synthesis of realistic human faces, consistent with the training distribution (CelebA-HQ dataset). No textual prompt or conditional input is required for generation.  \\n\\nIn summary, the tool\\u2019s response is fundamentally a high-resolution image object (or a collection thereof), optionally accompanied by metadata describing the generation process, intended for saving, display, or further processing.\", \"examples\": [{\"user_intent\": \"Generate a high-resolution realistic portrait of a person using AI.\", \"tool_call\": {\"name\": \"DiffusionPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"CompVis/ldm-celebahq-256\"}}}, {\"user_intent\": \"I want to create a batch of AI-generated celebrity-style images for testing a facial recognition model.\", \"tool_call\": {\"name\": \"DiffusionPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"CompVis/ldm-celebahq-256\"}}}, {\"user_intent\": \"Generate a realistic face image for use in an artwork project without providing any initial image or prompt.\", \"tool_call\": {\"name\": \"DiffusionPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"CompVis/ldm-celebahq-256\"}}}]}\n\n# User Query\nI'm working on a project that requires generating high-quality images of faces for a set of characters in a video game.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate high-quality, realistic images of human faces for my video game characters.",
                    "tool_description": "Use a Latent Diffusion Model (LDM) trained on the CelebA-HQ dataset to produce high-resolution, unconditional images of human faces, suitable for character design and visualization.",
                    "expected_response": "A collection of high-resolution image objects representing realistic human faces, optionally with metadata about generation parameters."
                }
            ],
            "extracted_arguments": {
                "model_id": "The identifier of the pretrained Latent Diffusion Model (string) to load for generating human face images."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_458",
        "query": "People in my company need an automatic solution to categorize videos based on their content. The system should be able to recognize the main theme of a video with high accuracy.",
        "instruction": "Given a `video categorization` task, retrieve tools that utilize video classification capabilities to accurately recognize and categorize the main themes of videos, processing video inputs to deliver highly reliable classification results.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')",
                    "api_arguments": [
                        "video"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\ninputs = processor(video, return_tensors='pt')\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-400",
                        "accuracy": {
                            "top-1": 79.4,
                            "top-5": 94.1
                        }
                    },
                    "description": "VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_328",
                    "name": "MCG-NJU/videomae-base-short-finetuned-kinetics"
                },
                "id": "gorilla_huggingface_tool_324",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 1: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.",
            "Tool 2: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.",
            "Tool 3: This is a fine-tuned video classification model based on MCG-NJU/videomae-base, designed for classifying videos. It utilizes the Hugging Face Transformers framework and is suitable for tasks that involve categorizing video content, achieving an accuracy of 89.68% on an unknown dataset.",
            "Tool 4: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 5: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 6: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 7: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 8: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 9: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 10: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.",
            "Tool 11: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 12: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 13: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 14: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 15: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 16: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 17: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 18: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 19: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "parameters": {
                "learning_rate": {
                    "type": "number",
                    "description": "The learning rate used during training. Controls the step size for weight updates in the optimizer.",
                    "required": true,
                    "example": 5e-05
                },
                "train_batch_size": {
                    "type": "number",
                    "description": "The number of samples processed together in one forward/backward pass during training.",
                    "required": true,
                    "example": 8
                },
                "eval_batch_size": {
                    "type": "number",
                    "description": "The number of samples processed together in one forward pass during evaluation.",
                    "required": true,
                    "example": 8
                },
                "seed": {
                    "type": "number",
                    "description": "A random seed used for initializing the model and data shuffling, ensuring reproducibility of results.",
                    "required": true,
                    "example": 42
                },
                "optimizer": {
                    "type": "string",
                    "description": "The optimizer algorithm used for training, including additional parameters such as beta values and epsilon.",
                    "required": true,
                    "example": "Adam with betas=(0.9,0.999) and epsilon=1e-08"
                },
                "lr_scheduler_type": {
                    "type": "string",
                    "description": "The type of learning rate scheduler used to adjust the learning rate during training.",
                    "required": true,
                    "example": "linear"
                },
                "lr_scheduler_warmup_ratio": {
                    "type": "number",
                    "description": "The fraction of total training steps used for learning rate warmup, where the learning rate gradually increases from 0 to the initial learning rate.",
                    "required": true,
                    "example": 0.1
                },
                "training_steps": {
                    "type": "number",
                    "description": "The total number of training steps to perform.",
                    "required": true,
                    "example": 111
                }
            },
            "response": "The tool performs video classification using a pretrained Hugging Face Transformers model. Its response would typically consist of a structured prediction output for a given input video, including the following components:  \n\n1. **Predicted Label(s)** – One or more class names representing the predicted action(s) or category(ies) in the input video. This is usually a string or list of strings corresponding to the dataset classes the model was trained on.  \n\n2. **Confidence Scores / Probabilities** – A numeric value or array indicating the model’s confidence in each predicted class. This is often represented as a probability distribution over all possible classes.  \n\n3. **Optional Metadata** – Depending on the implementation, the response may include additional information such as the mapping of class indices to human-readable labels, processing timestamps, or intermediate features used for prediction.  \n\nOverall, the response is structured to convey both the predicted video category and the associated confidence, enabling downstream applications to interpret and act on the classification results.",
            "examples": [
                {
                    "user_intent": "I want to use a pre-trained video classification model for a project. Can you help me with setting it up?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset"
                        }
                    }
                },
                {
                    "user_intent": "I'm training a custom video classification model using the Hugging Face framework. What parameters should I use for fine-tuning?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset",
                            "learning_rate": 5e-05,
                            "train_batch_size": 8,
                            "eval_batch_size": 8,
                            "seed": 42,
                            "optimizer": "Adam with betas=(0.9,0.999) and epsilon=1e-08",
                            "lr_scheduler_type": "linear",
                            "lr_scheduler_warmup_ratio": 0.1,
                            "training_steps": 111
                        }
                    }
                },
                {
                    "user_intent": "Can I get the performance details of the video classification model 'sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset'?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 1: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.\nTool 2: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.\nTool 3: This is a fine-tuned video classification model based on MCG-NJU/videomae-base, designed for classifying videos. It utilizes the Hugging Face Transformers framework and is suitable for tasks that involve categorizing video content, achieving an accuracy of 89.68% on an unknown dataset.\nTool 4: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 5: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 6: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 7: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 8: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 9: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 10: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.\nTool 11: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 12: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 13: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 14: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 15: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 16: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 17: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 18: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 19: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\", \"parameters\": {\"learning_rate\": {\"type\": \"number\", \"description\": \"The learning rate used during training. Controls the step size for weight updates in the optimizer.\", \"required\": true, \"example\": 5e-05}, \"train_batch_size\": {\"type\": \"number\", \"description\": \"The number of samples processed together in one forward/backward pass during training.\", \"required\": true, \"example\": 8}, \"eval_batch_size\": {\"type\": \"number\", \"description\": \"The number of samples processed together in one forward pass during evaluation.\", \"required\": true, \"example\": 8}, \"seed\": {\"type\": \"number\", \"description\": \"A random seed used for initializing the model and data shuffling, ensuring reproducibility of results.\", \"required\": true, \"example\": 42}, \"optimizer\": {\"type\": \"string\", \"description\": \"The optimizer algorithm used for training, including additional parameters such as beta values and epsilon.\", \"required\": true, \"example\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\"}, \"lr_scheduler_type\": {\"type\": \"string\", \"description\": \"The type of learning rate scheduler used to adjust the learning rate during training.\", \"required\": true, \"example\": \"linear\"}, \"lr_scheduler_warmup_ratio\": {\"type\": \"number\", \"description\": \"The fraction of total training steps used for learning rate warmup, where the learning rate gradually increases from 0 to the initial learning rate.\", \"required\": true, \"example\": 0.1}, \"training_steps\": {\"type\": \"number\", \"description\": \"The total number of training steps to perform.\", \"required\": true, \"example\": 111}}, \"response\": \"The tool performs video classification using a pretrained Hugging Face Transformers model. Its response would typically consist of a structured prediction output for a given input video, including the following components:  \\n\\n1. **Predicted Label(s)** \\u2013 One or more class names representing the predicted action(s) or category(ies) in the input video. This is usually a string or list of strings corresponding to the dataset classes the model was trained on.  \\n\\n2. **Confidence Scores / Probabilities** \\u2013 A numeric value or array indicating the model\\u2019s confidence in each predicted class. This is often represented as a probability distribution over all possible classes.  \\n\\n3. **Optional Metadata** \\u2013 Depending on the implementation, the response may include additional information such as the mapping of class indices to human-readable labels, processing timestamps, or intermediate features used for prediction.  \\n\\nOverall, the response is structured to convey both the predicted video category and the associated confidence, enabling downstream applications to interpret and act on the classification results.\", \"examples\": [{\"user_intent\": \"I want to use a pre-trained video classification model for a project. Can you help me with setting it up?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\"}}}, {\"user_intent\": \"I'm training a custom video classification model using the Hugging Face framework. What parameters should I use for fine-tuning?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\", \"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 111}}}, {\"user_intent\": \"Can I get the performance details of the video classification model 'sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset'?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\"}}}]}\n\n# User Query\nPeople in my company need an automatic solution to categorize videos based on their content. The system should be able to recognize the main theme of a video with high accuracy.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to categorize videos automatically based on their content.",
                    "tool_description": "Use a pre-trained video classification model to process input videos and predict their categories based on pretrained features from large-scale video datasets.",
                    "expected_response": "A collection of predicted labels representing the main themes or categories of the input videos, along with associated confidence scores."
                }
            ],
            "extracted_arguments": {
                "video_content": "The video file or video data to be classified.",
                "accuracy_requirement": "The required level of accuracy for the video classification process."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_465",
        "query": "Our customer is a fitness platform. We need to analyze workout videos for offering customized workout plans.",
        "instruction": "Given a `video analysis` task, retrieve tools that perform video classification by processing video inputs to extract relevant features for generating customized workout plans aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "MCG-NJU/videomae-base-short"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers"
                        ]
                    },
                    "example_code": "from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short)\npixel_values = processor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss",
                    "performance": {
                        "dataset": "Kinetics-400",
                        "accuracy": "Not provided"
                    },
                    "description": "VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.",
                    "id": "huggingface_api_326",
                    "name": "MCG-NJU/videomae-base-short"
                },
                "id": "gorilla_huggingface_tool_322",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 1: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 2: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 3: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 4: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 5: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 7: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 9: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 10: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 11: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 12: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 13: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 14: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 15: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 16: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 17: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 18: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.",
            "Tool 19: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions."
        ],
        "tool_view": {
            "description": "Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A textual description or prompt that guides the video generation process. This should describe the content or scene to be generated in the video.",
                    "required": true,
                    "example": "(redshift style) spider man is skiing"
                },
                "video_length": {
                    "type": "int",
                    "description": "The length of the generated video, measured in seconds.",
                    "required": true,
                    "example": 8
                },
                "height": {
                    "type": "int",
                    "description": "The height (in pixels) of the generated video frames.",
                    "required": true,
                    "example": 512
                },
                "width": {
                    "type": "int",
                    "description": "The width (in pixels) of the generated video frames.",
                    "required": true,
                    "example": 512
                },
                "num_inference_steps": {
                    "type": "int",
                    "description": "The number of inference steps to be used during the video generation process. A higher value generally leads to more detailed results, but requires more computation time.",
                    "required": true,
                    "example": 50
                },
                "guidance_scale": {
                    "type": "float",
                    "description": "A float value that controls how strongly the model should follow the prompt. Higher values encourage the model to stick closer to the prompt, while lower values allow more creativity in the generated video.",
                    "required": true,
                    "example": 7.5
                }
            },
            "response": "The tool produces a structured response containing the video(s) generated from the provided textual prompt. The response structure can be described as follows:\n\n- **videos**: This is the primary component of the response. It contains one or more video outputs generated by the model according to the input parameters. Each video is represented as a multi-dimensional array or a video object suitable for further processing, visualization, or saving to disk. The video length, resolution, and quality are determined by the corresponding input parameters: `video_length`, `height`, `width`, `num_inference_steps`, and `guidance_scale`.\n\nIn principle, the `videos` field encapsulates all generated video frames in temporal order, allowing downstream code to render, save, or manipulate the video. There are no additional metadata fields explicitly defined in the documentation, but the video content is aligned with the semantic content described in the `prompt`.  \n\nOverall, the response is essentially a container for the generated video data, structured to be directly compatible with video handling utilities like `save_videos_grid`.",
            "examples": [
                {
                    "user_intent": "I want to generate a short video of a person skiing in the redshift style.",
                    "tool_call": {
                        "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))",
                        "api_arguments": {
                            "prompt": "redshift style man skiing",
                            "video_length": 6,
                            "height": 512,
                            "width": 512,
                            "num_inference_steps": 50,
                            "guidance_scale": 7.5
                        }
                    }
                },
                {
                    "user_intent": "Create a video where Spider-Man is skiing in the redshift style for 10 seconds.",
                    "tool_call": {
                        "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))",
                        "api_arguments": {
                            "prompt": "(redshift style) spider man is skiing",
                            "video_length": 10,
                            "height": 512,
                            "width": 512,
                            "num_inference_steps": 75,
                            "guidance_scale": 8.0
                        }
                    }
                },
                {
                    "user_intent": "Generate a video of a man skiing in the redshift style with a higher guidance scale.",
                    "tool_call": {
                        "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))",
                        "api_arguments": {
                            "prompt": "redshift style man skiing",
                            "video_length": 8,
                            "height": 512,
                            "width": 512,
                            "num_inference_steps": 60,
                            "guidance_scale": 9.0
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 1: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 2: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 3: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 4: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 5: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 7: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 9: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 10: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 11: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 12: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 13: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 14: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 15: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 16: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 17: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 18: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.\nTool 19: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A textual description or prompt that guides the video generation process. This should describe the content or scene to be generated in the video.\", \"required\": true, \"example\": \"(redshift style) spider man is skiing\"}, \"video_length\": {\"type\": \"int\", \"description\": \"The length of the generated video, measured in seconds.\", \"required\": true, \"example\": 8}, \"height\": {\"type\": \"int\", \"description\": \"The height (in pixels) of the generated video frames.\", \"required\": true, \"example\": 512}, \"width\": {\"type\": \"int\", \"description\": \"The width (in pixels) of the generated video frames.\", \"required\": true, \"example\": 512}, \"num_inference_steps\": {\"type\": \"int\", \"description\": \"The number of inference steps to be used during the video generation process. A higher value generally leads to more detailed results, but requires more computation time.\", \"required\": true, \"example\": 50}, \"guidance_scale\": {\"type\": \"float\", \"description\": \"A float value that controls how strongly the model should follow the prompt. Higher values encourage the model to stick closer to the prompt, while lower values allow more creativity in the generated video.\", \"required\": true, \"example\": 7.5}}, \"response\": \"The tool produces a structured response containing the video(s) generated from the provided textual prompt. The response structure can be described as follows:\\n\\n- **videos**: This is the primary component of the response. It contains one or more video outputs generated by the model according to the input parameters. Each video is represented as a multi-dimensional array or a video object suitable for further processing, visualization, or saving to disk. The video length, resolution, and quality are determined by the corresponding input parameters: `video_length`, `height`, `width`, `num_inference_steps`, and `guidance_scale`.\\n\\nIn principle, the `videos` field encapsulates all generated video frames in temporal order, allowing downstream code to render, save, or manipulate the video. There are no additional metadata fields explicitly defined in the documentation, but the video content is aligned with the semantic content described in the `prompt`.  \\n\\nOverall, the response is essentially a container for the generated video data, structured to be directly compatible with video handling utilities like `save_videos_grid`.\", \"examples\": [{\"user_intent\": \"I want to generate a short video of a person skiing in the redshift style.\", \"tool_call\": {\"api_call\": \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", \"api_arguments\": {\"prompt\": \"redshift style man skiing\", \"video_length\": 6, \"height\": 512, \"width\": 512, \"num_inference_steps\": 50, \"guidance_scale\": 7.5}}}, {\"user_intent\": \"Create a video where Spider-Man is skiing in the redshift style for 10 seconds.\", \"tool_call\": {\"api_call\": \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", \"api_arguments\": {\"prompt\": \"(redshift style) spider man is skiing\", \"video_length\": 10, \"height\": 512, \"width\": 512, \"num_inference_steps\": 75, \"guidance_scale\": 8.0}}}, {\"user_intent\": \"Generate a video of a man skiing in the redshift style with a higher guidance scale.\", \"tool_call\": {\"api_call\": \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", \"api_arguments\": {\"prompt\": \"redshift style man skiing\", \"video_length\": 8, \"height\": 512, \"width\": 512, \"num_inference_steps\": 60, \"guidance_scale\": 9.0}}}]}\n\n# User Query\nOur customer is a fitness platform. We need to analyze workout videos for offering customized workout plans.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze workout videos to identify and classify different exercises performed.",
                    "tool_description": "Use a pre-trained video classification model, such as TimeSformer, to analyze the temporal and spatial patterns in workout videos and predict the corresponding exercise categories.",
                    "expected_response": "A list of predicted action classes for each video, indicating the types of exercises performed."
                },
                {
                    "user_intent": "Generate insights from classified workout videos to create customized workout plans for users.",
                    "tool_description": "Process the classified exercise data from videos to provide personalized recommendations and structured workout plans tailored to individual user needs.",
                    "expected_response": "Structured workout plans or recommendations based on the detected exercises in the videos."
                }
            ],
            "extracted_arguments": {
                "video_input": "The video file or stream containing the user's workout session, used as input for classification.",
                "exercise_categories": "The predefined set of exercise classes that the model can predict from video data."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_284",
        "query": "We have a dataset with customer reviews of our financial service app, and we'd like to analyze their sentiment.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that can assess customer feedback by processing text data to determine the sentiment expressed in reviews, especially focusing on financial service contexts.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "financial-sentiment-analysis",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')",
                    "performance": {
                        "dataset": "Financial PhraseBank",
                        "accuracy": "Not provided"
                    },
                    "description": "FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.",
                    "id": "huggingface_api_378",
                    "name": "ProsusAI/finbert"
                },
                "id": "gorilla_huggingface_tool_374",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 1: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 2: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 3: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 4: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 5: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 6: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 7: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 8: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 9: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 10: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 11: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 12: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 13: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.",
            "Tool 14: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 15: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 16: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 17: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 18: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications."
        ],
        "tool_view": {
            "description": "FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text to be analyzed for financial sentiment. This is the main content that the model will process to determine sentiment.",
                    "required": true,
                    "example": "The stock market showed a significant increase in value today."
                }
            },
            "response": "The tool returns a sentiment classification for a given financial text input. The response is typically structured as a collection of one or more prediction objects, each representing the model’s analysis of the input text. Each prediction object usually contains the following components:\n\n1. **`label`** – A string indicating the predicted sentiment of the text. In the context of financial sentiment analysis, this could represent categories such as positive, negative, or neutral sentiment.  \n\n2. **`score`** – A numerical value representing the model’s confidence in the predicted label. This is often a probability or normalized confidence score between 0 and 1, indicating how strongly the model believes the input belongs to the predicted sentiment class.  \n\nThe response may include multiple prediction objects if the tool is configured to process batched inputs, with each object corresponding to one text entry. In essence, the output provides both a categorical sentiment interpretation and a confidence measure for each piece of financial text analyzed.",
            "examples": [
                {
                    "user_intent": "I want to check if this earnings report excerpt is positive, negative, or neutral: 'The company reported a 15% increase in revenue compared to last year.'",
                    "tool_call": {
                        "name": "financial-sentiment-analysis",
                        "arguments": {
                            "text": "The company reported a 15% increase in revenue compared to last year."
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of this financial news headline: 'Tech startup faces unexpected regulatory hurdles, causing investor concern.'",
                    "tool_call": {
                        "name": "financial-sentiment-analysis",
                        "arguments": {
                            "text": "Tech startup faces unexpected regulatory hurdles, causing investor concern."
                        }
                    }
                },
                {
                    "user_intent": "Classify the sentiment of this analyst commentary: 'Despite recent market volatility, the long-term growth prospects for the sector remain strong.'",
                    "tool_call": {
                        "name": "financial-sentiment-analysis",
                        "arguments": {
                            "text": "Despite recent market volatility, the long-term growth prospects for the sector remain strong."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 1: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 2: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 3: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 4: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 5: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 6: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 7: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 8: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 9: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 10: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 11: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 12: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 13: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.\nTool 14: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 15: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 16: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 17: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 18: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text to be analyzed for financial sentiment. This is the main content that the model will process to determine sentiment.\", \"required\": true, \"example\": \"The stock market showed a significant increase in value today.\"}}, \"response\": \"The tool returns a sentiment classification for a given financial text input. The response is typically structured as a collection of one or more prediction objects, each representing the model\\u2019s analysis of the input text. Each prediction object usually contains the following components:\\n\\n1. **`label`** \\u2013 A string indicating the predicted sentiment of the text. In the context of financial sentiment analysis, this could represent categories such as positive, negative, or neutral sentiment.  \\n\\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence in the predicted label. This is often a probability or normalized confidence score between 0 and 1, indicating how strongly the model believes the input belongs to the predicted sentiment class.  \\n\\nThe response may include multiple prediction objects if the tool is configured to process batched inputs, with each object corresponding to one text entry. In essence, the output provides both a categorical sentiment interpretation and a confidence measure for each piece of financial text analyzed.\", \"examples\": [{\"user_intent\": \"I want to check if this earnings report excerpt is positive, negative, or neutral: 'The company reported a 15% increase in revenue compared to last year.'\", \"tool_call\": {\"name\": \"financial-sentiment-analysis\", \"arguments\": {\"text\": \"The company reported a 15% increase in revenue compared to last year.\"}}}, {\"user_intent\": \"Analyze the sentiment of this financial news headline: 'Tech startup faces unexpected regulatory hurdles, causing investor concern.'\", \"tool_call\": {\"name\": \"financial-sentiment-analysis\", \"arguments\": {\"text\": \"Tech startup faces unexpected regulatory hurdles, causing investor concern.\"}}}, {\"user_intent\": \"Classify the sentiment of this analyst commentary: 'Despite recent market volatility, the long-term growth prospects for the sector remain strong.'\", \"tool_call\": {\"name\": \"financial-sentiment-analysis\", \"arguments\": {\"text\": \"Despite recent market volatility, the long-term growth prospects for the sector remain strong.\"}}}]}\n\n# User Query\nWe have a dataset with customer reviews of our financial service app, and we'd like to analyze their sentiment.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze the sentiment of customer reviews related to our financial service app.",
                    "tool_description": "Analyze a set of customer reviews to classify their sentiment (positive, negative, or neutral) based on financial-related content.",
                    "expected_response": "A collection of sentiment prediction objects, each containing a sentiment label (positive, negative, or neutral) and a confidence score for each review."
                }
            ],
            "extracted_arguments": {
                "text": "The customer review text to be analyzed for sentiment classification."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_356",
        "query": "### Instruction: We are a retailer in Spain. We need to understand the sentiment of our Spanish-speaking customers when they provide feedback.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that utilize text classification techniques to accurately assess and interpret the sentiment expressed in Spanish-language customer feedback.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')",
                    "api_arguments": [
                        "model_path"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nmodel_path = cardiffnlp/twitter-xlm-roberta-base-sentiment\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\nsentiment_task(T'estimo!)",
                    "performance": {
                        "dataset": "Twitter",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).",
                    "id": "huggingface_api_373",
                    "name": "cardiffnlp/twitter-xlm-roberta-base-sentiment"
                },
                "id": "gorilla_huggingface_tool_369",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 2: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 4: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 5: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 6: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 7: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 8: Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.",
            "Tool 9: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 10: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 11: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 12: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 13: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.",
            "Tool 14: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 15: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 16: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 17: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 18: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 19: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 2: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 4: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 5: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 6: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 7: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 8: Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.\nTool 9: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 10: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 11: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 12: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 13: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\nTool 14: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 15: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 16: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 17: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 18: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 19: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\n### Instruction: We are a retailer in Spain. We need to understand the sentiment of our Spanish-speaking customers when they provide feedback.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the sentiment of customer feedback written in Spanish.",
                    "tool_description": "Perform sentiment analysis on Spanish-language text, predicting the sentiment expressed in customer feedback to evaluate satisfaction or opinion.",
                    "expected_response": "A structured sentiment result indicating positive, negative, or neutral sentiment with associated confidence scores."
                }
            ],
            "extracted_arguments": {
                "text": "The Spanish-language customer feedback text whose sentiment needs to be analyzed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_253",
        "query": "We have customer reviews of various software products. We want to extract company names in those reviews.",
        "instruction": "Given a `entity extraction` task, retrieve tools that utilize natural language processing to identify and extract specific entities, such as company names, from textual data based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Entity Extraction",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548')",
                    "api_arguments": {
                        "inputs": "I love AutoTrain"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTokenClassification, AutoTokenizer"
                    },
                    "example_code": "from transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "ismail-lucifer011/autotrain-data-company_all",
                        "accuracy": 0.9979930566588805
                    },
                    "description": "A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.",
                    "id": "huggingface_api_414",
                    "name": "903429548"
                },
                "id": "gorilla_huggingface_tool_410",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 3: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 4: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 5: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 6: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 7: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 8: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 9: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 10: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 11: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.",
            "Tool 12: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 13: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 14: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 15: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 16: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 17: TAPEX is a pretrained model designed for table-based question answering, enabling users to query structured data in natural language and receive accurate answers. It leverages a neural SQL executor learned during pretraining to reason over tables, making it effective for tasks that require understanding and extracting information from tabular datasets, such as those in the WikiSQL benchmark.",
            "Tool 18: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.",
            "Tool 19: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 3: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 4: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 5: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 6: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 7: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 8: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 9: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 10: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 11: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.\nTool 12: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 13: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 14: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 15: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 16: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 17: TAPEX is a pretrained model designed for table-based question answering, enabling users to query structured data in natural language and receive accurate answers. It leverages a neural SQL executor learned during pretraining to reason over tables, making it effective for tasks that require understanding and extracting information from tabular datasets, such as those in the WikiSQL benchmark.\nTool 18: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.\nTool 19: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nWe have customer reviews of various software products. We want to extract company names in those reviews.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract company names mentioned in customer reviews of software products.",
                    "tool_description": "Identify and extract named entities corresponding to company names from unstructured text using a token classification NER model.",
                    "expected_response": "A list of company names detected in each review, typically as an array of entity strings or objects with entity type and position."
                }
            ],
            "extracted_arguments": {
                "text": "The customer review text containing potential company mentions, provided as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_178",
        "query": "I need assistance in predicting carbon emissions of a city based on historical data. Use the dataset provided to predict future carbon emissions.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools that perform regression analysis using historical data to forecast future carbon emissions, ensuring compatibility with tabular data inputs and leveraging trained models for accurate predictions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "pcoloc/autotrain-data-mikrotik-7-7",
                        "accuracy": {
                            "Loss": 48.213,
                            "R2": 0.654,
                            "MSE": 2324.518,
                            "MAE": 32.634,
                            "RMSLE": 0.586
                        }
                    },
                    "description": "A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.",
                    "id": "huggingface_api_874",
                    "name": "pcoloc/autotrain-mikrotik-7-7-1860563588"
                },
                "id": "gorilla_huggingface_tool_867",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 1: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 2: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 3: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 4: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 5: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 6: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 7: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 8: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 9: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 10: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 11: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 12: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 13: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 14: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 15: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 16: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 17: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 18: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 19: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%."
        ],
        "tool_view": {
            "description": "A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "parameters": {
                "data": {
                    "type": "object",
                    "description": "The input data to be used for making predictions with the regression model. The data should include the features required by the model, formatted as a pandas DataFrame.",
                    "required": true,
                    "example": "{ 'feature1': [value1, value2], 'feature2': [value3, value4] }"
                }
            },
            "response": "The tool does not explicitly define a response format. However, based on its functionality, a reasonable and typical response from this tool would contain:\n\n1. **Predictions**: The primary output of the model is likely a set of predicted carbon emissions values, based on the input data provided to the model. These predictions would typically be in numerical form (e.g., in grams of CO2 emissions).\n\n2. **Performance Metrics**: The response may include key performance metrics of the model, such as:\n   - **Loss**: A numerical value representing the loss function (in this case, a regression loss measure).\n   - **R2**: The R-squared score, which indicates how well the model explains the variance in the target variable. In this case, a low R2 (0.013) suggests the model's predictive power is weak.\n   - **MSE**: Mean Squared Error, which measures the average of the squared differences between predicted and actual values.\n   - **MAE**: Mean Absolute Error, indicating the average absolute difference between predicted and actual values.\n   - **RMSLE**: Root Mean Squared Logarithmic Error, a metric sensitive to large errors.\n\n3. **Model Information**: The model's name, in this case, \"kochetkovIT/autotrain-ironhack-49741119788,\" could also be part of the response for identification or logging purposes.\n\nIn summary, the response would likely include:\n- The **predictions** from the model based on input data.\n- A set of **performance metrics** that evaluate the quality of the model.\n- **Model identification** (name) for reference or tracking.\n\nThe response would be structured to convey both the results of the prediction task and relevant performance data for assessing model accuracy and suitability.",
            "examples": [
                {
                    "user_intent": "Predict the carbon emissions for my new vehicle dataset using the trained model.",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "vehicle_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV of industrial factory parameters, can you estimate the associated carbon emissions?",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "factory_emissions_input.csv"
                        }
                    }
                },
                {
                    "user_intent": "Use the AutoTrain regression model to predict carbon emissions for a set of building energy consumption data.",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "building_energy.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 1: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 2: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 3: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 4: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 5: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 6: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 7: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 8: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 9: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 10: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 11: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 12: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 13: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 14: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 15: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 16: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 17: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 18: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 19: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\", \"parameters\": {\"data\": {\"type\": \"object\", \"description\": \"The input data to be used for making predictions with the regression model. The data should include the features required by the model, formatted as a pandas DataFrame.\", \"required\": true, \"example\": \"{ 'feature1': [value1, value2], 'feature2': [value3, value4] }\"}}, \"response\": \"The tool does not explicitly define a response format. However, based on its functionality, a reasonable and typical response from this tool would contain:\\n\\n1. **Predictions**: The primary output of the model is likely a set of predicted carbon emissions values, based on the input data provided to the model. These predictions would typically be in numerical form (e.g., in grams of CO2 emissions).\\n\\n2. **Performance Metrics**: The response may include key performance metrics of the model, such as:\\n   - **Loss**: A numerical value representing the loss function (in this case, a regression loss measure).\\n   - **R2**: The R-squared score, which indicates how well the model explains the variance in the target variable. In this case, a low R2 (0.013) suggests the model's predictive power is weak.\\n   - **MSE**: Mean Squared Error, which measures the average of the squared differences between predicted and actual values.\\n   - **MAE**: Mean Absolute Error, indicating the average absolute difference between predicted and actual values.\\n   - **RMSLE**: Root Mean Squared Logarithmic Error, a metric sensitive to large errors.\\n\\n3. **Model Information**: The model's name, in this case, \\\"kochetkovIT/autotrain-ironhack-49741119788,\\\" could also be part of the response for identification or logging purposes.\\n\\nIn summary, the response would likely include:\\n- The **predictions** from the model based on input data.\\n- A set of **performance metrics** that evaluate the quality of the model.\\n- **Model identification** (name) for reference or tracking.\\n\\nThe response would be structured to convey both the results of the prediction task and relevant performance data for assessing model accuracy and suitability.\", \"examples\": [{\"user_intent\": \"Predict the carbon emissions for my new vehicle dataset using the trained model.\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"vehicle_data.csv\"}}}, {\"user_intent\": \"I have a CSV of industrial factory parameters, can you estimate the associated carbon emissions?\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"factory_emissions_input.csv\"}}}, {\"user_intent\": \"Use the AutoTrain regression model to predict carbon emissions for a set of building energy consumption data.\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"building_energy.csv\"}}}]}\n\n# User Query\nI need assistance in predicting carbon emissions of a city based on historical data. Use the dataset provided to predict future carbon emissions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Predict the carbon emissions for a city using historical data.",
                    "tool_description": "Use a tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured historical data, enabling estimation of future emissions based on input features.",
                    "expected_response": "A set of predicted carbon emissions values for the city, optionally accompanied by model performance metrics such as R², MSE, MAE, and RMSLE, along with model identification information."
                }
            ],
            "extracted_arguments": {
                "data": "A structured dataset containing historical features relevant for predicting carbon emissions, formatted as a pandas DataFrame or CSV input."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_148",
        "query": "We have some clients asking for estimating CO2 emissions based on their historic data, which is in a CSV file.",
        "instruction": "Given an `emission estimation` task, retrieve tools that can process tabular data inputs, typically from CSV files, to estimate CO2 emissions using regression models, ensuring compatibility with the task's requirements and data format.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "pcoloc/autotrain-data-dragino-7-7",
                        "accuracy": {
                            "Loss": 84.433,
                            "R2": 0.54,
                            "MSE": 7129.004,
                            "MAE": 62.626,
                            "RMSLE": 0.418
                        }
                    },
                    "description": "A tabular regression model trained using AutoTrain for predicting carbon emissions. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and has an R2 score of 0.540.",
                    "id": "huggingface_api_879",
                    "name": "autotrain-dragino-7-7-1860763606"
                },
                "id": "gorilla_huggingface_tool_872",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "Tool 1: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.",
            "Tool 2: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 3: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 4: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 5: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 6: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 7: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.",
            "Tool 8: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 9: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 10: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 11: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 13: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 14: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 15: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.",
            "Tool 16: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 17: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 18: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 19: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features."
        ],
        "tool_view": {
            "description": "This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "parameters": {
                "data": {
                    "type": "object",
                    "description": "The input data for making predictions with the trained model. This should be a structured dataset, typically in the form of a DataFrame containing features selected for prediction.",
                    "required": true,
                    "example": {
                        "feat_1": 1.5,
                        "feat_2": 2.3,
                        "feat_3": 4.1
                    }
                }
            },
            "response": "The tool response provides the predicted carbon emissions (in grams) based on input data passed to the model. The structure and meaning of the response components are as follows:\n\n1. **Predictions**: \n   - The tool will return a set of predicted values corresponding to the carbon emissions (CO2 in grams) for each data point provided as input.\n   - These predictions are derived from applying the trained model (loaded using `joblib.load('model.joblib')`) to the input features, which are typically tabular data formatted in a specific way (with columns corresponding to features of the model).\n\n2. **Metadata (Optional)**: \n   - While the response itself primarily consists of the predictions, additional metadata can be included that provides context on the model's performance and usage. This can include:\n     - **Accuracy Metrics**: These might include various performance metrics like Loss, R2 (R-squared), MSE (Mean Squared Error), MAE (Mean Absolute Error), and RMSLE (Root Mean Squared Logarithmic Error), as indicated in the performance section. These metrics help evaluate the model's prediction quality on the provided dataset.\n     - **Model Information**: Details about the model, such as its name (`pcoloc/autotrain-600-dragino-1839063122`) and the dataset it was trained on (`pcoloc/autotrain-data-600-dragino`), might be included for reference.\n\n3. **Structure**:\n   - **Predictions**: This will likely be a list or array of numerical values, where each element corresponds to the predicted carbon emission for a single row of input data.\n   - **Optional Metadata (Accuracy Metrics)**: A dictionary or object containing performance evaluation metrics like:\n     - **Loss**: A numerical value representing the loss function used in model training (lower is better).\n     - **R2**: A measure of how well the model explains the variance of the data (values closer to 1 are better).\n     - **MSE**: Mean Squared Error, which quantifies the average squared difference between predicted and actual values.\n     - **MAE**: Mean Absolute Error, which quantifies the average absolute difference between predicted and actual values.\n     - **RMSLE**: Root Mean Squared Logarithmic Error, which focuses on the relative error in the logarithmic scale, useful for data with wide ranges.\n\n4. **Additional Information**:\n   - The response may also include an indication of any issues or errors if the model or data is incompatible or fails during the prediction process, although this is not explicitly outlined in the documentation.\n\nIn summary, the response contains:\n- A list or array of predicted carbon emissions values (in grams) for each input sample.\n- Optionally, a set of performance metrics summarizing the model's accuracy and predictive power.",
            "examples": [
                {
                    "user_intent": "Predict CO2 emissions for a new batch of industrial machines using the trained model.",
                    "tool_call": {
                        "name": "pcoloc/autotrain-600-dragino-1839063122",
                        "arguments": {
                            "data": "new_machines_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "Estimate carbon emissions for a factory dataset I have in a CSV file.",
                    "tool_call": {
                        "name": "pcoloc/autotrain-600-dragino-1839063122",
                        "arguments": {
                            "data": "factory_emissions.csv"
                        }
                    }
                },
                {
                    "user_intent": "Run the carbon emissions regression on the latest transportation fleet data.",
                    "tool_call": {
                        "name": "pcoloc/autotrain-600-dragino-1839063122",
                        "arguments": {
                            "data": "fleet_transport_data.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\nTool 1: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\nTool 2: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 3: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 4: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 5: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 6: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 7: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.\nTool 8: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 9: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 10: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 11: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 13: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 14: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 15: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.\nTool 16: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 17: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 18: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 19: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\", \"parameters\": {\"data\": {\"type\": \"object\", \"description\": \"The input data for making predictions with the trained model. This should be a structured dataset, typically in the form of a DataFrame containing features selected for prediction.\", \"required\": true, \"example\": {\"feat_1\": 1.5, \"feat_2\": 2.3, \"feat_3\": 4.1}}}, \"response\": \"The tool response provides the predicted carbon emissions (in grams) based on input data passed to the model. The structure and meaning of the response components are as follows:\\n\\n1. **Predictions**: \\n   - The tool will return a set of predicted values corresponding to the carbon emissions (CO2 in grams) for each data point provided as input.\\n   - These predictions are derived from applying the trained model (loaded using `joblib.load('model.joblib')`) to the input features, which are typically tabular data formatted in a specific way (with columns corresponding to features of the model).\\n\\n2. **Metadata (Optional)**: \\n   - While the response itself primarily consists of the predictions, additional metadata can be included that provides context on the model's performance and usage. This can include:\\n     - **Accuracy Metrics**: These might include various performance metrics like Loss, R2 (R-squared), MSE (Mean Squared Error), MAE (Mean Absolute Error), and RMSLE (Root Mean Squared Logarithmic Error), as indicated in the performance section. These metrics help evaluate the model's prediction quality on the provided dataset.\\n     - **Model Information**: Details about the model, such as its name (`pcoloc/autotrain-600-dragino-1839063122`) and the dataset it was trained on (`pcoloc/autotrain-data-600-dragino`), might be included for reference.\\n\\n3. **Structure**:\\n   - **Predictions**: This will likely be a list or array of numerical values, where each element corresponds to the predicted carbon emission for a single row of input data.\\n   - **Optional Metadata (Accuracy Metrics)**: A dictionary or object containing performance evaluation metrics like:\\n     - **Loss**: A numerical value representing the loss function used in model training (lower is better).\\n     - **R2**: A measure of how well the model explains the variance of the data (values closer to 1 are better).\\n     - **MSE**: Mean Squared Error, which quantifies the average squared difference between predicted and actual values.\\n     - **MAE**: Mean Absolute Error, which quantifies the average absolute difference between predicted and actual values.\\n     - **RMSLE**: Root Mean Squared Logarithmic Error, which focuses on the relative error in the logarithmic scale, useful for data with wide ranges.\\n\\n4. **Additional Information**:\\n   - The response may also include an indication of any issues or errors if the model or data is incompatible or fails during the prediction process, although this is not explicitly outlined in the documentation.\\n\\nIn summary, the response contains:\\n- A list or array of predicted carbon emissions values (in grams) for each input sample.\\n- Optionally, a set of performance metrics summarizing the model's accuracy and predictive power.\", \"examples\": [{\"user_intent\": \"Predict CO2 emissions for a new batch of industrial machines using the trained model.\", \"tool_call\": {\"name\": \"pcoloc/autotrain-600-dragino-1839063122\", \"arguments\": {\"data\": \"new_machines_data.csv\"}}}, {\"user_intent\": \"Estimate carbon emissions for a factory dataset I have in a CSV file.\", \"tool_call\": {\"name\": \"pcoloc/autotrain-600-dragino-1839063122\", \"arguments\": {\"data\": \"factory_emissions.csv\"}}}, {\"user_intent\": \"Run the carbon emissions regression on the latest transportation fleet data.\", \"tool_call\": {\"name\": \"pcoloc/autotrain-600-dragino-1839063122\", \"arguments\": {\"data\": \"fleet_transport_data.csv\"}}}]}\n\n# User Query\nWe have some clients asking for estimating CO2 emissions based on their historic data, which is in a CSV file.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Estimate carbon emissions based on historic data provided in a CSV file.",
                    "tool_description": "Use a trained regression model to predict CO2 emissions based on input features from a CSV file containing historic data.",
                    "expected_response": "A list or array of predicted carbon emissions (in grams) for each data point in the CSV file, along with optional metadata on model performance."
                }
            ],
            "extracted_arguments": {
                "data": "The structured historic data in CSV format containing the features needed for predicting CO2 emissions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_444",
        "query": "We are creating an AI assistant for banking clients. The customers should be able to talk to the bot to do various tasks. We need to have a meaningful dialogue with them.",
        "instruction": "Given a `conversational AI development` task, retrieve tools that enable natural language processing capabilities for generating meaningful dialogues, focusing on processing text-based inputs aligned with banking client interactions and returning contextually appropriate responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Conversational",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')",
                    "api_arguments": [
                        "instruction",
                        "knowledge",
                        "dialog"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\ndef generate(instruction, knowledge, dialog):\n if knowledge != '':\n knowledge = '[KNOWLEDGE] ' + knowledge\n dialog = ' EOS '.join(dialog)\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n return output",
                    "performance": {
                        "dataset": "Reddit discussion thread, instruction and knowledge grounded dialogs",
                        "accuracy": "N/A"
                    },
                    "description": "GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.",
                    "id": "huggingface_api_593",
                    "name": "microsoft/GODEL-v1_1-base-seq2seq"
                },
                "id": "gorilla_huggingface_tool_588",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 1: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 2: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 3: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 4: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 5: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 6: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 7: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 8: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 9: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 10: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 11: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 12: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 13: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.",
            "Tool 14: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 15: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 17: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 18: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.",
            "Tool 19: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors."
        ],
        "tool_view": {
            "description": "BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "parameters": {
                "message": {
                    "type": "string",
                    "description": "The input text message from the user that the chatbot will respond to.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a text-based conversational response generated by the BlenderBot-400M-distill model. The response structure is a single text string representing the model’s reply to the user-provided input message. Semantically, this response is intended to continue the dialogue in an open-domain conversational manner, reflecting understanding of the input message and generating coherent, contextually relevant text.  \n\nKey components and their meaning:  \n\n- **message input (api_arguments: 'message')**: The user-provided text that serves as the prompt or query for the chatbot.  \n- **response text**: A string generated by the model that constitutes the chatbot’s reply. This is the primary output and represents the model’s interpretation and continuation of the conversation.  \n- **conversational context (implicit, optional)**: If the tool maintains multi-turn context, the response may implicitly incorporate prior dialogue history to produce coherent and context-aware replies.  \n\nThe tool does not explicitly return structured metadata, confidence scores, or other auxiliary fields in its default usage; the response is solely the generated text output suitable for conversational interaction.",
            "examples": [
                {
                    "user_intent": "I want to have a casual chat and get some friendly advice about productivity.",
                    "tool_call": {
                        "name": "facebook/blenderbot-400M-distill",
                        "arguments": {
                            "message": "Hi! Do you have any tips for staying productive while working from home?"
                        }
                    }
                },
                {
                    "user_intent": "Ask a fun, general knowledge question to see how the chatbot responds.",
                    "tool_call": {
                        "name": "facebook/blenderbot-400M-distill",
                        "arguments": {
                            "message": "Can you tell me a fun fact about space?"
                        }
                    }
                },
                {
                    "user_intent": "Engage the chatbot in a light-hearted conversation about movies.",
                    "tool_call": {
                        "name": "facebook/blenderbot-400M-distill",
                        "arguments": {
                            "message": "What's your favorite movie genre, and why?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 1: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 2: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 3: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 4: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 5: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 6: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 7: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 8: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 9: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 10: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 11: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 12: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 13: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.\nTool 14: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 15: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 17: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 18: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.\nTool 19: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\", \"parameters\": {\"message\": {\"type\": \"string\", \"description\": \"The input text message from the user that the chatbot will respond to.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a text-based conversational response generated by the BlenderBot-400M-distill model. The response structure is a single text string representing the model\\u2019s reply to the user-provided input message. Semantically, this response is intended to continue the dialogue in an open-domain conversational manner, reflecting understanding of the input message and generating coherent, contextually relevant text.  \\n\\nKey components and their meaning:  \\n\\n- **message input (api_arguments: 'message')**: The user-provided text that serves as the prompt or query for the chatbot.  \\n- **response text**: A string generated by the model that constitutes the chatbot\\u2019s reply. This is the primary output and represents the model\\u2019s interpretation and continuation of the conversation.  \\n- **conversational context (implicit, optional)**: If the tool maintains multi-turn context, the response may implicitly incorporate prior dialogue history to produce coherent and context-aware replies.  \\n\\nThe tool does not explicitly return structured metadata, confidence scores, or other auxiliary fields in its default usage; the response is solely the generated text output suitable for conversational interaction.\", \"examples\": [{\"user_intent\": \"I want to have a casual chat and get some friendly advice about productivity.\", \"tool_call\": {\"name\": \"facebook/blenderbot-400M-distill\", \"arguments\": {\"message\": \"Hi! Do you have any tips for staying productive while working from home?\"}}}, {\"user_intent\": \"Ask a fun, general knowledge question to see how the chatbot responds.\", \"tool_call\": {\"name\": \"facebook/blenderbot-400M-distill\", \"arguments\": {\"message\": \"Can you tell me a fun fact about space?\"}}}, {\"user_intent\": \"Engage the chatbot in a light-hearted conversation about movies.\", \"tool_call\": {\"name\": \"facebook/blenderbot-400M-distill\", \"arguments\": {\"message\": \"What's your favorite movie genre, and why?\"}}}]}\n\n# User Query\nWe are creating an AI assistant for banking clients. The customers should be able to talk to the bot to do various tasks. We need to have a meaningful dialogue with them.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Engage banking clients in meaningful conversation and answer their queries.",
                    "tool_description": "Use a conversational AI model to generate context-aware text responses for client interactions in a banking domain, enabling multi-turn dialogue and coherent conversation flow.",
                    "expected_response": "A text-based reply from the AI assistant that is contextually relevant to the user's banking-related message, continuing the dialogue naturally."
                },
                {
                    "user_intent": "Provide guidance or perform simple banking tasks through dialogue with the client.",
                    "tool_description": "Leverage the AI assistant to interpret client messages and respond with informative or action-oriented text relevant to banking inquiries, such as account information, transactions, or advice.",
                    "expected_response": "A text-based response conveying banking information, guidance, or next steps, maintaining conversational clarity and client engagement."
                }
            ],
            "extracted_arguments": {
                "message": "The input text message from the banking client that the AI assistant will respond to."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_399",
        "query": "We are trying to create a solution for an HR department to predict whether a candidate would be a potential employee based on a list of background information.",
        "instruction": "Given a `candidate prediction` task, retrieve tools that perform binary classification by processing tabular data of background information to predict a candidate's potential as an employee, ensuring accuracy and alignment with the query's HR-focused requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Binary Classification",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas",
                        "json"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "rajistics/autotrain-data-Adult",
                        "accuracy": 0.8628221244500315
                    },
                    "description": "This model is trained for binary classification on the Adult dataset using AutoTrain. It is designed to predict CO2 emissions based on input features.",
                    "id": "huggingface_api_853",
                    "name": "abhishek/autotrain-adult-census-xgboost"
                },
                "id": "gorilla_huggingface_tool_838",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 1: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 2: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 3: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.",
            "Tool 4: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 5: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 6: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 7: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 8: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 9: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 10: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 11: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 12: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 13: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 14: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 15: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 16: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 17: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 18: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.",
            "Tool 19: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding."
        ],
        "tool_view": {
            "description": "A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "parameters": {
                "data": {
                    "type": "DataFrame",
                    "description": "A pandas DataFrame containing the input features for the Titanic survival prediction model. Columns should match the features specified in the model's configuration.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_age': [22, 38], 'feat_gender': [0, 1], 'feat_pclass': [3, 1]})"
                }
            },
            "response": "The tool produces a **binary classification output** for tabular data, indicating whether each input instance belongs to one of two classes (e.g., survived vs. did not survive). The response typically has the following structure and components:  \n\n1. **Predictions Array**:  \n   - A sequence (e.g., list, array, or Series) where each element corresponds to one row of the input data.  \n   - Each element represents the predicted class label for that row, encoded in a binary format (e.g., 0 or 1, False or True, depending on implementation).  \n\n2. **Optional Probabilities / Scores (if supported)**:  \n   - Some implementations may provide an array of probabilities alongside the predicted labels, representing the model’s confidence that the instance belongs to the positive class.  \n   - This would be structured as either a single probability per instance or a two-element vector for [negative_class_prob, positive_class_prob].  \n\n3. **Index Alignment**:  \n   - The predictions are aligned with the input data rows, preserving the original ordering of the dataset.  \n\nIn summary, the response is a structured output mapping each input row of tabular features to a predicted binary class, optionally including confidence scores for each prediction. This enables downstream tasks such as evaluation, thresholding, or further analysis.",
            "examples": [
                {
                    "user_intent": "Predict whether passengers survived the Titanic using a CSV file with passenger details.",
                    "tool_call": {
                        "name": "harithapliyal/autotrain-tatanic-survival-51030121311",
                        "arguments": {
                            "data": "titanic_passengers.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a dataset with age, sex, and ticket class of passengers. Check who survived.",
                    "tool_call": {
                        "name": "harithapliyal/autotrain-tatanic-survival-51030121311",
                        "arguments": {
                            "data": "passenger_info.csv"
                        }
                    }
                },
                {
                    "user_intent": "Classify a batch of Titanic passengers from an Excel export to see survival predictions.",
                    "tool_call": {
                        "name": "harithapliyal/autotrain-tatanic-survival-51030121311",
                        "arguments": {
                            "data": "batch_passengers.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 1: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 2: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 3: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.\nTool 4: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 5: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 6: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 7: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 8: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 9: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 10: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 11: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 12: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 13: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 14: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 15: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 16: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 17: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 18: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.\nTool 19: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\", \"parameters\": {\"data\": {\"type\": \"DataFrame\", \"description\": \"A pandas DataFrame containing the input features for the Titanic survival prediction model. Columns should match the features specified in the model's configuration.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_age': [22, 38], 'feat_gender': [0, 1], 'feat_pclass': [3, 1]})\"}}, \"response\": \"The tool produces a **binary classification output** for tabular data, indicating whether each input instance belongs to one of two classes (e.g., survived vs. did not survive). The response typically has the following structure and components:  \\n\\n1. **Predictions Array**:  \\n   - A sequence (e.g., list, array, or Series) where each element corresponds to one row of the input data.  \\n   - Each element represents the predicted class label for that row, encoded in a binary format (e.g., 0 or 1, False or True, depending on implementation).  \\n\\n2. **Optional Probabilities / Scores (if supported)**:  \\n   - Some implementations may provide an array of probabilities alongside the predicted labels, representing the model\\u2019s confidence that the instance belongs to the positive class.  \\n   - This would be structured as either a single probability per instance or a two-element vector for [negative_class_prob, positive_class_prob].  \\n\\n3. **Index Alignment**:  \\n   - The predictions are aligned with the input data rows, preserving the original ordering of the dataset.  \\n\\nIn summary, the response is a structured output mapping each input row of tabular features to a predicted binary class, optionally including confidence scores for each prediction. This enables downstream tasks such as evaluation, thresholding, or further analysis.\", \"examples\": [{\"user_intent\": \"Predict whether passengers survived the Titanic using a CSV file with passenger details.\", \"tool_call\": {\"name\": \"harithapliyal/autotrain-tatanic-survival-51030121311\", \"arguments\": {\"data\": \"titanic_passengers.csv\"}}}, {\"user_intent\": \"I have a dataset with age, sex, and ticket class of passengers. Check who survived.\", \"tool_call\": {\"name\": \"harithapliyal/autotrain-tatanic-survival-51030121311\", \"arguments\": {\"data\": \"passenger_info.csv\"}}}, {\"user_intent\": \"Classify a batch of Titanic passengers from an Excel export to see survival predictions.\", \"tool_call\": {\"name\": \"harithapliyal/autotrain-tatanic-survival-51030121311\", \"arguments\": {\"data\": \"batch_passengers.csv\"}}}]}\n\n# User Query\nWe are trying to create a solution for an HR department to predict whether a candidate would be a potential employee based on a list of background information.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Predict whether job candidates are likely to be suitable employees based on their background information.",
                    "tool_description": "Use a binary classification model trained on structured HR candidate data to predict potential employee suitability from features such as experience, education, skills, and other background attributes.",
                    "expected_response": "A binary classification output for each candidate, indicating suitability (e.g., suitable vs. not suitable), optionally including confidence scores."
                }
            ],
            "extracted_arguments": {
                "candidate_data": "A structured dataset (e.g., DataFrame) containing background information for each candidate, with columns representing relevant features such as experience, education, and skills."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_457",
        "query": "The marketing team needs different variations of a product image to use in advertising and promotional materials.",
        "instruction": "Given an `image variation generation` task, retrieve tools that utilize image processing techniques to create different variations of a provided product image for advertising and promotional purposes, ensuring they align with the team's specifications and creativity needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image Variations",
                    "api_call": "StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')",
                    "api_arguments": {
                        "revision": "v2.0"
                    },
                    "python_environment_requirements": "Diffusers >=0.8.0",
                    "example_code": "from diffusers import StableDiffusionImageVariationPipeline\nfrom PIL import Image\ndevice = cuda:0\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n lambdalabs/sd-image-variations-diffusers,\n revision=v2.0,\n)\nsd_pipe = sd_pipe.to(device)\nim = Image.open(path/to/image.jpg)\ntform = transforms.Compose([\n transforms.ToTensor(),\n transforms.Resize(\n  (224, 224),\n  interpolation=transforms.InterpolationMode.BICUBIC,\n  antialias=False,\n ),\n transforms.Normalize(\n  [0.48145466, 0.4578275, 0.40821073],\n  [0.26862954, 0.26130258, 0.27577711]),\n])\ninp = tform(im).to(device).unsqueeze(0)\nout = sd_pipe(inp, guidance_scale=3)\nout[images][0].save(result.jpg)",
                    "performance": {
                        "dataset": "ChristophSchuhmann/improved_aesthetics_6plus",
                        "accuracy": "N/A"
                    },
                    "description": "This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.",
                    "id": "huggingface_api_260",
                    "name": "lambdalabs/sd-image-variations-diffusers"
                },
                "id": "gorilla_huggingface_tool_256",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 1: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 2: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.",
            "Tool 3: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 4: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.",
            "Tool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 6: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image Transformer as an encoder and a text Transformer as a decoder, leveraging pre-trained weights from BEiT for the encoder and RoBERTa for the decoder. This model is suitable for extracting text from scanned documents or images, enabling automated text recognition and digitization of printed materials.",
            "Tool 7: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 8: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.",
            "Tool 9: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 10: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 11: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 12: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 13: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.",
            "Tool 14: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 15: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.",
            "Tool 16: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 17: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 18: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 19: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions."
        ],
        "tool_view": {
            "description": "This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "parameters": {
                "revision": {
                    "type": "string",
                    "description": "The specific version of the model to be used for image variations. This determines which version of the model is loaded for the process.",
                    "required": true,
                    "default": "v2.0",
                    "example": "v2.0"
                }
            },
            "response": "The tool produces one or more generated images that are variations of a given input image. The response is typically structured as an object or collection containing the generated images. Each component of the response has the following meaning:\n\n- **images**: A list or array of image objects representing the generated variations. Each element corresponds to a single image output created by the pipeline based on the input image and any additional parameters (e.g., guidance scale). These images are typically in a standard image format that can be saved or further processed, such as PIL Image objects.  \n\nThe primary semantic content of the response is the visual data of the image variations. Optional metadata (not explicitly defined in the documentation) could include information about generation parameters, but the core response is focused on returning the generated image objects.",
            "examples": [
                {
                    "user_intent": "I want to generate variations of an image using the Stable Diffusion model with the latest version.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                },
                {
                    "user_intent": "Create new variations of a given image using the Stable Diffusion image variation pipeline.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                },
                {
                    "user_intent": "Generate different styles of an input image with the fine-tuned Stable Diffusion model from Lambda Labs.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 1: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 2: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.\nTool 3: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 4: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.\nTool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 6: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image Transformer as an encoder and a text Transformer as a decoder, leveraging pre-trained weights from BEiT for the encoder and RoBERTa for the decoder. This model is suitable for extracting text from scanned documents or images, enabling automated text recognition and digitization of printed materials.\nTool 7: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 8: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.\nTool 9: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 10: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 11: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 12: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 13: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.\nTool 14: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 15: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.\nTool 16: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 17: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 18: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 19: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL\\u00b7E 2. The tool is based on the \\\"lambdalabs/sd-image-variations-diffusers\\\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\", \"parameters\": {\"revision\": {\"type\": \"string\", \"description\": \"The specific version of the model to be used for image variations. This determines which version of the model is loaded for the process.\", \"required\": true, \"default\": \"v2.0\", \"example\": \"v2.0\"}}, \"response\": \"The tool produces one or more generated images that are variations of a given input image. The response is typically structured as an object or collection containing the generated images. Each component of the response has the following meaning:\\n\\n- **images**: A list or array of image objects representing the generated variations. Each element corresponds to a single image output created by the pipeline based on the input image and any additional parameters (e.g., guidance scale). These images are typically in a standard image format that can be saved or further processed, such as PIL Image objects.  \\n\\nThe primary semantic content of the response is the visual data of the image variations. Optional metadata (not explicitly defined in the documentation) could include information about generation parameters, but the core response is focused on returning the generated image objects.\", \"examples\": [{\"user_intent\": \"I want to generate variations of an image using the Stable Diffusion model with the latest version.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}, {\"user_intent\": \"Create new variations of a given image using the Stable Diffusion image variation pipeline.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}, {\"user_intent\": \"Generate different styles of an input image with the fine-tuned Stable Diffusion model from Lambda Labs.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}]}\n\n# User Query\nThe marketing team needs different variations of a product image to use in advertising and promotional materials.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate different variations of a product image for marketing and promotional use.",
                    "tool_description": "Create multiple visual variations of a given image using a Stable Diffusion model fine-tuned to accept CLIP image embeddings, enabling diverse outputs suitable for advertising purposes.",
                    "expected_response": "A collection of generated image objects, each representing a unique variation of the original product image."
                }
            ],
            "extracted_arguments": {
                "revision": "The specific version of the Stable Diffusion image variation model to use for generating image variants."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_484",
        "query": "Our company is cooperating with a German partner. We have some materials in English, but need to translate them into German.",
        "instruction": "Given a `translation` task, retrieve tools that facilitate language conversion by processing text inputs in a source language to produce accurate translations in a target language, aligned with the query's linguistic requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Multilingual Sequence-to-Sequence",
                    "api_call": "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')",
                    "api_arguments": {
                        "src_lang": "en_XX",
                        "tgt_lang": "ro_RO"
                    },
                    "python_environment_requirements": {
                        "transformers": "MBartForConditionalGeneration, MBart50TokenizerFast"
                    },
                    "example_code": "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nmodel = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\ntokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50', src_lang='en_XX', tgt_lang='ro_RO')\nsrc_text = ' UN Chief Says There Is No Military Solution in Syria'\ntgt_text = 'Şeful ONU declară că nu există o soluţie militară în Siria'\nmodel_inputs = tokenizer(src_text, return_tensors='pt')\nwith tokenizer.as_target_tokenizer():\n labels = tokenizer(tgt_text, return_tensors='pt').input_ids\nmodel(**model_inputs, labels=labels)",
                    "performance": {
                        "dataset": "Multilingual Denoising Pretraining",
                        "accuracy": "Not specified"
                    },
                    "description": "mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the 'Multilingual Denoising Pretraining' objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.",
                    "id": "huggingface_api_645",
                    "name": "facebook/mbart-large-50"
                },
                "id": "gorilla_huggingface_tool_640",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 1: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.",
            "Tool 2: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 3: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 4: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.",
            "Tool 5: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 6: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 7: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 8: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 9: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 10: A German-to-Spanish translation tool that uses a transformer-based model trained on the OPUS dataset, providing accurate translations with preprocessing that includes normalization and SentencePiece tokenization.",
            "Tool 11: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 12: A German text summarization tool based on a fine-tuned T5 model, designed to generate concise summaries of input text using the Hugging Face Transformers framework.",
            "Tool 13: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 14: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.",
            "Tool 15: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 16: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 17: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.",
            "Tool 18: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 19: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio."
        ],
        "tool_view": {
            "description": "A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "parameters": {},
            "response": "The tool is designed for text-to-text generation, specifically English-to-German translation, using a model from the Hugging Face Transformers library.\n\n**Response Description:**\n\nThe response from this tool would consist of a translated German text corresponding to the input English text. The model accepts an English sentence as input and outputs its translation in German.\n\nKey components of the response:\n\n- **Input Text**: The original English text passed to the translation model.\n- **Translated Text**: The corresponding German translation generated by the model.\n\nWhile the exact structure of the response is not specified in the raw documentation, it would typically follow this general form:\n\n- **Output Format**: The translation response is typically a string containing the translated German text.\n- **Possible Additional Metadata**: Depending on the specific configuration or implementation, the response may include metadata such as:\n  - **Model Information**: Information about the model used (e.g., 'sshleifer/tiny-marian-en-de').\n  - **Translation Quality Metrics**: If applicable, some tools may return quality scores or confidence levels for the translation, though this is not explicitly stated in the documentation.\n\nThus, the expected output will primarily consist of the translated German text in response to the provided English input.",
            "examples": [
                {
                    "user_intent": "Translate the sentence 'The weather is nice today' from English to German.",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "The weather is nice today"
                        }
                    }
                },
                {
                    "user_intent": "I want to translate this paragraph into German: 'Artificial intelligence is transforming the world rapidly.'",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "Artificial intelligence is transforming the world rapidly."
                        }
                    }
                },
                {
                    "user_intent": "Convert this short English dialogue to German: 'Hello! How are you doing?'",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "Hello! How are you doing?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 1: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.\nTool 2: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 3: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 4: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.\nTool 5: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 6: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 7: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 8: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 9: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 10: A German-to-Spanish translation tool that uses a transformer-based model trained on the OPUS dataset, providing accurate translations with preprocessing that includes normalization and SentencePiece tokenization.\nTool 11: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 12: A German text summarization tool based on a fine-tuned T5 model, designed to generate concise summaries of input text using the Hugging Face Transformers framework.\nTool 13: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 14: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.\nTool 15: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 16: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 17: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.\nTool 18: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 19: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\", \"parameters\": {}, \"response\": \"The tool is designed for text-to-text generation, specifically English-to-German translation, using a model from the Hugging Face Transformers library.\\n\\n**Response Description:**\\n\\nThe response from this tool would consist of a translated German text corresponding to the input English text. The model accepts an English sentence as input and outputs its translation in German.\\n\\nKey components of the response:\\n\\n- **Input Text**: The original English text passed to the translation model.\\n- **Translated Text**: The corresponding German translation generated by the model.\\n\\nWhile the exact structure of the response is not specified in the raw documentation, it would typically follow this general form:\\n\\n- **Output Format**: The translation response is typically a string containing the translated German text.\\n- **Possible Additional Metadata**: Depending on the specific configuration or implementation, the response may include metadata such as:\\n  - **Model Information**: Information about the model used (e.g., 'sshleifer/tiny-marian-en-de').\\n  - **Translation Quality Metrics**: If applicable, some tools may return quality scores or confidence levels for the translation, though this is not explicitly stated in the documentation.\\n\\nThus, the expected output will primarily consist of the translated German text in response to the provided English input.\", \"examples\": [{\"user_intent\": \"Translate the sentence 'The weather is nice today' from English to German.\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"The weather is nice today\"}}}, {\"user_intent\": \"I want to translate this paragraph into German: 'Artificial intelligence is transforming the world rapidly.'\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"Artificial intelligence is transforming the world rapidly.\"}}}, {\"user_intent\": \"Convert this short English dialogue to German: 'Hello! How are you doing?'\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"Hello! How are you doing?\"}}}]}\n\n# User Query\nOur company is cooperating with a German partner. We have some materials in English, but need to translate them into German.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate our company's English materials into German for collaboration with a German partner.",
                    "tool_description": "Use an English-to-German machine translation model to convert text from English into German accurately and efficiently, suitable for business communication.",
                    "expected_response": "Translated German text corresponding to each input English material."
                }
            ],
            "extracted_arguments": {
                "source_text": "The English text content that needs to be translated into German."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_435",
        "query": "We need a picture that has a nostalgic look in high quality for the cover of our upcoming magazine.",
        "instruction": "Given an `image generation` task, retrieve tools that create high-quality visual outputs with specific aesthetic attributes, such as a nostalgic look, to align with the query's requirements for use in a magazine cover.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Diffusers",
                    "api_call": "DDPMPipeline.from_pretrained('pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs')",
                    "api_arguments": "",
                    "python_environment_requirements": "diffusers",
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "Example Fine-Tuned Model for Unit 2 of the Diffusion Models Class",
                    "id": "huggingface_api_311",
                    "name": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs"
                },
                "id": "gorilla_huggingface_tool_307",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 1: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.",
            "Tool 2: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 3: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 4: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 5: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 6: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 7: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 8: The \"google/ddpm-ema-church-256\" is a Denoising Diffusion Probabilistic Model (DDPM) used for high-quality image generation. Based on latent variable models inspired by nonequilibrium thermodynamics, it can synthesize images effectively using various noise schedulers and pipelines for optimized performance. This model, available through Hugging Face's Diffusers library, is designed for high-speed and high-quality image generation, with notable performance on datasets like CIFAR10, achieving an Inception score of 9.46 and an FID score of 3.17.",
            "Tool 9: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 10: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 12: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 13: This tool provides a question-answering model based on Roberta Base, specifically trained on the SQuAD task, allowing it to accurately answer questions from text. It uses the Hugging Face Transformers framework and is compatible with the latest version of the `transformers` library. The model has demonstrated strong performance, with high accuracy on datasets like SQuADv1 and MoviesQA.",
            "Tool 14: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 15: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.",
            "Tool 16: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 17: A ConvNeXt image classification model pretrained on ImageNet-1k, designed for classifying images, extracting feature maps, and generating image embeddings, suitable for computer vision tasks requiring high-quality visual representations.",
            "Tool 18: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.",
            "Tool 19: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically."
        ],
        "tool_view": {
            "description": "This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The model identifier to be used for loading the pre-trained Stable Diffusion model. This points to the specific version of the model stored on Hugging Face's repository.",
                    "required": true,
                    "example": "CompVis/stable-diffusion-v1-4"
                },
                "vae": {
                    "type": "string",
                    "description": "The pre-trained VAE (Variational Autoencoder) to be used for decoding. This parameter specifies the VAE model, which helps improve the quality of generated images.",
                    "required": true,
                    "example": "AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema')"
                }
            },
            "response": "The tool described in the raw JSON is a multimodal text-to-image generation system, leveraging the Stable Diffusion model pipeline. Based on the given information, the response would likely provide an image generated from a textual input. Here's a description of the response structure and its components:\n\n1. **Generated Image**: \n   - The core of the response is the image produced by the model, based on the provided textual input. The image is the result of a text-to-image transformation where the model interprets the prompt and generates visual content accordingly.\n\n2. **Image Metadata** (optional):\n   - **Dimensions**: The resolution of the generated image, likely in the form of width and height (e.g., 256x256, 512x512).\n   - **Model Used**: Information about the specific model that generated the image (e.g., StableDiffusionPipeline from CompVis/stable-diffusion-v1-4).\n   - **VAE Information**: The decoder used (e.g., AutoencoderKL from stabilityai/sd-vae-ft-ema) might be noted, especially if it influences the style or quality of the generated image.\n\n3. **Performance Metrics** (optional, not always included in the output but possible for more technical feedback):\n   - Metrics such as accuracy (rFID, PSNR, SSIM, PSIM) might be included, indicating the quality of the generated image, especially in comparison with datasets like COCO 2017 or LAION-Aesthetics 5+. These metrics provide insights into the image’s fidelity, structural quality, and aesthetic value.\n\n4. **Processing Information** (optional):\n   - Details on the processing environment or any notable configurations used during generation could be included. This might involve the model's configuration, the hardware used for the generation, or any additional modifications to the default settings.\n\n5. **Error Handling** (if applicable):\n   - In case of an error during the generation process, an error message might be returned, describing what went wrong (e.g., invalid input format, failure to load the model, etc.).\n\nIn summary, the response would consist of the generated image and possibly accompanying metadata, such as model and VAE details, image dimensions, and optional performance metrics.",
            "examples": [
                {
                    "user_intent": "Generate a high-resolution fantasy landscape with mountains, waterfalls, and a sunset sky.",
                    "tool_call": {
                        "name": "stabilityai/sd-vae-ft-ema",
                        "arguments": {
                            "prompt": "A breathtaking fantasy landscape with towering mountains, cascading waterfalls, and a vivid sunset sky, ultra-detailed, cinematic lighting"
                        }
                    }
                },
                {
                    "user_intent": "Create a futuristic cityscape at night with neon lights and flying cars.",
                    "tool_call": {
                        "name": "stabilityai/sd-vae-ft-ema",
                        "arguments": {
                            "prompt": "A futuristic cyberpunk city at night, glowing neon signs, flying cars, rainy streets, highly detailed, cinematic perspective"
                        }
                    }
                },
                {
                    "user_intent": "Generate a realistic portrait of an elderly woman smiling in natural sunlight.",
                    "tool_call": {
                        "name": "stabilityai/sd-vae-ft-ema",
                        "arguments": {
                            "prompt": "A photorealistic portrait of an elderly woman smiling, soft natural sunlight, warm tones, ultra-realistic skin texture"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 1: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.\nTool 2: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 3: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 4: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 5: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 6: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 7: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 8: The \"google/ddpm-ema-church-256\" is a Denoising Diffusion Probabilistic Model (DDPM) used for high-quality image generation. Based on latent variable models inspired by nonequilibrium thermodynamics, it can synthesize images effectively using various noise schedulers and pipelines for optimized performance. This model, available through Hugging Face's Diffusers library, is designed for high-speed and high-quality image generation, with notable performance on datasets like CIFAR10, achieving an Inception score of 9.46 and an FID score of 3.17.\nTool 9: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 10: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 12: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 13: This tool provides a question-answering model based on Roberta Base, specifically trained on the SQuAD task, allowing it to accurately answer questions from text. It uses the Hugging Face Transformers framework and is compatible with the latest version of the `transformers` library. The model has demonstrated strong performance, with high accuracy on datasets like SQuADv1 and MoviesQA.\nTool 14: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 15: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.\nTool 16: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 17: A ConvNeXt image classification model pretrained on ImageNet-1k, designed for classifying images, extracting feature maps, and generating image embeddings, suitable for computer vision tasks requiring high-quality visual representations.\nTool 18: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.\nTool 19: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The model identifier to be used for loading the pre-trained Stable Diffusion model. This points to the specific version of the model stored on Hugging Face's repository.\", \"required\": true, \"example\": \"CompVis/stable-diffusion-v1-4\"}, \"vae\": {\"type\": \"string\", \"description\": \"The pre-trained VAE (Variational Autoencoder) to be used for decoding. This parameter specifies the VAE model, which helps improve the quality of generated images.\", \"required\": true, \"example\": \"AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema')\"}}, \"response\": \"The tool described in the raw JSON is a multimodal text-to-image generation system, leveraging the Stable Diffusion model pipeline. Based on the given information, the response would likely provide an image generated from a textual input. Here's a description of the response structure and its components:\\n\\n1. **Generated Image**: \\n   - The core of the response is the image produced by the model, based on the provided textual input. The image is the result of a text-to-image transformation where the model interprets the prompt and generates visual content accordingly.\\n\\n2. **Image Metadata** (optional):\\n   - **Dimensions**: The resolution of the generated image, likely in the form of width and height (e.g., 256x256, 512x512).\\n   - **Model Used**: Information about the specific model that generated the image (e.g., StableDiffusionPipeline from CompVis/stable-diffusion-v1-4).\\n   - **VAE Information**: The decoder used (e.g., AutoencoderKL from stabilityai/sd-vae-ft-ema) might be noted, especially if it influences the style or quality of the generated image.\\n\\n3. **Performance Metrics** (optional, not always included in the output but possible for more technical feedback):\\n   - Metrics such as accuracy (rFID, PSNR, SSIM, PSIM) might be included, indicating the quality of the generated image, especially in comparison with datasets like COCO 2017 or LAION-Aesthetics 5+. These metrics provide insights into the image\\u2019s fidelity, structural quality, and aesthetic value.\\n\\n4. **Processing Information** (optional):\\n   - Details on the processing environment or any notable configurations used during generation could be included. This might involve the model's configuration, the hardware used for the generation, or any additional modifications to the default settings.\\n\\n5. **Error Handling** (if applicable):\\n   - In case of an error during the generation process, an error message might be returned, describing what went wrong (e.g., invalid input format, failure to load the model, etc.).\\n\\nIn summary, the response would consist of the generated image and possibly accompanying metadata, such as model and VAE details, image dimensions, and optional performance metrics.\", \"examples\": [{\"user_intent\": \"Generate a high-resolution fantasy landscape with mountains, waterfalls, and a sunset sky.\", \"tool_call\": {\"name\": \"stabilityai/sd-vae-ft-ema\", \"arguments\": {\"prompt\": \"A breathtaking fantasy landscape with towering mountains, cascading waterfalls, and a vivid sunset sky, ultra-detailed, cinematic lighting\"}}}, {\"user_intent\": \"Create a futuristic cityscape at night with neon lights and flying cars.\", \"tool_call\": {\"name\": \"stabilityai/sd-vae-ft-ema\", \"arguments\": {\"prompt\": \"A futuristic cyberpunk city at night, glowing neon signs, flying cars, rainy streets, highly detailed, cinematic perspective\"}}}, {\"user_intent\": \"Generate a realistic portrait of an elderly woman smiling in natural sunlight.\", \"tool_call\": {\"name\": \"stabilityai/sd-vae-ft-ema\", \"arguments\": {\"prompt\": \"A photorealistic portrait of an elderly woman smiling, soft natural sunlight, warm tones, ultra-realistic skin texture\"}}}]}\n\n# User Query\nWe need a picture that has a nostalgic look in high quality for the cover of our upcoming magazine.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a high-quality image with a nostalgic aesthetic suitable for a magazine cover.",
                    "tool_description": "Produce a visually rich, high-resolution image from a textual prompt using a fine-tuned Variational Autoencoder (VAE) within the Stable Diffusion text-to-image pipeline, optimized for aesthetic fidelity and image quality.",
                    "expected_response": "A generated image with the requested nostalgic style, including optional metadata such as resolution, model, and VAE details."
                }
            ],
            "extracted_arguments": {
                "prompt": "A descriptive text prompt detailing the desired nostalgic style and subject matter for the magazine cover.",
                "model": "The identifier of the pre-trained Stable Diffusion model to use for generation.",
                "vae": "The pre-trained VAE model to decode latent representations and enhance image quality."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_396",
        "query": "Now I need to create a summary of my chat with my friend last night.",
        "instruction": "Given a `summarization` task, retrieve tools that can process conversational text inputs to generate concise summaries, leveraging models trained on dialogue datasets to provide coherent abstracts of interactions.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Summarization",
                    "api_call": "pipeline('summarization', model='lidiya/bart-large-xsum-samsum')",
                    "api_arguments": "conversation",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline(summarization, model=lidiya/bart-large-xsum-samsum)\nconversation = '''Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye <br />\n'''\nsummarizer(conversation)",
                    "performance": {
                        "dataset": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization",
                        "accuracy": {
                            "rouge1": 53.306,
                            "rouge2": 28.355,
                            "rougeL": 44.095
                        }
                    },
                    "description": "This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.",
                    "id": "huggingface_api_555",
                    "name": "lidiya/bart-large-xsum-samsum"
                },
                "id": "gorilla_huggingface_tool_551",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 1: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 2: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 4: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 5: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "Tool 6: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 7: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 9: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 10: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 11: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 12: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 13: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 14: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 15: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 16: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 17: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 18: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 19: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms."
        ],
        "tool_view": {
            "description": "ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "parameters": {
                "message": {
                    "type": "string",
                    "description": "The input message or query that the user sends to the chatbot for generating a response.",
                    "required": true,
                    "example": "What is your name?"
                }
            },
            "response": "The tool produces a response representing the chatbot’s generated reply to a user-provided message. The response typically includes the following components:\n\n1. **User Input / Message**: The original text or query provided to the chatbot. This represents the conversational context or prompt that the model is responding to.  \n\n2. **Generated Response / Reply**: The text output produced by the model in response to the user input. This is the main content of the response and represents the chatbot’s attempt to provide a coherent, contextually relevant, and human-like answer.  \n\n3. **Conversation History (Optional)**: Depending on the implementation of the conversational pipeline, the response may include a record of previous exchanges in the chat session. This allows the model to maintain context across multiple turns of dialogue.  \n\n4. **Metadata (Optional)**: Some implementations may include additional information such as identifiers for the conversation, timestamps, or model-specific details (e.g., confidence scores or token-level information), though these are not explicitly required.  \n\nIn essence, the response is a structured object that contains the user’s message, the chatbot’s reply, and optionally any historical context to support multi-turn conversation continuity.",
            "examples": [
                {
                    "user_intent": "I want to have a casual chat and ask the bot how its day is going.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "Hi ShuanBot, how's your day going?"
                        }
                    }
                },
                {
                    "user_intent": "Ask the chatbot for advice on improving my productivity.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "Can you give me some tips to be more productive at work?"
                        }
                    }
                },
                {
                    "user_intent": "Start a conversation with ShuanBot about a fun topic like movies.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "What's your favorite movie genre and why?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 1: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 2: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 4: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 5: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\nTool 6: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 7: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 9: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 10: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 11: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 12: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 13: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 14: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 15: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 16: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 17: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 18: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 19: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\", \"parameters\": {\"message\": {\"type\": \"string\", \"description\": \"The input message or query that the user sends to the chatbot for generating a response.\", \"required\": true, \"example\": \"What is your name?\"}}, \"response\": \"The tool produces a response representing the chatbot\\u2019s generated reply to a user-provided message. The response typically includes the following components:\\n\\n1. **User Input / Message**: The original text or query provided to the chatbot. This represents the conversational context or prompt that the model is responding to.  \\n\\n2. **Generated Response / Reply**: The text output produced by the model in response to the user input. This is the main content of the response and represents the chatbot\\u2019s attempt to provide a coherent, contextually relevant, and human-like answer.  \\n\\n3. **Conversation History (Optional)**: Depending on the implementation of the conversational pipeline, the response may include a record of previous exchanges in the chat session. This allows the model to maintain context across multiple turns of dialogue.  \\n\\n4. **Metadata (Optional)**: Some implementations may include additional information such as identifiers for the conversation, timestamps, or model-specific details (e.g., confidence scores or token-level information), though these are not explicitly required.  \\n\\nIn essence, the response is a structured object that contains the user\\u2019s message, the chatbot\\u2019s reply, and optionally any historical context to support multi-turn conversation continuity.\", \"examples\": [{\"user_intent\": \"I want to have a casual chat and ask the bot how its day is going.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"Hi ShuanBot, how's your day going?\"}}}, {\"user_intent\": \"Ask the chatbot for advice on improving my productivity.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"Can you give me some tips to be more productive at work?\"}}}, {\"user_intent\": \"Start a conversation with ShuanBot about a fun topic like movies.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"What's your favorite movie genre and why?\"}}}]}\n\n# User Query\nNow I need to create a summary of my chat with my friend last night.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a concise summary of a chat conversation.",
                    "tool_description": "Transform a full chat transcript into a brief and coherent summary, highlighting the main points and key exchanges.",
                    "expected_response": "A textual summary of the conversation that captures the essential content in a condensed form."
                }
            ],
            "extracted_arguments": {
                "chat_transcript": "The full text of the chat conversation to be summarized, including all messages exchanged between participants."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_107",
        "query": "We have a French news agency and we want to categorize the news articles based on sports, politics, and science.",
        "instruction": "Given a `news categorization` task, retrieve tools that utilize natural language processing to classify news articles into specified categories such as sports, politics, and science, leveraging models capable of handling French text.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')",
                    "api_arguments": {
                        "sequence": "str",
                        "candidate_labels": "List[str]",
                        "hypothesis_template": "str"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "sequence = L'équipe de France joue aujourd'hui au Parc des Princes\ncandidate_labels = [sport,politique,science]\nhypothesis_template = Ce texte parle de {}.\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)",
                    "performance": {
                        "dataset": "xnli",
                        "accuracy": {
                            "validation": 81.4,
                            "test": 81.7
                        }
                    },
                    "description": "Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French.",
                    "id": "huggingface_api_492",
                    "name": "BaptisteDoyen/camembert-base-xnli"
                },
                "id": "gorilla_huggingface_tool_488",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 3: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 4: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 5: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 6: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 7: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 8: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 9: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 10: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 11: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 12: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 13: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 14: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.",
            "Tool 15: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 16: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 17: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 18: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.",
            "Tool 19: Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks."
        ],
        "tool_view": {
            "description": "A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text or article that you want the model to summarize.",
                    "required": true,
                    "example": "The stock market saw significant gains today as tech companies reported higher than expected earnings."
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens for the generated summary. Controls the length of the output.",
                    "required": false,
                    "default": 150,
                    "example": 100
                }
            },
            "response": "The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \n\nKey aspects of the response:  \n\n- **Type:** String (text)  \n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \n\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.",
            "examples": [
                {
                    "user_intent": "Summarize a recent news article about the global climate summit in under 100 words.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.",
                            "max_length": 100
                        }
                    }
                },
                {
                    "user_intent": "Create a concise summary of a long article about advancements in AI and robotics.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.",
                            "max_length": 120
                        }
                    }
                },
                {
                    "user_intent": "Provide a short summary for a news article about a major sports event.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 3: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 4: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 5: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 6: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 7: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 8: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 9: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 10: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 11: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 12: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 13: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 14: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\nTool 15: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 16: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 17: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 18: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.\nTool 19: Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text or article that you want the model to summarize.\", \"required\": true, \"example\": \"The stock market saw significant gains today as tech companies reported higher than expected earnings.\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens for the generated summary. Controls the length of the output.\", \"required\": false, \"default\": 150, \"example\": 100}}, \"response\": \"The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \\n\\nKey aspects of the response:  \\n\\n- **Type:** String (text)  \\n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \\n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \\n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \\n\\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.\", \"examples\": [{\"user_intent\": \"Summarize a recent news article about the global climate summit in under 100 words.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.\", \"max_length\": 100}}}, {\"user_intent\": \"Create a concise summary of a long article about advancements in AI and robotics.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.\", \"max_length\": 120}}}, {\"user_intent\": \"Provide a short summary for a news article about a major sports event.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.\", \"max_length\": 80}}}]}\n\n# User Query\nWe have a French news agency and we want to categorize the news articles based on sports, politics, and science.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify French news articles into categories such as sports, politics, and science.",
                    "tool_description": "Perform zero-shot text classification on French-language news articles, assigning each article to one of the predefined categories without requiring task-specific training data.",
                    "expected_response": "A structured response listing each article along with its predicted category label and confidence scores."
                }
            ],
            "extracted_arguments": {
                "text": "The French-language news article content to classify.",
                "categories": "A list of target category labels such as sports, politics, and science."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_215",
        "query": "Your company has launched a new chatbot service. We need to create conversations with the customers to provide information about our products and answer their queries.",
        "instruction": "Given a `customer interaction` task, retrieve tools that facilitate creating conversational dialogues with customers by employing text-generation models designed to simulate natural and informative exchanges based on input prompts and dialogue contexts.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face Transformers",
                    "functionality": "text-generation",
                    "api_call": "pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')",
                    "api_arguments": "input_prompt",
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\n[DIALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:",
                    "performance": {
                        "dataset": "56MB of dialogue data",
                        "accuracy": "Not provided"
                    },
                    "description": "Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.",
                    "id": "huggingface_api_584",
                    "name": "pygmalion-1.3b"
                },
                "id": "gorilla_huggingface_tool_579",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 1: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 2: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 3: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 6: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 8: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 9: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 10: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 11: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 12: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 13: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 14: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 15: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 16: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 17: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.",
            "Tool 18: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 19: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets."
        ],
        "tool_view": {
            "description": "A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path or URL of the image on which the visual question will be asked.",
                    "required": true,
                    "example": "images/sample.jpg"
                },
                "question": {
                    "type": "string",
                    "description": "The text of the question to ask about the image, in Polish.",
                    "required": true,
                    "example": "Co jest na obrazie?"
                }
            },
            "response": "The tool returns a structured response containing the answer to a visual question based on the provided image and question text. The typical response structure includes the following components:\n\n1. **answer** – A textual string representing the model’s predicted answer to the input question. This is the primary output conveying the model’s interpretation of the image in the context of the question.  \n\n2. **score** – A numerical value indicating the model’s confidence in the predicted answer. This score usually ranges between 0 and 1 and reflects the model’s certainty about its response.  \n\nOptional or extended implementations might include additional metadata such as:  \n\n- **logits** or probability distributions over possible answers (if available), providing deeper insight into alternative predictions.  \n- **question** and **image_reference** fields, echoing the input for clarity in multi-query pipelines.  \n\nIn essence, the response communicates both the textual answer derived from the image-question pair and the associated confidence level, allowing downstream applications to interpret or filter results accordingly.",
            "examples": [
                {
                    "user_intent": "I have a photo of my living room and I want to know how many chairs are in it.",
                    "tool_call": {
                        "name": "azwierzc/vilt-b32-finetuned-vqa-pl",
                        "arguments": {
                            "image": "/home/user/images/living_room.jpg",
                            "question": "Ile jest krzeseł na tym zdjęciu?"
                        }
                    }
                },
                {
                    "user_intent": "Check what brand of cereal is on the kitchen shelf in this picture.",
                    "tool_call": {
                        "name": "azwierzc/vilt-b32-finetuned-vqa-pl",
                        "arguments": {
                            "image": "/home/user/images/kitchen_shelf.png",
                            "question": "Jaka marka płatków śniadaniowych znajduje się na tym zdjęciu?"
                        }
                    }
                },
                {
                    "user_intent": "I took a photo of a park. I want to know if there are any dogs in the image.",
                    "tool_call": {
                        "name": "azwierzc/vilt-b32-finetuned-vqa-pl",
                        "arguments": {
                            "image": "/home/user/images/park.jpg",
                            "question": "Czy na tym zdjęciu są psy?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 1: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 2: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 3: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 6: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 8: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 9: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 10: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 11: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 12: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 13: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 14: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 15: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 16: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 17: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.\nTool 18: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 19: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path or URL of the image on which the visual question will be asked.\", \"required\": true, \"example\": \"images/sample.jpg\"}, \"question\": {\"type\": \"string\", \"description\": \"The text of the question to ask about the image, in Polish.\", \"required\": true, \"example\": \"Co jest na obrazie?\"}}, \"response\": \"The tool returns a structured response containing the answer to a visual question based on the provided image and question text. The typical response structure includes the following components:\\n\\n1. **answer** \\u2013 A textual string representing the model\\u2019s predicted answer to the input question. This is the primary output conveying the model\\u2019s interpretation of the image in the context of the question.  \\n\\n2. **score** \\u2013 A numerical value indicating the model\\u2019s confidence in the predicted answer. This score usually ranges between 0 and 1 and reflects the model\\u2019s certainty about its response.  \\n\\nOptional or extended implementations might include additional metadata such as:  \\n\\n- **logits** or probability distributions over possible answers (if available), providing deeper insight into alternative predictions.  \\n- **question** and **image_reference** fields, echoing the input for clarity in multi-query pipelines.  \\n\\nIn essence, the response communicates both the textual answer derived from the image-question pair and the associated confidence level, allowing downstream applications to interpret or filter results accordingly.\", \"examples\": [{\"user_intent\": \"I have a photo of my living room and I want to know how many chairs are in it.\", \"tool_call\": {\"name\": \"azwierzc/vilt-b32-finetuned-vqa-pl\", \"arguments\": {\"image\": \"/home/user/images/living_room.jpg\", \"question\": \"Ile jest krzese\\u0142 na tym zdj\\u0119ciu?\"}}}, {\"user_intent\": \"Check what brand of cereal is on the kitchen shelf in this picture.\", \"tool_call\": {\"name\": \"azwierzc/vilt-b32-finetuned-vqa-pl\", \"arguments\": {\"image\": \"/home/user/images/kitchen_shelf.png\", \"question\": \"Jaka marka p\\u0142atk\\u00f3w \\u015bniadaniowych znajduje si\\u0119 na tym zdj\\u0119ciu?\"}}}, {\"user_intent\": \"I took a photo of a park. I want to know if there are any dogs in the image.\", \"tool_call\": {\"name\": \"azwierzc/vilt-b32-finetuned-vqa-pl\", \"arguments\": {\"image\": \"/home/user/images/park.jpg\", \"question\": \"Czy na tym zdj\\u0119ciu s\\u0105 psy?\"}}}]}\n\n# User Query\nYour company has launched a new chatbot service. We need to create conversations with the customers to provide information about our products and answer their queries.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a chatbot that can interact with customers and answer questions about our products.",
                    "tool_description": "Use a conversational AI model to generate natural language responses for multi-turn dialogues, capable of understanding user queries and providing relevant information about products or services.",
                    "expected_response": "Textual responses from the AI model that are contextually appropriate, informative, and coherent for each user message in the conversation."
                }
            ],
            "extracted_arguments": {
                "product_information": "Structured or unstructured details about the company's products, including features, pricing, and specifications, used as a knowledge source for the chatbot."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_451",
        "query": "I'm working on a phonebot, and I need the bot to be able to read a sensitive warning message to the users.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that convert text inputs into audio outputs capable of reading messages aloud, emphasizing clarity and accuracy aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "TTSHubInterface.get_prediction('facebook/fastspeech2-en-ljspeech')",
                    "api_arguments": {
                        "task": "task",
                        "model": "model",
                        "generator": "generator",
                        "sample": "sample"
                    },
                    "python_environment_requirements": [
                        "fairseq",
                        "IPython"
                    ],
                    "example_code": "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\nimport IPython.display as ipd\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/fastspeech2-en-ljspeech,\n arg_overrides={vocoder: hifigan, fp16: False}\n)\nmodel = models[0]\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\ngenerator = task.build_generator(model, cfg)\ntext = Hello, this is a test run.\nsample = TTSHubInterface.get_model_input(task, text)\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\nipd.Audio(wav, rate=rate)",
                    "performance": {
                        "dataset": "LJSpeech",
                        "accuracy": "N/A"
                    },
                    "description": "FastSpeech 2 text-to-speech model from fairseq S^2. English single-speaker female voice trained on LJSpeech.",
                    "id": "huggingface_api_720",
                    "name": "fastspeech2-en-ljspeech"
                },
                "id": "gorilla_huggingface_tool_715",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 1: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 2: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 3: The `bert-base-multilingual-cased` model is a case-sensitive BERT model pretrained on Wikipedia content from 104 languages using masked language modeling. It can predict missing words in text and serve as a foundation for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 6: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 7: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 8: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 9: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 10: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 11: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 12: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 13: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 14: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 15: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 16: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 17: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 18: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 19: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features."
        ],
        "tool_view": {
            "description": "`bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or path of the pre-trained transformer model to use for masked language modeling. This specifies which model weights and configuration to load.",
                    "required": true,
                    "example": "bert-base-cased"
                }
            },
            "response": "The tool produces a structured response representing the predictions for a masked token in a given input sequence. Each response typically contains the following components:\n\n1. **Sequence**: The input text where the masked token has been replaced with a predicted candidate. This shows the full sentence with the model’s suggested substitution in place of the `[MASK]`.\n\n2. **Score**: A numerical value representing the model’s confidence or probability for the predicted token in that context. Higher scores indicate higher confidence that the candidate token fits the masked position.\n\n3. **Token**: The integer ID corresponding to the predicted token in the model’s vocabulary. This links the prediction to the underlying tokenizer used by the model.\n\n4. **Token_str**: The textual representation of the predicted token. This is the human-readable form that fills the `[MASK]` in the input sequence.\n\nThe response is usually a **list of such prediction objects**, often sorted in descending order of score, so that the most likely predictions appear first. Each prediction provides both a quantitative confidence measure and the corresponding text substitution, allowing downstream applications to select or rank candidates for the masked position.",
            "examples": [
                {
                    "user_intent": "I want to predict the missing word in this sentence: 'The weather today is [MASK]'.",
                    "tool_call": {
                        "name": "fill-mask",
                        "arguments": {
                            "model": "bert-base-cased",
                            "input_text": "The weather today is [MASK]."
                        }
                    }
                },
                {
                    "user_intent": "Find a suitable word to complete this sentence: 'She opened the [MASK] to let in some fresh air.'",
                    "tool_call": {
                        "name": "fill-mask",
                        "arguments": {
                            "model": "bert-base-cased",
                            "input_text": "She opened the [MASK] to let in some fresh air."
                        }
                    }
                },
                {
                    "user_intent": "Predict what word fits in the sentence: 'Artificial [MASK] is transforming industries.'",
                    "tool_call": {
                        "name": "fill-mask",
                        "arguments": {
                            "model": "bert-base-cased",
                            "input_text": "Artificial [MASK] is transforming industries."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 1: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 2: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 3: The `bert-base-multilingual-cased` model is a case-sensitive BERT model pretrained on Wikipedia content from 104 languages using masked language modeling. It can predict missing words in text and serve as a foundation for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 6: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 7: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 8: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 9: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 10: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 11: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 12: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 13: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 14: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 15: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 16: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 17: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 18: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 19: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"`bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or path of the pre-trained transformer model to use for masked language modeling. This specifies which model weights and configuration to load.\", \"required\": true, \"example\": \"bert-base-cased\"}}, \"response\": \"The tool produces a structured response representing the predictions for a masked token in a given input sequence. Each response typically contains the following components:\\n\\n1. **Sequence**: The input text where the masked token has been replaced with a predicted candidate. This shows the full sentence with the model\\u2019s suggested substitution in place of the `[MASK]`.\\n\\n2. **Score**: A numerical value representing the model\\u2019s confidence or probability for the predicted token in that context. Higher scores indicate higher confidence that the candidate token fits the masked position.\\n\\n3. **Token**: The integer ID corresponding to the predicted token in the model\\u2019s vocabulary. This links the prediction to the underlying tokenizer used by the model.\\n\\n4. **Token_str**: The textual representation of the predicted token. This is the human-readable form that fills the `[MASK]` in the input sequence.\\n\\nThe response is usually a **list of such prediction objects**, often sorted in descending order of score, so that the most likely predictions appear first. Each prediction provides both a quantitative confidence measure and the corresponding text substitution, allowing downstream applications to select or rank candidates for the masked position.\", \"examples\": [{\"user_intent\": \"I want to predict the missing word in this sentence: 'The weather today is [MASK]'.\", \"tool_call\": {\"name\": \"fill-mask\", \"arguments\": {\"model\": \"bert-base-cased\", \"input_text\": \"The weather today is [MASK].\"}}}, {\"user_intent\": \"Find a suitable word to complete this sentence: 'She opened the [MASK] to let in some fresh air.'\", \"tool_call\": {\"name\": \"fill-mask\", \"arguments\": {\"model\": \"bert-base-cased\", \"input_text\": \"She opened the [MASK] to let in some fresh air.\"}}}, {\"user_intent\": \"Predict what word fits in the sentence: 'Artificial [MASK] is transforming industries.'\", \"tool_call\": {\"name\": \"fill-mask\", \"arguments\": {\"model\": \"bert-base-cased\", \"input_text\": \"Artificial [MASK] is transforming industries.\"}}}]}\n\n# User Query\nI'm working on a phonebot, and I need the bot to be able to read a sensitive warning message to the users.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want the phonebot to read a sensitive warning message aloud to users.",
                    "tool_description": "Convert a given text message into audible speech using text-to-speech synthesis, enabling the bot to communicate warnings or notifications vocally.",
                    "expected_response": "An audio output stream or file containing the spoken version of the input text message."
                }
            ],
            "extracted_arguments": {
                "warning_text": "The sensitive message content that needs to be spoken by the phonebot, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_425",
        "query": "Create a program to generate a description for an image provided as input.",
        "instruction": "Given an `image-to-text description generation` task, retrieve tools that process image inputs to generate descriptive textual outputs, detailing the contents of the image as required by the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('text-generation', model='microsoft/git-large-r-textcaps')",
                    "api_arguments": "image",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "TextCaps",
                        "accuracy": ""
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).",
                    "id": "huggingface_api_85",
                    "name": "git-large-r-textcaps"
                },
                "id": "gorilla_huggingface_tool_85",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 1: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 2: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 3: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 4: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 5: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 6: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 7: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 8: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 9: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 10: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 11: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.",
            "Tool 12: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 13: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 14: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 15: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 16: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 17: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 18: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 19: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests."
        ],
        "tool_view": {
            "description": "Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "parameters": {},
            "response": "The tool produces an image generated unconditionally by the diffusion model. The response is an object or structure that contains at least the following key components:  \n\n1. **images** – A collection (often a list) of generated images. Each element represents a single generated image produced by the diffusion process.  \n   - The images are typically in a standard image format object used in the Python environment (e.g., PIL Image objects).  \n   - They may include additional metadata such as image mode (RGB, RGBA), size, or channels.  \n\n2. **Optional pipeline metadata** – Depending on the implementation, the response might include additional information about the generation process, such as:  \n   - Number of diffusion steps used  \n   - Seed or randomness state for reproducibility  \n   - Timing or performance metrics for the generation  \n\nThe core content of the response is the generated image(s), which can be directly manipulated, displayed, or saved. In typical use, the first element of the `images` list is accessed as the primary output.  \n\nOverall, the response encapsulates the visual output of the unconditional Minecraft skin diffusion model, formatted as standard image objects compatible with Python imaging libraries.",
            "examples": [
                {
                    "user_intent": "Generate a new Minecraft skin using a diffusion model.",
                    "tool_call": {
                        "name": "Minecraft-Skin-Diffusion",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "I want to create a custom Minecraft skin using the WiNE-iNEFF model.",
                    "tool_call": {
                        "name": "Minecraft-Skin-Diffusion",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Generate a random Minecraft skin with the latest diffusion pipeline.",
                    "tool_call": {
                        "name": "Minecraft-Skin-Diffusion",
                        "arguments": {}
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 1: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 2: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 3: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 4: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 5: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 6: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 7: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 8: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 9: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 10: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 11: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.\nTool 12: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 13: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 14: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 15: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 16: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 17: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 18: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 19: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\", \"parameters\": {}, \"response\": \"The tool produces an image generated unconditionally by the diffusion model. The response is an object or structure that contains at least the following key components:  \\n\\n1. **images** \\u2013 A collection (often a list) of generated images. Each element represents a single generated image produced by the diffusion process.  \\n   - The images are typically in a standard image format object used in the Python environment (e.g., PIL Image objects).  \\n   - They may include additional metadata such as image mode (RGB, RGBA), size, or channels.  \\n\\n2. **Optional pipeline metadata** \\u2013 Depending on the implementation, the response might include additional information about the generation process, such as:  \\n   - Number of diffusion steps used  \\n   - Seed or randomness state for reproducibility  \\n   - Timing or performance metrics for the generation  \\n\\nThe core content of the response is the generated image(s), which can be directly manipulated, displayed, or saved. In typical use, the first element of the `images` list is accessed as the primary output.  \\n\\nOverall, the response encapsulates the visual output of the unconditional Minecraft skin diffusion model, formatted as standard image objects compatible with Python imaging libraries.\", \"examples\": [{\"user_intent\": \"Generate a new Minecraft skin using a diffusion model.\", \"tool_call\": {\"name\": \"Minecraft-Skin-Diffusion\", \"arguments\": {}}}, {\"user_intent\": \"I want to create a custom Minecraft skin using the WiNE-iNEFF model.\", \"tool_call\": {\"name\": \"Minecraft-Skin-Diffusion\", \"arguments\": {}}}, {\"user_intent\": \"Generate a random Minecraft skin with the latest diffusion pipeline.\", \"tool_call\": {\"name\": \"Minecraft-Skin-Diffusion\", \"arguments\": {}}}]}\n\n# User Query\nCreate a program to generate a description for an image provided as input.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a description for an image that is provided.",
                    "tool_description": "This tool uses a Hugging Face Transformers image-to-text model to generate captions for input images, providing an automated description of the image's content.",
                    "expected_response": "A textual description that summarizes the content and key features of the provided image."
                }
            ],
            "extracted_arguments": {
                "image": "The image input for which a description is to be generated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_209",
        "query": "Develop a text-to-speech model for our mobile app to read news articles for our users.",
        "instruction": "Given a `text-to-speech development` task, retrieve tools that transform text inputs into spoken output for integration into mobile applications, focusing on their ability to handle diverse language datasets and deliver synthesized speech suited for user engagement with written content.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "fairseq",
                        "IPython"
                    ],
                    "example_code": "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\nimport IPython.display as ipd\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/fastspeech2-en-200_speaker-cv4,\n arg_overrides={vocoder: hifigan, fp16: False}\n)\nmodel = models[0]\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\ngenerator = task.build_generator(model, cfg)\ntext = Hello, this is a test run.\nsample = TTSHubInterface.get_model_input(task, text)\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\nipd.Audio(wav, rate=rate)",
                    "performance": {
                        "dataset": "common_voice",
                        "accuracy": null
                    },
                    "description": "FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.",
                    "id": "huggingface_api_735",
                    "name": "fastspeech2-en-male1"
                },
                "id": "gorilla_huggingface_tool_730",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 2: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 3: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 4: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 5: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 6: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 7: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.",
            "Tool 8: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 9: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 10: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 11: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 12: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 13: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 14: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.",
            "Tool 15: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 16: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 17: A Text-to-Speech tool using the ESPnet framework, trained on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.",
            "Tool 18: The \"mio/Artoria\" is a text-to-speech model trained using the ESPnet framework and the Fate recipe, capable of converting text input into speech output. It can be accessed through the ESPnet pipeline API, specifically for text-to-speech tasks, and requires the Transformers library in a Python environment.",
            "Tool 19: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems."
        ],
        "tool_view": {
            "description": "A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text or article that you want the model to summarize.",
                    "required": true,
                    "example": "The stock market saw significant gains today as tech companies reported higher than expected earnings."
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens for the generated summary. Controls the length of the output.",
                    "required": false,
                    "default": 150,
                    "example": 100
                }
            },
            "response": "The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \n\nKey aspects of the response:  \n\n- **Type:** String (text)  \n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \n\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.",
            "examples": [
                {
                    "user_intent": "Summarize a recent news article about the global climate summit in under 100 words.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.",
                            "max_length": 100
                        }
                    }
                },
                {
                    "user_intent": "Create a concise summary of a long article about advancements in AI and robotics.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.",
                            "max_length": 120
                        }
                    }
                },
                {
                    "user_intent": "Provide a short summary for a news article about a major sports event.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 2: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 3: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 4: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 5: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 6: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 7: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.\nTool 8: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 9: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 10: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 11: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 12: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 13: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 14: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.\nTool 15: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 16: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 17: A Text-to-Speech tool using the ESPnet framework, trained on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.\nTool 18: The \"mio/Artoria\" is a text-to-speech model trained using the ESPnet framework and the Fate recipe, capable of converting text input into speech output. It can be accessed through the ESPnet pipeline API, specifically for text-to-speech tasks, and requires the Transformers library in a Python environment.\nTool 19: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text or article that you want the model to summarize.\", \"required\": true, \"example\": \"The stock market saw significant gains today as tech companies reported higher than expected earnings.\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens for the generated summary. Controls the length of the output.\", \"required\": false, \"default\": 150, \"example\": 100}}, \"response\": \"The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \\n\\nKey aspects of the response:  \\n\\n- **Type:** String (text)  \\n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \\n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \\n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \\n\\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.\", \"examples\": [{\"user_intent\": \"Summarize a recent news article about the global climate summit in under 100 words.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.\", \"max_length\": 100}}}, {\"user_intent\": \"Create a concise summary of a long article about advancements in AI and robotics.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.\", \"max_length\": 120}}}, {\"user_intent\": \"Provide a short summary for a news article about a major sports event.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.\", \"max_length\": 80}}}]}\n\n# User Query\nDevelop a text-to-speech model for our mobile app to read news articles for our users.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a text-to-speech model that can read news articles aloud.",
                    "tool_description": "Develop a text-to-speech model that converts written news articles into natural-sounding speech using a suitable pre-trained text-to-speech framework.",
                    "expected_response": "A working model that can convert text input (e.g., news articles) into audio output, ready for integration into a mobile app."
                }
            ],
            "extracted_arguments": {
                "text_input": "The news articles or text content that will be converted to speech.",
                "platform": "The mobile app platform where the model will be deployed (e.g., iOS, Android)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_317",
        "query": "Someone asked the question \"What is the capital of Sweden?\" in the context \"Stockholm is the beautiful capital of Sweden, which is known for its high living standards and great attractions.\"",
        "instruction": "Given a `question answering` task, retrieve tools that employ natural language processing techniques to provide accurate answers by analyzing the context and query to deliver high-quality results.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')",
                    "api_arguments": {
                        "model_name_or_path": "bert-large-uncased-whole-word-masking",
                        "dataset_name": "squad",
                        "do_train": true,
                        "do_eval": true,
                        "learning_rate": 3e-05,
                        "num_train_epochs": 2,
                        "max_seq_length": 384,
                        "doc_stride": 128,
                        "output_dir": "./examples/models/wwm_uncased_finetuned_squad/",
                        "per_device_eval_batch_size": 3,
                        "per_device_train_batch_size": 3
                    },
                    "python_environment_requirements": {
                        "torch": "1.9.0",
                        "transformers": "4.9.2"
                    },
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\nprint(result)",
                    "performance": {
                        "dataset": "SQuAD",
                        "accuracy": {
                            "f1": 93.15,
                            "exact_match": 86.91
                        }
                    },
                    "description": "BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model.",
                    "id": "huggingface_api_466",
                    "name": "bert-large-uncased-whole-word-masking-finetuned-squad"
                },
                "id": "gorilla_huggingface_tool_462",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 1: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 2: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.",
            "Tool 3: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 4: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 5: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 6: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 7: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 8: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.",
            "Tool 9: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 10: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 11: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.",
            "Tool 12: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 13: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.",
            "Tool 14: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 15: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 16: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 17: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 18: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 19: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning."
        ],
        "tool_view": {
            "description": "A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "parameters": {
                "answer": {
                    "type": "string",
                    "description": "The answer text based on which the model will generate a corresponding question.",
                    "required": true,
                    "example": "Paris"
                },
                "context": {
                    "type": "string",
                    "description": "The context or passage in which the answer is located, providing background for question generation.",
                    "required": true,
                    "example": "Paris is the capital city of France, known for its art, culture, and landmarks like the Eiffel Tower."
                },
                "max_length": {
                    "type": "integer",
                    "description": "The maximum length of the generated question in tokens. If not specified, defaults to 64.",
                    "required": false,
                    "default": 64,
                    "example": 32
                }
            },
            "response": "The tool is designed for question generation using a fine-tuned version of Google's T5 model. The response format is not explicitly defined in the raw tool documentation, but based on the functionality and typical outputs for question generation tasks, a reasonable and typical response would include the following components:\n\n### Structure and Components of the Response:\n\n1. **Generated Question**:\n   - **Field Name**: `\"question\"`\n   - **Type**: String\n   - **Description**: The question generated by the model based on the provided `answer` and `context`. The question is intended to be relevant and logically derived from the input answer and context.\n\n2. **Confidence or Score (optional)**:\n   - **Field Name**: `\"confidence_score\"` (or a similar term depending on implementation)\n   - **Type**: Float (optional)\n   - **Description**: A numerical score indicating the model's confidence in the generated question. This field may not be included in all responses, depending on the implementation.\n\n3. **Metadata (optional)**:\n   - **Field Name**: `\"metadata\"`\n   - **Type**: Object (optional)\n   - **Description**: Additional metadata related to the generation process. This could include things like the model version, processing time, or other relevant information about the generation process.\n\n### Expected Information in the Response:\n- The **generated question** will be a string that is directly related to the `answer` and `context` provided as inputs.\n- The response may include additional optional fields, such as a **confidence score** to indicate the model’s certainty about the quality or relevance of the generated question.\n- **Metadata** may be included for tracking purposes, though this would depend on the specific implementation of the tool.\n\nThis response structure ensures that the output directly supports the task of question generation, which is the core functionality of the tool.",
            "examples": [
                {
                    "user_intent": "Generate a question for the answer 'Paris' given the context about French cities.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-question-generation-ap",
                        "arguments": {
                            "answer": "Paris",
                            "context": "France has many beautiful cities, but Paris is known as the capital and a major cultural center.",
                            "max_length": 50
                        }
                    }
                },
                {
                    "user_intent": "Create a question for the answer 'Albert Einstein' in the context of famous scientists.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-question-generation-ap",
                        "arguments": {
                            "answer": "Albert Einstein",
                            "context": "Among the most influential physicists of the 20th century was Albert Einstein, who developed the theory of relativity.",
                            "max_length": 64
                        }
                    }
                },
                {
                    "user_intent": "Formulate a question for the answer 'Photosynthesis' based on plant biology information.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-question-generation-ap",
                        "arguments": {
                            "answer": "Photosynthesis",
                            "context": "Plants convert sunlight into chemical energy through a process called photosynthesis, which is vital for life on Earth."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 1: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 2: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.\nTool 3: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 4: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 5: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 6: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 7: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 8: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.\nTool 9: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 10: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 11: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.\nTool 12: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 13: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.\nTool 14: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 15: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 16: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 17: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 18: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 19: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\", \"parameters\": {\"answer\": {\"type\": \"string\", \"description\": \"The answer text based on which the model will generate a corresponding question.\", \"required\": true, \"example\": \"Paris\"}, \"context\": {\"type\": \"string\", \"description\": \"The context or passage in which the answer is located, providing background for question generation.\", \"required\": true, \"example\": \"Paris is the capital city of France, known for its art, culture, and landmarks like the Eiffel Tower.\"}, \"max_length\": {\"type\": \"integer\", \"description\": \"The maximum length of the generated question in tokens. If not specified, defaults to 64.\", \"required\": false, \"default\": 64, \"example\": 32}}, \"response\": \"The tool is designed for question generation using a fine-tuned version of Google's T5 model. The response format is not explicitly defined in the raw tool documentation, but based on the functionality and typical outputs for question generation tasks, a reasonable and typical response would include the following components:\\n\\n### Structure and Components of the Response:\\n\\n1. **Generated Question**:\\n   - **Field Name**: `\\\"question\\\"`\\n   - **Type**: String\\n   - **Description**: The question generated by the model based on the provided `answer` and `context`. The question is intended to be relevant and logically derived from the input answer and context.\\n\\n2. **Confidence or Score (optional)**:\\n   - **Field Name**: `\\\"confidence_score\\\"` (or a similar term depending on implementation)\\n   - **Type**: Float (optional)\\n   - **Description**: A numerical score indicating the model's confidence in the generated question. This field may not be included in all responses, depending on the implementation.\\n\\n3. **Metadata (optional)**:\\n   - **Field Name**: `\\\"metadata\\\"`\\n   - **Type**: Object (optional)\\n   - **Description**: Additional metadata related to the generation process. This could include things like the model version, processing time, or other relevant information about the generation process.\\n\\n### Expected Information in the Response:\\n- The **generated question** will be a string that is directly related to the `answer` and `context` provided as inputs.\\n- The response may include additional optional fields, such as a **confidence score** to indicate the model\\u2019s certainty about the quality or relevance of the generated question.\\n- **Metadata** may be included for tracking purposes, though this would depend on the specific implementation of the tool.\\n\\nThis response structure ensures that the output directly supports the task of question generation, which is the core functionality of the tool.\", \"examples\": [{\"user_intent\": \"Generate a question for the answer 'Paris' given the context about French cities.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-question-generation-ap\", \"arguments\": {\"answer\": \"Paris\", \"context\": \"France has many beautiful cities, but Paris is known as the capital and a major cultural center.\", \"max_length\": 50}}}, {\"user_intent\": \"Create a question for the answer 'Albert Einstein' in the context of famous scientists.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-question-generation-ap\", \"arguments\": {\"answer\": \"Albert Einstein\", \"context\": \"Among the most influential physicists of the 20th century was Albert Einstein, who developed the theory of relativity.\", \"max_length\": 64}}}, {\"user_intent\": \"Formulate a question for the answer 'Photosynthesis' based on plant biology information.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-question-generation-ap\", \"arguments\": {\"answer\": \"Photosynthesis\", \"context\": \"Plants convert sunlight into chemical energy through a process called photosynthesis, which is vital for life on Earth.\"}}}]}\n\n# User Query\nSomeone asked the question \"What is the capital of Sweden?\" in the context \"Stockholm is the beautiful capital of Sweden, which is known for its high living standards and great attractions.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a question for the answer 'Stockholm' given the context about Sweden.",
                    "tool_description": "Use a T5-based question generation model to produce a question from a given answer and its context, allowing automated creation of relevant questions from text.",
                    "expected_response": "A generated question string that corresponds logically to the provided answer and context, optionally including a confidence score and metadata."
                }
            ],
            "extracted_arguments": {
                "answer": "The answer text based on which the model will generate a corresponding question.",
                "context": "The context or passage in which the answer is located, providing background for question generation.",
                "max_length": "The maximum length of the generated question in tokens, optional."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_207",
        "query": "In surveillance operations, transcripts are used to turn the audio feed into chunks such that after processing transcripts there are no speakers talking over each other in the output.",
        "instruction": "Given a `speaker diarization` task, retrieve tools that process audio feeds to identify and separate individual speakers, ensuring no overlap in the transcription outputs by utilizing segmentation and diarization techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "pyannote.audio",
                    "functionality": "Speaker diarization",
                    "api_call": "Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')",
                    "api_arguments": [
                        "num_speakers",
                        "min_speakers",
                        "max_speakers",
                        "segmentation_onset"
                    ],
                    "python_environment_requirements": "pyannote.audio 2.0",
                    "example_code": {
                        "load_pipeline": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)",
                        "apply_pipeline": "diarization = pipeline(audio.wav)",
                        "save_output": "with open(audio.rttm, w) as rttm:\n  diarization.write_rttm(rttm)"
                    },
                    "performance": {
                        "dataset": [
                            {
                                "name": "AISHELL-4",
                                "accuracy": {
                                    "DER%": 14.61,
                                    "FA%": 3.31,
                                    "Miss%": 4.35,
                                    "Conf%": 6.95
                                }
                            },
                            {
                                "name": "AMI Mix-Headset only_words",
                                "accuracy": {
                                    "DER%": 18.21,
                                    "FA%": 3.28,
                                    "Miss%": 11.07,
                                    "Conf%": 3.87
                                }
                            },
                            {
                                "name": "AMI Array1-01 only_words",
                                "accuracy": {
                                    "DER%": 29.0,
                                    "FA%": 2.71,
                                    "Miss%": 21.61,
                                    "Conf%": 4.68
                                }
                            },
                            {
                                "name": "CALLHOME Part2",
                                "accuracy": {
                                    "DER%": 30.24,
                                    "FA%": 3.71,
                                    "Miss%": 16.86,
                                    "Conf%": 9.66
                                }
                            },
                            {
                                "name": "DIHARD 3 Full",
                                "accuracy": {
                                    "DER%": 20.99,
                                    "FA%": 4.25,
                                    "Miss%": 10.74,
                                    "Conf%": 6.0
                                }
                            },
                            {
                                "name": "REPERE Phase 2",
                                "accuracy": {
                                    "DER%": 12.62,
                                    "FA%": 1.55,
                                    "Miss%": 3.3,
                                    "Conf%": 7.76
                                }
                            },
                            {
                                "name": "VoxConverse v0.0.2",
                                "accuracy": {
                                    "DER%": 12.76,
                                    "FA%": 3.45,
                                    "Miss%": 3.85,
                                    "Conf%": 5.46
                                }
                            }
                        ]
                    },
                    "description": "This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.",
                    "id": "huggingface_api_846",
                    "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint"
                },
                "id": "gorilla_huggingface_tool_840",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 1: This tool provides a pre-trained HiFIGAN vocoder model for converting spectrograms into audio waveforms. It is typically used after a Text-to-Speech (TTS) model to transform the spectrogram output into natural-sounding speech. The model is trained on the LJSpeech dataset and operates at a sampling frequency of 22,050 Hz.",
            "Tool 2: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 3: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 4: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 5: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 6: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 7: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 8: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.",
            "Tool 9: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 10: A Keras-based TabTransformer model for supervised and semi-supervised learning on tabular data, capable of handling both numerical and categorical features. Categorical features are embedded and processed through Transformer blocks to capture contextual relationships, then combined with numerical features and passed through a final feed-forward network for prediction, with a SoftMax output for classification. This model is designed to leverage self-attention mechanisms to improve learning from structured datasets.",
            "Tool 11: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.",
            "Tool 12: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.",
            "Tool 13: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.",
            "Tool 14: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 15: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 16: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 17: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 18: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 19: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs."
        ],
        "tool_view": {
            "description": "This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "parameters": {
                "num_speakers": {
                    "type": "number",
                    "description": "The exact number of speakers to detect in the audio. This overrides the min_speakers and max_speakers settings if provided.",
                    "required": true,
                    "example": 2
                },
                "min_speakers": {
                    "type": "number",
                    "description": "The minimum number of speakers expected in the audio. Used when the exact number of speakers is unknown.",
                    "required": true,
                    "example": 1
                },
                "max_speakers": {
                    "type": "number",
                    "description": "The maximum number of speakers expected in the audio. Helps constrain the diarization process when the exact number is unknown.",
                    "required": true,
                    "example": 5
                },
                "segmentation_onset": {
                    "type": "number",
                    "description": "A threshold parameter that adjusts the sensitivity for detecting the start of speech segments. Lower values make the system more sensitive to short speech segments.",
                    "required": true,
                    "example": 0.5
                }
            },
            "response": "The tool response would likely contain the following components:\n\n1. **Diarization Results**: This part of the response would include the speaker diarization output for the provided audio file. The diarization output typically includes:\n   - **Segments**: Time intervals where different speakers are active, with each segment linked to a specific speaker.\n   - **Speaker Labels**: Identifiers assigned to each speaker in the audio, such as \"Speaker 1,\" \"Speaker 2,\" etc., or possibly user-defined labels depending on the tool configuration.\n   - **Speech Time**: The total time a particular speaker is active during the audio, often associated with each speaker label.\n\n2. **RTTM File Format**: The diarization results will be structured in **RTTM (Rich Transcription Time Marked)** format, which typically includes:\n   - **File Name**: The audio file being processed.\n   - **Start Time**: The timestamp when the speaker begins speaking.\n   - **Duration**: The length of time the speaker was speaking.\n   - **Speaker ID**: A unique identifier for the speaker (e.g., \"Speaker 1\").\n   - **Speech/Non-Speech Label**: Indicating whether the segment contains speech or non-speech (e.g., \"SPEECH\" or \"NON_SPEECH\").\n\n3. **Performance Metrics**: The response might include performance metrics based on the evaluation on different datasets:\n   - **Dataset Name**: Identifiers for datasets used for evaluating the model.\n   - **Accuracy Metrics**: For each dataset, the performance metrics such as:\n     - **DER% (Diarization Error Rate)**: A measure of the overall error in the diarization.\n     - **FA% (False Alarm Rate)**: The rate of incorrectly detected speech from a non-speech segment.\n     - **Miss% (Missed Detection Rate)**: The rate of missed speech segments.\n     - **Conf% (Confusion Rate)**: The rate of confusion in speaker identification.\n\n4. **Error Handling**: If an error occurs during processing (e.g., invalid audio file, failure in the diarization process), the response may include an error message indicating the nature of the problem.\n\nThe output would summarize these results to provide insight into the audio's speaker segmentation and performance of the diarization model. It will also indicate the model's accuracy based on various evaluation datasets, helping the user assess its reliability.",
            "examples": [
                {
                    "user_intent": "I need to transcribe a conversation with 3 speakers from an audio file.",
                    "tool_call": {
                        "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint",
                        "arguments": {
                            "num_speakers": 3
                        }
                    }
                },
                {
                    "user_intent": "Process an audio file with an unknown number of speakers, but I expect there to be between 2 and 4 speakers.",
                    "tool_call": {
                        "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint",
                        "arguments": {
                            "min_speakers": 2,
                            "max_speakers": 4
                        }
                    }
                },
                {
                    "user_intent": "Diarize an audio recording with a low segmentation onset threshold for more precise speaker segmentation.",
                    "tool_call": {
                        "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint",
                        "arguments": {
                            "segmentation_onset": 0.3
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 1: This tool provides a pre-trained HiFIGAN vocoder model for converting spectrograms into audio waveforms. It is typically used after a Text-to-Speech (TTS) model to transform the spectrogram output into natural-sounding speech. The model is trained on the LJSpeech dataset and operates at a sampling frequency of 22,050 Hz.\nTool 2: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 3: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 4: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 5: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 6: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 7: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 8: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.\nTool 9: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 10: A Keras-based TabTransformer model for supervised and semi-supervised learning on tabular data, capable of handling both numerical and categorical features. Categorical features are embedded and processed through Transformer blocks to capture contextual relationships, then combined with numerical features and passed through a final feed-forward network for prediction, with a SoftMax output for classification. This model is designed to leverage self-attention mechanisms to improve learning from structured datasets.\nTool 11: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.\nTool 12: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.\nTool 13: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.\nTool 14: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 15: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 16: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 17: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 18: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 19: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\", \"parameters\": {\"num_speakers\": {\"type\": \"number\", \"description\": \"The exact number of speakers to detect in the audio. This overrides the min_speakers and max_speakers settings if provided.\", \"required\": true, \"example\": 2}, \"min_speakers\": {\"type\": \"number\", \"description\": \"The minimum number of speakers expected in the audio. Used when the exact number of speakers is unknown.\", \"required\": true, \"example\": 1}, \"max_speakers\": {\"type\": \"number\", \"description\": \"The maximum number of speakers expected in the audio. Helps constrain the diarization process when the exact number is unknown.\", \"required\": true, \"example\": 5}, \"segmentation_onset\": {\"type\": \"number\", \"description\": \"A threshold parameter that adjusts the sensitivity for detecting the start of speech segments. Lower values make the system more sensitive to short speech segments.\", \"required\": true, \"example\": 0.5}}, \"response\": \"The tool response would likely contain the following components:\\n\\n1. **Diarization Results**: This part of the response would include the speaker diarization output for the provided audio file. The diarization output typically includes:\\n   - **Segments**: Time intervals where different speakers are active, with each segment linked to a specific speaker.\\n   - **Speaker Labels**: Identifiers assigned to each speaker in the audio, such as \\\"Speaker 1,\\\" \\\"Speaker 2,\\\" etc., or possibly user-defined labels depending on the tool configuration.\\n   - **Speech Time**: The total time a particular speaker is active during the audio, often associated with each speaker label.\\n\\n2. **RTTM File Format**: The diarization results will be structured in **RTTM (Rich Transcription Time Marked)** format, which typically includes:\\n   - **File Name**: The audio file being processed.\\n   - **Start Time**: The timestamp when the speaker begins speaking.\\n   - **Duration**: The length of time the speaker was speaking.\\n   - **Speaker ID**: A unique identifier for the speaker (e.g., \\\"Speaker 1\\\").\\n   - **Speech/Non-Speech Label**: Indicating whether the segment contains speech or non-speech (e.g., \\\"SPEECH\\\" or \\\"NON_SPEECH\\\").\\n\\n3. **Performance Metrics**: The response might include performance metrics based on the evaluation on different datasets:\\n   - **Dataset Name**: Identifiers for datasets used for evaluating the model.\\n   - **Accuracy Metrics**: For each dataset, the performance metrics such as:\\n     - **DER% (Diarization Error Rate)**: A measure of the overall error in the diarization.\\n     - **FA% (False Alarm Rate)**: The rate of incorrectly detected speech from a non-speech segment.\\n     - **Miss% (Missed Detection Rate)**: The rate of missed speech segments.\\n     - **Conf% (Confusion Rate)**: The rate of confusion in speaker identification.\\n\\n4. **Error Handling**: If an error occurs during processing (e.g., invalid audio file, failure in the diarization process), the response may include an error message indicating the nature of the problem.\\n\\nThe output would summarize these results to provide insight into the audio's speaker segmentation and performance of the diarization model. It will also indicate the model's accuracy based on various evaluation datasets, helping the user assess its reliability.\", \"examples\": [{\"user_intent\": \"I need to transcribe a conversation with 3 speakers from an audio file.\", \"tool_call\": {\"name\": \"johnislarry/cloned-pyannote-speaker-diarization-endpoint\", \"arguments\": {\"num_speakers\": 3}}}, {\"user_intent\": \"Process an audio file with an unknown number of speakers, but I expect there to be between 2 and 4 speakers.\", \"tool_call\": {\"name\": \"johnislarry/cloned-pyannote-speaker-diarization-endpoint\", \"arguments\": {\"min_speakers\": 2, \"max_speakers\": 4}}}, {\"user_intent\": \"Diarize an audio recording with a low segmentation onset threshold for more precise speaker segmentation.\", \"tool_call\": {\"name\": \"johnislarry/cloned-pyannote-speaker-diarization-endpoint\", \"arguments\": {\"segmentation_onset\": 0.3}}}]}\n\n# User Query\nIn surveillance operations, transcripts are used to turn the audio feed into chunks such that after processing transcripts there are no speakers talking over each other in the output.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to process surveillance audio to separate overlapping speech into distinct speaker segments.",
                    "tool_description": "Perform speaker diarization on audio recordings to identify and segment each speaker, including detection of overlapping speech, ensuring that the resulting segments contain only one active speaker at a time.",
                    "expected_response": "A diarization output in RTTM format containing time-aligned segments for each speaker, with overlaps resolved so that no segment contains multiple speakers speaking simultaneously."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The input audio feed to be analyzed and segmented.",
                "num_speakers": "The exact number of speakers expected in the audio, if known; otherwise undefined.",
                "min_speakers": "The minimum number of speakers expected in the audio when the exact number is unknown.",
                "max_speakers": "The maximum number of speakers expected in the audio when the exact number is unknown.",
                "segmentation_onset": "A numeric threshold to control sensitivity for detecting speech segments, influencing segmentation precision."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_184",
        "query": "Our client is an AI gaming company and we need to develop a bot for the game Valorant. The bot should detect objects like dropped spike, enemy, planted spike, and teammate within the game.",
        "instruction": "Given a `game bot development` task, retrieve tools that perform object detection within a gaming environment by recognizing specific in-game elements such as dropped spikes, enemies, planted spikes, and teammates, aligning with the role of detecting these features in games like Valorant.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8m-valorant-detection')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic_nms": false,
                        "max_det": 1000
                    },
                    "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21",
                    "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "valorant-object-detection",
                        "accuracy": 0.965
                    },
                    "description": "A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.",
                    "id": "huggingface_api_215",
                    "name": "keremberke/yolov8m-valorant-detection"
                },
                "id": "gorilla_huggingface_tool_211",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 2: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 4: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 5: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 6: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 7: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 8: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 9: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 10: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 11: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 12: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.",
            "Tool 13: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 14: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 15: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.",
            "Tool 16: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 17: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 18: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The tool’s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\n\n1. **Boxes / Bounding Boxes**  \n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \n   - These coordinates define the rectangular region enclosing each detected object.\n\n2. **Class / Label**  \n   - Each detected object is assigned a class label indicating its category, such as \"dropped spike,\" \"enemy,\" \"planted spike,\" or \"teammate.\"  \n   - The label identifies the type of object detected by the model.\n\n3. **Confidence Score**  \n   - A numerical value representing the model’s confidence that the detected object belongs to the predicted class.  \n   - This value is constrained by the `conf` threshold specified in the API arguments.\n\n4. **Optional Additional Metadata**  \n   - Depending on the framework, the response may include additional information such as:\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\n     - **Tracking ID** if sequential detection or object tracking is enabled.\n     - **Mask or segmentation data** if the model supports instance segmentation.\n\n5. **Rendering / Visualization (Optional)**  \n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\n\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.",
            "examples": [
                {
                    "user_intent": "I want to detect all enemies and teammates in a Valorant gameplay screenshot.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000
                        }
                    }
                },
                {
                    "user_intent": "Scan a live stream frame from Valorant to check if the spike has been dropped or planted.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500
                        }
                    }
                },
                {
                    "user_intent": "Analyze a sequence of Valorant game images to locate all objects with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 1000
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 2: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 4: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 5: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 6: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 7: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 8: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 9: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 10: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 11: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 12: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.\nTool 13: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 14: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 15: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.\nTool 16: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 17: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 18: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The tool\\u2019s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\\n\\n1. **Boxes / Bounding Boxes**  \\n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \\n   - These coordinates define the rectangular region enclosing each detected object.\\n\\n2. **Class / Label**  \\n   - Each detected object is assigned a class label indicating its category, such as \\\"dropped spike,\\\" \\\"enemy,\\\" \\\"planted spike,\\\" or \\\"teammate.\\\"  \\n   - The label identifies the type of object detected by the model.\\n\\n3. **Confidence Score**  \\n   - A numerical value representing the model\\u2019s confidence that the detected object belongs to the predicted class.  \\n   - This value is constrained by the `conf` threshold specified in the API arguments.\\n\\n4. **Optional Additional Metadata**  \\n   - Depending on the framework, the response may include additional information such as:\\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\\n     - **Tracking ID** if sequential detection or object tracking is enabled.\\n     - **Mask or segmentation data** if the model supports instance segmentation.\\n\\n5. **Rendering / Visualization (Optional)**  \\n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \\n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\\n\\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.\", \"examples\": [{\"user_intent\": \"I want to detect all enemies and teammates in a Valorant gameplay screenshot.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}}}, {\"user_intent\": \"Scan a live stream frame from Valorant to check if the spike has been dropped or planted.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500}}}, {\"user_intent\": \"Analyze a sequence of Valorant game images to locate all objects with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 1000}}}]}\n\n# User Query\nOur client is an AI gaming company and we need to develop a bot for the game Valorant. The bot should detect objects like dropped spike, enemy, planted spike, and teammate within the game.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect all in-game objects such as enemies, teammates, dropped spikes, and planted spikes in Valorant screenshots or video frames.",
                    "tool_description": "Use a YOLOv8-based object detection model trained on a Valorant dataset to automatically detect and classify key in-game objects within images or frames.",
                    "expected_response": "A structured list of detected objects, each containing bounding box coordinates, class label (enemy, teammate, dropped spike, planted spike), and confidence score."
                }
            ],
            "extracted_arguments": {
                "conf": "The minimum confidence threshold for detected objects (numeric).",
                "iou": "The Intersection over Union threshold for non-maximum suppression to control bounding box overlap (numeric).",
                "agnostic_nms": "Boolean flag indicating whether to perform class-agnostic non-maximum suppression.",
                "max_det": "The maximum number of detections allowed per image (numeric)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_4",
        "query": "We need to generate a motivational quote related to sports.",
        "instruction": "Given a `text generation` task, retrieve tools that specialize in producing motivational content by generating coherent and contextually relevant text outputs aligned with the query's theme of sports.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "This model can be loaded on the Inference API on-demand.",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%.",
                    "id": "huggingface_api_613",
                    "name": "TehVenom/PPO_Pygway-V8p4_Dev-6b"
                },
                "id": "gorilla_huggingface_tool_608",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 1: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 2: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 3: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 6: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 7: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 8: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 9: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 10: This tool uses the \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" model to convert sentences and paragraphs into 384-dimensional vector embeddings, which can be applied to tasks such as clustering and semantic search. It is based on the Hugging Face Transformers framework and is useful for comparing sentence similarity or finding related content in large datasets.",
            "Tool 11: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 12: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 13: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 14: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.",
            "Tool 15: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 16: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 17: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 18: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 19: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language."
        ],
        "tool_view": {
            "description": "OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "parameters": {
                "table": {
                    "type": "pd.DataFrame",
                    "description": "A pandas DataFrame containing the table data on which the question answering will be performed. Each column should represent a feature and each row a record.",
                    "required": true,
                    "example": "pd.DataFrame({'year': [1896, 1900], 'city': ['athens', 'paris']})"
                },
                "query": {
                    "type": "string",
                    "description": "A natural language question string to be answered using the provided table data.",
                    "required": true,
                    "example": "In which year did Beijing host the Olympic Games?"
                }
            },
            "response": "The tool produces a textual answer to a natural language question about the contents of a table. The response is typically structured as a sequence of text tokens decoded into a human-readable string. Specifically:  \n\n- **Primary content:** A single or multi-sentence string that directly answers the input query based on the data in the provided table.  \n- **Format:** The output is returned as a decoded string (from token IDs generated by the model), with special tokens removed if requested (e.g., `skip_special_tokens=True`).  \n- **Optional batch structure:** If multiple queries or tables are provided, the output can be a list of strings, each corresponding to one query-table pair.  \n- **Semantic role:** Each output string conveys the model’s prediction of the answer to the user’s question, derived by reasoning over the table structure, headers, and cell values.  \n\nThere are no additional metadata fields returned by the model by default; the response is purely the generated answer text.",
            "examples": [
                {
                    "user_intent": "I have a table of countries and their populations. I want to know which country has the largest population.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Country': ['USA', 'China', 'India', 'Brazil'], 'Population': [331000000, 1441000000, 1393000000, 212000000]})",
                            "query": "Which country has the largest population?"
                        }
                    }
                },
                {
                    "user_intent": "Given a table of sales data per quarter, I want to find out in which quarter the company made the highest revenue.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Quarter': ['Q1', 'Q2', 'Q3', 'Q4'], 'Revenue': [120000, 150000, 130000, 170000]})",
                            "query": "Which quarter had the highest revenue?"
                        }
                    }
                },
                {
                    "user_intent": "I have Olympic host cities and years in a table. I want to know when London hosted the Olympics.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Year': [1896, 1900, 1904, 2004, 2008, 2012], 'City': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']})",
                            "query": "In which year did London host the Olympic Games?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 1: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 2: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 3: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 6: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 7: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 8: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 9: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 10: This tool uses the \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" model to convert sentences and paragraphs into 384-dimensional vector embeddings, which can be applied to tasks such as clustering and semantic search. It is based on the Hugging Face Transformers framework and is useful for comparing sentence similarity or finding related content in large datasets.\nTool 11: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 12: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 13: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 14: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.\nTool 15: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 16: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 17: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 18: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 19: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\", \"parameters\": {\"table\": {\"type\": \"pd.DataFrame\", \"description\": \"A pandas DataFrame containing the table data on which the question answering will be performed. Each column should represent a feature and each row a record.\", \"required\": true, \"example\": \"pd.DataFrame({'year': [1896, 1900], 'city': ['athens', 'paris']})\"}, \"query\": {\"type\": \"string\", \"description\": \"A natural language question string to be answered using the provided table data.\", \"required\": true, \"example\": \"In which year did Beijing host the Olympic Games?\"}}, \"response\": \"The tool produces a textual answer to a natural language question about the contents of a table. The response is typically structured as a sequence of text tokens decoded into a human-readable string. Specifically:  \\n\\n- **Primary content:** A single or multi-sentence string that directly answers the input query based on the data in the provided table.  \\n- **Format:** The output is returned as a decoded string (from token IDs generated by the model), with special tokens removed if requested (e.g., `skip_special_tokens=True`).  \\n- **Optional batch structure:** If multiple queries or tables are provided, the output can be a list of strings, each corresponding to one query-table pair.  \\n- **Semantic role:** Each output string conveys the model\\u2019s prediction of the answer to the user\\u2019s question, derived by reasoning over the table structure, headers, and cell values.  \\n\\nThere are no additional metadata fields returned by the model by default; the response is purely the generated answer text.\", \"examples\": [{\"user_intent\": \"I have a table of countries and their populations. I want to know which country has the largest population.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Country': ['USA', 'China', 'India', 'Brazil'], 'Population': [331000000, 1441000000, 1393000000, 212000000]})\", \"query\": \"Which country has the largest population?\"}}}, {\"user_intent\": \"Given a table of sales data per quarter, I want to find out in which quarter the company made the highest revenue.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Quarter': ['Q1', 'Q2', 'Q3', 'Q4'], 'Revenue': [120000, 150000, 130000, 170000]})\", \"query\": \"Which quarter had the highest revenue?\"}}}, {\"user_intent\": \"I have Olympic host cities and years in a table. I want to know when London hosted the Olympics.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Year': [1896, 1900, 1904, 2004, 2008, 2012], 'City': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']})\", \"query\": \"In which year did London host the Olympic Games?\"}}}]}\n\n# User Query\nWe need to generate a motivational quote related to sports.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a motivational quote related to sports.",
                    "tool_description": "Produce a context-aware, inspiring textual quote that encourages sportsmanship, perseverance, or athletic excellence.",
                    "expected_response": "A single or multi-sentence motivational quote expressed as a human-readable text string."
                }
            ],
            "extracted_arguments": {
                "topic": "The thematic focus of the quote, in this case related to sports."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_100",
        "query": "My daughter left her laptop logged in, and I found her English essay unfinished. There's a sentence that goes like \"In the story, the antagonist represents the <mask> nature of humanity.\" Can you help me complete her sentence with an appropriate word?",
        "instruction": "Given a `fill-in-the-blank` task, retrieve tools that utilize masked language modeling to process text inputs with missing elements and provide contextually appropriate word completions to enhance the narrative coherence.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='roberta-base')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='roberta-base')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "MNLI",
                                "accuracy": 87.6
                            },
                            {
                                "name": "QQP",
                                "accuracy": 91.9
                            },
                            {
                                "name": "QNLI",
                                "accuracy": 92.8
                            },
                            {
                                "name": "SST-2",
                                "accuracy": 94.8
                            },
                            {
                                "name": "CoLA",
                                "accuracy": 63.6
                            },
                            {
                                "name": "STS-B",
                                "accuracy": 91.2
                            },
                            {
                                "name": "MRPC",
                                "accuracy": 90.2
                            },
                            {
                                "name": "RTE",
                                "accuracy": 78.7
                            }
                        ]
                    },
                    "description": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task.",
                    "id": "huggingface_api_659",
                    "name": "roberta-base"
                },
                "id": "gorilla_huggingface_tool_654",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 1: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 2: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 3: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 4: The \"microsoft/deberta-v2-xlarge\" is a large-scale DeBERTa model optimized for natural language understanding tasks. It leverages disentangled attention and an enhanced mask decoder to outperform BERT and RoBERTa on a variety of NLU benchmarks. This model is designed to handle tasks such as question answering, sentence classification, and textual entailment, offering high performance across multiple datasets like SQuAD, MNLI, and SST-2. It can be loaded on-demand using the Inference API with PyTorch or TensorFlow, and is particularly suited for fill-mask tasks, where missing words in a sentence are predicted based on context.",
            "Tool 5: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 6: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 7: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 8: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 9: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.",
            "Tool 10: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 11: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.",
            "Tool 12: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 13: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 14: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 15: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 16: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.",
            "Tool 17: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.",
            "Tool 18: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.",
            "Tool 19: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search."
        ],
        "tool_view": {
            "description": "CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.",
                    "required": true,
                    "example": "camembert-base"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.",
                    "required": true,
                    "example": "camembert-base"
                }
            },
            "response": "The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \n\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \n- **score**: A numerical value representing the model’s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \n- **token**: The identifier of the predicted token according to the model’s vocabulary.  \n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \n\nThe response is usually ordered by descending score, so the first entry is the model’s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.",
            "examples": [
                {
                    "user_intent": "Predict the missing word in the French sentence about cheese.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Le camembert est <mask> :)"
                        }
                    }
                },
                {
                    "user_intent": "Complete this French sentence describing the weather.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Aujourd'hui, il fait très <mask> dehors."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for a French sentence about happiness.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Je me sens vraiment <mask> aujourd'hui."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 1: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 2: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 3: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 4: The \"microsoft/deberta-v2-xlarge\" is a large-scale DeBERTa model optimized for natural language understanding tasks. It leverages disentangled attention and an enhanced mask decoder to outperform BERT and RoBERTa on a variety of NLU benchmarks. This model is designed to handle tasks such as question answering, sentence classification, and textual entailment, offering high performance across multiple datasets like SQuAD, MNLI, and SST-2. It can be loaded on-demand using the Inference API with PyTorch or TensorFlow, and is particularly suited for fill-mask tasks, where missing words in a sentence are predicted based on context.\nTool 5: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 6: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 7: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 8: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 9: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.\nTool 10: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 11: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.\nTool 12: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 13: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 14: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 15: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 16: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.\nTool 17: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.\nTool 18: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.\nTool 19: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.\", \"required\": true, \"example\": \"camembert-base\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.\", \"required\": true, \"example\": \"camembert-base\"}}, \"response\": \"The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \\n\\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \\n- **score**: A numerical value representing the model\\u2019s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \\n- **token**: The identifier of the predicted token according to the model\\u2019s vocabulary.  \\n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \\n\\nThe response is usually ordered by descending score, so the first entry is the model\\u2019s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.\", \"examples\": [{\"user_intent\": \"Predict the missing word in the French sentence about cheese.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Le camembert est <mask> :)\"}}}, {\"user_intent\": \"Complete this French sentence describing the weather.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Aujourd'hui, il fait tr\\u00e8s <mask> dehors.\"}}}, {\"user_intent\": \"Fill in the blank for a French sentence about happiness.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Je me sens vraiment <mask> aujourd'hui.\"}}}]}\n\n# User Query\nMy daughter left her laptop logged in, and I found her English essay unfinished. There's a sentence that goes like \"In the story, the antagonist represents the <mask> nature of humanity.\" Can you help me complete her sentence with an appropriate word?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict the missing word in a sentence describing the antagonist's role in a story.",
                    "tool_description": "Use a masked language model to predict the missing word in the sentence, based on context provided in the sentence.",
                    "expected_response": "A ranked list of candidate words that could complete the sentence, including their confidence scores."
                }
            ],
            "extracted_arguments": {
                "model": "The name or path of the pre-trained English language model for fill-mask tasks.",
                "tokenizer": "The name or path of the tokenizer corresponding to the English language model.",
                "text": "The sentence with a masked token, which the model will use to predict a suitable word to fill the blank."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_74",
        "query": "Extract entities from a provided sentence mentioning various companies and their CEOs.",
        "instruction": "Given an `entity extraction` task, retrieve tools that specialize in processing text inputs to identify and extract named entities, such as companies and their CEOs, from sentences based on natural language processing techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Entity Extraction",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577')",
                    "api_arguments": {
                        "inputs": "I love AutoTrain"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTokenClassification",
                        "tokenizer": "AutoTokenizer"
                    },
                    "example_code": "from transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "ismail-lucifer011/autotrain-data-name_all",
                        "accuracy": 0.9989316041363876
                    },
                    "description": "This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams.",
                    "id": "huggingface_api_413",
                    "name": "904029577"
                },
                "id": "gorilla_huggingface_tool_409",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.",
            "Tool 3: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 4: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 5: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 6: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 7: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 8: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.",
            "Tool 9: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.",
            "Tool 10: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "Tool 11: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 12: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.",
            "Tool 13: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 14: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 15: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 16: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 17: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 18: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 19: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks."
        ],
        "tool_view": {
            "description": "The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "parameters": {
                "sentence": {
                    "type": "string",
                    "description": "The input text on which the Named Entity Recognition (NER) model will perform entity detection. This should be a single sentence or a piece of text containing entities to recognize.",
                    "required": true,
                    "example": "On September 1st George Washington won 1 dollar."
                }
            },
            "response": "The tool returns a structured representation of named entities detected within the input text (sentence). The response typically consists of the following components:  \n\n1. **Entities/Spans**: A list of individual entities recognized in the text. Each entity represents a contiguous sequence of tokens that the model has classified as a particular named entity type.  \n\n2. **Entity Attributes**: Each entity includes several key pieces of information:  \n   - **Text/Span**: The exact substring from the input sentence corresponding to the entity.  \n   - **Entity Type/Label**: The predicted category of the entity, drawn from the 18 OntoNotes classes (e.g., person, organization, date, location, money, event, product, etc.).  \n   - **Start and End Positions**: The character indices or token positions in the original sentence that mark the beginning and end of the entity span.  \n   - **Confidence Score (optional)**: A numerical score representing the model’s confidence in the entity classification, if available.  \n\n3. **Sentence Object**: The response may also retain a reference to the original sentence object, which can include the text, tokenization, and any other annotations produced by the Flair framework.  \n\nIn essence, the response provides a detailed mapping from segments of the input sentence to their corresponding named entity classifications, along with metadata about span location and optionally prediction confidence. This structured output enables downstream tasks such as information extraction, text annotation, or semantic analysis.",
            "examples": [
                {
                    "user_intent": "I want to identify entities like dates, people, and organizations in the sentence 'Apple was founded by Steve Jobs on April 1st, 1976.'",
                    "tool_call": {
                        "name": "flair/ner-english-ontonotes-fast",
                        "arguments": {
                            "sentence": "Apple was founded by Steve Jobs on April 1st, 1976."
                        }
                    }
                },
                {
                    "user_intent": "Can you extract the named entities from the sentence 'The Eiffel Tower is in Paris, and it attracts millions of tourists every year.'?",
                    "tool_call": {
                        "name": "flair/ner-english-ontonotes-fast",
                        "arguments": {
                            "sentence": "The Eiffel Tower is in Paris, and it attracts millions of tourists every year."
                        }
                    }
                },
                {
                    "user_intent": "I need to identify any money-related entities in the sentence 'The company earned 500 million dollars in profit last quarter.'",
                    "tool_call": {
                        "name": "flair/ner-english-ontonotes-fast",
                        "arguments": {
                            "sentence": "The company earned 500 million dollars in profit last quarter."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.\nTool 3: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 4: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 5: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 6: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 7: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 8: The XLM-RoBERTa-large model, fine-tuned on the conll2003 dataset in English, is designed for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. This model leverages the multilingual capabilities of XLM-RoBERTa, trained on extensive CommonCrawl data, to classify individual tokens in text. It can be used for various NLP applications that require identifying entities or parts of speech within sentences.\nTool 9: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.\nTool 10: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\nTool 11: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 12: The xhyi/layoutlmv3_docvqa_t11c5000 is a LayoutLMv3 model designed for document question answering, enabling the extraction of answers from documents based on a provided question and context. It utilizes the Hugging Face Transformers framework and is optimized for the DocVQA dataset.\nTool 13: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 14: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 15: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 16: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 17: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 18: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 19: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"flair/ner-english-ontonotes-fast\\\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\", \"parameters\": {\"sentence\": {\"type\": \"string\", \"description\": \"The input text on which the Named Entity Recognition (NER) model will perform entity detection. This should be a single sentence or a piece of text containing entities to recognize.\", \"required\": true, \"example\": \"On September 1st George Washington won 1 dollar.\"}}, \"response\": \"The tool returns a structured representation of named entities detected within the input text (sentence). The response typically consists of the following components:  \\n\\n1. **Entities/Spans**: A list of individual entities recognized in the text. Each entity represents a contiguous sequence of tokens that the model has classified as a particular named entity type.  \\n\\n2. **Entity Attributes**: Each entity includes several key pieces of information:  \\n   - **Text/Span**: The exact substring from the input sentence corresponding to the entity.  \\n   - **Entity Type/Label**: The predicted category of the entity, drawn from the 18 OntoNotes classes (e.g., person, organization, date, location, money, event, product, etc.).  \\n   - **Start and End Positions**: The character indices or token positions in the original sentence that mark the beginning and end of the entity span.  \\n   - **Confidence Score (optional)**: A numerical score representing the model\\u2019s confidence in the entity classification, if available.  \\n\\n3. **Sentence Object**: The response may also retain a reference to the original sentence object, which can include the text, tokenization, and any other annotations produced by the Flair framework.  \\n\\nIn essence, the response provides a detailed mapping from segments of the input sentence to their corresponding named entity classifications, along with metadata about span location and optionally prediction confidence. This structured output enables downstream tasks such as information extraction, text annotation, or semantic analysis.\", \"examples\": [{\"user_intent\": \"I want to identify entities like dates, people, and organizations in the sentence 'Apple was founded by Steve Jobs on April 1st, 1976.'\", \"tool_call\": {\"name\": \"flair/ner-english-ontonotes-fast\", \"arguments\": {\"sentence\": \"Apple was founded by Steve Jobs on April 1st, 1976.\"}}}, {\"user_intent\": \"Can you extract the named entities from the sentence 'The Eiffel Tower is in Paris, and it attracts millions of tourists every year.'?\", \"tool_call\": {\"name\": \"flair/ner-english-ontonotes-fast\", \"arguments\": {\"sentence\": \"The Eiffel Tower is in Paris, and it attracts millions of tourists every year.\"}}}, {\"user_intent\": \"I need to identify any money-related entities in the sentence 'The company earned 500 million dollars in profit last quarter.'\", \"tool_call\": {\"name\": \"flair/ner-english-ontonotes-fast\", \"arguments\": {\"sentence\": \"The company earned 500 million dollars in profit last quarter.\"}}}]}\n\n# User Query\nExtract entities from a provided sentence mentioning various companies and their CEOs.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to identify entities related to companies and persons, particularly CEOs, in the sentence.",
                    "tool_description": "Extract named entities, specifically company names and person names, from a given sentence using Named Entity Recognition (NER).",
                    "expected_response": "A structured list of identified entities, including companies and persons, with additional metadata such as entity type and span positions."
                }
            ],
            "extracted_arguments": {
                "sentence": "The input text containing mentions of companies and their CEOs."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_22",
        "query": "I have a diary entry and want to identify the names of people and locations mentioned in it.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that process textual inputs to identify and categorize named entities such as people and locations within the text.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load('flair/ner-english')\n\n# make example sentence\nsentence = Sentence('George Washington went to Washington')\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)",
                    "performance": {
                        "dataset": "conll2003",
                        "accuracy": "93.06"
                    },
                    "description": "This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_426",
                    "name": "flair/ner-english"
                },
                "id": "gorilla_huggingface_tool_422",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 2: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 3: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 4: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 6: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.",
            "Tool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 8: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 9: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 10: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 11: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 12: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 13: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.",
            "Tool 14: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 15: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 16: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 17: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.",
            "Tool 18: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.",
            "Tool 19: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding."
        ],
        "tool_view": {
            "description": "The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "parameters": {
                "sentence": {
                    "type": "string",
                    "description": "The input text sentence on which the Named Entity Recognition (NER) model will perform entity tagging.",
                    "required": true,
                    "example": "George Washington went to Washington"
                }
            },
            "response": "The tool’s response consists of an annotated sentence object that includes recognized named entities identified within the input text. Specifically, after the model processes the input sentence, the response encapsulates the sentence along with labeled entity spans corresponding to one of four predefined categories: PER (person names), LOC (location names), ORG (organization names), and MISC (miscellaneous names).\n\nEach entity span in the response includes at least the following components:\n\n- The text segment from the sentence identified as an entity.\n- The entity label indicating the type of named entity (PER, LOC, ORG, or MISC).\n- The position or span indices within the sentence marking where the entity occurs.\n- A confidence score or probability reflecting the model’s certainty about the prediction (typically included in Flair’s entity spans).\n\nOverall, the response structure is a richly annotated sentence object that integrates the original text with its token-level classification into named entity categories, enabling downstream use such as extraction, display, or further analysis of recognized entities.",
            "examples": [
                {
                    "user_intent": "Identify all the named entities in the sentence 'Barack Obama visited Paris last summer.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Barack Obama visited Paris last summer."
                        }
                    }
                },
                {
                    "user_intent": "I want to extract organizations and locations mentioned in 'Apple announced a new product in New York today.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Apple announced a new product in New York today."
                        }
                    }
                },
                {
                    "user_intent": "Find all the people, places, and miscellaneous entities in 'Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 2: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 3: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 4: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 6: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.\nTool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 8: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 9: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 10: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 11: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 12: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 13: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.\nTool 14: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 15: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 16: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 17: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.\nTool 18: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.\nTool 19: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"flair/ner-english\\\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\", \"parameters\": {\"sentence\": {\"type\": \"string\", \"description\": \"The input text sentence on which the Named Entity Recognition (NER) model will perform entity tagging.\", \"required\": true, \"example\": \"George Washington went to Washington\"}}, \"response\": \"The tool\\u2019s response consists of an annotated sentence object that includes recognized named entities identified within the input text. Specifically, after the model processes the input sentence, the response encapsulates the sentence along with labeled entity spans corresponding to one of four predefined categories: PER (person names), LOC (location names), ORG (organization names), and MISC (miscellaneous names).\\n\\nEach entity span in the response includes at least the following components:\\n\\n- The text segment from the sentence identified as an entity.\\n- The entity label indicating the type of named entity (PER, LOC, ORG, or MISC).\\n- The position or span indices within the sentence marking where the entity occurs.\\n- A confidence score or probability reflecting the model\\u2019s certainty about the prediction (typically included in Flair\\u2019s entity spans).\\n\\nOverall, the response structure is a richly annotated sentence object that integrates the original text with its token-level classification into named entity categories, enabling downstream use such as extraction, display, or further analysis of recognized entities.\", \"examples\": [{\"user_intent\": \"Identify all the named entities in the sentence 'Barack Obama visited Paris last summer.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Barack Obama visited Paris last summer.\"}}}, {\"user_intent\": \"I want to extract organizations and locations mentioned in 'Apple announced a new product in New York today.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Apple announced a new product in New York today.\"}}}, {\"user_intent\": \"Find all the people, places, and miscellaneous entities in 'Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.\"}}}]}\n\n# User Query\nI have a diary entry and want to identify the names of people and locations mentioned in it.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract the names of people and locations from a diary entry.",
                    "tool_description": "Retrieve named entities, specifically person names and location names, from an input text using a Named Entity Recognition (NER) model.",
                    "expected_response": "A sentence object annotated with recognized named entities, including person names (PER) and location names (LOC), along with their corresponding spans and confidence scores."
                }
            ],
            "extracted_arguments": {
                "sentence": "The diary entry text on which the Named Entity Recognition (NER) model will perform entity tagging."
            }
        }
    }
]